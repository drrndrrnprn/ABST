2022-02-28 16:21:40 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 10, 'log_format': None, 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 8192, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 5000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 12288, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.abst/dev', 'restore_file': 'bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 10, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 2, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.0, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=0.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst/dev', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_epoch=15, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='aspect_base_denoising', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.0, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=0.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst/dev', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_epoch=15, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 20000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-28 16:21:40 | INFO | bartabst.tasks.aspect_base_denoising | dictionary: 51200 types
2022-02-28 16:21:43 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-28 16:21:43 | INFO | fairseq_cli.train | task: AspectBaseDenoisingTask
2022-02-28 16:21:43 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-28 16:21:43 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-28 16:21:43 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-28 16:21:43 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-28 16:21:43 | INFO | bartabst.data.data_utils | loaded 598 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-02-28 16:21:43 | INFO | bartabst.tasks.aspect_base_denoising | Split: valid, Loaded 598 samples of denoising_dataset
2022-02-28 16:21:54 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-02-28 16:21:54 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-28 16:21:54 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-02-28 16:21:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 16:21:54 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-02-28 16:21:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 16:21:54 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-28 16:21:54 | INFO | fairseq_cli.train | max tokens per device = 8192 and max sentences per device = 32
2022-02-28 16:21:54 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:21:56 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 58 @ 0 updates)
2022-02-28 16:21:56 | INFO | fairseq.trainer | loading train data for epoch 1
no aos file, no transfer aos used
2022-02-28 16:21:58 | INFO | bartabst.data.data_utils | loaded 1,910 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-02-28 16:21:58 | INFO | bartabst.tasks.aspect_base_denoising | Split: train, Loaded 1910 samples of denoising_dataset
2022-02-28 16:21:58 | INFO | fairseq.trainer | begin training epoch 1
2022-02-28 16:21:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:21:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-28 16:21:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-28 16:21:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-28 16:21:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-28 16:21:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-28 16:21:59 | INFO | train_inner | epoch 001:     15 / 60 loss=6.766, ppl=108.85, wps=8073.4, ups=11.74, wpb=665.5, bsz=32, num_updates=10, lr=6e-07, gnorm=26.635, clip=100, loss_scale=4, train_wall=1, gb_free=20.8, wall=6
2022-02-28 16:22:00 | INFO | train_inner | epoch 001:     25 / 60 loss=6.668, ppl=101.66, wps=8506.8, ups=12.36, wpb=688.3, bsz=32, num_updates=20, lr=1.2e-06, gnorm=27.361, clip=100, loss_scale=4, train_wall=1, gb_free=20.8, wall=6
2022-02-28 16:22:01 | INFO | train_inner | epoch 001:     35 / 60 loss=5.98, ppl=63.1, wps=7521.1, ups=11.78, wpb=638.2, bsz=32, num_updates=30, lr=1.8e-06, gnorm=29.712, clip=100, loss_scale=4, train_wall=1, gb_free=20.8, wall=7
2022-02-28 16:22:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-02-28 16:22:02 | INFO | train_inner | epoch 001:     46 / 60 loss=4.599, ppl=24.24, wps=7057.5, ups=10.98, wpb=642.7, bsz=32, num_updates=40, lr=2.4e-06, gnorm=27.189, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=8
2022-02-28 16:22:03 | INFO | train_inner | epoch 001:     56 / 60 loss=3.647, ppl=12.53, wps=8884.1, ups=12.62, wpb=704.2, bsz=32, num_updates=50, lr=3e-06, gnorm=24.793, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=9
2022-02-28 16:22:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:22:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.369 | ppl 2.58 | wps 27308.4 | wpb 653.8 | bsz 31.5 | num_updates 54
2022-02-28 16:22:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 54 updates
2022-02-28 16:22:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:22:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:22:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 1 @ 54 updates, score 1.369) (writing took 7.486177606973797 seconds)
2022-02-28 16:22:11 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-28 16:22:11 | INFO | train | epoch 001 | loss 5.399 | ppl 42.21 | wps 2784.6 | ups 4.24 | wpb 653.3 | bsz 32 | num_updates 54 | lr 3.24e-06 | gnorm 26.649 | clip 100 | loss_scale 2 | train_wall 5 | gb_free 20.8 | wall 17
2022-02-28 16:22:11 | INFO | fairseq.trainer | begin training epoch 2
2022-02-28 16:22:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:22:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2022-02-28 16:22:12 | INFO | train_inner | epoch 002:      7 / 60 loss=3.386, ppl=10.45, wps=480, ups=1.12, wpb=428.9, bsz=32, num_updates=60, lr=3.6e-06, gnorm=20.361, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=18
2022-02-28 16:22:12 | INFO | train_inner | epoch 002:     17 / 60 loss=2.951, ppl=7.73, wps=7736.1, ups=15.77, wpb=490.5, bsz=32, num_updates=70, lr=4.2e-06, gnorm=26.849, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=18
2022-02-28 16:22:13 | INFO | train_inner | epoch 002:     27 / 60 loss=1.862, ppl=3.64, wps=12171.2, ups=14.57, wpb=835.4, bsz=32, num_updates=80, lr=4.8e-06, gnorm=16.382, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=19
2022-02-28 16:22:14 | INFO | train_inner | epoch 002:     37 / 60 loss=2.011, ppl=4.03, wps=8599.3, ups=14.81, wpb=580.8, bsz=32, num_updates=90, lr=5.4e-06, gnorm=13.972, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=20
2022-02-28 16:22:14 | INFO | train_inner | epoch 002:     47 / 60 loss=1.664, ppl=3.17, wps=10262.4, ups=14.07, wpb=729.6, bsz=32, num_updates=100, lr=6e-06, gnorm=19.198, clip=100, loss_scale=1, train_wall=1, gb_free=20.5, wall=21
2022-02-28 16:22:15 | INFO | train_inner | epoch 002:     57 / 60 loss=1.537, ppl=2.9, wps=8717.4, ups=11.26, wpb=774.2, bsz=31, num_updates=110, lr=6.6e-06, gnorm=21.99, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=21
2022-02-28 16:22:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:22:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.119 | ppl 2.17 | wps 28776.9 | wpb 653.8 | bsz 31.5 | num_updates 113 | best_loss 1.119
2022-02-28 16:22:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 113 updates
2022-02-28 16:22:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:22:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:22:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 2 @ 113 updates, score 1.119) (writing took 7.077620244002901 seconds)
2022-02-28 16:22:23 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-28 16:22:23 | INFO | train | epoch 002 | loss 2.011 | ppl 4.03 | wps 3208.6 | ups 4.9 | wpb 654.8 | bsz 31.8 | num_updates 113 | lr 6.78e-06 | gnorm 19.21 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 29
2022-02-28 16:22:23 | INFO | fairseq.trainer | begin training epoch 3
2022-02-28 16:22:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:22:24 | INFO | train_inner | epoch 003:      7 / 60 loss=1.48, ppl=2.79, wps=902.7, ups=1.18, wpb=764.1, bsz=32, num_updates=120, lr=7.2e-06, gnorm=13.274, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=30
2022-02-28 16:22:24 | INFO | train_inner | epoch 003:     17 / 60 loss=1.713, ppl=3.28, wps=8745.7, ups=15.23, wpb=574.4, bsz=32, num_updates=130, lr=7.8e-06, gnorm=40.419, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=31
2022-02-28 16:22:25 | INFO | train_inner | epoch 003:     27 / 60 loss=1.586, ppl=3, wps=9868.6, ups=15.09, wpb=653.9, bsz=32, num_updates=140, lr=8.4e-06, gnorm=8.821, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=31
2022-02-28 16:22:26 | INFO | train_inner | epoch 003:     37 / 60 loss=1.199, ppl=2.3, wps=11339.2, ups=13.7, wpb=827.6, bsz=31, num_updates=150, lr=9e-06, gnorm=8.411, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=32
2022-02-28 16:22:26 | INFO | train_inner | epoch 003:     47 / 60 loss=1.238, ppl=2.36, wps=11563.3, ups=14.22, wpb=813.1, bsz=32, num_updates=160, lr=9.6e-06, gnorm=6.573, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=33
2022-02-28 16:22:27 | INFO | train_inner | epoch 003:     57 / 60 loss=2.009, ppl=4.02, wps=5245.6, ups=11.57, wpb=453.3, bsz=32, num_updates=170, lr=1.02e-05, gnorm=11.079, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=34
2022-02-28 16:22:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:22:28 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.028 | ppl 2.04 | wps 28307.1 | wpb 653.8 | bsz 31.5 | num_updates 173 | best_loss 1.028
2022-02-28 16:22:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 173 updates
2022-02-28 16:22:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:22:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:22:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 3 @ 173 updates, score 1.028) (writing took 5.765097497962415 seconds)
2022-02-28 16:22:34 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-28 16:22:34 | INFO | train | epoch 003 | loss 1.489 | ppl 2.81 | wps 3693.6 | ups 5.55 | wpb 666.1 | bsz 31.8 | num_updates 173 | lr 1.038e-05 | gnorm 14.852 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 40
2022-02-28 16:22:34 | INFO | fairseq.trainer | begin training epoch 4
2022-02-28 16:22:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:22:35 | INFO | train_inner | epoch 004:      7 / 60 loss=1.308, ppl=2.48, wps=960.3, ups=1.37, wpb=700.1, bsz=32, num_updates=180, lr=1.08e-05, gnorm=9.184, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=41
2022-02-28 16:22:35 | INFO | train_inner | epoch 004:     17 / 60 loss=1.575, ppl=2.98, wps=8004, ups=14.84, wpb=539.5, bsz=32, num_updates=190, lr=1.14e-05, gnorm=22.95, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=42
2022-02-28 16:22:36 | INFO | train_inner | epoch 004:     27 / 60 loss=1.275, ppl=2.42, wps=9887, ups=14.67, wpb=673.8, bsz=31, num_updates=200, lr=1.2e-05, gnorm=9.315, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=42
2022-02-28 16:22:37 | INFO | train_inner | epoch 004:     37 / 60 loss=1.145, ppl=2.21, wps=10741.1, ups=13.29, wpb=808.1, bsz=32, num_updates=210, lr=1.26e-05, gnorm=14.561, clip=100, loss_scale=1, train_wall=1, gb_free=19.7, wall=43
2022-02-28 16:22:38 | INFO | train_inner | epoch 004:     47 / 60 loss=1.231, ppl=2.35, wps=7922, ups=11.4, wpb=695.2, bsz=32, num_updates=220, lr=1.32e-05, gnorm=5.907, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=44
2022-02-28 16:22:38 | INFO | train_inner | epoch 004:     57 / 60 loss=1.757, ppl=3.38, wps=5800.1, ups=11.82, wpb=490.6, bsz=32, num_updates=230, lr=1.38e-05, gnorm=11.207, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=45
2022-02-28 16:22:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:22:39 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.922 | ppl 1.9 | wps 30117.9 | wpb 653.8 | bsz 31.5 | num_updates 233 | best_loss 0.922
2022-02-28 16:22:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 233 updates
2022-02-28 16:22:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:22:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:22:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 4 @ 233 updates, score 0.922) (writing took 4.488826401997358 seconds)
2022-02-28 16:22:44 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-28 16:22:44 | INFO | train | epoch 004 | loss 1.312 | ppl 2.48 | wps 4076.7 | ups 6.12 | wpb 666.1 | bsz 31.8 | num_updates 233 | lr 1.398e-05 | gnorm 11.958 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 50
2022-02-28 16:22:44 | INFO | fairseq.trainer | begin training epoch 5
2022-02-28 16:22:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:22:44 | INFO | train_inner | epoch 005:      7 / 60 loss=1.091, ppl=2.13, wps=1257.6, ups=1.72, wpb=731, bsz=32, num_updates=240, lr=1.44e-05, gnorm=6.145, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=51
2022-02-28 16:22:45 | INFO | train_inner | epoch 005:     17 / 60 loss=1.239, ppl=2.36, wps=9972.8, ups=15.5, wpb=643.6, bsz=32, num_updates=250, lr=1.5e-05, gnorm=7.391, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=51
2022-02-28 16:22:46 | INFO | train_inner | epoch 005:     27 / 60 loss=1.33, ppl=2.51, wps=9307.7, ups=15.09, wpb=616.8, bsz=32, num_updates=260, lr=1.56e-05, gnorm=6.917, clip=100, loss_scale=1, train_wall=1, gb_free=19.7, wall=52
2022-02-28 16:22:46 | INFO | train_inner | epoch 005:     37 / 60 loss=1.678, ppl=3.2, wps=6110.7, ups=15.2, wpb=401.9, bsz=32, num_updates=270, lr=1.62e-05, gnorm=12.739, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=52
2022-02-28 16:22:47 | INFO | train_inner | epoch 005:     47 / 60 loss=1.125, ppl=2.18, wps=11210.3, ups=15.07, wpb=744.1, bsz=32, num_updates=280, lr=1.68e-05, gnorm=7.394, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=53
2022-02-28 16:22:48 | INFO | train_inner | epoch 005:     57 / 60 loss=0.901, ppl=1.87, wps=10051.8, ups=11.63, wpb=864, bsz=32, num_updates=290, lr=1.74e-05, gnorm=5.542, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=54
2022-02-28 16:22:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:22:49 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.866 | ppl 1.82 | wps 26337 | wpb 653.8 | bsz 31.5 | num_updates 293 | best_loss 0.866
2022-02-28 16:22:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 293 updates
2022-02-28 16:22:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:22:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:22:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 5 @ 293 updates, score 0.866) (writing took 4.557788042991888 seconds)
2022-02-28 16:22:53 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-28 16:22:53 | INFO | train | epoch 005 | loss 1.169 | ppl 2.25 | wps 4236.9 | ups 6.36 | wpb 666.1 | bsz 31.8 | num_updates 293 | lr 1.758e-05 | gnorm 7.678 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 59
2022-02-28 16:22:53 | INFO | fairseq.trainer | begin training epoch 6
2022-02-28 16:22:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:22:54 | INFO | train_inner | epoch 006:      7 / 60 loss=1.216, ppl=2.32, wps=911.4, ups=1.69, wpb=539, bsz=31, num_updates=300, lr=1.8e-05, gnorm=9.294, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=60
2022-02-28 16:22:54 | INFO | train_inner | epoch 006:     17 / 60 loss=1.192, ppl=2.28, wps=9161.9, ups=14.53, wpb=630.7, bsz=32, num_updates=310, lr=1.86e-05, gnorm=6.437, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=61
2022-02-28 16:22:55 | INFO | train_inner | epoch 006:     27 / 60 loss=0.951, ppl=1.93, wps=10826.7, ups=15, wpb=721.9, bsz=32, num_updates=320, lr=1.92e-05, gnorm=6.139, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=61
2022-02-28 16:22:56 | INFO | train_inner | epoch 006:     37 / 60 loss=1.081, ppl=2.12, wps=9992.3, ups=14.68, wpb=680.6, bsz=32, num_updates=330, lr=1.98e-05, gnorm=8.674, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=62
2022-02-28 16:22:56 | INFO | train_inner | epoch 006:     47 / 60 loss=0.972, ppl=1.96, wps=10283.8, ups=13.65, wpb=753.2, bsz=32, num_updates=340, lr=2.04e-05, gnorm=5.247, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=63
2022-02-28 16:22:57 | INFO | train_inner | epoch 006:     57 / 60 loss=1.004, ppl=2, wps=8895.1, ups=12.46, wpb=713.7, bsz=31, num_updates=350, lr=2.1e-05, gnorm=7.809, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=64
2022-02-28 16:22:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:22:58 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 0.861 | ppl 1.82 | wps 29116.1 | wpb 653.8 | bsz 31.5 | num_updates 353 | best_loss 0.861
2022-02-28 16:22:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 353 updates
2022-02-28 16:22:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:23:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:23:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 6 @ 353 updates, score 0.861) (writing took 4.792572881036904 seconds)
2022-02-28 16:23:03 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-28 16:23:03 | INFO | train | epoch 006 | loss 1.072 | ppl 2.1 | wps 4138 | ups 6.21 | wpb 666.1 | bsz 31.8 | num_updates 353 | lr 2.118e-05 | gnorm 7.54 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 69
2022-02-28 16:23:03 | INFO | fairseq.trainer | begin training epoch 7
2022-02-28 16:23:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:23:03 | INFO | train_inner | epoch 007:      7 / 60 loss=1.21, ppl=2.31, wps=920.1, ups=1.64, wpb=561.6, bsz=32, num_updates=360, lr=2.16e-05, gnorm=7.501, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=70
2022-02-28 16:23:04 | INFO | train_inner | epoch 007:     17 / 60 loss=1.217, ppl=2.33, wps=8007.2, ups=14.47, wpb=553.3, bsz=32, num_updates=370, lr=2.22e-05, gnorm=7.554, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=70
2022-02-28 16:23:05 | INFO | train_inner | epoch 007:     27 / 60 loss=1.447, ppl=2.73, wps=7288.7, ups=16.02, wpb=455, bsz=32, num_updates=380, lr=2.28e-05, gnorm=6.689, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=71
2022-02-28 16:23:05 | INFO | train_inner | epoch 007:     37 / 60 loss=0.841, ppl=1.79, wps=11396.6, ups=13.85, wpb=822.8, bsz=31, num_updates=390, lr=2.34e-05, gnorm=5.285, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=72
2022-02-28 16:23:06 | INFO | train_inner | epoch 007:     47 / 60 loss=1.041, ppl=2.06, wps=10207.9, ups=14.4, wpb=708.8, bsz=32, num_updates=400, lr=2.4e-05, gnorm=5.489, clip=100, loss_scale=1, train_wall=1, gb_free=20.3, wall=72
2022-02-28 16:23:07 | INFO | train_inner | epoch 007:     57 / 60 loss=0.733, ppl=1.66, wps=9216.6, ups=11.28, wpb=817.4, bsz=32, num_updates=410, lr=2.46e-05, gnorm=5.451, clip=100, loss_scale=1, train_wall=1, gb_free=20.4, wall=73
2022-02-28 16:23:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:23:08 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 0.836 | ppl 1.79 | wps 25882.1 | wpb 653.8 | bsz 31.5 | num_updates 413 | best_loss 0.836
2022-02-28 16:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 413 updates
2022-02-28 16:23:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:23:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:23:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 7 @ 413 updates, score 0.836) (writing took 5.424744791002013 seconds)
2022-02-28 16:23:13 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-28 16:23:13 | INFO | train | epoch 007 | loss 1.013 | ppl 2.02 | wps 3833.1 | ups 5.75 | wpb 666.1 | bsz 31.8 | num_updates 413 | lr 2.478e-05 | gnorm 6.001 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 79
2022-02-28 16:23:13 | INFO | fairseq.trainer | begin training epoch 8
2022-02-28 16:23:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:23:14 | INFO | train_inner | epoch 008:      7 / 60 loss=0.918, ppl=1.89, wps=1015, ups=1.47, wpb=691.4, bsz=32, num_updates=420, lr=2.52e-05, gnorm=9.793, clip=100, loss_scale=1, train_wall=1, gb_free=20, wall=80
2022-02-28 16:23:15 | INFO | train_inner | epoch 008:     17 / 60 loss=0.945, ppl=1.92, wps=10000.2, ups=14.52, wpb=688.5, bsz=32, num_updates=430, lr=2.58e-05, gnorm=4.611, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=81
2022-02-28 16:23:15 | INFO | train_inner | epoch 008:     27 / 60 loss=1.002, ppl=2, wps=10193.6, ups=15.16, wpb=672.6, bsz=32, num_updates=440, lr=2.64e-05, gnorm=6.363, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=81
2022-02-28 16:23:16 | INFO | train_inner | epoch 008:     37 / 60 loss=0.806, ppl=1.75, wps=11222.1, ups=14.87, wpb=754.8, bsz=31, num_updates=450, lr=2.7e-05, gnorm=6.358, clip=100, loss_scale=1, train_wall=1, gb_free=20.5, wall=82
2022-02-28 16:23:17 | INFO | train_inner | epoch 008:     47 / 60 loss=1.112, ppl=2.16, wps=9276.8, ups=16.03, wpb=578.8, bsz=32, num_updates=460, lr=2.76e-05, gnorm=8.924, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=83
2022-02-28 16:23:17 | INFO | train_inner | epoch 008:     57 / 60 loss=0.839, ppl=1.79, wps=7950.2, ups=12.05, wpb=659.5, bsz=32, num_updates=470, lr=2.82e-05, gnorm=6.772, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=84
2022-02-28 16:23:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:23:18 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 0.801 | ppl 1.74 | wps 30476.3 | wpb 653.8 | bsz 31.5 | num_updates 473 | best_loss 0.801
2022-02-28 16:23:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 473 updates
2022-02-28 16:23:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:23:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:23:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 8 @ 473 updates, score 0.801) (writing took 4.331482760957442 seconds)
2022-02-28 16:23:22 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-28 16:23:22 | INFO | train | epoch 008 | loss 0.943 | ppl 1.92 | wps 4379.5 | ups 6.57 | wpb 666.1 | bsz 31.8 | num_updates 473 | lr 2.838e-05 | gnorm 7.141 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.6 | wall 89
2022-02-28 16:23:22 | INFO | fairseq.trainer | begin training epoch 9
2022-02-28 16:23:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:23:23 | INFO | train_inner | epoch 009:      7 / 60 loss=0.83, ppl=1.78, wps=1352.1, ups=1.77, wpb=762.2, bsz=31, num_updates=480, lr=2.88e-05, gnorm=5.455, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=89
2022-02-28 16:23:24 | INFO | train_inner | epoch 009:     17 / 60 loss=1.064, ppl=2.09, wps=7798.9, ups=14.45, wpb=539.8, bsz=32, num_updates=490, lr=2.94e-05, gnorm=6.941, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=90
2022-02-28 16:23:24 | INFO | train_inner | epoch 009:     27 / 60 loss=0.925, ppl=1.9, wps=9424.9, ups=14.98, wpb=629, bsz=32, num_updates=500, lr=3e-05, gnorm=8.68, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=91
2022-02-28 16:23:25 | INFO | train_inner | epoch 009:     37 / 60 loss=0.985, ppl=1.98, wps=9837.3, ups=15.36, wpb=640.5, bsz=32, num_updates=510, lr=2.99846e-05, gnorm=5.383, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=91
2022-02-28 16:23:26 | INFO | train_inner | epoch 009:     47 / 60 loss=1.009, ppl=2.01, wps=8893.5, ups=15.14, wpb=587.5, bsz=32, num_updates=520, lr=2.99692e-05, gnorm=6.117, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=92
2022-02-28 16:23:27 | INFO | train_inner | epoch 009:     57 / 60 loss=0.738, ppl=1.67, wps=8871.3, ups=11.77, wpb=753.6, bsz=32, num_updates=530, lr=2.99538e-05, gnorm=4.607, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=93
2022-02-28 16:23:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:23:27 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 0.804 | ppl 1.75 | wps 28080 | wpb 653.8 | bsz 31.5 | num_updates 533 | best_loss 0.801
2022-02-28 16:23:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 533 updates
2022-02-28 16:23:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 16:23:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 16:23:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 9 @ 533 updates, score 0.804) (writing took 2.6914851969922893 seconds)
2022-02-28 16:23:30 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-28 16:23:30 | INFO | train | epoch 009 | loss 0.879 | ppl 1.84 | wps 5280.4 | ups 7.93 | wpb 666.1 | bsz 31.8 | num_updates 533 | lr 2.99492e-05 | gnorm 6.063 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20 | wall 96
2022-02-28 16:23:30 | INFO | fairseq.trainer | begin training epoch 10
2022-02-28 16:23:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:23:30 | INFO | train_inner | epoch 010:      7 / 60 loss=0.834, ppl=1.78, wps=1476.7, ups=2.51, wpb=587.9, bsz=32, num_updates=540, lr=2.99385e-05, gnorm=5.79, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=97
2022-02-28 16:23:31 | INFO | train_inner | epoch 010:     17 / 60 loss=0.702, ppl=1.63, wps=10997.8, ups=13.72, wpb=801.8, bsz=31, num_updates=550, lr=2.99231e-05, gnorm=5.45, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=97
2022-02-28 16:23:32 | INFO | train_inner | epoch 010:     27 / 60 loss=0.862, ppl=1.82, wps=9677.1, ups=15.18, wpb=637.4, bsz=32, num_updates=560, lr=2.99077e-05, gnorm=5.355, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=98
2022-02-28 16:23:33 | INFO | train_inner | epoch 010:     37 / 60 loss=0.97, ppl=1.96, wps=8116.2, ups=14.2, wpb=571.6, bsz=32, num_updates=570, lr=2.98923e-05, gnorm=8.432, clip=100, loss_scale=1, train_wall=1, gb_free=20.4, wall=99
2022-02-28 16:23:33 | INFO | train_inner | epoch 010:     47 / 60 loss=0.714, ppl=1.64, wps=10884.7, ups=14.98, wpb=726.6, bsz=32, num_updates=580, lr=2.98769e-05, gnorm=3.924, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=99
2022-02-28 16:23:34 | INFO | train_inner | epoch 010:     57 / 60 loss=0.818, ppl=1.76, wps=9157.5, ups=12.3, wpb=744.4, bsz=32, num_updates=590, lr=2.98615e-05, gnorm=4.924, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=100
2022-02-28 16:23:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:23:35 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.785 | ppl 1.72 | wps 26663.4 | wpb 653.8 | bsz 31.5 | num_updates 593 | best_loss 0.785
2022-02-28 16:23:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 593 updates
2022-02-28 16:23:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:23:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 16:23:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 10 @ 593 updates, score 0.785) (writing took 7.355989514966495 seconds)
2022-02-28 16:23:42 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-28 16:23:42 | INFO | train | epoch 010 | loss 0.83 | ppl 1.78 | wps 3263.5 | ups 4.9 | wpb 666.1 | bsz 31.8 | num_updates 593 | lr 2.98569e-05 | gnorm 5.802 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.4 | wall 108
2022-02-28 16:23:42 | INFO | fairseq.trainer | begin training epoch 11
2022-02-28 16:23:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:23:43 | INFO | train_inner | epoch 011:      7 / 60 loss=0.736, ppl=1.67, wps=899.1, ups=1.14, wpb=788.3, bsz=32, num_updates=600, lr=2.98462e-05, gnorm=4.178, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=109
2022-02-28 16:23:43 | INFO | train_inner | epoch 011:     17 / 60 loss=0.944, ppl=1.92, wps=8352.4, ups=15.51, wpb=538.5, bsz=32, num_updates=610, lr=2.98308e-05, gnorm=10.154, clip=100, loss_scale=1, train_wall=1, gb_free=20.1, wall=110
2022-02-28 16:23:44 | INFO | train_inner | epoch 011:     27 / 60 loss=0.823, ppl=1.77, wps=9231.7, ups=15.52, wpb=594.8, bsz=32, num_updates=620, lr=2.98154e-05, gnorm=12.79, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=110
2022-02-28 16:23:45 | INFO | train_inner | epoch 011:     37 / 60 loss=0.754, ppl=1.69, wps=9861.4, ups=13.34, wpb=739.3, bsz=32, num_updates=630, lr=2.98e-05, gnorm=5.977, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=111
2022-02-28 16:23:46 | INFO | train_inner | epoch 011:     47 / 60 loss=0.895, ppl=1.86, wps=9650.3, ups=14.94, wpb=645.9, bsz=32, num_updates=640, lr=2.97846e-05, gnorm=4.627, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=112
2022-02-28 16:23:46 | INFO | train_inner | epoch 011:     57 / 60 loss=0.746, ppl=1.68, wps=7875.2, ups=11.75, wpb=670.5, bsz=31, num_updates=650, lr=2.97692e-05, gnorm=4.175, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=113
2022-02-28 16:23:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:23:47 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.79 | ppl 1.73 | wps 29898.2 | wpb 653.8 | bsz 31.5 | num_updates 653 | best_loss 0.785
2022-02-28 16:23:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 653 updates
2022-02-28 16:23:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 16:23:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 16:23:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 11 @ 653 updates, score 0.79) (writing took 4.410372362995986 seconds)
2022-02-28 16:23:52 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-28 16:23:52 | INFO | train | epoch 011 | loss 0.794 | ppl 1.73 | wps 4271.6 | ups 6.41 | wpb 666.1 | bsz 31.8 | num_updates 653 | lr 2.97646e-05 | gnorm 6.897 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 118
2022-02-28 16:23:52 | INFO | fairseq.trainer | begin training epoch 12
2022-02-28 16:23:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:23:52 | INFO | train_inner | epoch 012:      7 / 60 loss=0.581, ppl=1.5, wps=1498.9, ups=1.74, wpb=859.6, bsz=31, num_updates=660, lr=2.97538e-05, gnorm=7.391, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=118
2022-02-28 16:23:53 | INFO | train_inner | epoch 012:     17 / 60 loss=0.549, ppl=1.46, wps=11600.7, ups=12.61, wpb=919.6, bsz=32, num_updates=670, lr=2.97385e-05, gnorm=3.421, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=119
2022-02-28 16:23:54 | INFO | train_inner | epoch 012:     27 / 60 loss=0.747, ppl=1.68, wps=9205.7, ups=11.79, wpb=780.5, bsz=32, num_updates=680, lr=2.97231e-05, gnorm=3.73, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=120
2022-02-28 16:23:54 | INFO | train_inner | epoch 012:     37 / 60 loss=0.905, ppl=1.87, wps=8409.2, ups=15.03, wpb=559.6, bsz=32, num_updates=690, lr=2.97077e-05, gnorm=7.786, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=121
2022-02-28 16:23:55 | INFO | train_inner | epoch 012:     47 / 60 loss=0.951, ppl=1.93, wps=7831.3, ups=14.51, wpb=539.9, bsz=32, num_updates=700, lr=2.96923e-05, gnorm=5.88, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=121
2022-02-28 16:23:56 | INFO | train_inner | epoch 012:     57 / 60 loss=1.169, ppl=2.25, wps=4763.3, ups=11.4, wpb=417.9, bsz=32, num_updates=710, lr=2.96769e-05, gnorm=8.523, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=122
2022-02-28 16:23:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:23:57 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.791 | ppl 1.73 | wps 30003.1 | wpb 653.8 | bsz 31.5 | num_updates 713 | best_loss 0.785
2022-02-28 16:23:57 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 2 runs
2022-02-28 16:23:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 713 updates
2022-02-28 16:23:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 16:24:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 16:24:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 12 @ 713 updates, score 0.791) (writing took 3.3129061840008944 seconds)
2022-02-28 16:24:00 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-28 16:24:00 | INFO | train | epoch 012 | loss 0.768 | ppl 1.7 | wps 4676.8 | ups 7.02 | wpb 666.1 | bsz 31.8 | num_updates 713 | lr 2.96723e-05 | gnorm 6.256 | clip 100 | loss_scale 1 | train_wall 5 | gb_free 20.6 | wall 126
2022-02-28 16:24:00 | INFO | fairseq_cli.train | done training in 122.3 seconds
