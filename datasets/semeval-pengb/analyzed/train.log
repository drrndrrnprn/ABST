2022-02-26 19:16:19 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 10, 'log_format': None, 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 8192, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 5000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 12288, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.abst/dev', 'restore_file': 'bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 10, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 2, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.0, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=3.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst/dev', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_epoch=5, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='aspect_base_denoising', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.0, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=3.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst/dev', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_epoch=5, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 20000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-26 19:16:19 | INFO | bartabst.tasks.aspect_base_denoising | dictionary: 51200 types
2022-02-26 19:16:21 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-26 19:16:21 | INFO | fairseq_cli.train | task: AspectBaseDenoisingTask
2022-02-26 19:16:21 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-26 19:16:21 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-26 19:16:21 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-26 19:16:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-26 19:16:22 | INFO | bartabst.data.data_utils | loaded 598 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-02-26 19:16:22 | INFO | bartabst.tasks.aspect_base_denoising | Split: valid, Loaded 598 samples of denoising_dataset
2022-02-26 19:16:28 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-02-26 19:16:28 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-26 19:16:28 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-02-26 19:16:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-26 19:16:28 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-02-26 19:16:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-26 19:16:28 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-26 19:16:28 | INFO | fairseq_cli.train | max tokens per device = 8192 and max sentences per device = 32
2022-02-26 19:16:28 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:16:32 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 64 @ 0 updates)
2022-02-26 19:16:32 | INFO | fairseq.trainer | loading train data for epoch 1
no aos file, no transfer aos used
2022-02-26 19:16:34 | INFO | bartabst.data.data_utils | loaded 1,910 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-02-26 19:16:34 | INFO | bartabst.tasks.aspect_base_denoising | Split: train, Loaded 1910 samples of denoising_dataset
2022-02-26 19:16:34 | INFO | fairseq.trainer | begin training epoch 1
2022-02-26 19:16:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:16:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-26 19:16:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-26 19:16:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-26 19:16:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-26 19:16:35 | INFO | train_inner | epoch 001:     14 / 60 loss=7.187, ppl=145.73, wps=7928.1, ups=12.71, wpb=645.3, bsz=32, num_updates=10, lr=6e-07, gnorm=25.684, clip=100, loss_scale=8, train_wall=1, gb_free=20.4, wall=7
2022-02-26 19:16:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-26 19:16:36 | INFO | train_inner | epoch 001:     25 / 60 loss=6.936, ppl=122.42, wps=8689.5, ups=14.63, wpb=594.1, bsz=32, num_updates=20, lr=1.2e-06, gnorm=25.478, clip=100, loss_scale=4, train_wall=1, gb_free=20.8, wall=8
2022-02-26 19:16:36 | INFO | train_inner | epoch 001:     35 / 60 loss=6.457, ppl=87.88, wps=9893.5, ups=15.5, wpb=638.2, bsz=32, num_updates=30, lr=1.8e-06, gnorm=26.265, clip=100, loss_scale=4, train_wall=1, gb_free=20.8, wall=8
2022-02-26 19:16:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-02-26 19:16:37 | INFO | train_inner | epoch 001:     46 / 60 loss=5.203, ppl=36.84, wps=8837.6, ups=13.75, wpb=642.7, bsz=32, num_updates=40, lr=2.4e-06, gnorm=28.071, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=9
2022-02-26 19:16:38 | INFO | train_inner | epoch 001:     56 / 60 loss=4.244, ppl=18.95, wps=7704.7, ups=10.94, wpb=704.2, bsz=32, num_updates=50, lr=3e-06, gnorm=20.778, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=10
2022-02-26 19:16:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:16:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.629 | ppl 3.09 | wps 25264.7 | wpb 653.8 | bsz 31.5 | num_updates 54
2022-02-26 19:16:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 54 updates
2022-02-26 19:16:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:16:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:16:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 1 @ 54 updates, score 1.629) (writing took 4.513333170005353 seconds)
2022-02-26 19:16:44 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-26 19:16:44 | INFO | train | epoch 001 | loss 5.829 | ppl 56.84 | wps 3652.1 | ups 5.81 | wpb 632.1 | bsz 32 | num_updates 54 | lr 3.24e-06 | gnorm 24.961 | clip 100 | loss_scale 2 | train_wall 4 | gb_free 20.8 | wall 15
2022-02-26 19:16:44 | INFO | fairseq.trainer | begin training epoch 2
2022-02-26 19:16:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:16:44 | INFO | train_inner | epoch 002:      6 / 60 loss=3.422, ppl=10.71, wps=909.9, ups=1.7, wpb=536.1, bsz=32, num_updates=60, lr=3.6e-06, gnorm=19.578, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=16
2022-02-26 19:16:45 | INFO | train_inner | epoch 002:     16 / 60 loss=3.289, ppl=9.77, wps=7790.9, ups=16.29, wpb=478.2, bsz=32, num_updates=70, lr=4.2e-06, gnorm=16.082, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=16
2022-02-26 19:16:45 | INFO | train_inner | epoch 002:     26 / 60 loss=2.388, ppl=5.23, wps=11454.2, ups=14.7, wpb=779.3, bsz=32, num_updates=80, lr=4.8e-06, gnorm=28.639, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=17
2022-02-26 19:16:46 | INFO | train_inner | epoch 002:     36 / 60 loss=2.165, ppl=4.49, wps=8101.8, ups=12.93, wpb=626.6, bsz=32, num_updates=90, lr=5.4e-06, gnorm=18.77, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=18
2022-02-26 19:16:47 | INFO | train_inner | epoch 002:     46 / 60 loss=2.159, ppl=4.47, wps=10264.2, ups=15.31, wpb=670.6, bsz=32, num_updates=100, lr=6e-06, gnorm=24.77, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=19
2022-02-26 19:16:48 | INFO | train_inner | epoch 002:     56 / 60 loss=1.769, ppl=3.41, wps=9871.5, ups=12.42, wpb=794.7, bsz=31, num_updates=110, lr=6.6e-06, gnorm=15.363, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=19
2022-02-26 19:16:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:16:48 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.242 | ppl 2.37 | wps 27176.9 | wpb 653.8 | bsz 31.5 | num_updates 114 | best_loss 1.242
2022-02-26 19:16:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 114 updates
2022-02-26 19:16:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:16:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:16:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 2 @ 114 updates, score 1.242) (writing took 4.287648946992704 seconds)
2022-02-26 19:16:53 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-26 19:16:53 | INFO | train | epoch 002 | loss 2.324 | ppl 5.01 | wps 4350.3 | ups 6.53 | wpb 666.1 | bsz 31.8 | num_updates 114 | lr 6.84e-06 | gnorm 19.785 | clip 100 | loss_scale 2 | train_wall 4 | gb_free 20.8 | wall 24
2022-02-26 19:16:53 | INFO | fairseq.trainer | begin training epoch 3
2022-02-26 19:16:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:16:53 | INFO | train_inner | epoch 003:      6 / 60 loss=1.584, ppl=3, wps=1367.2, ups=1.77, wpb=771.3, bsz=32, num_updates=120, lr=7.2e-06, gnorm=9.943, clip=100, loss_scale=2, train_wall=1, gb_free=20.5, wall=25
2022-02-26 19:16:54 | INFO | train_inner | epoch 003:     16 / 60 loss=1.75, ppl=3.36, wps=9358.5, ups=14.83, wpb=631, bsz=32, num_updates=130, lr=7.8e-06, gnorm=11.388, clip=100, loss_scale=2, train_wall=1, gb_free=20.4, wall=26
2022-02-26 19:16:54 | INFO | train_inner | epoch 003:     26 / 60 loss=1.755, ppl=3.38, wps=9843.4, ups=15.88, wpb=619.7, bsz=32, num_updates=140, lr=8.4e-06, gnorm=9.661, clip=100, loss_scale=2, train_wall=1, gb_free=20.1, wall=26
2022-02-26 19:16:55 | INFO | train_inner | epoch 003:     36 / 60 loss=1.433, ppl=2.7, wps=12073.4, ups=15.33, wpb=787.7, bsz=31, num_updates=150, lr=9e-06, gnorm=8.299, clip=100, loss_scale=2, train_wall=1, gb_free=20.2, wall=27
2022-02-26 19:16:56 | INFO | train_inner | epoch 003:     46 / 60 loss=1.333, ppl=2.52, wps=12253.4, ups=14.92, wpb=821, bsz=32, num_updates=160, lr=9.6e-06, gnorm=7.72, clip=100, loss_scale=2, train_wall=1, gb_free=20.6, wall=28
2022-02-26 19:16:57 | INFO | train_inner | epoch 003:     56 / 60 loss=2.06, ppl=4.17, wps=5636.3, ups=11.75, wpb=479.8, bsz=32, num_updates=170, lr=1.02e-05, gnorm=15.907, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=28
2022-02-26 19:16:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:16:57 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.089 | ppl 2.13 | wps 28017.2 | wpb 653.8 | bsz 31.5 | num_updates 174 | best_loss 1.089
2022-02-26 19:16:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 174 updates
2022-02-26 19:16:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 3 @ 174 updates, score 1.089) (writing took 4.038159432995599 seconds)
2022-02-26 19:17:02 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-26 19:17:02 | INFO | train | epoch 003 | loss 1.63 | ppl 3.1 | wps 4527.9 | ups 6.8 | wpb 666.1 | bsz 31.8 | num_updates 174 | lr 1.044e-05 | gnorm 11.123 | clip 100 | loss_scale 2 | train_wall 4 | gb_free 20.8 | wall 33
2022-02-26 19:17:02 | INFO | fairseq.trainer | begin training epoch 4
2022-02-26 19:17:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:17:02 | INFO | train_inner | epoch 004:      6 / 60 loss=1.494, ppl=2.82, wps=1318.5, ups=1.86, wpb=707.7, bsz=32, num_updates=180, lr=1.08e-05, gnorm=16.43, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=34
2022-02-26 19:17:03 | INFO | train_inner | epoch 004:     16 / 60 loss=1.7, ppl=3.25, wps=8415.3, ups=15.57, wpb=540.6, bsz=32, num_updates=190, lr=1.14e-05, gnorm=10.655, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=34
2022-02-26 19:17:03 | INFO | train_inner | epoch 004:     26 / 60 loss=1.451, ppl=2.73, wps=10252.7, ups=15.62, wpb=656.2, bsz=31, num_updates=200, lr=1.2e-05, gnorm=12.912, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=35
2022-02-26 19:17:04 | INFO | train_inner | epoch 004:     36 / 60 loss=1.314, ppl=2.49, wps=11373.7, ups=15.83, wpb=718.7, bsz=32, num_updates=210, lr=1.26e-05, gnorm=9.898, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=36
2022-02-26 19:17:05 | INFO | train_inner | epoch 004:     46 / 60 loss=1.223, ppl=2.33, wps=11907.9, ups=14.96, wpb=795.8, bsz=32, num_updates=220, lr=1.32e-05, gnorm=15.201, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=36
2022-02-26 19:17:05 | INFO | train_inner | epoch 004:     56 / 60 loss=1.771, ppl=3.41, wps=5926.8, ups=11.77, wpb=503.6, bsz=32, num_updates=230, lr=1.38e-05, gnorm=8.319, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=37
2022-02-26 19:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:17:06 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.02 | ppl 2.03 | wps 25576.5 | wpb 653.8 | bsz 31.5 | num_updates 234 | best_loss 1.02
2022-02-26 19:17:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 234 updates
2022-02-26 19:17:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 4 @ 234 updates, score 1.02) (writing took 4.444704098001239 seconds)
2022-02-26 19:17:11 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-26 19:17:11 | INFO | train | epoch 004 | loss 1.422 | ppl 2.68 | wps 4313.7 | ups 6.48 | wpb 666.1 | bsz 31.8 | num_updates 234 | lr 1.404e-05 | gnorm 11.408 | clip 100 | loss_scale 2 | train_wall 4 | gb_free 20.8 | wall 43
2022-02-26 19:17:11 | INFO | fairseq.trainer | begin training epoch 5
2022-02-26 19:17:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:17:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2022-02-26 19:17:11 | INFO | train_inner | epoch 005:      7 / 60 loss=1.604, ppl=3.04, wps=1130.2, ups=1.71, wpb=662.5, bsz=32, num_updates=240, lr=1.44e-05, gnorm=9.89, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=43
2022-02-26 19:17:12 | INFO | train_inner | epoch 005:     17 / 60 loss=1.716, ppl=3.28, wps=10249.9, ups=15.93, wpb=643.6, bsz=32, num_updates=250, lr=1.5e-05, gnorm=11.944, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=44
2022-02-26 19:17:13 | INFO | train_inner | epoch 005:     27 / 60 loss=1.875, ppl=3.67, wps=9526.1, ups=15.44, wpb=616.8, bsz=32, num_updates=260, lr=1.56e-05, gnorm=7.709, clip=100, loss_scale=1, train_wall=1, gb_free=19.7, wall=44
2022-02-26 19:17:13 | INFO | train_inner | epoch 005:     37 / 60 loss=2.081, ppl=4.23, wps=6193.3, ups=15.41, wpb=401.9, bsz=32, num_updates=270, lr=1.62e-05, gnorm=18.073, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=45
2022-02-26 19:17:14 | INFO | train_inner | epoch 005:     47 / 60 loss=1.578, ppl=2.99, wps=11450.4, ups=15.39, wpb=744.1, bsz=32, num_updates=280, lr=1.68e-05, gnorm=6.85, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=46
2022-02-26 19:17:15 | INFO | train_inner | epoch 005:     57 / 60 loss=1.303, ppl=2.47, wps=10329.3, ups=11.96, wpb=864, bsz=32, num_updates=290, lr=1.74e-05, gnorm=26.713, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=47
2022-02-26 19:17:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:17:16 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.979 | ppl 1.97 | wps 25857 | wpb 653.8 | bsz 31.5 | num_updates 293 | best_loss 0.979
2022-02-26 19:17:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 293 updates
2022-02-26 19:17:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 5 @ 293 updates, score 0.979) (writing took 4.935181266002473 seconds)
2022-02-26 19:17:21 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-26 19:17:21 | INFO | train | epoch 005 | loss 1.667 | ppl 3.17 | wps 3979.9 | ups 6.05 | wpb 657.7 | bsz 31.8 | num_updates 293 | lr 1.758e-05 | gnorm 14.563 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 52
2022-02-26 19:17:21 | INFO | fairseq.trainer | begin training epoch 6
2022-02-26 19:17:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:17:21 | INFO | train_inner | epoch 006:      7 / 60 loss=1.741, ppl=3.34, wps=822, ups=1.53, wpb=539, bsz=31, num_updates=300, lr=1.8e-05, gnorm=13.505, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=53
2022-02-26 19:17:22 | INFO | train_inner | epoch 006:     17 / 60 loss=1.686, ppl=3.22, wps=10140.6, ups=16.08, wpb=630.7, bsz=32, num_updates=310, lr=1.86e-05, gnorm=37.333, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=54
2022-02-26 19:17:23 | INFO | train_inner | epoch 006:     27 / 60 loss=1.492, ppl=2.81, wps=11442.9, ups=15.85, wpb=721.9, bsz=32, num_updates=320, lr=1.92e-05, gnorm=7.336, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=54
2022-02-26 19:17:23 | INFO | train_inner | epoch 006:     37 / 60 loss=1.612, ppl=3.06, wps=10926.7, ups=16.05, wpb=680.6, bsz=32, num_updates=330, lr=1.98e-05, gnorm=6.677, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=55
2022-02-26 19:17:24 | INFO | train_inner | epoch 006:     47 / 60 loss=1.423, ppl=2.68, wps=10980.6, ups=14.58, wpb=753.2, bsz=32, num_updates=340, lr=2.04e-05, gnorm=6.284, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=56
2022-02-26 19:17:25 | INFO | train_inner | epoch 006:     57 / 60 loss=1.473, ppl=2.78, wps=9094.9, ups=12.74, wpb=713.7, bsz=31, num_updates=350, lr=2.1e-05, gnorm=7.195, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=56
2022-02-26 19:17:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:17:25 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 0.942 | ppl 1.92 | wps 25798 | wpb 653.8 | bsz 31.5 | num_updates 353 | best_loss 0.942
2022-02-26 19:17:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 353 updates
2022-02-26 19:17:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 6 @ 353 updates, score 0.942) (writing took 4.324204649994499 seconds)
2022-02-26 19:17:30 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-26 19:17:30 | INFO | train | epoch 006 | loss 1.556 | ppl 2.94 | wps 4432 | ups 6.65 | wpb 666.1 | bsz 31.8 | num_updates 353 | lr 2.118e-05 | gnorm 12.262 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 62
2022-02-26 19:17:30 | INFO | fairseq.trainer | begin training epoch 7
2022-02-26 19:17:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:17:30 | INFO | train_inner | epoch 007:      7 / 60 loss=1.529, ppl=2.89, wps=984.2, ups=1.75, wpb=561.6, bsz=32, num_updates=360, lr=2.16e-05, gnorm=7.353, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=62
2022-02-26 19:17:31 | INFO | train_inner | epoch 007:     17 / 60 loss=1.552, ppl=2.93, wps=8761.5, ups=15.83, wpb=553.3, bsz=32, num_updates=370, lr=2.22e-05, gnorm=8.272, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=63
2022-02-26 19:17:32 | INFO | train_inner | epoch 007:     27 / 60 loss=1.877, ppl=3.67, wps=7482.1, ups=16.44, wpb=455, bsz=32, num_updates=380, lr=2.28e-05, gnorm=7.473, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=63
2022-02-26 19:17:32 | INFO | train_inner | epoch 007:     37 / 60 loss=1.358, ppl=2.56, wps=11143.6, ups=13.54, wpb=822.8, bsz=31, num_updates=390, lr=2.34e-05, gnorm=24.973, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=64
2022-02-26 19:17:33 | INFO | train_inner | epoch 007:     47 / 60 loss=1.419, ppl=2.67, wps=10982.6, ups=15.49, wpb=708.8, bsz=32, num_updates=400, lr=2.4e-05, gnorm=43.807, clip=100, loss_scale=1, train_wall=1, gb_free=20.2, wall=65
2022-02-26 19:17:34 | INFO | train_inner | epoch 007:     57 / 60 loss=1.135, ppl=2.2, wps=9923, ups=12.14, wpb=817.4, bsz=32, num_updates=410, lr=2.46e-05, gnorm=5.684, clip=100, loss_scale=1, train_wall=1, gb_free=20.4, wall=66
2022-02-26 19:17:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:17:35 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 0.948 | ppl 1.93 | wps 25839.3 | wpb 653.8 | bsz 31.5 | num_updates 413 | best_loss 0.942
2022-02-26 19:17:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 413 updates
2022-02-26 19:17:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-26 19:17:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-26 19:17:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 7 @ 413 updates, score 0.948) (writing took 2.5416838290111627 seconds)
2022-02-26 19:17:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-26 19:17:37 | INFO | train | epoch 007 | loss 1.416 | ppl 2.67 | wps 5369.8 | ups 8.06 | wpb 666.1 | bsz 31.8 | num_updates 413 | lr 2.478e-05 | gnorm 16.107 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 69
2022-02-26 19:17:37 | INFO | fairseq.trainer | begin training epoch 8
2022-02-26 19:17:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:17:38 | INFO | train_inner | epoch 008:      7 / 60 loss=1.289, ppl=2.44, wps=1765.7, ups=2.55, wpb=691.4, bsz=32, num_updates=420, lr=2.52e-05, gnorm=7.211, clip=100, loss_scale=1, train_wall=1, gb_free=20, wall=70
2022-02-26 19:17:38 | INFO | train_inner | epoch 008:     17 / 60 loss=1.365, ppl=2.58, wps=10506.4, ups=15.26, wpb=688.5, bsz=32, num_updates=430, lr=2.58e-05, gnorm=6.321, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=70
2022-02-26 19:17:39 | INFO | train_inner | epoch 008:     27 / 60 loss=1.511, ppl=2.85, wps=10229.6, ups=15.21, wpb=672.6, bsz=32, num_updates=440, lr=2.64e-05, gnorm=7.539, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=71
2022-02-26 19:17:40 | INFO | train_inner | epoch 008:     37 / 60 loss=1.274, ppl=2.42, wps=10993.6, ups=14.56, wpb=754.8, bsz=31, num_updates=450, lr=2.7e-05, gnorm=6.965, clip=100, loss_scale=1, train_wall=1, gb_free=20.5, wall=72
2022-02-26 19:17:40 | INFO | train_inner | epoch 008:     47 / 60 loss=1.449, ppl=2.73, wps=9372.2, ups=16.19, wpb=578.8, bsz=32, num_updates=460, lr=2.76e-05, gnorm=6.044, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=72
2022-02-26 19:17:41 | INFO | train_inner | epoch 008:     57 / 60 loss=1.204, ppl=2.3, wps=8517.3, ups=12.91, wpb=659.5, bsz=32, num_updates=470, lr=2.82e-05, gnorm=8.343, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=73
2022-02-26 19:17:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:17:42 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 0.898 | ppl 1.86 | wps 28732.7 | wpb 653.8 | bsz 31.5 | num_updates 473 | best_loss 0.898
2022-02-26 19:17:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 473 updates
2022-02-26 19:17:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 8 @ 473 updates, score 0.898) (writing took 3.78755225999339 seconds)
2022-02-26 19:17:46 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-26 19:17:46 | INFO | train | epoch 008 | loss 1.363 | ppl 2.57 | wps 4705.8 | ups 7.06 | wpb 666.1 | bsz 31.8 | num_updates 473 | lr 2.838e-05 | gnorm 7.038 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.6 | wall 78
2022-02-26 19:17:46 | INFO | fairseq.trainer | begin training epoch 9
2022-02-26 19:17:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:17:46 | INFO | train_inner | epoch 009:      7 / 60 loss=1.239, ppl=2.36, wps=1483.7, ups=1.95, wpb=762.2, bsz=31, num_updates=480, lr=2.88e-05, gnorm=6.506, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=78
2022-02-26 19:17:47 | INFO | train_inner | epoch 009:     17 / 60 loss=1.398, ppl=2.63, wps=8610.3, ups=15.95, wpb=539.8, bsz=32, num_updates=490, lr=2.94e-05, gnorm=8.35, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=79
2022-02-26 19:17:48 | INFO | train_inner | epoch 009:     27 / 60 loss=1.248, ppl=2.38, wps=8891.8, ups=14.14, wpb=629, bsz=32, num_updates=500, lr=3e-05, gnorm=7.521, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=79
2022-02-26 19:17:48 | INFO | train_inner | epoch 009:     37 / 60 loss=1.41, ppl=2.66, wps=8852.5, ups=13.82, wpb=640.5, bsz=32, num_updates=510, lr=2.99846e-05, gnorm=8.073, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=80
2022-02-26 19:17:49 | INFO | train_inner | epoch 009:     47 / 60 loss=1.413, ppl=2.66, wps=8131.2, ups=13.84, wpb=587.5, bsz=32, num_updates=520, lr=2.99692e-05, gnorm=8.403, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=81
2022-02-26 19:17:50 | INFO | train_inner | epoch 009:     57 / 60 loss=1.124, ppl=2.18, wps=7179.4, ups=9.53, wpb=753.6, bsz=32, num_updates=530, lr=2.99538e-05, gnorm=7.099, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=82
2022-02-26 19:17:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:17:51 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 0.846 | ppl 1.8 | wps 25886 | wpb 653.8 | bsz 31.5 | num_updates 533 | best_loss 0.846
2022-02-26 19:17:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 533 updates
2022-02-26 19:17:51 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:17:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 9 @ 533 updates, score 0.846) (writing took 4.266187425993849 seconds)
2022-02-26 19:17:55 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-26 19:17:55 | INFO | train | epoch 009 | loss 1.261 | ppl 2.4 | wps 4202.6 | ups 6.31 | wpb 666.1 | bsz 31.8 | num_updates 533 | lr 2.99492e-05 | gnorm 7.621 | clip 100 | loss_scale 1 | train_wall 5 | gb_free 20 | wall 87
2022-02-26 19:17:55 | INFO | fairseq.trainer | begin training epoch 10
2022-02-26 19:17:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:17:56 | INFO | train_inner | epoch 010:      7 / 60 loss=1.219, ppl=2.33, wps=1050.1, ups=1.79, wpb=587.9, bsz=32, num_updates=540, lr=2.99385e-05, gnorm=6.324, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=88
2022-02-26 19:17:56 | INFO | train_inner | epoch 010:     17 / 60 loss=1.105, ppl=2.15, wps=11209.2, ups=13.98, wpb=801.8, bsz=31, num_updates=550, lr=2.99231e-05, gnorm=6.324, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=88
2022-02-26 19:17:57 | INFO | train_inner | epoch 010:     27 / 60 loss=1.236, ppl=2.36, wps=10317.3, ups=16.19, wpb=637.4, bsz=32, num_updates=560, lr=2.99077e-05, gnorm=5.857, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=89
2022-02-26 19:17:58 | INFO | train_inner | epoch 010:     37 / 60 loss=1.317, ppl=2.49, wps=9141, ups=15.99, wpb=571.6, bsz=32, num_updates=570, lr=2.98923e-05, gnorm=9.886, clip=100, loss_scale=1, train_wall=1, gb_free=20.3, wall=90
2022-02-26 19:17:58 | INFO | train_inner | epoch 010:     47 / 60 loss=1.082, ppl=2.12, wps=11106.5, ups=15.29, wpb=726.6, bsz=32, num_updates=580, lr=2.98769e-05, gnorm=5.254, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=90
2022-02-26 19:17:59 | INFO | train_inner | epoch 010:     57 / 60 loss=1.298, ppl=2.46, wps=8788.7, ups=11.81, wpb=744.4, bsz=32, num_updates=590, lr=2.98615e-05, gnorm=34.147, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=91
2022-02-26 19:18:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:18:00 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.842 | ppl 1.79 | wps 26384.7 | wpb 653.8 | bsz 31.5 | num_updates 593 | best_loss 0.842
2022-02-26 19:18:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 593 updates
2022-02-26 19:18:00 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:18:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:18:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 10 @ 593 updates, score 0.842) (writing took 4.033129938994534 seconds)
2022-02-26 19:18:04 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-26 19:18:04 | INFO | train | epoch 010 | loss 1.217 | ppl 2.32 | wps 4526.8 | ups 6.8 | wpb 666.1 | bsz 31.8 | num_updates 593 | lr 2.98569e-05 | gnorm 16.697 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.4 | wall 96
2022-02-26 19:18:04 | INFO | fairseq.trainer | begin training epoch 11
2022-02-26 19:18:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:18:05 | INFO | train_inner | epoch 011:      7 / 60 loss=1.112, ppl=2.16, wps=1449.7, ups=1.84, wpb=788.3, bsz=32, num_updates=600, lr=2.98462e-05, gnorm=37.054, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=96
2022-02-26 19:18:05 | INFO | train_inner | epoch 011:     17 / 60 loss=1.302, ppl=2.47, wps=7248.7, ups=13.46, wpb=538.5, bsz=32, num_updates=610, lr=2.98308e-05, gnorm=7.31, clip=100, loss_scale=1, train_wall=1, gb_free=20.1, wall=97
2022-02-26 19:18:06 | INFO | train_inner | epoch 011:     27 / 60 loss=1.173, ppl=2.25, wps=8605.1, ups=14.47, wpb=594.8, bsz=32, num_updates=620, lr=2.98154e-05, gnorm=48.283, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=98
2022-02-26 19:18:07 | INFO | train_inner | epoch 011:     37 / 60 loss=1.056, ppl=2.08, wps=9753.7, ups=13.19, wpb=739.3, bsz=32, num_updates=630, lr=2.98e-05, gnorm=6.273, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=99
2022-02-26 19:18:08 | INFO | train_inner | epoch 011:     47 / 60 loss=1.272, ppl=2.42, wps=9636.1, ups=14.92, wpb=645.9, bsz=32, num_updates=640, lr=2.97846e-05, gnorm=5.444, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=99
2022-02-26 19:18:09 | INFO | train_inner | epoch 011:     57 / 60 loss=1.206, ppl=2.31, wps=7184.3, ups=10.71, wpb=670.5, bsz=31, num_updates=650, lr=2.97692e-05, gnorm=5.628, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=100
2022-02-26 19:18:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:18:09 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.848 | ppl 1.8 | wps 28747.4 | wpb 653.8 | bsz 31.5 | num_updates 653 | best_loss 0.842
2022-02-26 19:18:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 653 updates
2022-02-26 19:18:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-26 19:18:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-26 19:18:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 11 @ 653 updates, score 0.848) (writing took 2.693469803009066 seconds)
2022-02-26 19:18:12 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-26 19:18:12 | INFO | train | epoch 011 | loss 1.171 | ppl 2.25 | wps 5061.7 | ups 7.6 | wpb 666.1 | bsz 31.8 | num_updates 653 | lr 2.97646e-05 | gnorm 12.904 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 104
2022-02-26 19:18:12 | INFO | fairseq.trainer | begin training epoch 12
2022-02-26 19:18:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:18:13 | INFO | train_inner | epoch 012:      7 / 60 loss=0.957, ppl=1.94, wps=2123.4, ups=2.47, wpb=859.6, bsz=31, num_updates=660, lr=2.97538e-05, gnorm=6.345, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=104
2022-02-26 19:18:13 | INFO | train_inner | epoch 012:     17 / 60 loss=0.916, ppl=1.89, wps=13420.6, ups=14.59, wpb=919.6, bsz=32, num_updates=670, lr=2.97385e-05, gnorm=4.745, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=105
2022-02-26 19:18:14 | INFO | train_inner | epoch 012:     27 / 60 loss=1.092, ppl=2.13, wps=12308.4, ups=15.77, wpb=780.5, bsz=32, num_updates=680, lr=2.97231e-05, gnorm=4.73, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=106
2022-02-26 19:18:15 | INFO | train_inner | epoch 012:     37 / 60 loss=1.174, ppl=2.26, wps=8730.8, ups=15.6, wpb=559.6, bsz=32, num_updates=690, lr=2.97077e-05, gnorm=5.719, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=106
2022-02-26 19:18:15 | INFO | train_inner | epoch 012:     47 / 60 loss=1.226, ppl=2.34, wps=8706.1, ups=16.13, wpb=539.9, bsz=32, num_updates=700, lr=2.96923e-05, gnorm=6.206, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=107
2022-02-26 19:18:16 | INFO | train_inner | epoch 012:     57 / 60 loss=1.465, ppl=2.76, wps=4839.9, ups=11.58, wpb=417.9, bsz=32, num_updates=710, lr=2.96769e-05, gnorm=17.588, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=108
2022-02-26 19:18:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:18:17 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.841 | ppl 1.79 | wps 25256.4 | wpb 653.8 | bsz 31.5 | num_updates 713 | best_loss 0.841
2022-02-26 19:18:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 713 updates
2022-02-26 19:18:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:18:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:18:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 12 @ 713 updates, score 0.841) (writing took 3.8222594399994705 seconds)
2022-02-26 19:18:21 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-26 19:18:21 | INFO | train | epoch 012 | loss 1.093 | ppl 2.13 | wps 4584.7 | ups 6.88 | wpb 666.1 | bsz 31.8 | num_updates 713 | lr 2.96723e-05 | gnorm 7.655 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.6 | wall 113
2022-02-26 19:18:21 | INFO | fairseq.trainer | begin training epoch 13
2022-02-26 19:18:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:18:21 | INFO | train_inner | epoch 013:      7 / 60 loss=1.254, ppl=2.39, wps=874.9, ups=1.9, wpb=461.2, bsz=32, num_updates=720, lr=2.96615e-05, gnorm=6.223, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=113
2022-02-26 19:18:22 | INFO | train_inner | epoch 013:     17 / 60 loss=1.022, ppl=2.03, wps=10887, ups=15.29, wpb=712, bsz=32, num_updates=730, lr=2.96462e-05, gnorm=5.262, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=114
2022-02-26 19:18:23 | INFO | train_inner | epoch 013:     27 / 60 loss=1.054, ppl=2.08, wps=9774, ups=16.53, wpb=591.2, bsz=32, num_updates=740, lr=2.96308e-05, gnorm=5.813, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=114
2022-02-26 19:18:23 | INFO | train_inner | epoch 013:     37 / 60 loss=0.909, ppl=1.88, wps=10612.6, ups=15.67, wpb=677.2, bsz=32, num_updates=750, lr=2.96154e-05, gnorm=5.083, clip=100, loss_scale=1, train_wall=1, gb_free=20.4, wall=115
2022-02-26 19:18:24 | INFO | train_inner | epoch 013:     47 / 60 loss=0.922, ppl=1.9, wps=12075.6, ups=14.73, wpb=819.6, bsz=31, num_updates=760, lr=2.96e-05, gnorm=4.508, clip=100, loss_scale=1, train_wall=1, gb_free=19.2, wall=116
2022-02-26 19:18:25 | INFO | train_inner | epoch 013:     57 / 60 loss=0.986, ppl=1.98, wps=9278.6, ups=13.04, wpb=711.6, bsz=32, num_updates=770, lr=2.95846e-05, gnorm=4.119, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=116
2022-02-26 19:18:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:18:25 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 0.839 | ppl 1.79 | wps 28308.8 | wpb 653.8 | bsz 31.5 | num_updates 773 | best_loss 0.839
2022-02-26 19:18:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 773 updates
2022-02-26 19:18:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:18:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:18:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 13 @ 773 updates, score 0.839) (writing took 3.9011213689955184 seconds)
2022-02-26 19:18:29 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-26 19:18:29 | INFO | train | epoch 013 | loss 1.013 | ppl 2.02 | wps 4642.3 | ups 6.97 | wpb 666.1 | bsz 31.8 | num_updates 773 | lr 2.958e-05 | gnorm 5.083 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 121
2022-02-26 19:18:29 | INFO | fairseq.trainer | begin training epoch 14
2022-02-26 19:18:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:18:30 | INFO | train_inner | epoch 014:      7 / 60 loss=1.058, ppl=2.08, wps=1171.2, ups=1.89, wpb=621.1, bsz=32, num_updates=780, lr=2.95692e-05, gnorm=4.665, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=122
2022-02-26 19:18:31 | INFO | train_inner | epoch 014:     17 / 60 loss=0.834, ppl=1.78, wps=12076.3, ups=14.85, wpb=813.1, bsz=31, num_updates=790, lr=2.95538e-05, gnorm=4.599, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=122
2022-02-26 19:18:31 | INFO | train_inner | epoch 014:     27 / 60 loss=0.909, ppl=1.88, wps=9743.1, ups=15.88, wpb=613.5, bsz=32, num_updates=800, lr=2.95385e-05, gnorm=5.445, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=123
2022-02-26 19:18:32 | INFO | train_inner | epoch 014:     37 / 60 loss=0.922, ppl=1.9, wps=11236.5, ups=15.48, wpb=725.9, bsz=32, num_updates=810, lr=2.95231e-05, gnorm=5.017, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=124
2022-02-26 19:18:33 | INFO | train_inner | epoch 014:     47 / 60 loss=1.118, ppl=2.17, wps=7488.9, ups=13.9, wpb=538.7, bsz=32, num_updates=820, lr=2.95077e-05, gnorm=5.693, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=124
2022-02-26 19:18:34 | INFO | train_inner | epoch 014:     57 / 60 loss=1.037, ppl=2.05, wps=7727.9, ups=11.13, wpb=694.3, bsz=32, num_updates=830, lr=2.94923e-05, gnorm=8.266, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=125
2022-02-26 19:18:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:18:34 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 0.837 | ppl 1.79 | wps 27360.9 | wpb 653.8 | bsz 31.5 | num_updates 833 | best_loss 0.837
2022-02-26 19:18:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 833 updates
2022-02-26 19:18:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:18:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:18:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 14 @ 833 updates, score 0.837) (writing took 3.8412711279961513 seconds)
2022-02-26 19:18:38 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-26 19:18:38 | INFO | train | epoch 014 | loss 0.973 | ppl 1.96 | wps 4561.6 | ups 6.85 | wpb 666.1 | bsz 31.8 | num_updates 833 | lr 2.94877e-05 | gnorm 5.685 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 130
2022-02-26 19:18:38 | INFO | fairseq.trainer | begin training epoch 15
2022-02-26 19:18:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:18:39 | INFO | train_inner | epoch 015:      7 / 60 loss=0.952, ppl=1.94, wps=1234.8, ups=1.93, wpb=640.5, bsz=32, num_updates=840, lr=2.94769e-05, gnorm=4.997, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=131
2022-02-26 19:18:39 | INFO | train_inner | epoch 015:     17 / 60 loss=0.877, ppl=1.84, wps=11352.4, ups=15.09, wpb=752.1, bsz=32, num_updates=850, lr=2.94615e-05, gnorm=4.397, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=131
2022-02-26 19:18:40 | INFO | train_inner | epoch 015:     27 / 60 loss=0.877, ppl=1.84, wps=11306.8, ups=15.18, wpb=744.9, bsz=32, num_updates=860, lr=2.94462e-05, gnorm=6.688, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=132
2022-02-26 19:18:41 | INFO | train_inner | epoch 015:     37 / 60 loss=1.144, ppl=2.21, wps=7875.9, ups=16.64, wpb=473.3, bsz=32, num_updates=870, lr=2.94308e-05, gnorm=6.412, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=132
2022-02-26 19:18:41 | INFO | train_inner | epoch 015:     47 / 60 loss=0.909, ppl=1.88, wps=9943.7, ups=15.83, wpb=628, bsz=32, num_updates=880, lr=2.94154e-05, gnorm=5.336, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=133
2022-02-26 19:18:42 | INFO | train_inner | epoch 015:     57 / 60 loss=0.908, ppl=1.88, wps=8164.6, ups=10.28, wpb=794.6, bsz=31, num_updates=890, lr=2.94e-05, gnorm=5.083, clip=100, loss_scale=1, train_wall=1, gb_free=19.7, wall=134
2022-02-26 19:18:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:18:43 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 0.814 | ppl 1.76 | wps 26271.4 | wpb 653.8 | bsz 31.5 | num_updates 893 | best_loss 0.814
2022-02-26 19:18:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 893 updates
2022-02-26 19:18:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:18:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:18:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 15 @ 893 updates, score 0.814) (writing took 4.577091961007682 seconds)
2022-02-26 19:18:48 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-26 19:18:48 | INFO | train | epoch 015 | loss 0.931 | ppl 1.91 | wps 4227.3 | ups 6.35 | wpb 666.1 | bsz 31.8 | num_updates 893 | lr 2.93954e-05 | gnorm 5.456 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 139
2022-02-26 19:18:48 | INFO | fairseq.trainer | begin training epoch 16
2022-02-26 19:18:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:18:48 | INFO | train_inner | epoch 016:      7 / 60 loss=0.993, ppl=1.99, wps=947.2, ups=1.69, wpb=560.4, bsz=32, num_updates=900, lr=2.93846e-05, gnorm=7.646, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=140
2022-02-26 19:18:49 | INFO | train_inner | epoch 016:     17 / 60 loss=0.916, ppl=1.89, wps=9400, ups=15.89, wpb=591.5, bsz=32, num_updates=910, lr=2.93692e-05, gnorm=5.083, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=141
2022-02-26 19:18:49 | INFO | train_inner | epoch 016:     27 / 60 loss=1.026, ppl=2.04, wps=9089.4, ups=16.41, wpb=553.8, bsz=32, num_updates=920, lr=2.93538e-05, gnorm=5.75, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=141
2022-02-26 19:18:50 | INFO | train_inner | epoch 016:     37 / 60 loss=0.781, ppl=1.72, wps=13395.1, ups=14.69, wpb=911.6, bsz=32, num_updates=930, lr=2.93385e-05, gnorm=4, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=142
2022-02-26 19:18:51 | INFO | train_inner | epoch 016:     47 / 60 loss=0.907, ppl=1.88, wps=9268.7, ups=15.93, wpb=581.8, bsz=32, num_updates=940, lr=2.93231e-05, gnorm=5.443, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=143
2022-02-26 19:18:52 | INFO | train_inner | epoch 016:     57 / 60 loss=0.877, ppl=1.84, wps=8631.2, ups=11.7, wpb=737.7, bsz=31, num_updates=950, lr=2.93077e-05, gnorm=4.945, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=143
2022-02-26 19:18:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:18:52 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 0.819 | ppl 1.76 | wps 27099 | wpb 653.8 | bsz 31.5 | num_updates 953 | best_loss 0.814
2022-02-26 19:18:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 953 updates
2022-02-26 19:18:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-26 19:18:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-26 19:18:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 16 @ 953 updates, score 0.819) (writing took 2.58459730699542 seconds)
2022-02-26 19:18:55 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-26 19:18:55 | INFO | train | epoch 016 | loss 0.893 | ppl 1.86 | wps 5424.9 | ups 8.14 | wpb 666.1 | bsz 31.8 | num_updates 953 | lr 2.93031e-05 | gnorm 5.401 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 147
2022-02-26 19:18:55 | INFO | fairseq.trainer | begin training epoch 17
2022-02-26 19:18:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:18:56 | INFO | train_inner | epoch 017:      7 / 60 loss=0.87, ppl=1.83, wps=1653.4, ups=2.55, wpb=648.6, bsz=32, num_updates=960, lr=2.92923e-05, gnorm=5.117, clip=100, loss_scale=1, train_wall=1, gb_free=20.2, wall=147
2022-02-26 19:18:56 | INFO | train_inner | epoch 017:     17 / 60 loss=0.809, ppl=1.75, wps=10203.1, ups=15.8, wpb=645.6, bsz=32, num_updates=970, lr=2.92769e-05, gnorm=5.047, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=148
2022-02-26 19:18:57 | INFO | train_inner | epoch 017:     27 / 60 loss=0.813, ppl=1.76, wps=13240.2, ups=14.49, wpb=913.6, bsz=31, num_updates=980, lr=2.92615e-05, gnorm=3.922, clip=100, loss_scale=1, train_wall=1, gb_free=19.7, wall=149
2022-02-26 19:18:58 | INFO | train_inner | epoch 017:     37 / 60 loss=0.9, ppl=1.87, wps=8904.1, ups=14.89, wpb=598, bsz=32, num_updates=990, lr=2.92462e-05, gnorm=4.817, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=149
2022-02-26 19:18:58 | INFO | train_inner | epoch 017:     47 / 60 loss=0.968, ppl=1.96, wps=8423.8, ups=16.21, wpb=519.8, bsz=32, num_updates=1000, lr=2.92308e-05, gnorm=6.391, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=150
2022-02-26 19:18:59 | INFO | train_inner | epoch 017:     57 / 60 loss=0.855, ppl=1.81, wps=7891.5, ups=12.5, wpb=631.4, bsz=32, num_updates=1010, lr=2.92154e-05, gnorm=7.16, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=151
2022-02-26 19:18:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:19:00 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 0.805 | ppl 1.75 | wps 25548.3 | wpb 653.8 | bsz 31.5 | num_updates 1013 | best_loss 0.805
2022-02-26 19:19:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1013 updates
2022-02-26 19:19:00 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:19:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-26 19:19:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 17 @ 1013 updates, score 0.805) (writing took 3.8075955089880154 seconds)
2022-02-26 19:19:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-26 19:19:04 | INFO | train | epoch 017 | loss 0.847 | ppl 1.8 | wps 4647.5 | ups 6.98 | wpb 666.1 | bsz 31.8 | num_updates 1013 | lr 2.92108e-05 | gnorm 5.403 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.5 | wall 155
2022-02-26 19:19:04 | INFO | fairseq.trainer | begin training epoch 18
2022-02-26 19:19:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:19:05 | INFO | train_inner | epoch 018:      7 / 60 loss=0.738, ppl=1.67, wps=1290.2, ups=1.7, wpb=757.7, bsz=32, num_updates=1020, lr=2.92e-05, gnorm=4.136, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=157
2022-02-26 19:19:06 | INFO | train_inner | epoch 018:     17 / 60 loss=0.702, ppl=1.63, wps=12878.2, ups=14.14, wpb=910.6, bsz=31, num_updates=1030, lr=2.91846e-05, gnorm=3.523, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=157
2022-02-26 19:19:06 | INFO | train_inner | epoch 018:     27 / 60 loss=0.908, ppl=1.88, wps=7478.6, ups=16.17, wpb=462.5, bsz=32, num_updates=1040, lr=2.91692e-05, gnorm=5.456, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=158
2022-02-26 19:19:07 | INFO | train_inner | epoch 018:     37 / 60 loss=0.764, ppl=1.7, wps=11102.1, ups=15.45, wpb=718.6, bsz=32, num_updates=1050, lr=2.91538e-05, gnorm=4.084, clip=100, loss_scale=1, train_wall=1, gb_free=20.2, wall=159
2022-02-26 19:19:07 | INFO | train_inner | epoch 018:     47 / 60 loss=0.784, ppl=1.72, wps=11363.5, ups=15.77, wpb=720.7, bsz=32, num_updates=1060, lr=2.91385e-05, gnorm=6.59, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=159
2022-02-26 19:19:08 | INFO | train_inner | epoch 018:     57 / 60 loss=0.962, ppl=1.95, wps=7255.8, ups=13.01, wpb=557.7, bsz=32, num_updates=1070, lr=2.91231e-05, gnorm=4.507, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=160
2022-02-26 19:19:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:19:09 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 0.818 | ppl 1.76 | wps 27622.8 | wpb 653.8 | bsz 31.5 | num_updates 1073 | best_loss 0.805
2022-02-26 19:19:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1073 updates
2022-02-26 19:19:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-26 19:19:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-26 19:19:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 18 @ 1073 updates, score 0.818) (writing took 2.6782500650006114 seconds)
2022-02-26 19:19:12 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-26 19:19:12 | INFO | train | epoch 018 | loss 0.808 | ppl 1.75 | wps 5354.1 | ups 8.04 | wpb 666.1 | bsz 31.8 | num_updates 1073 | lr 2.91185e-05 | gnorm 4.92 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 164
2022-02-26 19:19:12 | INFO | fairseq.trainer | begin training epoch 19
2022-02-26 19:19:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:19:12 | INFO | train_inner | epoch 019:      7 / 60 loss=0.855, ppl=1.81, wps=1272.4, ups=2.44, wpb=522.4, bsz=32, num_updates=1080, lr=2.91077e-05, gnorm=5.269, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=164
2022-02-26 19:19:13 | INFO | train_inner | epoch 019:     17 / 60 loss=0.905, ppl=1.87, wps=8361.2, ups=16.58, wpb=504.4, bsz=32, num_updates=1090, lr=2.90923e-05, gnorm=4.887, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=165
2022-02-26 19:19:14 | INFO | train_inner | epoch 019:     27 / 60 loss=0.739, ppl=1.67, wps=10221.6, ups=16.01, wpb=638.6, bsz=32, num_updates=1100, lr=2.90769e-05, gnorm=4.475, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=165
2022-02-26 19:19:14 | INFO | train_inner | epoch 019:     37 / 60 loss=0.82, ppl=1.77, wps=10076.7, ups=13.51, wpb=746.1, bsz=31, num_updates=1110, lr=2.90615e-05, gnorm=5.4, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=166
2022-02-26 19:19:15 | INFO | train_inner | epoch 019:     47 / 60 loss=0.741, ppl=1.67, wps=10050.7, ups=13.58, wpb=740, bsz=32, num_updates=1120, lr=2.90462e-05, gnorm=4.927, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=167
2022-02-26 19:19:16 | INFO | train_inner | epoch 019:     57 / 60 loss=0.757, ppl=1.69, wps=9129.5, ups=12.05, wpb=757.5, bsz=32, num_updates=1130, lr=2.90308e-05, gnorm=4.425, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=168
2022-02-26 19:19:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:19:17 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 0.82 | ppl 1.77 | wps 26677 | wpb 653.8 | bsz 31.5 | num_updates 1133 | best_loss 0.805
2022-02-26 19:19:17 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 2 runs
2022-02-26 19:19:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1133 updates
2022-02-26 19:19:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-26 19:19:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-26 19:19:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 19 @ 1133 updates, score 0.82) (writing took 2.560934441993595 seconds)
2022-02-26 19:19:19 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-26 19:19:19 | INFO | train | epoch 019 | loss 0.791 | ppl 1.73 | wps 5348.4 | ups 8.03 | wpb 666.1 | bsz 31.8 | num_updates 1133 | lr 2.90262e-05 | gnorm 4.755 | clip 100 | loss_scale 1 | train_wall 4 | gb_free 20.8 | wall 171
2022-02-26 19:19:19 | INFO | fairseq_cli.train | done training in 165.4 seconds
