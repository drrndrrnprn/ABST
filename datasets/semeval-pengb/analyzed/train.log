2022-02-28 00:02:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 10, 'log_format': None, 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 8192, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 5000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 12288, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.abst/dev', 'restore_file': 'bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 10, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 2, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.0, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=3.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst/dev', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_epoch=999, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='aspect_base_denoising', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.0, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=3.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst/dev', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_epoch=999, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 20000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-28 00:02:30 | INFO | bartabst.tasks.aspect_base_denoising | dictionary: 51200 types
2022-02-28 00:02:32 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-28 00:02:32 | INFO | fairseq_cli.train | task: AspectBaseDenoisingTask
2022-02-28 00:02:32 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-28 00:02:32 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-02-28 00:02:32 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-28 00:02:32 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-28 00:02:33 | INFO | bartabst.data.data_utils | loaded 598 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-02-28 00:02:33 | INFO | bartabst.tasks.aspect_base_denoising | Split: valid, Loaded 598 samples of denoising_dataset
2022-02-28 00:02:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-02-28 00:02:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-28 00:02:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-02-28 00:02:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 00:02:36 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-02-28 00:02:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 00:02:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-28 00:02:36 | INFO | fairseq_cli.train | max tokens per device = 8192 and max sentences per device = 32
2022-02-28 00:02:36 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 00:02:36 | INFO | fairseq.trainer | No existing checkpoint found bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 00:02:36 | INFO | fairseq.trainer | loading train data for epoch 1
no aos file, no transfer aos used
2022-02-28 00:02:38 | INFO | bartabst.data.data_utils | loaded 1,910 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-02-28 00:02:38 | INFO | bartabst.tasks.aspect_base_denoising | Split: train, Loaded 1910 samples of denoising_dataset
2022-02-28 00:02:38 | INFO | fairseq.trainer | begin training epoch 1
2022-02-28 00:02:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:02:39 | INFO | train_inner | epoch 001:     10 / 60 loss=15.931, ppl=62463.5, wps=8303.7, ups=13.98, wpb=603.3, bsz=32, num_updates=10, lr=6e-07, gnorm=4.695, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=3
2022-02-28 00:02:40 | INFO | train_inner | epoch 001:     20 / 60 loss=15.953, ppl=63426.6, wps=9145.2, ups=12.78, wpb=715.7, bsz=32, num_updates=20, lr=1.2e-06, gnorm=4.624, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=3
2022-02-28 00:02:40 | INFO | train_inner | epoch 001:     30 / 60 loss=15.781, ppl=56295.3, wps=7880.3, ups=11.87, wpb=663.8, bsz=32, num_updates=30, lr=1.8e-06, gnorm=4.523, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=4
2022-02-28 00:02:41 | INFO | train_inner | epoch 001:     40 / 60 loss=15.6, ppl=49678.5, wps=8828.1, ups=11.93, wpb=739.7, bsz=31, num_updates=40, lr=2.4e-06, gnorm=4.329, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=5
2022-02-28 00:02:42 | INFO | train_inner | epoch 001:     50 / 60 loss=15.359, ppl=42038.6, wps=8444.7, ups=11.7, wpb=721.9, bsz=32, num_updates=50, lr=3e-06, gnorm=4.517, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=6
2022-02-28 00:02:43 | INFO | train_inner | epoch 001:     60 / 60 loss=14.927, ppl=31150.9, wps=6146.8, ups=11.13, wpb=552.3, bsz=32, num_updates=60, lr=3.6e-06, gnorm=5.457, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=7
2022-02-28 00:02:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:02:44 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.518 | ppl 23463.4 | wps 27868.7 | wpb 653.8 | bsz 31.5 | num_updates 60
2022-02-28 00:02:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 60 updates
2022-02-28 00:02:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:02:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:02:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 1 @ 60 updates, score 14.518) (writing took 9.356417182978475 seconds)
2022-02-28 00:02:53 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-28 00:02:53 | INFO | train | epoch 001 | loss 15.607 | ppl 49898.8 | wps 2661.5 | ups 4 | wpb 666.1 | bsz 31.8 | num_updates 60 | lr 3.6e-06 | gnorm 4.691 | clip 100 | loss_scale 128 | train_wall 5 | gb_free 20.8 | wall 17
2022-02-28 00:02:53 | INFO | fairseq.trainer | begin training epoch 2
2022-02-28 00:02:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:02:54 | INFO | train_inner | epoch 002:     10 / 60 loss=14.563, ppl=24210, wps=517.9, ups=0.94, wpb=549.5, bsz=32, num_updates=70, lr=4.2e-06, gnorm=5.279, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=17
2022-02-28 00:02:54 | INFO | train_inner | epoch 002:     20 / 60 loss=14.137, ppl=18013.4, wps=8674.7, ups=15.77, wpb=550, bsz=32, num_updates=80, lr=4.8e-06, gnorm=4.661, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=18
2022-02-28 00:02:55 | INFO | train_inner | epoch 002:     30 / 60 loss=14.2, ppl=18821.4, wps=12452.3, ups=15.66, wpb=795.1, bsz=32, num_updates=90, lr=5.4e-06, gnorm=3.47, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=19
2022-02-28 00:02:56 | INFO | train_inner | epoch 002:     40 / 60 loss=13.547, ppl=11969.6, wps=7631.1, ups=13.54, wpb=563.5, bsz=32, num_updates=100, lr=6e-06, gnorm=4.007, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=19
2022-02-28 00:02:56 | INFO | train_inner | epoch 002:     50 / 60 loss=13.652, ppl=12874.2, wps=10346.6, ups=14.6, wpb=708.5, bsz=32, num_updates=110, lr=6.6e-06, gnorm=3.766, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=20
2022-02-28 00:02:57 | INFO | train_inner | epoch 002:     60 / 60 loss=13.659, ppl=12931.2, wps=9425.5, ups=11.35, wpb=830.1, bsz=31, num_updates=120, lr=7.2e-06, gnorm=3.336, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=21
2022-02-28 00:02:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:02:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.991 | ppl 8142.73 | wps 27034.7 | wpb 653.8 | bsz 31.5 | num_updates 120 | best_loss 12.991
2022-02-28 00:02:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 120 updates
2022-02-28 00:02:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 2 @ 120 updates, score 12.991) (writing took 7.93759584997315 seconds)
2022-02-28 00:03:06 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-28 00:03:06 | INFO | train | epoch 002 | loss 13.94 | ppl 15712.8 | wps 3123 | ups 4.69 | wpb 666.1 | bsz 31.8 | num_updates 120 | lr 7.2e-06 | gnorm 4.087 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 29
2022-02-28 00:03:06 | INFO | fairseq.trainer | begin training epoch 3
2022-02-28 00:03:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:03:07 | INFO | train_inner | epoch 003:     10 / 60 loss=13.214, ppl=9502.31, wps=751.9, ups=1.08, wpb=698.4, bsz=32, num_updates=130, lr=7.8e-06, gnorm=3.467, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=30
2022-02-28 00:03:07 | INFO | train_inner | epoch 003:     20 / 60 loss=13.018, ppl=8293.6, wps=10083.1, ups=15.7, wpb=642.4, bsz=32, num_updates=140, lr=8.4e-06, gnorm=4.118, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=31
2022-02-28 00:03:08 | INFO | train_inner | epoch 003:     30 / 60 loss=12.998, ppl=8180.97, wps=10937.3, ups=15.47, wpb=707, bsz=31, num_updates=150, lr=9e-06, gnorm=3.261, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=32
2022-02-28 00:03:09 | INFO | train_inner | epoch 003:     40 / 60 loss=13.044, ppl=8447.59, wps=13492.4, ups=14.98, wpb=900.6, bsz=32, num_updates=160, lr=9.6e-06, gnorm=2.974, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=32
2022-02-28 00:03:09 | INFO | train_inner | epoch 003:     50 / 60 loss=12.423, ppl=5490.11, wps=8651, ups=14.5, wpb=596.8, bsz=32, num_updates=170, lr=1.02e-05, gnorm=3.335, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=33
2022-02-28 00:03:10 | INFO | train_inner | epoch 003:     60 / 60 loss=11.725, ppl=3384.44, wps=4899.9, ups=10.85, wpb=451.5, bsz=32, num_updates=180, lr=1.08e-05, gnorm=3.876, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=34
2022-02-28 00:03:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:03:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.226 | ppl 4790.9 | wps 28458.7 | wpb 653.8 | bsz 31.5 | num_updates 180 | best_loss 12.226
2022-02-28 00:03:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 180 updates
2022-02-28 00:03:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 3 @ 180 updates, score 12.226) (writing took 6.793636724993121 seconds)
2022-02-28 00:03:17 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-28 00:03:17 | INFO | train | epoch 003 | loss 12.82 | ppl 7229.21 | wps 3418.1 | ups 5.13 | wpb 666.1 | bsz 31.8 | num_updates 180 | lr 1.08e-05 | gnorm 3.505 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 41
2022-02-28 00:03:17 | INFO | fairseq.trainer | begin training epoch 4
2022-02-28 00:03:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:03:18 | INFO | train_inner | epoch 004:     10 / 60 loss=12.503, ppl=5804.76, wps=902.6, ups=1.24, wpb=729.5, bsz=32, num_updates=190, lr=1.14e-05, gnorm=3.263, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=42
2022-02-28 00:03:19 | INFO | train_inner | epoch 004:     20 / 60 loss=11.804, ppl=3575.48, wps=8698.5, ups=16.04, wpb=542.2, bsz=32, num_updates=200, lr=1.2e-05, gnorm=3.449, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=43
2022-02-28 00:03:20 | INFO | train_inner | epoch 004:     30 / 60 loss=12.494, ppl=5767.15, wps=12585.6, ups=14.64, wpb=859.7, bsz=31, num_updates=210, lr=1.26e-05, gnorm=3.243, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=43
2022-02-28 00:03:20 | INFO | train_inner | epoch 004:     40 / 60 loss=11.983, ppl=4046.86, wps=10706.7, ups=15.3, wpb=700, bsz=32, num_updates=220, lr=1.32e-05, gnorm=3.145, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=44
2022-02-28 00:03:21 | INFO | train_inner | epoch 004:     50 / 60 loss=11.414, ppl=2729.06, wps=8912.1, ups=15.34, wpb=580.8, bsz=32, num_updates=230, lr=1.38e-05, gnorm=3.017, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=45
2022-02-28 00:03:22 | INFO | train_inner | epoch 004:     60 / 60 loss=11.232, ppl=2405.23, wps=7559, ups=12.93, wpb=584.5, bsz=32, num_updates=240, lr=1.44e-05, gnorm=2.87, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=45
2022-02-28 00:03:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:03:22 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.356 | ppl 2620.44 | wps 28176 | wpb 653.8 | bsz 31.5 | num_updates 240 | best_loss 11.356
2022-02-28 00:03:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 240 updates
2022-02-28 00:03:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 4 @ 240 updates, score 11.356) (writing took 7.119441075017676 seconds)
2022-02-28 00:03:29 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-28 00:03:29 | INFO | train | epoch 004 | loss 11.971 | ppl 4014.2 | wps 3386.2 | ups 5.08 | wpb 666.1 | bsz 31.8 | num_updates 240 | lr 1.44e-05 | gnorm 3.164 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 53
2022-02-28 00:03:29 | INFO | fairseq.trainer | begin training epoch 5
2022-02-28 00:03:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:03:30 | INFO | train_inner | epoch 005:     10 / 60 loss=11.55, ppl=2998.59, wps=862.8, ups=1.19, wpb=724.7, bsz=32, num_updates=250, lr=1.5e-05, gnorm=2.52, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=54
2022-02-28 00:03:31 | INFO | train_inner | epoch 005:     20 / 60 loss=11.02, ppl=2076.29, wps=8548.1, ups=14.46, wpb=591.1, bsz=32, num_updates=260, lr=1.56e-05, gnorm=2.567, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=54
2022-02-28 00:03:31 | INFO | train_inner | epoch 005:     30 / 60 loss=10.972, ppl=2009.14, wps=9506.6, ups=15.33, wpb=620, bsz=32, num_updates=270, lr=1.62e-05, gnorm=2.67, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=55
2022-02-28 00:03:32 | INFO | train_inner | epoch 005:     40 / 60 loss=10.003, ppl=1026.04, wps=6265.4, ups=15.54, wpb=403.2, bsz=32, num_updates=280, lr=1.68e-05, gnorm=3.383, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=56
2022-02-28 00:03:33 | INFO | train_inner | epoch 005:     50 / 60 loss=11.181, ppl=2322.32, wps=11559.6, ups=13.94, wpb=829.4, bsz=32, num_updates=290, lr=1.74e-05, gnorm=2.431, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=57
2022-02-28 00:03:34 | INFO | train_inner | epoch 005:     60 / 60 loss=11.086, ppl=2174.03, wps=8830.6, ups=10.66, wpb=828.3, bsz=31, num_updates=300, lr=1.8e-05, gnorm=2.514, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=57
2022-02-28 00:03:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:03:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.525 | ppl 1473.21 | wps 28057.3 | wpb 653.8 | bsz 31.5 | num_updates 300 | best_loss 10.525
2022-02-28 00:03:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 300 updates
2022-02-28 00:03:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 5 @ 300 updates, score 10.525) (writing took 6.753343852004036 seconds)
2022-02-28 00:03:41 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-28 00:03:41 | INFO | train | epoch 005 | loss 11.053 | ppl 2125.05 | wps 3423.4 | ups 5.14 | wpb 666.1 | bsz 31.8 | num_updates 300 | lr 1.8e-05 | gnorm 2.681 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 65
2022-02-28 00:03:41 | INFO | fairseq.trainer | begin training epoch 6
2022-02-28 00:03:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:03:42 | INFO | train_inner | epoch 006:     10 / 60 loss=10.104, ppl=1100.6, wps=648.8, ups=1.24, wpb=522.2, bsz=32, num_updates=310, lr=1.86e-05, gnorm=2.766, clip=100, loss_scale=128, train_wall=1, gb_free=20.1, wall=65
2022-02-28 00:03:42 | INFO | train_inner | epoch 006:     20 / 60 loss=10.1, ppl=1097.31, wps=9182.2, ups=16.32, wpb=562.5, bsz=32, num_updates=320, lr=1.92e-05, gnorm=2.407, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=66
2022-02-28 00:03:43 | INFO | train_inner | epoch 006:     30 / 60 loss=10.521, ppl=1469.46, wps=12441.4, ups=15.09, wpb=824.3, bsz=32, num_updates=330, lr=1.98e-05, gnorm=2.364, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=67
2022-02-28 00:03:44 | INFO | train_inner | epoch 006:     40 / 60 loss=9.883, ppl=944.18, wps=9408.9, ups=16.19, wpb=581.3, bsz=32, num_updates=340, lr=2.04e-05, gnorm=2.467, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=67
2022-02-28 00:03:44 | INFO | train_inner | epoch 006:     50 / 60 loss=10.278, ppl=1241.72, wps=10956.9, ups=13.47, wpb=813.7, bsz=32, num_updates=350, lr=2.1e-05, gnorm=2.202, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=68
2022-02-28 00:03:45 | INFO | train_inner | epoch 006:     60 / 60 loss=9.899, ppl=954.71, wps=8861.1, ups=12.79, wpb=692.7, bsz=31, num_updates=360, lr=2.16e-05, gnorm=2.784, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=69
2022-02-28 00:03:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:03:46 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.64 | ppl 797.79 | wps 28922.5 | wpb 653.8 | bsz 31.5 | num_updates 360 | best_loss 9.64
2022-02-28 00:03:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 360 updates
2022-02-28 00:03:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 6 @ 360 updates, score 9.64) (writing took 5.630421926995041 seconds)
2022-02-28 00:03:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-28 00:03:51 | INFO | train | epoch 006 | loss 10.157 | ppl 1141.87 | wps 3873.2 | ups 5.81 | wpb 666.1 | bsz 31.8 | num_updates 360 | lr 2.16e-05 | gnorm 2.499 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 75
2022-02-28 00:03:51 | INFO | fairseq.trainer | begin training epoch 7
2022-02-28 00:03:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:03:52 | INFO | train_inner | epoch 007:     10 / 60 loss=9.327, ppl=642.47, wps=888.8, ups=1.47, wpb=605.9, bsz=32, num_updates=370, lr=2.22e-05, gnorm=2.083, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=76
2022-02-28 00:03:53 | INFO | train_inner | epoch 007:     20 / 60 loss=8.993, ppl=509.59, wps=7890, ups=16.18, wpb=487.7, bsz=32, num_updates=380, lr=2.28e-05, gnorm=2.693, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=76
2022-02-28 00:03:53 | INFO | train_inner | epoch 007:     30 / 60 loss=9.42, ppl=685.01, wps=10169.8, ups=15.36, wpb=662, bsz=31, num_updates=390, lr=2.34e-05, gnorm=2.159, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=77
2022-02-28 00:03:54 | INFO | train_inner | epoch 007:     40 / 60 loss=9.506, ppl=726.95, wps=10216.6, ups=14.97, wpb=682.6, bsz=32, num_updates=400, lr=2.4e-05, gnorm=2.071, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=78
2022-02-28 00:03:55 | INFO | train_inner | epoch 007:     50 / 60 loss=9.128, ppl=559.35, wps=9869.1, ups=13.93, wpb=708.3, bsz=32, num_updates=410, lr=2.46e-05, gnorm=1.923, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=78
2022-02-28 00:03:56 | INFO | train_inner | epoch 007:     60 / 60 loss=9.204, ppl=589.78, wps=10224.5, ups=12.03, wpb=850.2, bsz=32, num_updates=420, lr=2.52e-05, gnorm=1.834, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=79
2022-02-28 00:03:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:03:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.96 | ppl 497.84 | wps 27783.4 | wpb 653.8 | bsz 31.5 | num_updates 420 | best_loss 8.96
2022-02-28 00:03:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 420 updates
2022-02-28 00:03:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:03:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 7 @ 420 updates, score 8.96) (writing took 5.874305097007891 seconds)
2022-02-28 00:04:02 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-28 00:04:02 | INFO | train | epoch 007 | loss 9.271 | ppl 617.7 | wps 3744.4 | ups 5.62 | wpb 666.1 | bsz 31.8 | num_updates 420 | lr 2.52e-05 | gnorm 2.127 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 86
2022-02-28 00:04:02 | INFO | fairseq.trainer | begin training epoch 8
2022-02-28 00:04:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:04:03 | INFO | train_inner | epoch 008:     10 / 60 loss=8.863, ppl=465.51, wps=885, ups=1.35, wpb=654, bsz=32, num_updates=430, lr=2.58e-05, gnorm=2.421, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=87
2022-02-28 00:04:04 | INFO | train_inner | epoch 008:     20 / 60 loss=9.023, ppl=520.26, wps=8526.2, ups=10.37, wpb=822.2, bsz=32, num_updates=440, lr=2.64e-05, gnorm=1.789, clip=100, loss_scale=128, train_wall=1, gb_free=19.6, wall=88
2022-02-28 00:04:05 | INFO | train_inner | epoch 008:     30 / 60 loss=8.379, ppl=332.8, wps=5831.5, ups=10.83, wpb=538.6, bsz=32, num_updates=450, lr=2.7e-05, gnorm=2.105, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=89
2022-02-28 00:04:06 | INFO | train_inner | epoch 008:     40 / 60 loss=8.825, ppl=453.4, wps=7729.8, ups=10.52, wpb=734.5, bsz=31, num_updates=460, lr=2.76e-05, gnorm=2.435, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=89
2022-02-28 00:04:07 | INFO | train_inner | epoch 008:     50 / 60 loss=8.179, ppl=289.77, wps=6560.7, ups=10.81, wpb=607, bsz=32, num_updates=470, lr=2.82e-05, gnorm=2.093, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=90
2022-02-28 00:04:08 | INFO | train_inner | epoch 008:     60 / 60 loss=8.16, ppl=286.07, wps=6672.1, ups=10.42, wpb=640.4, bsz=32, num_updates=480, lr=2.88e-05, gnorm=2.174, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=91
2022-02-28 00:04:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:04:08 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.452 | ppl 350.15 | wps 27610.8 | wpb 653.8 | bsz 31.5 | num_updates 480 | best_loss 8.452
2022-02-28 00:04:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 480 updates
2022-02-28 00:04:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 8 @ 480 updates, score 8.452) (writing took 5.001144228008343 seconds)
2022-02-28 00:04:13 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-28 00:04:13 | INFO | train | epoch 008 | loss 8.607 | ppl 389.91 | wps 3581.1 | ups 5.38 | wpb 666.1 | bsz 31.8 | num_updates 480 | lr 2.88e-05 | gnorm 2.169 | clip 100 | loss_scale 128 | train_wall 5 | gb_free 20.6 | wall 97
2022-02-28 00:04:13 | INFO | fairseq.trainer | begin training epoch 9
2022-02-28 00:04:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:04:14 | INFO | train_inner | epoch 009:     10 / 60 loss=8.507, ppl=363.72, wps=1243.4, ups=1.59, wpb=780, bsz=31, num_updates=490, lr=2.94e-05, gnorm=2.242, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=98
2022-02-28 00:04:15 | INFO | train_inner | epoch 009:     20 / 60 loss=7.964, ppl=249.66, wps=8468.5, ups=15.44, wpb=548.5, bsz=32, num_updates=500, lr=3e-05, gnorm=2.437, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=98
2022-02-28 00:04:15 | INFO | train_inner | epoch 009:     30 / 60 loss=7.612, ppl=195.6, wps=8493.6, ups=16.48, wpb=515.3, bsz=32, num_updates=510, lr=2.99846e-05, gnorm=2.645, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=99
2022-02-28 00:04:16 | INFO | train_inner | epoch 009:     40 / 60 loss=7.994, ppl=254.9, wps=10590.1, ups=16.22, wpb=653.1, bsz=32, num_updates=520, lr=2.99692e-05, gnorm=2.208, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=100
2022-02-28 00:04:17 | INFO | train_inner | epoch 009:     50 / 60 loss=8.212, ppl=296.58, wps=10341.4, ups=13.5, wpb=765.9, bsz=32, num_updates=530, lr=2.99538e-05, gnorm=1.782, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=100
2022-02-28 00:04:17 | INFO | train_inner | epoch 009:     60 / 60 loss=8.159, ppl=285.78, wps=8282.5, ups=11.29, wpb=733.9, bsz=32, num_updates=540, lr=2.99385e-05, gnorm=3.53, clip=100, loss_scale=128, train_wall=1, gb_free=20, wall=101
2022-02-28 00:04:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:04:18 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.148 | ppl 283.74 | wps 27724.5 | wpb 653.8 | bsz 31.5 | num_updates 540 | best_loss 8.148
2022-02-28 00:04:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 540 updates
2022-02-28 00:04:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 9 @ 540 updates, score 8.148) (writing took 5.711708094982896 seconds)
2022-02-28 00:04:24 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-28 00:04:24 | INFO | train | epoch 009 | loss 8.113 | ppl 276.8 | wps 3813.1 | ups 5.72 | wpb 666.1 | bsz 31.8 | num_updates 540 | lr 2.99385e-05 | gnorm 2.474 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20 | wall 107
2022-02-28 00:04:24 | INFO | fairseq.trainer | begin training epoch 10
2022-02-28 00:04:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:04:25 | INFO | train_inner | epoch 010:     10 / 60 loss=7.849, ppl=230.57, wps=875.6, ups=1.42, wpb=617.4, bsz=31, num_updates=550, lr=2.99231e-05, gnorm=3.47, clip=100, loss_scale=128, train_wall=1, gb_free=19.2, wall=108
2022-02-28 00:04:25 | INFO | train_inner | epoch 010:     20 / 60 loss=7.855, ppl=231.54, wps=10503, ups=15.06, wpb=697.2, bsz=32, num_updates=560, lr=2.99077e-05, gnorm=2.722, clip=100, loss_scale=128, train_wall=1, gb_free=20.2, wall=109
2022-02-28 00:04:26 | INFO | train_inner | epoch 010:     30 / 60 loss=7.602, ppl=194.34, wps=6165.5, ups=10.89, wpb=566.1, bsz=32, num_updates=570, lr=2.98923e-05, gnorm=2.733, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=110
2022-02-28 00:04:27 | INFO | train_inner | epoch 010:     40 / 60 loss=7.582, ppl=191.65, wps=6479.8, ups=11.03, wpb=587.7, bsz=32, num_updates=580, lr=2.98769e-05, gnorm=2.444, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=111
2022-02-28 00:04:28 | INFO | train_inner | epoch 010:     50 / 60 loss=7.987, ppl=253.72, wps=12137.3, ups=14.31, wpb=848.2, bsz=32, num_updates=590, lr=2.98615e-05, gnorm=1.698, clip=100, loss_scale=128, train_wall=1, gb_free=20, wall=111
2022-02-28 00:04:28 | INFO | train_inner | epoch 010:     60 / 60 loss=7.702, ppl=208.23, wps=8732, ups=12.84, wpb=680.1, bsz=32, num_updates=600, lr=2.98462e-05, gnorm=2.418, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=112
2022-02-28 00:04:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:04:29 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.944 | ppl 246.25 | wps 27797.5 | wpb 653.8 | bsz 31.5 | num_updates 600 | best_loss 7.944
2022-02-28 00:04:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 600 updates
2022-02-28 00:04:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 10 @ 600 updates, score 7.944) (writing took 5.550630268000532 seconds)
2022-02-28 00:04:35 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-28 00:04:35 | INFO | train | epoch 010 | loss 7.78 | ppl 219.83 | wps 3687.6 | ups 5.54 | wpb 666.1 | bsz 31.8 | num_updates 600 | lr 2.98462e-05 | gnorm 2.581 | clip 100 | loss_scale 128 | train_wall 5 | gb_free 20.4 | wall 118
2022-02-28 00:04:35 | INFO | fairseq.trainer | begin training epoch 11
2022-02-28 00:04:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:04:35 | INFO | train_inner | epoch 011:     10 / 60 loss=7.824, ppl=226.63, wps=1055.9, ups=1.46, wpb=722.2, bsz=32, num_updates=610, lr=2.98308e-05, gnorm=2.09, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=119
2022-02-28 00:04:36 | INFO | train_inner | epoch 011:     20 / 60 loss=7.141, ppl=141.19, wps=9199.7, ups=16.26, wpb=565.7, bsz=32, num_updates=620, lr=2.98154e-05, gnorm=2.288, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=120
2022-02-28 00:04:37 | INFO | train_inner | epoch 011:     30 / 60 loss=7.421, ppl=171.41, wps=10312.4, ups=15.87, wpb=650, bsz=32, num_updates=630, lr=2.98e-05, gnorm=2.512, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=120
2022-02-28 00:04:37 | INFO | train_inner | epoch 011:     40 / 60 loss=7.677, ppl=204.65, wps=12043.6, ups=15.53, wpb=775.5, bsz=32, num_updates=640, lr=2.97846e-05, gnorm=1.92, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=121
2022-02-28 00:04:38 | INFO | train_inner | epoch 011:     50 / 60 loss=7.379, ppl=166.41, wps=9311, ups=15.33, wpb=607.4, bsz=31, num_updates=650, lr=2.97692e-05, gnorm=2.062, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=122
2022-02-28 00:04:39 | INFO | train_inner | epoch 011:     60 / 60 loss=7.355, ppl=163.68, wps=8804.9, ups=13.03, wpb=675.9, bsz=32, num_updates=660, lr=2.97538e-05, gnorm=2.331, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=122
2022-02-28 00:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:04:39 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.667 | ppl 203.21 | wps 27865.4 | wpb 653.8 | bsz 31.5 | num_updates 660 | best_loss 7.667
2022-02-28 00:04:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 660 updates
2022-02-28 00:04:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 11 @ 660 updates, score 7.667) (writing took 5.09420850299648 seconds)
2022-02-28 00:04:44 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-28 00:04:44 | INFO | train | epoch 011 | loss 7.486 | ppl 179.32 | wps 4126.3 | ups 6.19 | wpb 666.1 | bsz 31.8 | num_updates 660 | lr 2.97538e-05 | gnorm 2.2 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 128
2022-02-28 00:04:44 | INFO | fairseq.trainer | begin training epoch 12
2022-02-28 00:04:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:04:45 | INFO | train_inner | epoch 012:     10 / 60 loss=7.635, ppl=198.81, wps=1275.5, ups=1.54, wpb=828, bsz=31, num_updates=670, lr=2.97385e-05, gnorm=2.061, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=129
2022-02-28 00:04:46 | INFO | train_inner | epoch 012:     20 / 60 loss=7.611, ppl=195.46, wps=11759, ups=12.33, wpb=953.7, bsz=32, num_updates=680, lr=2.97231e-05, gnorm=1.688, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=130
2022-02-28 00:04:47 | INFO | train_inner | epoch 012:     30 / 60 loss=7.332, ppl=161.17, wps=8606.4, ups=12.16, wpb=707.6, bsz=32, num_updates=690, lr=2.97077e-05, gnorm=2.263, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=131
2022-02-28 00:04:48 | INFO | train_inner | epoch 012:     40 / 60 loss=7.127, ppl=139.76, wps=7548.1, ups=13.11, wpb=575.6, bsz=32, num_updates=700, lr=2.96923e-05, gnorm=2.946, clip=100, loss_scale=128, train_wall=1, gb_free=20.2, wall=131
2022-02-28 00:04:48 | INFO | train_inner | epoch 012:     50 / 60 loss=6.699, ppl=103.93, wps=7591.8, ups=15.1, wpb=502.8, bsz=32, num_updates=710, lr=2.96769e-05, gnorm=2.957, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=132
2022-02-28 00:04:49 | INFO | train_inner | epoch 012:     60 / 60 loss=6.591, ppl=96.4, wps=5503.4, ups=12.83, wpb=429, bsz=32, num_updates=720, lr=2.96615e-05, gnorm=3.475, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=133
2022-02-28 00:04:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:04:50 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.531 | ppl 184.94 | wps 27640.6 | wpb 653.8 | bsz 31.5 | num_updates 720 | best_loss 7.531
2022-02-28 00:04:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 720 updates
2022-02-28 00:04:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 12 @ 720 updates, score 7.531) (writing took 5.097580752015347 seconds)
2022-02-28 00:04:55 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-28 00:04:55 | INFO | train | epoch 012 | loss 7.273 | ppl 154.64 | wps 3860.7 | ups 5.8 | wpb 666.1 | bsz 31.8 | num_updates 720 | lr 2.96615e-05 | gnorm 2.565 | clip 100 | loss_scale 128 | train_wall 5 | gb_free 20.6 | wall 138
2022-02-28 00:04:55 | INFO | fairseq.trainer | begin training epoch 13
2022-02-28 00:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:04:55 | INFO | train_inner | epoch 013:     10 / 60 loss=6.263, ppl=76.78, wps=691.1, ups=1.58, wpb=437.6, bsz=32, num_updates=730, lr=2.96462e-05, gnorm=2.862, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=139
2022-02-28 00:04:56 | INFO | train_inner | epoch 013:     20 / 60 loss=7.284, ppl=155.86, wps=11042.8, ups=15.38, wpb=717.9, bsz=32, num_updates=740, lr=2.96308e-05, gnorm=2.525, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=140
2022-02-28 00:04:57 | INFO | train_inner | epoch 013:     30 / 60 loss=6.992, ppl=127.29, wps=9880, ups=15.5, wpb=637.5, bsz=32, num_updates=750, lr=2.96154e-05, gnorm=2.028, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=140
2022-02-28 00:04:57 | INFO | train_inner | epoch 013:     40 / 60 loss=7.228, ppl=149.93, wps=11605.9, ups=14.57, wpb=796.5, bsz=32, num_updates=760, lr=2.96e-05, gnorm=2.241, clip=100, loss_scale=128, train_wall=1, gb_free=20.1, wall=141
2022-02-28 00:04:58 | INFO | train_inner | epoch 013:     50 / 60 loss=7.272, ppl=154.54, wps=10985, ups=14.74, wpb=745, bsz=31, num_updates=770, lr=2.95846e-05, gnorm=2.278, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=142
2022-02-28 00:04:59 | INFO | train_inner | epoch 013:     60 / 60 loss=6.93, ppl=121.97, wps=8502.5, ups=12.84, wpb=662.2, bsz=32, num_updates=780, lr=2.95692e-05, gnorm=2.204, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=143
2022-02-28 00:04:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:04:59 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.311 | ppl 158.84 | wps 26585.4 | wpb 653.8 | bsz 31.5 | num_updates 780 | best_loss 7.311
2022-02-28 00:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 780 updates
2022-02-28 00:04:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 13 @ 780 updates, score 7.311) (writing took 5.538872776989592 seconds)
2022-02-28 00:05:05 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-28 00:05:05 | INFO | train | epoch 013 | loss 7.054 | ppl 132.85 | wps 3901 | ups 5.86 | wpb 666.1 | bsz 31.8 | num_updates 780 | lr 2.95692e-05 | gnorm 2.356 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 149
2022-02-28 00:05:05 | INFO | fairseq.trainer | begin training epoch 14
2022-02-28 00:05:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:05:06 | INFO | train_inner | epoch 014:     10 / 60 loss=6.995, ppl=127.54, wps=1056.9, ups=1.47, wpb=720.9, bsz=31, num_updates=790, lr=2.95538e-05, gnorm=2.185, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=149
2022-02-28 00:05:06 | INFO | train_inner | epoch 014:     20 / 60 loss=7.068, ppl=134.18, wps=11564.3, ups=14.5, wpb=797.6, bsz=32, num_updates=800, lr=2.95385e-05, gnorm=2.481, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=150
2022-02-28 00:05:07 | INFO | train_inner | epoch 014:     30 / 60 loss=6.421, ppl=85.71, wps=8201.5, ups=16.11, wpb=509.1, bsz=32, num_updates=810, lr=2.95231e-05, gnorm=3.379, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=151
2022-02-28 00:05:08 | INFO | train_inner | epoch 014:     40 / 60 loss=6.972, ppl=125.51, wps=10944.2, ups=14.96, wpb=731.4, bsz=32, num_updates=820, lr=2.95077e-05, gnorm=2.618, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=151
2022-02-28 00:05:08 | INFO | train_inner | epoch 014:     50 / 60 loss=7.042, ppl=131.81, wps=9961.9, ups=14.35, wpb=694.1, bsz=32, num_updates=830, lr=2.94923e-05, gnorm=2.243, clip=100, loss_scale=128, train_wall=1, gb_free=19.6, wall=152
2022-02-28 00:05:09 | INFO | train_inner | epoch 014:     60 / 60 loss=6.47, ppl=88.66, wps=6065.1, ups=11.16, wpb=543.6, bsz=32, num_updates=840, lr=2.94769e-05, gnorm=2.883, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=153
2022-02-28 00:05:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:05:10 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.156 | ppl 142.6 | wps 28099.7 | wpb 653.8 | bsz 31.5 | num_updates 840 | best_loss 7.156
2022-02-28 00:05:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 840 updates
2022-02-28 00:05:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 14 @ 840 updates, score 7.156) (writing took 4.179612646985333 seconds)
2022-02-28 00:05:14 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-28 00:05:14 | INFO | train | epoch 014 | loss 6.869 | ppl 116.89 | wps 4427.8 | ups 6.65 | wpb 666.1 | bsz 31.8 | num_updates 840 | lr 2.94769e-05 | gnorm 2.632 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 158
2022-02-28 00:05:14 | INFO | fairseq.trainer | begin training epoch 15
2022-02-28 00:05:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:05:15 | INFO | train_inner | epoch 015:     10 / 60 loss=6.648, ppl=100.3, wps=1223, ups=1.81, wpb=676, bsz=32, num_updates=850, lr=2.94615e-05, gnorm=2.157, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=158
2022-02-28 00:05:15 | INFO | train_inner | epoch 015:     20 / 60 loss=6.935, ppl=122.39, wps=10766.6, ups=13.91, wpb=774.2, bsz=32, num_updates=860, lr=2.94462e-05, gnorm=2.024, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=159
2022-02-28 00:05:16 | INFO | train_inner | epoch 015:     30 / 60 loss=6.61, ppl=97.7, wps=10585.7, ups=14.78, wpb=716.3, bsz=32, num_updates=870, lr=2.94308e-05, gnorm=2.311, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=160
2022-02-28 00:05:17 | INFO | train_inner | epoch 015:     40 / 60 loss=5.755, ppl=53.99, wps=6152.5, ups=14.25, wpb=431.8, bsz=32, num_updates=880, lr=2.94154e-05, gnorm=3.343, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=161
2022-02-28 00:05:18 | INFO | train_inner | epoch 015:     50 / 60 loss=6.653, ppl=100.64, wps=9077.8, ups=13.33, wpb=680.8, bsz=32, num_updates=890, lr=2.94e-05, gnorm=3.066, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=161
2022-02-28 00:05:18 | INFO | train_inner | epoch 015:     60 / 60 loss=7.033, ppl=130.92, wps=8244.3, ups=11.49, wpb=717.6, bsz=31, num_updates=900, lr=2.93846e-05, gnorm=2.918, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=162
2022-02-28 00:05:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:05:19 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.015 | ppl 129.38 | wps 26716.6 | wpb 653.8 | bsz 31.5 | num_updates 900 | best_loss 7.015
2022-02-28 00:05:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 900 updates
2022-02-28 00:05:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 15 @ 900 updates, score 7.015) (writing took 3.986878097988665 seconds)
2022-02-28 00:05:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-28 00:05:23 | INFO | train | epoch 015 | loss 6.67 | ppl 101.85 | wps 4399.1 | ups 6.6 | wpb 666.1 | bsz 31.8 | num_updates 900 | lr 2.93846e-05 | gnorm 2.637 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 167
2022-02-28 00:05:23 | INFO | fairseq.trainer | begin training epoch 16
2022-02-28 00:05:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:05:24 | INFO | train_inner | epoch 016:     10 / 60 loss=6.438, ppl=86.72, wps=1186.9, ups=1.89, wpb=628.7, bsz=32, num_updates=910, lr=2.93692e-05, gnorm=3.029, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=167
2022-02-28 00:05:24 | INFO | train_inner | epoch 016:     20 / 60 loss=6.154, ppl=71.2, wps=9097.8, ups=16.47, wpb=552.3, bsz=32, num_updates=920, lr=2.93538e-05, gnorm=2.501, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=168
2022-02-28 00:05:25 | INFO | train_inner | epoch 016:     30 / 60 loss=6.396, ppl=84.23, wps=10260.3, ups=15.83, wpb=648, bsz=32, num_updates=930, lr=2.93385e-05, gnorm=3.108, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=169
2022-02-28 00:05:26 | INFO | train_inner | epoch 016:     40 / 60 loss=6.742, ppl=107.02, wps=10169.4, ups=12.77, wpb=796.3, bsz=32, num_updates=940, lr=2.93231e-05, gnorm=2.706, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=170
2022-02-28 00:05:26 | INFO | train_inner | epoch 016:     50 / 60 loss=6.442, ppl=86.92, wps=8572, ups=14.42, wpb=594.5, bsz=32, num_updates=950, lr=2.93077e-05, gnorm=2.661, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=170
2022-02-28 00:05:27 | INFO | train_inner | epoch 016:     60 / 60 loss=6.697, ppl=103.77, wps=9120.9, ups=11.74, wpb=776.9, bsz=31, num_updates=960, lr=2.92923e-05, gnorm=2.205, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=171
2022-02-28 00:05:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:05:28 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.894 | ppl 118.96 | wps 24422.8 | wpb 653.8 | bsz 31.5 | num_updates 960 | best_loss 6.894
2022-02-28 00:05:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 960 updates
2022-02-28 00:05:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 16 @ 960 updates, score 6.894) (writing took 4.168924666999374 seconds)
2022-02-28 00:05:32 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-28 00:05:32 | INFO | train | epoch 016 | loss 6.503 | ppl 90.73 | wps 4387.4 | ups 6.59 | wpb 666.1 | bsz 31.8 | num_updates 960 | lr 2.92923e-05 | gnorm 2.701 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 176
2022-02-28 00:05:32 | INFO | fairseq.trainer | begin training epoch 17
2022-02-28 00:05:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:05:33 | INFO | train_inner | epoch 017:     10 / 60 loss=6.577, ppl=95.47, wps=1428.1, ups=1.82, wpb=785.8, bsz=32, num_updates=970, lr=2.92769e-05, gnorm=2.225, clip=100, loss_scale=128, train_wall=1, gb_free=20, wall=177
2022-02-28 00:05:33 | INFO | train_inner | epoch 017:     20 / 60 loss=6.057, ppl=66.57, wps=8943.7, ups=15.92, wpb=561.9, bsz=32, num_updates=980, lr=2.92615e-05, gnorm=2.784, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=177
2022-02-28 00:05:34 | INFO | train_inner | epoch 017:     30 / 60 loss=6.941, ppl=122.88, wps=13497.1, ups=14.5, wpb=931, bsz=31, num_updates=990, lr=2.92462e-05, gnorm=2.174, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=178
2022-02-28 00:05:35 | INFO | train_inner | epoch 017:     40 / 60 loss=5.887, ppl=59.18, wps=8247.1, ups=16.18, wpb=509.7, bsz=32, num_updates=1000, lr=2.92308e-05, gnorm=3.198, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=179
2022-02-28 00:05:35 | INFO | train_inner | epoch 017:     50 / 60 loss=5.972, ppl=62.75, wps=8854.4, ups=14.8, wpb=598.4, bsz=32, num_updates=1010, lr=2.92154e-05, gnorm=2.747, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=179
2022-02-28 00:05:36 | INFO | train_inner | epoch 017:     60 / 60 loss=6.078, ppl=67.56, wps=7361.1, ups=12.07, wpb=609.9, bsz=32, num_updates=1020, lr=2.92e-05, gnorm=2.52, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=180
2022-02-28 00:05:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:05:37 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.715 | ppl 105.04 | wps 25887.1 | wpb 653.8 | bsz 31.5 | num_updates 1020 | best_loss 6.715
2022-02-28 00:05:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1020 updates
2022-02-28 00:05:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 17 @ 1020 updates, score 6.715) (writing took 5.233572731987806 seconds)
2022-02-28 00:05:42 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-28 00:05:42 | INFO | train | epoch 017 | loss 6.334 | ppl 80.67 | wps 4000.5 | ups 6.01 | wpb 666.1 | bsz 31.8 | num_updates 1020 | lr 2.92e-05 | gnorm 2.608 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.5 | wall 186
2022-02-28 00:05:42 | INFO | fairseq.trainer | begin training epoch 18
2022-02-28 00:05:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:05:43 | INFO | train_inner | epoch 018:     10 / 60 loss=6.838, ppl=114.41, wps=1374.2, ups=1.5, wpb=916.6, bsz=31, num_updates=1030, lr=2.91846e-05, gnorm=2.305, clip=100, loss_scale=128, train_wall=1, gb_free=20, wall=187
2022-02-28 00:05:44 | INFO | train_inner | epoch 018:     20 / 60 loss=6.122, ppl=69.65, wps=10743.6, ups=15.44, wpb=695.9, bsz=32, num_updates=1040, lr=2.91692e-05, gnorm=2.443, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=187
2022-02-28 00:05:44 | INFO | train_inner | epoch 018:     30 / 60 loss=5.525, ppl=46.05, wps=7415.2, ups=16.11, wpb=460.4, bsz=32, num_updates=1050, lr=2.91538e-05, gnorm=2.736, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=188
2022-02-28 00:05:45 | INFO | train_inner | epoch 018:     40 / 60 loss=6.318, ppl=79.76, wps=11122.1, ups=14.46, wpb=769, bsz=32, num_updates=1060, lr=2.91385e-05, gnorm=2.4, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=189
2022-02-28 00:05:46 | INFO | train_inner | epoch 018:     50 / 60 loss=6.154, ppl=71.19, wps=9466.3, ups=13.26, wpb=714.1, bsz=32, num_updates=1070, lr=2.91231e-05, gnorm=2.745, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=189
2022-02-28 00:05:47 | INFO | train_inner | epoch 018:     60 / 60 loss=5.364, ppl=41.19, wps=5322.2, ups=12.08, wpb=440.7, bsz=32, num_updates=1080, lr=2.91077e-05, gnorm=3.215, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=190
2022-02-28 00:05:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:05:47 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.758 | ppl 108.23 | wps 26885.2 | wpb 653.8 | bsz 31.5 | num_updates 1080 | best_loss 6.715
2022-02-28 00:05:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1080 updates
2022-02-28 00:05:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:05:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:05:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 18 @ 1080 updates, score 6.758) (writing took 2.6006169379979838 seconds)
2022-02-28 00:05:50 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-28 00:05:50 | INFO | train | epoch 018 | loss 6.177 | ppl 72.36 | wps 5332 | ups 8 | wpb 666.1 | bsz 31.8 | num_updates 1080 | lr 2.91077e-05 | gnorm 2.641 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 193
2022-02-28 00:05:50 | INFO | fairseq.trainer | begin training epoch 19
2022-02-28 00:05:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:05:50 | INFO | train_inner | epoch 019:     10 / 60 loss=5.728, ppl=53.02, wps=1498.4, ups=2.54, wpb=589, bsz=32, num_updates=1090, lr=2.90923e-05, gnorm=3.074, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=194
2022-02-28 00:05:51 | INFO | train_inner | epoch 019:     20 / 60 loss=5.69, ppl=51.62, wps=7649.6, ups=13.51, wpb=566.3, bsz=32, num_updates=1100, lr=2.90769e-05, gnorm=2.524, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=195
2022-02-28 00:05:52 | INFO | train_inner | epoch 019:     30 / 60 loss=6.096, ppl=68.38, wps=9696.9, ups=13.98, wpb=693.4, bsz=32, num_updates=1110, lr=2.90615e-05, gnorm=2.886, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=196
2022-02-28 00:05:53 | INFO | train_inner | epoch 019:     40 / 60 loss=6.358, ppl=82.03, wps=9696.8, ups=13.59, wpb=713.5, bsz=31, num_updates=1120, lr=2.90462e-05, gnorm=3.361, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=196
2022-02-28 00:05:53 | INFO | train_inner | epoch 019:     50 / 60 loss=6.269, ppl=77.13, wps=9837.2, ups=12.97, wpb=758.7, bsz=32, num_updates=1130, lr=2.90308e-05, gnorm=2.635, clip=100, loss_scale=128, train_wall=1, gb_free=20.2, wall=197
2022-02-28 00:05:54 | INFO | train_inner | epoch 019:     60 / 60 loss=5.947, ppl=61.7, wps=7147.8, ups=10.58, wpb=675.8, bsz=32, num_updates=1140, lr=2.90154e-05, gnorm=2.516, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=198
2022-02-28 00:05:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:05:55 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.501 | ppl 90.57 | wps 26967.3 | wpb 653.8 | bsz 31.5 | num_updates 1140 | best_loss 6.501
2022-02-28 00:05:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1140 updates
2022-02-28 00:05:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:05:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 19 @ 1140 updates, score 6.501) (writing took 3.8996821689943317 seconds)
2022-02-28 00:05:59 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-28 00:05:59 | INFO | train | epoch 019 | loss 6.039 | ppl 65.74 | wps 4368.8 | ups 6.56 | wpb 666.1 | bsz 31.8 | num_updates 1140 | lr 2.90154e-05 | gnorm 2.833 | clip 100 | loss_scale 128 | train_wall 5 | gb_free 20.8 | wall 203
2022-02-28 00:05:59 | INFO | fairseq.trainer | begin training epoch 20
2022-02-28 00:05:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:06:00 | INFO | train_inner | epoch 020:     10 / 60 loss=6.299, ppl=78.74, wps=1502.5, ups=1.92, wpb=784.4, bsz=32, num_updates=1150, lr=2.9e-05, gnorm=2.557, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=203
2022-02-28 00:06:00 | INFO | train_inner | epoch 020:     20 / 60 loss=5.581, ppl=47.87, wps=9768.9, ups=16.16, wpb=604.6, bsz=32, num_updates=1160, lr=2.89846e-05, gnorm=2.535, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=204
2022-02-28 00:06:01 | INFO | train_inner | epoch 020:     30 / 60 loss=6.058, ppl=66.61, wps=10378.8, ups=15.55, wpb=667.6, bsz=31, num_updates=1170, lr=2.89692e-05, gnorm=2.606, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=205
2022-02-28 00:06:02 | INFO | train_inner | epoch 020:     40 / 60 loss=5.656, ppl=50.41, wps=10092, ups=16.13, wpb=625.7, bsz=32, num_updates=1180, lr=2.89538e-05, gnorm=2.649, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=205
2022-02-28 00:06:02 | INFO | train_inner | epoch 020:     50 / 60 loss=5.776, ppl=54.81, wps=8884.3, ups=14.55, wpb=610.4, bsz=32, num_updates=1190, lr=2.89385e-05, gnorm=2.783, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=206
2022-02-28 00:06:03 | INFO | train_inner | epoch 020:     60 / 60 loss=5.827, ppl=56.78, wps=8411.5, ups=11.95, wpb=704, bsz=32, num_updates=1200, lr=2.89231e-05, gnorm=2.396, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=207
2022-02-28 00:06:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:06:04 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.386 | ppl 83.64 | wps 28699.1 | wpb 653.8 | bsz 31.5 | num_updates 1200 | best_loss 6.386
2022-02-28 00:06:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1200 updates
2022-02-28 00:06:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 20 @ 1200 updates, score 6.386) (writing took 4.013688450009795 seconds)
2022-02-28 00:06:08 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-28 00:06:08 | INFO | train | epoch 020 | loss 5.886 | ppl 59.16 | wps 4572.6 | ups 6.86 | wpb 666.1 | bsz 31.8 | num_updates 1200 | lr 2.89231e-05 | gnorm 2.588 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 211
2022-02-28 00:06:08 | INFO | fairseq.trainer | begin training epoch 21
2022-02-28 00:06:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:06:08 | INFO | train_inner | epoch 021:     10 / 60 loss=5.938, ppl=61.29, wps=1560.8, ups=1.89, wpb=826.2, bsz=32, num_updates=1210, lr=2.89077e-05, gnorm=2.35, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=212
2022-02-28 00:06:09 | INFO | train_inner | epoch 021:     20 / 60 loss=5.783, ppl=55.06, wps=11101.9, ups=16.16, wpb=687.1, bsz=32, num_updates=1220, lr=2.88923e-05, gnorm=2.522, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=213
2022-02-28 00:06:10 | INFO | train_inner | epoch 021:     30 / 60 loss=5.876, ppl=58.74, wps=9318.3, ups=15.54, wpb=599.6, bsz=32, num_updates=1230, lr=2.88769e-05, gnorm=2.531, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=213
2022-02-28 00:06:10 | INFO | train_inner | epoch 021:     40 / 60 loss=5.528, ppl=46.14, wps=10229.6, ups=16.21, wpb=631, bsz=32, num_updates=1240, lr=2.88615e-05, gnorm=2.78, clip=100, loss_scale=128, train_wall=1, gb_free=20.3, wall=214
2022-02-28 00:06:11 | INFO | train_inner | epoch 021:     50 / 60 loss=5.926, ppl=60.82, wps=9846.4, ups=13.98, wpb=704.4, bsz=31, num_updates=1250, lr=2.88462e-05, gnorm=3.073, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=215
2022-02-28 00:06:12 | INFO | train_inner | epoch 021:     60 / 60 loss=5.291, ppl=39.15, wps=7001.8, ups=12.77, wpb=548.4, bsz=32, num_updates=1260, lr=2.88308e-05, gnorm=3.878, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=215
2022-02-28 00:06:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:06:12 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.26 | ppl 76.63 | wps 27731.6 | wpb 653.8 | bsz 31.5 | num_updates 1260 | best_loss 6.26
2022-02-28 00:06:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1260 updates
2022-02-28 00:06:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 21 @ 1260 updates, score 6.26) (writing took 4.60433346699574 seconds)
2022-02-28 00:06:17 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-28 00:06:17 | INFO | train | epoch 021 | loss 5.746 | ppl 53.69 | wps 4306.1 | ups 6.46 | wpb 666.1 | bsz 31.8 | num_updates 1260 | lr 2.88308e-05 | gnorm 2.856 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.7 | wall 221
2022-02-28 00:06:17 | INFO | fairseq.trainer | begin training epoch 22
2022-02-28 00:06:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:06:18 | INFO | train_inner | epoch 022:     10 / 60 loss=5.066, ppl=33.49, wps=932.1, ups=1.71, wpb=546, bsz=32, num_updates=1270, lr=2.88154e-05, gnorm=2.642, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=221
2022-02-28 00:06:18 | INFO | train_inner | epoch 022:     20 / 60 loss=5.845, ppl=57.47, wps=8474.6, ups=12.77, wpb=663.7, bsz=31, num_updates=1280, lr=2.88e-05, gnorm=2.812, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=222
2022-02-28 00:06:19 | INFO | train_inner | epoch 022:     30 / 60 loss=5.836, ppl=57.11, wps=9907.5, ups=13.13, wpb=754.4, bsz=32, num_updates=1290, lr=2.87846e-05, gnorm=2.801, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=223
2022-02-28 00:06:20 | INFO | train_inner | epoch 022:     40 / 60 loss=5.381, ppl=41.68, wps=8057.7, ups=13.4, wpb=601.3, bsz=32, num_updates=1300, lr=2.87692e-05, gnorm=2.745, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=224
2022-02-28 00:06:21 | INFO | train_inner | epoch 022:     50 / 60 loss=4.753, ppl=26.97, wps=6396.6, ups=13.35, wpb=479.3, bsz=32, num_updates=1310, lr=2.87538e-05, gnorm=2.692, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=224
2022-02-28 00:06:21 | INFO | train_inner | epoch 022:     60 / 60 loss=6.16, ppl=71.5, wps=12336.4, ups=12.96, wpb=952, bsz=32, num_updates=1320, lr=2.87385e-05, gnorm=2.287, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=225
2022-02-28 00:06:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:06:22 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.13 | ppl 70.06 | wps 27650.7 | wpb 653.8 | bsz 31.5 | num_updates 1320 | best_loss 6.13
2022-02-28 00:06:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1320 updates
2022-02-28 00:06:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 22 @ 1320 updates, score 6.13) (writing took 4.699195391993271 seconds)
2022-02-28 00:06:27 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-28 00:06:27 | INFO | train | epoch 022 | loss 5.611 | ppl 48.87 | wps 4087.4 | ups 6.14 | wpb 666.1 | bsz 31.8 | num_updates 1320 | lr 2.87385e-05 | gnorm 2.663 | clip 100 | loss_scale 128 | train_wall 4 | gb_free 20.8 | wall 230
2022-02-28 00:06:27 | INFO | fairseq.trainer | begin training epoch 23
2022-02-28 00:06:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:06:27 | INFO | train_inner | epoch 023:     10 / 60 loss=5.718, ppl=52.65, wps=1314, ups=1.66, wpb=790.6, bsz=31, num_updates=1330, lr=2.87231e-05, gnorm=2.366, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=231
2022-02-28 00:06:28 | INFO | train_inner | epoch 023:     20 / 60 loss=5.222, ppl=37.31, wps=9101.3, ups=15.03, wpb=605.7, bsz=32, num_updates=1340, lr=2.87077e-05, gnorm=2.811, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=232
2022-02-28 00:06:29 | INFO | train_inner | epoch 023:     30 / 60 loss=5.651, ppl=50.25, wps=10805.5, ups=14.37, wpb=751.9, bsz=32, num_updates=1350, lr=2.86923e-05, gnorm=2.89, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=233
2022-02-28 00:06:29 | INFO | train_inner | epoch 023:     40 / 60 loss=4.772, ppl=27.31, wps=7347.9, ups=16.24, wpb=452.5, bsz=32, num_updates=1360, lr=2.86769e-05, gnorm=3.409, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=233
2022-02-28 00:06:30 | INFO | train_inner | epoch 023:     50 / 60 loss=5.781, ppl=54.98, wps=8228.9, ups=11.56, wpb=711.7, bsz=32, num_updates=1370, lr=2.86615e-05, gnorm=4.796, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=234
2022-02-28 00:06:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-28 00:06:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:06:32 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.069 | ppl 67.15 | wps 27801 | wpb 653.8 | bsz 31.5 | num_updates 1379 | best_loss 6.069
2022-02-28 00:06:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1379 updates
2022-02-28 00:06:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 23 @ 1379 updates, score 6.069) (writing took 4.242405934986891 seconds)
2022-02-28 00:06:36 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-28 00:06:36 | INFO | train | epoch 023 | loss 5.485 | ppl 44.79 | wps 4110.6 | ups 6.3 | wpb 652.9 | bsz 31.8 | num_updates 1379 | lr 2.86477e-05 | gnorm 3.314 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.7 | wall 240
2022-02-28 00:06:36 | INFO | fairseq.trainer | begin training epoch 24
2022-02-28 00:06:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:06:36 | INFO | train_inner | epoch 024:      1 / 60 loss=5.51, ppl=45.56, wps=1071.1, ups=1.7, wpb=631.6, bsz=32, num_updates=1380, lr=2.86462e-05, gnorm=3.496, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=240
2022-02-28 00:06:37 | INFO | train_inner | epoch 024:     11 / 60 loss=5.75, ppl=53.82, wps=11197.3, ups=14.74, wpb=759.6, bsz=31, num_updates=1390, lr=2.86308e-05, gnorm=3.264, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=241
2022-02-28 00:06:38 | INFO | train_inner | epoch 024:     21 / 60 loss=5.189, ppl=36.47, wps=9723.6, ups=15.85, wpb=613.4, bsz=32, num_updates=1400, lr=2.86154e-05, gnorm=3.099, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=241
2022-02-28 00:06:38 | INFO | train_inner | epoch 024:     31 / 60 loss=5.111, ppl=34.55, wps=9179.3, ups=16.57, wpb=554.1, bsz=32, num_updates=1410, lr=2.86e-05, gnorm=3.427, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=242
2022-02-28 00:06:39 | INFO | train_inner | epoch 024:     41 / 60 loss=4.98, ppl=31.55, wps=9349.5, ups=16.05, wpb=582.6, bsz=32, num_updates=1420, lr=2.85846e-05, gnorm=2.865, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=242
2022-02-28 00:06:39 | INFO | train_inner | epoch 024:     51 / 60 loss=5.488, ppl=44.89, wps=10871.1, ups=14.41, wpb=754.2, bsz=32, num_updates=1430, lr=2.85692e-05, gnorm=2.659, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=243
2022-02-28 00:06:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:06:41 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.954 | ppl 62 | wps 27671.6 | wpb 653.8 | bsz 31.5 | num_updates 1439 | best_loss 5.954
2022-02-28 00:06:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1439 updates
2022-02-28 00:06:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 24 @ 1439 updates, score 5.954) (writing took 3.8606761199771427 seconds)
2022-02-28 00:06:45 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-28 00:06:45 | INFO | train | epoch 024 | loss 5.384 | ppl 41.77 | wps 4705.9 | ups 7.06 | wpb 666.1 | bsz 31.8 | num_updates 1439 | lr 2.85554e-05 | gnorm 2.977 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 248
2022-02-28 00:06:45 | INFO | fairseq.trainer | begin training epoch 25
2022-02-28 00:06:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:06:45 | INFO | train_inner | epoch 025:      1 / 60 loss=5.444, ppl=43.54, wps=1334.8, ups=1.9, wpb=703.8, bsz=32, num_updates=1440, lr=2.85538e-05, gnorm=2.584, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=248
2022-02-28 00:06:45 | INFO | train_inner | epoch 025:     11 / 60 loss=5.308, ppl=39.61, wps=8737.8, ups=13.93, wpb=627.2, bsz=31, num_updates=1450, lr=2.85385e-05, gnorm=3.147, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=249
2022-02-28 00:06:46 | INFO | train_inner | epoch 025:     21 / 60 loss=5.447, ppl=43.61, wps=9322.7, ups=14.27, wpb=653.1, bsz=32, num_updates=1460, lr=2.85231e-05, gnorm=3.737, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=250
2022-02-28 00:06:47 | INFO | train_inner | epoch 025:     31 / 60 loss=5.522, ppl=45.96, wps=10380.1, ups=14.59, wpb=711.4, bsz=32, num_updates=1470, lr=2.85077e-05, gnorm=2.7, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=251
2022-02-28 00:06:47 | INFO | train_inner | epoch 025:     41 / 60 loss=5.189, ppl=36.48, wps=11005.4, ups=15.12, wpb=727.7, bsz=32, num_updates=1480, lr=2.84923e-05, gnorm=2.537, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=251
2022-02-28 00:06:48 | INFO | train_inner | epoch 025:     51 / 60 loss=5.278, ppl=38.81, wps=10831, ups=14.11, wpb=767.6, bsz=32, num_updates=1490, lr=2.84769e-05, gnorm=2.51, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=252
2022-02-28 00:06:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:06:50 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.883 | ppl 59.02 | wps 27785.6 | wpb 653.8 | bsz 31.5 | num_updates 1499 | best_loss 5.883
2022-02-28 00:06:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1499 updates
2022-02-28 00:06:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:06:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 25 @ 1499 updates, score 5.883) (writing took 4.246102715987945 seconds)
2022-02-28 00:06:54 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-28 00:06:54 | INFO | train | epoch 025 | loss 5.259 | ppl 38.3 | wps 4355.1 | ups 6.54 | wpb 666.1 | bsz 31.8 | num_updates 1499 | lr 2.84631e-05 | gnorm 2.93 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 257
2022-02-28 00:06:54 | INFO | fairseq.trainer | begin training epoch 26
2022-02-28 00:06:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:06:54 | INFO | train_inner | epoch 026:      1 / 60 loss=4.625, ppl=24.68, wps=829.5, ups=1.76, wpb=472.2, bsz=32, num_updates=1500, lr=2.84615e-05, gnorm=2.978, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=258
2022-02-28 00:06:55 | INFO | train_inner | epoch 026:     11 / 60 loss=4.974, ppl=31.43, wps=9218.3, ups=14.84, wpb=621.2, bsz=32, num_updates=1510, lr=2.84462e-05, gnorm=2.647, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=258
2022-02-28 00:06:55 | INFO | train_inner | epoch 026:     21 / 60 loss=5.342, ppl=40.57, wps=11046.1, ups=14.89, wpb=741.8, bsz=32, num_updates=1520, lr=2.84308e-05, gnorm=2.572, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=259
2022-02-28 00:06:56 | INFO | train_inner | epoch 026:     31 / 60 loss=4.965, ppl=31.22, wps=9910.4, ups=14.84, wpb=667.6, bsz=32, num_updates=1530, lr=2.84154e-05, gnorm=2.888, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=260
2022-02-28 00:06:57 | INFO | train_inner | epoch 026:     41 / 60 loss=5.534, ppl=46.34, wps=11688.9, ups=14.9, wpb=784.5, bsz=31, num_updates=1540, lr=2.84e-05, gnorm=2.771, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=260
2022-02-28 00:06:57 | INFO | train_inner | epoch 026:     51 / 60 loss=4.907, ppl=29.99, wps=7727.9, ups=14.41, wpb=536.3, bsz=32, num_updates=1550, lr=2.83846e-05, gnorm=3.012, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=261
2022-02-28 00:06:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:06:59 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.789 | ppl 55.28 | wps 28450.9 | wpb 653.8 | bsz 31.5 | num_updates 1559 | best_loss 5.789
2022-02-28 00:06:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1559 updates
2022-02-28 00:06:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 26 @ 1559 updates, score 5.789) (writing took 3.872757958015427 seconds)
2022-02-28 00:07:02 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-28 00:07:02 | INFO | train | epoch 026 | loss 5.137 | ppl 35.18 | wps 4616.3 | ups 6.93 | wpb 666.1 | bsz 31.8 | num_updates 1559 | lr 2.83708e-05 | gnorm 2.89 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.5 | wall 266
2022-02-28 00:07:02 | INFO | fairseq.trainer | begin training epoch 27
2022-02-28 00:07:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:07:03 | INFO | train_inner | epoch 027:      1 / 60 loss=4.911, ppl=30.08, wps=1218.9, ups=1.89, wpb=645.8, bsz=32, num_updates=1560, lr=2.83692e-05, gnorm=3.479, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=266
2022-02-28 00:07:03 | INFO | train_inner | epoch 027:     11 / 60 loss=5.196, ppl=36.65, wps=9930.6, ups=14.75, wpb=673.2, bsz=31, num_updates=1570, lr=2.83538e-05, gnorm=3.246, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=267
2022-02-28 00:07:04 | INFO | train_inner | epoch 027:     21 / 60 loss=4.866, ppl=29.17, wps=10446.5, ups=16, wpb=652.9, bsz=32, num_updates=1580, lr=2.83385e-05, gnorm=3.018, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=268
2022-02-28 00:07:05 | INFO | train_inner | epoch 027:     31 / 60 loss=4.389, ppl=20.95, wps=8318.1, ups=16.15, wpb=514.9, bsz=32, num_updates=1590, lr=2.83231e-05, gnorm=3.128, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=268
2022-02-28 00:07:05 | INFO | train_inner | epoch 027:     41 / 60 loss=5.28, ppl=38.86, wps=12196.5, ups=15.44, wpb=789.7, bsz=32, num_updates=1600, lr=2.83077e-05, gnorm=2.543, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=269
2022-02-28 00:07:06 | INFO | train_inner | epoch 027:     51 / 60 loss=5.129, ppl=34.99, wps=8478.7, ups=12.59, wpb=673.3, bsz=32, num_updates=1610, lr=2.82923e-05, gnorm=2.848, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=270
2022-02-28 00:07:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:07:07 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.687 | ppl 51.51 | wps 27253.3 | wpb 653.8 | bsz 31.5 | num_updates 1619 | best_loss 5.687
2022-02-28 00:07:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1619 updates
2022-02-28 00:07:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 27 @ 1619 updates, score 5.687) (writing took 4.037400553992484 seconds)
2022-02-28 00:07:11 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-28 00:07:11 | INFO | train | epoch 027 | loss 5.011 | ppl 32.24 | wps 4488.6 | ups 6.74 | wpb 666.1 | bsz 31.8 | num_updates 1619 | lr 2.82785e-05 | gnorm 2.919 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.3 | wall 275
2022-02-28 00:07:11 | INFO | fairseq.trainer | begin training epoch 28
2022-02-28 00:07:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:07:12 | INFO | train_inner | epoch 028:      1 / 60 loss=5.068, ppl=33.54, wps=1333.3, ups=1.81, wpb=737, bsz=32, num_updates=1620, lr=2.82769e-05, gnorm=2.717, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=275
2022-02-28 00:07:12 | INFO | train_inner | epoch 028:     11 / 60 loss=4.929, ppl=30.47, wps=9829.1, ups=15.1, wpb=650.9, bsz=32, num_updates=1630, lr=2.82615e-05, gnorm=2.764, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=276
2022-02-28 00:07:13 | INFO | train_inner | epoch 028:     21 / 60 loss=5.155, ppl=35.63, wps=11813, ups=15.4, wpb=766.9, bsz=32, num_updates=1640, lr=2.82462e-05, gnorm=2.885, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=277
2022-02-28 00:07:13 | INFO | train_inner | epoch 028:     31 / 60 loss=4.336, ppl=20.19, wps=9219.5, ups=16.2, wpb=569.2, bsz=32, num_updates=1650, lr=2.82308e-05, gnorm=2.748, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=277
2022-02-28 00:07:14 | INFO | train_inner | epoch 028:     41 / 60 loss=5.143, ppl=35.34, wps=11163.2, ups=15.2, wpb=734.6, bsz=31, num_updates=1660, lr=2.82154e-05, gnorm=2.662, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=278
2022-02-28 00:07:15 | INFO | train_inner | epoch 028:     51 / 60 loss=4.643, ppl=24.99, wps=8571.4, ups=14.69, wpb=583.4, bsz=32, num_updates=1670, lr=2.82e-05, gnorm=2.979, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=279
2022-02-28 00:07:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:07:16 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.571 | ppl 47.54 | wps 28244.1 | wpb 653.8 | bsz 31.5 | num_updates 1679 | best_loss 5.571
2022-02-28 00:07:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1679 updates
2022-02-28 00:07:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 28 @ 1679 updates, score 5.571) (writing took 3.948563470010413 seconds)
2022-02-28 00:07:20 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-28 00:07:20 | INFO | train | epoch 028 | loss 4.886 | ppl 29.58 | wps 4679.4 | ups 7.02 | wpb 666.1 | bsz 31.8 | num_updates 1679 | lr 2.81862e-05 | gnorm 2.82 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 284
2022-02-28 00:07:20 | INFO | fairseq.trainer | begin training epoch 29
2022-02-28 00:07:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:07:20 | INFO | train_inner | epoch 029:      1 / 60 loss=4.844, ppl=28.71, wps=1239.5, ups=1.9, wpb=651.3, bsz=32, num_updates=1680, lr=2.81846e-05, gnorm=2.844, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=284
2022-02-28 00:07:21 | INFO | train_inner | epoch 029:     11 / 60 loss=5.092, ppl=34.11, wps=10679.5, ups=14.89, wpb=717.4, bsz=31, num_updates=1690, lr=2.81692e-05, gnorm=2.955, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=284
2022-02-28 00:07:21 | INFO | train_inner | epoch 029:     21 / 60 loss=4.758, ppl=27.05, wps=11101.4, ups=15.89, wpb=698.8, bsz=32, num_updates=1700, lr=2.81538e-05, gnorm=2.932, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=285
2022-02-28 00:07:22 | INFO | train_inner | epoch 029:     31 / 60 loss=4.983, ppl=31.63, wps=11074.8, ups=15.89, wpb=696.9, bsz=32, num_updates=1710, lr=2.81385e-05, gnorm=2.893, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=286
2022-02-28 00:07:23 | INFO | train_inner | epoch 029:     41 / 60 loss=4.332, ppl=20.14, wps=9350.7, ups=16.46, wpb=568.2, bsz=32, num_updates=1720, lr=2.81231e-05, gnorm=2.781, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=286
2022-02-28 00:07:23 | INFO | train_inner | epoch 029:     51 / 60 loss=4.482, ppl=22.35, wps=8247.6, ups=14.29, wpb=577.1, bsz=32, num_updates=1730, lr=2.81077e-05, gnorm=2.914, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=287
2022-02-28 00:07:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:07:25 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.43 | ppl 43.11 | wps 27239.9 | wpb 653.8 | bsz 31.5 | num_updates 1739 | best_loss 5.43
2022-02-28 00:07:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1739 updates
2022-02-28 00:07:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 29 @ 1739 updates, score 5.43) (writing took 4.4494976100104395 seconds)
2022-02-28 00:07:29 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-28 00:07:29 | INFO | train | epoch 029 | loss 4.773 | ppl 27.33 | wps 4399.5 | ups 6.6 | wpb 666.1 | bsz 31.8 | num_updates 1739 | lr 2.80938e-05 | gnorm 2.838 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.7 | wall 293
2022-02-28 00:07:29 | INFO | fairseq.trainer | begin training epoch 30
2022-02-28 00:07:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:07:29 | INFO | train_inner | epoch 030:      1 / 60 loss=4.864, ppl=29.12, wps=1269.8, ups=1.7, wpb=744.9, bsz=32, num_updates=1740, lr=2.80923e-05, gnorm=2.596, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=293
2022-02-28 00:07:30 | INFO | train_inner | epoch 030:     11 / 60 loss=4.64, ppl=24.93, wps=9671.4, ups=14.41, wpb=671.2, bsz=32, num_updates=1750, lr=2.80769e-05, gnorm=2.924, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=294
2022-02-28 00:07:31 | INFO | train_inner | epoch 030:     21 / 60 loss=4.809, ppl=28.03, wps=10926, ups=15.71, wpb=695.6, bsz=31, num_updates=1760, lr=2.80615e-05, gnorm=2.756, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=294
2022-02-28 00:07:31 | INFO | train_inner | epoch 030:     31 / 60 loss=5.315, ppl=39.8, wps=14503.5, ups=14.36, wpb=1010, bsz=32, num_updates=1770, lr=2.80462e-05, gnorm=2.387, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=295
2022-02-28 00:07:32 | INFO | train_inner | epoch 030:     41 / 60 loss=4.247, ppl=18.98, wps=8558.6, ups=16.08, wpb=532.1, bsz=32, num_updates=1780, lr=2.80308e-05, gnorm=2.777, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=296
2022-02-28 00:07:33 | INFO | train_inner | epoch 030:     51 / 60 loss=4.24, ppl=18.89, wps=8059, ups=14.21, wpb=567.2, bsz=32, num_updates=1790, lr=2.80154e-05, gnorm=3.118, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=296
2022-02-28 00:07:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:07:34 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.323 | ppl 40.02 | wps 26499.1 | wpb 653.8 | bsz 31.5 | num_updates 1799 | best_loss 5.323
2022-02-28 00:07:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1799 updates
2022-02-28 00:07:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 30 @ 1799 updates, score 5.323) (writing took 4.820445778022986 seconds)
2022-02-28 00:07:39 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-28 00:07:39 | INFO | train | epoch 030 | loss 4.652 | ppl 25.15 | wps 4120.1 | ups 6.19 | wpb 666.1 | bsz 31.8 | num_updates 1799 | lr 2.80015e-05 | gnorm 2.804 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 302
2022-02-28 00:07:39 | INFO | fairseq.trainer | begin training epoch 31
2022-02-28 00:07:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:07:39 | INFO | train_inner | epoch 031:      1 / 60 loss=4.025, ppl=16.28, wps=821.8, ups=1.57, wpb=523.1, bsz=32, num_updates=1800, lr=2.8e-05, gnorm=2.932, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=303
2022-02-28 00:07:40 | INFO | train_inner | epoch 031:     11 / 60 loss=4.524, ppl=23.01, wps=9353.5, ups=14.63, wpb=639.2, bsz=32, num_updates=1810, lr=2.79846e-05, gnorm=3.102, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=303
2022-02-28 00:07:40 | INFO | train_inner | epoch 031:     21 / 60 loss=4.906, ppl=29.98, wps=9675.7, ups=12.73, wpb=760.3, bsz=31, num_updates=1820, lr=2.79692e-05, gnorm=2.748, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=304
2022-02-28 00:07:41 | INFO | train_inner | epoch 031:     31 / 60 loss=4.488, ppl=22.44, wps=8874.8, ups=12.48, wpb=711.2, bsz=32, num_updates=1830, lr=2.79538e-05, gnorm=2.65, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=305
2022-02-28 00:07:42 | INFO | train_inner | epoch 031:     41 / 60 loss=4.291, ppl=19.57, wps=7398.1, ups=12.69, wpb=582.8, bsz=32, num_updates=1840, lr=2.79385e-05, gnorm=2.915, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=306
2022-02-28 00:07:43 | INFO | train_inner | epoch 031:     51 / 60 loss=4.771, ppl=27.3, wps=10960.7, ups=12.92, wpb=848.6, bsz=32, num_updates=1850, lr=2.79231e-05, gnorm=2.565, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=306
2022-02-28 00:07:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:07:44 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.2 | ppl 36.77 | wps 26698 | wpb 653.8 | bsz 31.5 | num_updates 1859 | best_loss 5.2
2022-02-28 00:07:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1859 updates
2022-02-28 00:07:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 31 @ 1859 updates, score 5.2) (writing took 4.378946910990635 seconds)
2022-02-28 00:07:48 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-28 00:07:49 | INFO | train | epoch 031 | loss 4.539 | ppl 23.24 | wps 4151 | ups 6.23 | wpb 666.1 | bsz 31.8 | num_updates 1859 | lr 2.79092e-05 | gnorm 2.873 | clip 100 | loss_scale 64 | train_wall 5 | gb_free 20.8 | wall 312
2022-02-28 00:07:49 | INFO | fairseq.trainer | begin training epoch 32
2022-02-28 00:07:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:07:49 | INFO | train_inner | epoch 032:      1 / 60 loss=3.942, ppl=15.37, wps=795, ups=1.67, wpb=474.7, bsz=32, num_updates=1860, lr=2.79077e-05, gnorm=3.128, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=312
2022-02-28 00:07:49 | INFO | train_inner | epoch 032:     11 / 60 loss=4.407, ppl=21.22, wps=9923.9, ups=15.06, wpb=659, bsz=32, num_updates=1870, lr=2.78923e-05, gnorm=2.781, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=313
2022-02-28 00:07:50 | INFO | train_inner | epoch 032:     21 / 60 loss=3.631, ppl=12.39, wps=6479.8, ups=15.23, wpb=425.4, bsz=32, num_updates=1880, lr=2.78769e-05, gnorm=2.997, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=314
2022-02-28 00:07:51 | INFO | train_inner | epoch 032:     31 / 60 loss=4.593, ppl=24.13, wps=10791.4, ups=14.54, wpb=742, bsz=32, num_updates=1890, lr=2.78615e-05, gnorm=2.67, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=314
2022-02-28 00:07:51 | INFO | train_inner | epoch 032:     41 / 60 loss=4.145, ppl=17.69, wps=8874.5, ups=14.44, wpb=614.4, bsz=32, num_updates=1900, lr=2.78462e-05, gnorm=3.431, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=315
2022-02-28 00:07:52 | INFO | train_inner | epoch 032:     51 / 60 loss=4.844, ppl=28.71, wps=10468.2, ups=12.59, wpb=831.4, bsz=31, num_updates=1910, lr=2.78308e-05, gnorm=2.648, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=316
2022-02-28 00:07:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:07:54 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.101 | ppl 34.32 | wps 26124.8 | wpb 653.8 | bsz 31.5 | num_updates 1919 | best_loss 5.101
2022-02-28 00:07:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1919 updates
2022-02-28 00:07:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:07:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 32 @ 1919 updates, score 5.101) (writing took 4.210666092025349 seconds)
2022-02-28 00:07:58 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-28 00:07:58 | INFO | train | epoch 032 | loss 4.42 | ppl 21.41 | wps 4309.3 | ups 6.47 | wpb 666.1 | bsz 31.8 | num_updates 1919 | lr 2.78169e-05 | gnorm 2.845 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 322
2022-02-28 00:07:58 | INFO | fairseq.trainer | begin training epoch 33
2022-02-28 00:07:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:07:58 | INFO | train_inner | epoch 033:      1 / 60 loss=4.458, ppl=21.98, wps=1254.3, ups=1.74, wpb=722.9, bsz=32, num_updates=1920, lr=2.78154e-05, gnorm=2.532, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=322
2022-02-28 00:07:59 | INFO | train_inner | epoch 033:     11 / 60 loss=3.935, ppl=15.29, wps=8162.2, ups=14.94, wpb=546.2, bsz=32, num_updates=1930, lr=2.78e-05, gnorm=2.644, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=322
2022-02-28 00:07:59 | INFO | train_inner | epoch 033:     21 / 60 loss=4.647, ppl=25.06, wps=11168.4, ups=15.38, wpb=726.2, bsz=32, num_updates=1940, lr=2.77846e-05, gnorm=2.611, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=323
2022-02-28 00:08:00 | INFO | train_inner | epoch 033:     31 / 60 loss=4.195, ppl=18.31, wps=10274.5, ups=15.74, wpb=652.7, bsz=32, num_updates=1950, lr=2.77692e-05, gnorm=2.808, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=324
2022-02-28 00:08:01 | INFO | train_inner | epoch 033:     41 / 60 loss=4.342, ppl=20.28, wps=11282.1, ups=15.49, wpb=728.2, bsz=32, num_updates=1960, lr=2.77538e-05, gnorm=2.674, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=324
2022-02-28 00:08:01 | INFO | train_inner | epoch 033:     51 / 60 loss=4.46, ppl=22.01, wps=10756.9, ups=14.05, wpb=765.7, bsz=31, num_updates=1970, lr=2.77385e-05, gnorm=2.704, clip=100, loss_scale=64, train_wall=1, gb_free=19.2, wall=325
2022-02-28 00:08:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:08:03 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.04 | ppl 32.89 | wps 27013.8 | wpb 653.8 | bsz 31.5 | num_updates 1979 | best_loss 5.04
2022-02-28 00:08:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1979 updates
2022-02-28 00:08:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 33 @ 1979 updates, score 5.04) (writing took 4.40751711599296 seconds)
2022-02-28 00:08:07 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-28 00:08:07 | INFO | train | epoch 033 | loss 4.297 | ppl 19.66 | wps 4343.4 | ups 6.52 | wpb 666.1 | bsz 31.8 | num_updates 1979 | lr 2.77246e-05 | gnorm 2.699 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.1 | wall 331
2022-02-28 00:08:07 | INFO | fairseq.trainer | begin training epoch 34
2022-02-28 00:08:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:08:07 | INFO | train_inner | epoch 034:      1 / 60 loss=4.02, ppl=16.22, wps=936.7, ups=1.69, wpb=554.8, bsz=32, num_updates=1980, lr=2.77231e-05, gnorm=2.847, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=331
2022-02-28 00:08:08 | INFO | train_inner | epoch 034:     11 / 60 loss=4.599, ppl=24.23, wps=12309.9, ups=14.12, wpb=871.9, bsz=31, num_updates=1990, lr=2.77077e-05, gnorm=2.56, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=332
2022-02-28 00:08:09 | INFO | train_inner | epoch 034:     21 / 60 loss=3.725, ppl=13.22, wps=9759.4, ups=16.55, wpb=589.6, bsz=32, num_updates=2000, lr=2.76923e-05, gnorm=2.653, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=332
2022-02-28 00:08:09 | INFO | train_inner | epoch 034:     31 / 60 loss=4.088, ppl=17.01, wps=10651.9, ups=15.75, wpb=676.2, bsz=32, num_updates=2010, lr=2.76769e-05, gnorm=2.606, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=333
2022-02-28 00:08:10 | INFO | train_inner | epoch 034:     41 / 60 loss=4.218, ppl=18.61, wps=8970.3, ups=15.74, wpb=569.8, bsz=32, num_updates=2020, lr=2.76615e-05, gnorm=2.796, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=334
2022-02-28 00:08:11 | INFO | train_inner | epoch 034:     51 / 60 loss=4.253, ppl=19.06, wps=9960.5, ups=13.2, wpb=754.5, bsz=32, num_updates=2030, lr=2.76462e-05, gnorm=2.74, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=334
2022-02-28 00:08:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:08:12 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.899 | ppl 29.83 | wps 26602.9 | wpb 653.8 | bsz 31.5 | num_updates 2039 | best_loss 4.899
2022-02-28 00:08:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 2039 updates
2022-02-28 00:08:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 34 @ 2039 updates, score 4.899) (writing took 4.022325550002279 seconds)
2022-02-28 00:08:16 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-28 00:08:16 | INFO | train | epoch 034 | loss 4.19 | ppl 18.26 | wps 4539 | ups 6.81 | wpb 666.1 | bsz 31.8 | num_updates 2039 | lr 2.76323e-05 | gnorm 2.759 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 340
2022-02-28 00:08:16 | INFO | fairseq.trainer | begin training epoch 35
2022-02-28 00:08:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:08:16 | INFO | train_inner | epoch 035:      1 / 60 loss=4.086, ppl=16.99, wps=1059.5, ups=1.84, wpb=577.1, bsz=32, num_updates=2040, lr=2.76308e-05, gnorm=3.109, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=340
2022-02-28 00:08:17 | INFO | train_inner | epoch 035:     11 / 60 loss=4.425, ppl=21.48, wps=11421.1, ups=14.44, wpb=790.8, bsz=31, num_updates=2050, lr=2.76154e-05, gnorm=2.678, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=340
2022-02-28 00:08:17 | INFO | train_inner | epoch 035:     21 / 60 loss=4.115, ppl=17.33, wps=11377.7, ups=15.56, wpb=731, bsz=32, num_updates=2060, lr=2.76e-05, gnorm=2.788, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=341
2022-02-28 00:08:18 | INFO | train_inner | epoch 035:     31 / 60 loss=3.436, ppl=10.83, wps=7489.3, ups=16.25, wpb=460.8, bsz=32, num_updates=2070, lr=2.75846e-05, gnorm=2.974, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=342
2022-02-28 00:08:19 | INFO | train_inner | epoch 035:     41 / 60 loss=3.965, ppl=15.62, wps=8805.8, ups=15.11, wpb=582.9, bsz=32, num_updates=2080, lr=2.75692e-05, gnorm=3.021, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=342
2022-02-28 00:08:19 | INFO | train_inner | epoch 035:     51 / 60 loss=3.828, ppl=14.21, wps=8618.8, ups=13.94, wpb=618.1, bsz=32, num_updates=2090, lr=2.75538e-05, gnorm=2.893, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=343
2022-02-28 00:08:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:08:21 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.912 | ppl 30.12 | wps 28163.9 | wpb 653.8 | bsz 31.5 | num_updates 2099 | best_loss 4.899
2022-02-28 00:08:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 2099 updates
2022-02-28 00:08:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:08:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:08:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 35 @ 2099 updates, score 4.912) (writing took 2.677214905008441 seconds)
2022-02-28 00:08:23 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-28 00:08:23 | INFO | train | epoch 035 | loss 4.082 | ppl 16.94 | wps 5368.3 | ups 8.06 | wpb 666.1 | bsz 31.8 | num_updates 2099 | lr 2.754e-05 | gnorm 2.825 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 347
2022-02-28 00:08:23 | INFO | fairseq.trainer | begin training epoch 36
2022-02-28 00:08:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:08:23 | INFO | train_inner | epoch 036:      1 / 60 loss=4.332, ppl=20.14, wps=1980.3, ups=2.45, wpb=809.6, bsz=32, num_updates=2100, lr=2.75385e-05, gnorm=2.638, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=347
2022-02-28 00:08:24 | INFO | train_inner | epoch 036:     11 / 60 loss=3.457, ppl=10.98, wps=7588.4, ups=14.87, wpb=510.4, bsz=32, num_updates=2110, lr=2.75231e-05, gnorm=3.032, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=348
2022-02-28 00:08:25 | INFO | train_inner | epoch 036:     21 / 60 loss=4.123, ppl=17.42, wps=9981.3, ups=14.94, wpb=667.9, bsz=32, num_updates=2120, lr=2.75077e-05, gnorm=3.435, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=349
2022-02-28 00:08:26 | INFO | train_inner | epoch 036:     31 / 60 loss=4.255, ppl=19.1, wps=9928, ups=13.84, wpb=717.4, bsz=31, num_updates=2130, lr=2.74923e-05, gnorm=3.112, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=349
2022-02-28 00:08:26 | INFO | train_inner | epoch 036:     41 / 60 loss=4.128, ppl=17.48, wps=10441.5, ups=14.91, wpb=700.2, bsz=32, num_updates=2140, lr=2.74769e-05, gnorm=4.009, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=350
2022-02-28 00:08:27 | INFO | train_inner | epoch 036:     51 / 60 loss=4.245, ppl=18.97, wps=9612.1, ups=13.5, wpb=711.8, bsz=32, num_updates=2150, lr=2.74615e-05, gnorm=3.272, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=351
2022-02-28 00:08:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:08:28 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.823 | ppl 28.31 | wps 27034.4 | wpb 653.8 | bsz 31.5 | num_updates 2159 | best_loss 4.823
2022-02-28 00:08:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 2159 updates
2022-02-28 00:08:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 36 @ 2159 updates, score 4.823) (writing took 3.9757152989914175 seconds)
2022-02-28 00:08:32 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-28 00:08:32 | INFO | train | epoch 036 | loss 4.055 | ppl 16.63 | wps 4485.8 | ups 6.73 | wpb 666.1 | bsz 31.8 | num_updates 2159 | lr 2.74477e-05 | gnorm 3.437 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 356
2022-02-28 00:08:32 | INFO | fairseq.trainer | begin training epoch 37
2022-02-28 00:08:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:08:32 | INFO | train_inner | epoch 037:      1 / 60 loss=3.937, ppl=15.32, wps=1232.1, ups=1.84, wpb=669.8, bsz=32, num_updates=2160, lr=2.74462e-05, gnorm=3.826, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=356
2022-02-28 00:08:33 | INFO | train_inner | epoch 037:     11 / 60 loss=4.231, ppl=18.78, wps=12004.9, ups=14.8, wpb=811.3, bsz=32, num_updates=2170, lr=2.74308e-05, gnorm=2.699, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=357
2022-02-28 00:08:34 | INFO | train_inner | epoch 037:     21 / 60 loss=3.836, ppl=14.28, wps=10077.7, ups=16.1, wpb=626.1, bsz=32, num_updates=2180, lr=2.74154e-05, gnorm=2.959, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=357
2022-02-28 00:08:34 | INFO | train_inner | epoch 037:     31 / 60 loss=4.086, ppl=16.98, wps=11855, ups=15.7, wpb=755.1, bsz=32, num_updates=2190, lr=2.74e-05, gnorm=2.685, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=358
2022-02-28 00:08:35 | INFO | train_inner | epoch 037:     41 / 60 loss=3.663, ppl=12.66, wps=11144.2, ups=16.11, wpb=691.7, bsz=32, num_updates=2200, lr=2.73846e-05, gnorm=2.603, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=359
2022-02-28 00:08:36 | INFO | train_inner | epoch 037:     51 / 60 loss=3.54, ppl=11.63, wps=7956.1, ups=14.3, wpb=556.4, bsz=32, num_updates=2210, lr=2.73692e-05, gnorm=2.863, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=359
2022-02-28 00:08:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:08:37 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.736 | ppl 26.65 | wps 25295.8 | wpb 653.8 | bsz 31.5 | num_updates 2219 | best_loss 4.736
2022-02-28 00:08:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 2219 updates
2022-02-28 00:08:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 37 @ 2219 updates, score 4.736) (writing took 3.897879139985889 seconds)
2022-02-28 00:08:41 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-28 00:08:41 | INFO | train | epoch 037 | loss 3.907 | ppl 15 | wps 4626.5 | ups 6.95 | wpb 666.1 | bsz 31.8 | num_updates 2219 | lr 2.73554e-05 | gnorm 2.799 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 365
2022-02-28 00:08:41 | INFO | fairseq.trainer | begin training epoch 38
2022-02-28 00:08:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:08:41 | INFO | train_inner | epoch 038:      1 / 60 loss=3.96, ppl=15.56, wps=1055.5, ups=1.87, wpb=565.7, bsz=31, num_updates=2220, lr=2.73538e-05, gnorm=2.977, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=365
2022-02-28 00:08:42 | INFO | train_inner | epoch 038:     11 / 60 loss=4.216, ppl=18.58, wps=11273.8, ups=14.13, wpb=797.8, bsz=31, num_updates=2230, lr=2.73385e-05, gnorm=2.666, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=365
2022-02-28 00:08:42 | INFO | train_inner | epoch 038:     21 / 60 loss=3.463, ppl=11.03, wps=10004.5, ups=16.02, wpb=624.5, bsz=32, num_updates=2240, lr=2.73231e-05, gnorm=2.55, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=366
2022-02-28 00:08:43 | INFO | train_inner | epoch 038:     31 / 60 loss=4.07, ppl=16.8, wps=11498.5, ups=15.41, wpb=746, bsz=32, num_updates=2250, lr=2.73077e-05, gnorm=2.708, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=367
2022-02-28 00:08:44 | INFO | train_inner | epoch 038:     41 / 60 loss=3.801, ppl=13.94, wps=10825.3, ups=15.63, wpb=692.8, bsz=32, num_updates=2260, lr=2.72923e-05, gnorm=2.71, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=367
2022-02-28 00:08:44 | INFO | train_inner | epoch 038:     51 / 60 loss=3.237, ppl=9.43, wps=6964.5, ups=14.16, wpb=491.7, bsz=32, num_updates=2270, lr=2.72769e-05, gnorm=3.045, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=368
2022-02-28 00:08:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:08:46 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.616 | ppl 24.52 | wps 34582.3 | wpb 653.8 | bsz 31.5 | num_updates 2279 | best_loss 4.616
2022-02-28 00:08:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 2279 updates
2022-02-28 00:08:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 38 @ 2279 updates, score 4.616) (writing took 4.558990414021537 seconds)
2022-02-28 00:08:50 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-28 00:08:50 | INFO | train | epoch 038 | loss 3.796 | ppl 13.89 | wps 4314.1 | ups 6.48 | wpb 666.1 | bsz 31.8 | num_updates 2279 | lr 2.72631e-05 | gnorm 2.783 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.6 | wall 374
2022-02-28 00:08:50 | INFO | fairseq.trainer | begin training epoch 39
2022-02-28 00:08:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:08:50 | INFO | train_inner | epoch 039:      1 / 60 loss=3.771, ppl=13.65, wps=1130.2, ups=1.68, wpb=672, bsz=32, num_updates=2280, lr=2.72615e-05, gnorm=2.936, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=374
2022-02-28 00:08:51 | INFO | train_inner | epoch 039:     11 / 60 loss=3.212, ppl=9.27, wps=8572.6, ups=15.21, wpb=563.7, bsz=32, num_updates=2290, lr=2.72462e-05, gnorm=2.702, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=375
2022-02-28 00:08:52 | INFO | train_inner | epoch 039:     21 / 60 loss=2.789, ppl=6.91, wps=6939.9, ups=16.3, wpb=425.8, bsz=32, num_updates=2300, lr=2.72308e-05, gnorm=2.716, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=375
2022-02-28 00:08:52 | INFO | train_inner | epoch 039:     31 / 60 loss=3.845, ppl=14.37, wps=11070.3, ups=14.98, wpb=739.2, bsz=32, num_updates=2310, lr=2.72154e-05, gnorm=3.008, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=376
2022-02-28 00:08:53 | INFO | train_inner | epoch 039:     41 / 60 loss=3.155, ppl=8.91, wps=9031.9, ups=15.59, wpb=579.5, bsz=32, num_updates=2320, lr=2.72e-05, gnorm=2.583, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=377
2022-02-28 00:08:54 | INFO | train_inner | epoch 039:     51 / 60 loss=4.033, ppl=16.37, wps=10157.5, ups=12.45, wpb=815.7, bsz=32, num_updates=2330, lr=2.71846e-05, gnorm=2.591, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=377
2022-02-28 00:08:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:08:55 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.544 | ppl 23.33 | wps 25925.1 | wpb 653.8 | bsz 31.5 | num_updates 2339 | best_loss 4.544
2022-02-28 00:08:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 2339 updates
2022-02-28 00:08:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:08:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 39 @ 2339 updates, score 4.544) (writing took 3.999169096990954 seconds)
2022-02-28 00:08:59 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-28 00:08:59 | INFO | train | epoch 039 | loss 3.709 | ppl 13.07 | wps 4431.5 | ups 6.65 | wpb 666.1 | bsz 31.8 | num_updates 2339 | lr 2.71708e-05 | gnorm 2.715 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 383
2022-02-28 00:08:59 | INFO | fairseq.trainer | begin training epoch 40
2022-02-28 00:08:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:08:59 | INFO | train_inner | epoch 040:      1 / 60 loss=4.371, ppl=20.69, wps=1469.4, ups=1.77, wpb=828.7, bsz=31, num_updates=2340, lr=2.71692e-05, gnorm=2.708, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=383
2022-02-28 00:09:00 | INFO | train_inner | epoch 040:     11 / 60 loss=3.623, ppl=12.32, wps=9990.2, ups=15.28, wpb=653.8, bsz=32, num_updates=2350, lr=2.71538e-05, gnorm=3.061, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=384
2022-02-28 00:09:01 | INFO | train_inner | epoch 040:     21 / 60 loss=3.978, ppl=15.76, wps=11507.5, ups=15.11, wpb=761.4, bsz=32, num_updates=2360, lr=2.71385e-05, gnorm=2.978, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=384
2022-02-28 00:09:01 | INFO | train_inner | epoch 040:     31 / 60 loss=3.581, ppl=11.97, wps=10902.3, ups=15.61, wpb=698.6, bsz=32, num_updates=2370, lr=2.71231e-05, gnorm=3.406, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=385
2022-02-28 00:09:02 | INFO | train_inner | epoch 040:     41 / 60 loss=3.099, ppl=8.57, wps=8493.6, ups=15.96, wpb=532.1, bsz=32, num_updates=2380, lr=2.71077e-05, gnorm=3.503, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=386
2022-02-28 00:09:03 | INFO | train_inner | epoch 040:     51 / 60 loss=3.692, ppl=12.92, wps=10712.8, ups=13.83, wpb=774.4, bsz=32, num_updates=2390, lr=2.70923e-05, gnorm=2.71, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=386
2022-02-28 00:09:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:09:04 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.434 | ppl 21.61 | wps 26931.8 | wpb 653.8 | bsz 31.5 | num_updates 2399 | best_loss 4.434
2022-02-28 00:09:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2399 updates
2022-02-28 00:09:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 40 @ 2399 updates, score 4.434) (writing took 3.925931378005771 seconds)
2022-02-28 00:09:08 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-28 00:09:08 | INFO | train | epoch 040 | loss 3.631 | ppl 12.39 | wps 4589.8 | ups 6.89 | wpb 666.1 | bsz 31.8 | num_updates 2399 | lr 2.70785e-05 | gnorm 3.1 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 392
2022-02-28 00:09:08 | INFO | fairseq.trainer | begin training epoch 41
2022-02-28 00:09:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:09:08 | INFO | train_inner | epoch 041:      1 / 60 loss=3.745, ppl=13.4, wps=1157.5, ups=1.87, wpb=620.5, bsz=31, num_updates=2400, lr=2.70769e-05, gnorm=2.954, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=392
2022-02-28 00:09:09 | INFO | train_inner | epoch 041:     11 / 60 loss=3.49, ppl=11.24, wps=9869, ups=13.95, wpb=707.5, bsz=32, num_updates=2410, lr=2.70615e-05, gnorm=2.985, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=393
2022-02-28 00:09:09 | INFO | train_inner | epoch 041:     21 / 60 loss=3.186, ppl=9.1, wps=9605, ups=15.79, wpb=608.3, bsz=32, num_updates=2420, lr=2.70462e-05, gnorm=2.903, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=393
2022-02-28 00:09:10 | INFO | train_inner | epoch 041:     31 / 60 loss=3.182, ppl=9.07, wps=7180, ups=11.72, wpb=612.6, bsz=32, num_updates=2430, lr=2.70308e-05, gnorm=2.758, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=394
2022-02-28 00:09:11 | INFO | train_inner | epoch 041:     41 / 60 loss=3.197, ppl=9.17, wps=7598.1, ups=12.78, wpb=594.7, bsz=32, num_updates=2440, lr=2.70154e-05, gnorm=2.892, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=395
2022-02-28 00:09:12 | INFO | train_inner | epoch 041:     51 / 60 loss=4.161, ppl=17.89, wps=9588.4, ups=12.66, wpb=757.5, bsz=31, num_updates=2450, lr=2.7e-05, gnorm=2.705, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=396
2022-02-28 00:09:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:09:13 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.417 | ppl 21.36 | wps 28365.7 | wpb 653.8 | bsz 31.5 | num_updates 2459 | best_loss 4.417
2022-02-28 00:09:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2459 updates
2022-02-28 00:09:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 41 @ 2459 updates, score 4.417) (writing took 4.245037717017112 seconds)
2022-02-28 00:09:17 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-28 00:09:17 | INFO | train | epoch 041 | loss 3.537 | ppl 11.61 | wps 4194.2 | ups 6.3 | wpb 666.1 | bsz 31.8 | num_updates 2459 | lr 2.69862e-05 | gnorm 2.84 | clip 100 | loss_scale 64 | train_wall 5 | gb_free 20.8 | wall 401
2022-02-28 00:09:17 | INFO | fairseq.trainer | begin training epoch 42
2022-02-28 00:09:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:09:18 | INFO | train_inner | epoch 042:      1 / 60 loss=3.672, ppl=12.75, wps=1196, ups=1.75, wpb=684.8, bsz=32, num_updates=2460, lr=2.69846e-05, gnorm=2.778, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=401
2022-02-28 00:09:18 | INFO | train_inner | epoch 042:     11 / 60 loss=3.057, ppl=8.32, wps=9230.9, ups=15.03, wpb=614.3, bsz=32, num_updates=2470, lr=2.69692e-05, gnorm=2.676, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=402
2022-02-28 00:09:19 | INFO | train_inner | epoch 042:     21 / 60 loss=4.188, ppl=18.22, wps=11284.8, ups=14.47, wpb=779.7, bsz=31, num_updates=2480, lr=2.69538e-05, gnorm=2.78, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=403
2022-02-28 00:09:20 | INFO | train_inner | epoch 042:     31 / 60 loss=3.393, ppl=10.5, wps=9816.7, ups=14.84, wpb=661.5, bsz=32, num_updates=2490, lr=2.69385e-05, gnorm=2.886, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=403
2022-02-28 00:09:20 | INFO | train_inner | epoch 042:     41 / 60 loss=3.151, ppl=8.88, wps=9473.8, ups=15.9, wpb=595.8, bsz=32, num_updates=2500, lr=2.69231e-05, gnorm=2.772, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=404
2022-02-28 00:09:21 | INFO | train_inner | epoch 042:     51 / 60 loss=3.155, ppl=8.91, wps=8401.4, ups=14.65, wpb=573.5, bsz=32, num_updates=2510, lr=2.69077e-05, gnorm=3.324, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=405
2022-02-28 00:09:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:09:22 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.357 | ppl 20.5 | wps 26790.3 | wpb 653.8 | bsz 31.5 | num_updates 2519 | best_loss 4.357
2022-02-28 00:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2519 updates
2022-02-28 00:09:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 42 @ 2519 updates, score 4.357) (writing took 5.16624264602433 seconds)
2022-02-28 00:09:27 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-02-28 00:09:27 | INFO | train | epoch 042 | loss 3.444 | ppl 10.88 | wps 3997.9 | ups 6 | wpb 666.1 | bsz 31.8 | num_updates 2519 | lr 2.68938e-05 | gnorm 2.871 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 411
2022-02-28 00:09:27 | INFO | fairseq.trainer | begin training epoch 43
2022-02-28 00:09:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:09:28 | INFO | train_inner | epoch 043:      1 / 60 loss=3.525, ppl=11.51, wps=1121.2, ups=1.5, wpb=746.3, bsz=32, num_updates=2520, lr=2.68923e-05, gnorm=2.874, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=411
2022-02-28 00:09:28 | INFO | train_inner | epoch 043:     11 / 60 loss=3.565, ppl=11.83, wps=8405.7, ups=14.29, wpb=588.2, bsz=31, num_updates=2530, lr=2.68769e-05, gnorm=2.827, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=412
2022-02-28 00:09:29 | INFO | train_inner | epoch 043:     21 / 60 loss=2.996, ppl=7.98, wps=9318.6, ups=15.77, wpb=590.8, bsz=32, num_updates=2540, lr=2.68615e-05, gnorm=3.105, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=413
2022-02-28 00:09:30 | INFO | train_inner | epoch 043:     31 / 60 loss=3.616, ppl=12.26, wps=12731.6, ups=14.82, wpb=859.3, bsz=32, num_updates=2550, lr=2.68462e-05, gnorm=2.59, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=413
2022-02-28 00:09:30 | INFO | train_inner | epoch 043:     41 / 60 loss=3.005, ppl=8.03, wps=9619.8, ups=15.45, wpb=622.6, bsz=32, num_updates=2560, lr=2.68308e-05, gnorm=2.668, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=414
2022-02-28 00:09:31 | INFO | train_inner | epoch 043:     51 / 60 loss=3.404, ppl=10.59, wps=7820.4, ups=12.66, wpb=617.5, bsz=32, num_updates=2570, lr=2.68154e-05, gnorm=2.652, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=415
2022-02-28 00:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:09:32 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.245 | ppl 18.96 | wps 26298.6 | wpb 653.8 | bsz 31.5 | num_updates 2579 | best_loss 4.245
2022-02-28 00:09:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2579 updates
2022-02-28 00:09:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 43 @ 2579 updates, score 4.245) (writing took 4.032440350012621 seconds)
2022-02-28 00:09:37 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-02-28 00:09:37 | INFO | train | epoch 043 | loss 3.355 | ppl 10.23 | wps 4407.6 | ups 6.62 | wpb 666.1 | bsz 31.8 | num_updates 2579 | lr 2.68015e-05 | gnorm 2.77 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 420
2022-02-28 00:09:37 | INFO | fairseq.trainer | begin training epoch 44
2022-02-28 00:09:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:09:37 | INFO | train_inner | epoch 044:      1 / 60 loss=3.466, ppl=11.05, wps=1387, ups=1.78, wpb=777.6, bsz=32, num_updates=2580, lr=2.68e-05, gnorm=2.701, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=420
2022-02-28 00:09:37 | INFO | train_inner | epoch 044:     11 / 60 loss=3.207, ppl=9.23, wps=9945.2, ups=14.74, wpb=674.8, bsz=32, num_updates=2590, lr=2.67846e-05, gnorm=2.784, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=421
2022-02-28 00:09:38 | INFO | train_inner | epoch 044:     21 / 60 loss=3.944, ppl=15.39, wps=12166.3, ups=14.48, wpb=840, bsz=31, num_updates=2600, lr=2.67692e-05, gnorm=2.654, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=422
2022-02-28 00:09:39 | INFO | train_inner | epoch 044:     31 / 60 loss=3.235, ppl=9.41, wps=10478.4, ups=16.06, wpb=652.4, bsz=32, num_updates=2610, lr=2.67538e-05, gnorm=2.683, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=422
2022-02-28 00:09:39 | INFO | train_inner | epoch 044:     41 / 60 loss=2.832, ppl=7.12, wps=8437.1, ups=16.13, wpb=523.1, bsz=32, num_updates=2620, lr=2.67385e-05, gnorm=2.842, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=423
2022-02-28 00:09:40 | INFO | train_inner | epoch 044:     51 / 60 loss=3.145, ppl=8.85, wps=8035.4, ups=12.44, wpb=646.1, bsz=32, num_updates=2630, lr=2.67231e-05, gnorm=2.77, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=424
2022-02-28 00:09:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:09:41 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.17 | ppl 18 | wps 25938 | wpb 653.8 | bsz 31.5 | num_updates 2639 | best_loss 4.17
2022-02-28 00:09:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2639 updates
2022-02-28 00:09:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 44 @ 2639 updates, score 4.17) (writing took 4.067429821996484 seconds)
2022-02-28 00:09:46 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-02-28 00:09:46 | INFO | train | epoch 044 | loss 3.265 | ppl 9.61 | wps 4432.5 | ups 6.65 | wpb 666.1 | bsz 31.8 | num_updates 2639 | lr 2.67092e-05 | gnorm 2.766 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 429
2022-02-28 00:09:46 | INFO | fairseq.trainer | begin training epoch 45
2022-02-28 00:09:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:09:46 | INFO | train_inner | epoch 045:      1 / 60 loss=2.864, ppl=7.28, wps=1126.2, ups=1.78, wpb=631.6, bsz=32, num_updates=2640, lr=2.67077e-05, gnorm=2.884, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=429
2022-02-28 00:09:46 | INFO | train_inner | epoch 045:     11 / 60 loss=2.63, ppl=6.19, wps=7170.3, ups=15.46, wpb=463.9, bsz=32, num_updates=2650, lr=2.66923e-05, gnorm=2.904, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=430
2022-02-28 00:09:47 | INFO | train_inner | epoch 045:     21 / 60 loss=3.47, ppl=11.08, wps=11181.8, ups=15.5, wpb=721.2, bsz=32, num_updates=2660, lr=2.66769e-05, gnorm=2.848, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=431
2022-02-28 00:09:48 | INFO | train_inner | epoch 045:     31 / 60 loss=3.528, ppl=11.54, wps=11178.2, ups=15.22, wpb=734.6, bsz=31, num_updates=2670, lr=2.66615e-05, gnorm=2.988, clip=100, loss_scale=64, train_wall=1, gb_free=19.2, wall=431
2022-02-28 00:09:48 | INFO | train_inner | epoch 045:     41 / 60 loss=3.069, ppl=8.39, wps=10716.4, ups=15.43, wpb=694.6, bsz=32, num_updates=2680, lr=2.66462e-05, gnorm=2.706, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=432
2022-02-28 00:09:49 | INFO | train_inner | epoch 045:     51 / 60 loss=2.982, ppl=7.9, wps=9922.5, ups=14.43, wpb=687.6, bsz=32, num_updates=2690, lr=2.66308e-05, gnorm=2.77, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=433
2022-02-28 00:09:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:09:50 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 4.129 | ppl 17.5 | wps 27839.4 | wpb 653.8 | bsz 31.5 | num_updates 2699 | best_loss 4.129
2022-02-28 00:09:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2699 updates
2022-02-28 00:09:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:09:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 45 @ 2699 updates, score 4.129) (writing took 3.8589969459862914 seconds)
2022-02-28 00:09:54 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-02-28 00:09:54 | INFO | train | epoch 045 | loss 3.185 | ppl 9.1 | wps 4670.1 | ups 7.01 | wpb 666.1 | bsz 31.8 | num_updates 2699 | lr 2.66169e-05 | gnorm 2.814 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 438
2022-02-28 00:09:54 | INFO | fairseq.trainer | begin training epoch 46
2022-02-28 00:09:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:09:54 | INFO | train_inner | epoch 046:      1 / 60 loss=3.198, ppl=9.18, wps=1275.6, ups=1.89, wpb=673.8, bsz=32, num_updates=2700, lr=2.66154e-05, gnorm=2.655, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=438
2022-02-28 00:09:55 | INFO | train_inner | epoch 046:     11 / 60 loss=3.798, ppl=13.91, wps=11211.1, ups=14.29, wpb=784.8, bsz=31, num_updates=2710, lr=2.66e-05, gnorm=2.542, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=439
2022-02-28 00:09:56 | INFO | train_inner | epoch 046:     21 / 60 loss=3.428, ppl=10.76, wps=13330.9, ups=15.45, wpb=863.1, bsz=32, num_updates=2720, lr=2.65846e-05, gnorm=2.479, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=439
2022-02-28 00:09:56 | INFO | train_inner | epoch 046:     31 / 60 loss=2.683, ppl=6.42, wps=9237.8, ups=16.28, wpb=567.5, bsz=32, num_updates=2730, lr=2.65692e-05, gnorm=2.787, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=440
2022-02-28 00:09:57 | INFO | train_inner | epoch 046:     41 / 60 loss=2.616, ppl=6.13, wps=8550.9, ups=16.21, wpb=527.5, bsz=32, num_updates=2740, lr=2.65538e-05, gnorm=2.698, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=441
2022-02-28 00:09:58 | INFO | train_inner | epoch 046:     51 / 60 loss=2.805, ppl=6.99, wps=8830.8, ups=14.83, wpb=595.6, bsz=32, num_updates=2750, lr=2.65385e-05, gnorm=2.828, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=441
2022-02-28 00:09:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:09:59 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 4.072 | ppl 16.82 | wps 27588.5 | wpb 653.8 | bsz 31.5 | num_updates 2759 | best_loss 4.072
2022-02-28 00:09:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2759 updates
2022-02-28 00:09:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 46 @ 2759 updates, score 4.072) (writing took 4.586690144991735 seconds)
2022-02-28 00:10:03 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-02-28 00:10:03 | INFO | train | epoch 046 | loss 3.096 | ppl 8.55 | wps 4306.6 | ups 6.47 | wpb 666.1 | bsz 31.8 | num_updates 2759 | lr 2.65246e-05 | gnorm 2.715 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 447
2022-02-28 00:10:03 | INFO | fairseq.trainer | begin training epoch 47
2022-02-28 00:10:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:10:04 | INFO | train_inner | epoch 047:      1 / 60 loss=2.868, ppl=7.3, wps=1158.4, ups=1.67, wpb=693.4, bsz=32, num_updates=2760, lr=2.65231e-05, gnorm=2.941, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=447
2022-02-28 00:10:04 | INFO | train_inner | epoch 047:     11 / 60 loss=3.051, ppl=8.29, wps=10609.5, ups=14.79, wpb=717.3, bsz=32, num_updates=2770, lr=2.65077e-05, gnorm=2.767, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=448
2022-02-28 00:10:05 | INFO | train_inner | epoch 047:     21 / 60 loss=2.957, ppl=7.77, wps=9777.9, ups=16.03, wpb=610.1, bsz=32, num_updates=2780, lr=2.64923e-05, gnorm=2.791, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=449
2022-02-28 00:10:05 | INFO | train_inner | epoch 047:     31 / 60 loss=2.728, ppl=6.63, wps=9306.2, ups=16.19, wpb=574.8, bsz=32, num_updates=2790, lr=2.64769e-05, gnorm=2.755, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=449
2022-02-28 00:10:06 | INFO | train_inner | epoch 047:     41 / 60 loss=3.237, ppl=9.43, wps=11885.6, ups=15.52, wpb=765.8, bsz=31, num_updates=2800, lr=2.64615e-05, gnorm=2.593, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=450
2022-02-28 00:10:07 | INFO | train_inner | epoch 047:     51 / 60 loss=3.368, ppl=10.32, wps=10451.9, ups=14, wpb=746.4, bsz=32, num_updates=2810, lr=2.64462e-05, gnorm=2.742, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=451
2022-02-28 00:10:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:10:08 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.991 | ppl 15.9 | wps 29128.7 | wpb 653.8 | bsz 31.5 | num_updates 2819 | best_loss 3.991
2022-02-28 00:10:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 2819 updates
2022-02-28 00:10:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 47 @ 2819 updates, score 3.991) (writing took 4.332034992985427 seconds)
2022-02-28 00:10:12 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-02-28 00:10:12 | INFO | train | epoch 047 | loss 3 | ppl 8 | wps 4442.6 | ups 6.67 | wpb 666.1 | bsz 31.8 | num_updates 2819 | lr 2.64323e-05 | gnorm 2.731 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 456
2022-02-28 00:10:12 | INFO | fairseq.trainer | begin training epoch 48
2022-02-28 00:10:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:10:13 | INFO | train_inner | epoch 048:      1 / 60 loss=2.515, ppl=5.72, wps=1051.2, ups=1.75, wpb=599.8, bsz=32, num_updates=2820, lr=2.64308e-05, gnorm=2.776, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=456
2022-02-28 00:10:13 | INFO | train_inner | epoch 048:     11 / 60 loss=2.796, ppl=6.94, wps=10416.3, ups=15.75, wpb=661.3, bsz=32, num_updates=2830, lr=2.64154e-05, gnorm=2.599, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=457
2022-02-28 00:10:14 | INFO | train_inner | epoch 048:     21 / 60 loss=3.123, ppl=8.71, wps=11562.5, ups=15.18, wpb=761.6, bsz=32, num_updates=2840, lr=2.64e-05, gnorm=2.665, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=458
2022-02-28 00:10:15 | INFO | train_inner | epoch 048:     31 / 60 loss=3.173, ppl=9.02, wps=8765.7, ups=14.75, wpb=594.4, bsz=31, num_updates=2850, lr=2.63846e-05, gnorm=2.663, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=458
2022-02-28 00:10:15 | INFO | train_inner | epoch 048:     41 / 60 loss=3.168, ppl=8.99, wps=11938.5, ups=14.99, wpb=796.4, bsz=32, num_updates=2860, lr=2.63692e-05, gnorm=2.61, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=459
2022-02-28 00:10:16 | INFO | train_inner | epoch 048:     51 / 60 loss=2.51, ppl=5.7, wps=7491.3, ups=13.22, wpb=566.7, bsz=32, num_updates=2870, lr=2.63538e-05, gnorm=2.905, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=460
2022-02-28 00:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:10:17 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.981 | ppl 15.79 | wps 27782.9 | wpb 653.8 | bsz 31.5 | num_updates 2879 | best_loss 3.981
2022-02-28 00:10:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 2879 updates
2022-02-28 00:10:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 48 @ 2879 updates, score 3.981) (writing took 4.262305222015129 seconds)
2022-02-28 00:10:22 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-02-28 00:10:22 | INFO | train | epoch 048 | loss 2.926 | ppl 7.6 | wps 4386.1 | ups 6.58 | wpb 666.1 | bsz 31.8 | num_updates 2879 | lr 2.634e-05 | gnorm 2.713 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 465
2022-02-28 00:10:22 | INFO | fairseq.trainer | begin training epoch 49
2022-02-28 00:10:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:10:22 | INFO | train_inner | epoch 049:      1 / 60 loss=2.668, ppl=6.36, wps=992.6, ups=1.59, wpb=622.9, bsz=32, num_updates=2880, lr=2.63385e-05, gnorm=2.792, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=466
2022-02-28 00:10:23 | INFO | train_inner | epoch 049:     11 / 60 loss=2.397, ppl=5.27, wps=7966.2, ups=15.7, wpb=507.3, bsz=32, num_updates=2890, lr=2.63231e-05, gnorm=2.873, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=467
2022-02-28 00:10:24 | INFO | train_inner | epoch 049:     21 / 60 loss=2.353, ppl=5.11, wps=8200.5, ups=14.98, wpb=547.4, bsz=32, num_updates=2900, lr=2.63077e-05, gnorm=2.66, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=467
2022-02-28 00:10:24 | INFO | train_inner | epoch 049:     31 / 60 loss=2.423, ppl=5.36, wps=9024.7, ups=16.45, wpb=548.5, bsz=32, num_updates=2910, lr=2.62923e-05, gnorm=2.556, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=468
2022-02-28 00:10:25 | INFO | train_inner | epoch 049:     41 / 60 loss=3.309, ppl=9.91, wps=12160.2, ups=14.88, wpb=817.3, bsz=31, num_updates=2920, lr=2.62769e-05, gnorm=2.664, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=469
2022-02-28 00:10:26 | INFO | train_inner | epoch 049:     51 / 60 loss=2.801, ppl=6.97, wps=9034.4, ups=14.18, wpb=637, bsz=32, num_updates=2930, lr=2.62615e-05, gnorm=2.742, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=469
2022-02-28 00:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:10:27 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.926 | ppl 15.21 | wps 28352.4 | wpb 653.8 | bsz 31.5 | num_updates 2939 | best_loss 3.926
2022-02-28 00:10:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 2939 updates
2022-02-28 00:10:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 49 @ 2939 updates, score 3.926) (writing took 4.981254343991168 seconds)
2022-02-28 00:10:32 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-02-28 00:10:32 | INFO | train | epoch 049 | loss 2.858 | ppl 7.25 | wps 3993.5 | ups 6 | wpb 666.1 | bsz 31.8 | num_updates 2939 | lr 2.62477e-05 | gnorm 2.701 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.5 | wall 476
2022-02-28 00:10:32 | INFO | fairseq.trainer | begin training epoch 50
2022-02-28 00:10:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:10:32 | INFO | train_inner | epoch 050:      1 / 60 loss=3.291, ppl=9.79, wps=1438.3, ups=1.56, wpb=924.7, bsz=32, num_updates=2940, lr=2.62462e-05, gnorm=2.717, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=476
2022-02-28 00:10:33 | INFO | train_inner | epoch 050:     11 / 60 loss=2.549, ppl=5.85, wps=10223.9, ups=15.65, wpb=653.3, bsz=32, num_updates=2950, lr=2.62308e-05, gnorm=2.52, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=476
2022-02-28 00:10:33 | INFO | train_inner | epoch 050:     21 / 60 loss=2.915, ppl=7.54, wps=10559.9, ups=15.34, wpb=688.3, bsz=32, num_updates=2960, lr=2.62154e-05, gnorm=2.678, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=477
2022-02-28 00:10:34 | INFO | train_inner | epoch 050:     31 / 60 loss=3.086, ppl=8.49, wps=10735.1, ups=14.47, wpb=741.9, bsz=31, num_updates=2970, lr=2.62e-05, gnorm=2.867, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=478
2022-02-28 00:10:35 | INFO | train_inner | epoch 050:     41 / 60 loss=2.618, ppl=6.14, wps=10107.5, ups=15.8, wpb=639.9, bsz=32, num_updates=2980, lr=2.61846e-05, gnorm=2.89, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=478
2022-02-28 00:10:35 | INFO | train_inner | epoch 050:     51 / 60 loss=2.522, ppl=5.74, wps=8976.1, ups=13.51, wpb=664.5, bsz=32, num_updates=2990, lr=2.61692e-05, gnorm=2.891, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=479
2022-02-28 00:10:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:10:37 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.882 | ppl 14.74 | wps 27501.1 | wpb 653.8 | bsz 31.5 | num_updates 2999 | best_loss 3.882
2022-02-28 00:10:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2999 updates
2022-02-28 00:10:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 50 @ 2999 updates, score 3.882) (writing took 4.4977342679922 seconds)
2022-02-28 00:10:41 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-02-28 00:10:41 | INFO | train | epoch 050 | loss 2.782 | ppl 6.88 | wps 4304.9 | ups 6.46 | wpb 666.1 | bsz 31.8 | num_updates 2999 | lr 2.61554e-05 | gnorm 2.808 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 19.6 | wall 485
2022-02-28 00:10:41 | INFO | fairseq.trainer | begin training epoch 51
2022-02-28 00:10:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:10:41 | INFO | train_inner | epoch 051:      1 / 60 loss=2.883, ppl=7.38, wps=991, ups=1.69, wpb=585, bsz=32, num_updates=3000, lr=2.61538e-05, gnorm=3.009, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=485
2022-02-28 00:10:42 | INFO | train_inner | epoch 051:     11 / 60 loss=2.568, ppl=5.93, wps=9004.6, ups=13.98, wpb=644, bsz=32, num_updates=3010, lr=2.61385e-05, gnorm=2.849, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=486
2022-02-28 00:10:43 | INFO | train_inner | epoch 051:     21 / 60 loss=2.991, ppl=7.95, wps=10358.3, ups=14.77, wpb=701.3, bsz=31, num_updates=3020, lr=2.61231e-05, gnorm=2.699, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=486
2022-02-28 00:10:43 | INFO | train_inner | epoch 051:     31 / 60 loss=2.647, ppl=6.27, wps=10104.9, ups=14.56, wpb=694.1, bsz=32, num_updates=3030, lr=2.61077e-05, gnorm=2.78, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=487
2022-02-28 00:10:44 | INFO | train_inner | epoch 051:     41 / 60 loss=2.937, ppl=7.66, wps=11199.6, ups=15.29, wpb=732.7, bsz=32, num_updates=3040, lr=2.60923e-05, gnorm=2.643, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=488
2022-02-28 00:10:45 | INFO | train_inner | epoch 051:     51 / 60 loss=2.73, ppl=6.63, wps=9158.4, ups=13.82, wpb=662.5, bsz=32, num_updates=3050, lr=2.60769e-05, gnorm=2.965, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=488
2022-02-28 00:10:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:10:46 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.896 | ppl 14.89 | wps 25693.3 | wpb 653.8 | bsz 31.5 | num_updates 3059 | best_loss 3.882
2022-02-28 00:10:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 3059 updates
2022-02-28 00:10:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:10:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 51 @ 3059 updates, score 3.896) (writing took 2.6095207880134694 seconds)
2022-02-28 00:10:49 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-02-28 00:10:49 | INFO | train | epoch 051 | loss 2.715 | ppl 6.57 | wps 5317.7 | ups 7.98 | wpb 666.1 | bsz 31.8 | num_updates 3059 | lr 2.60631e-05 | gnorm 2.771 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 492
2022-02-28 00:10:49 | INFO | fairseq.trainer | begin training epoch 52
2022-02-28 00:10:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:10:49 | INFO | train_inner | epoch 052:      1 / 60 loss=2.282, ppl=4.86, wps=1296.1, ups=2.47, wpb=525.1, bsz=32, num_updates=3060, lr=2.60615e-05, gnorm=2.645, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=493
2022-02-28 00:10:50 | INFO | train_inner | epoch 052:     11 / 60 loss=3.027, ppl=8.15, wps=11021.4, ups=13.11, wpb=840.8, bsz=32, num_updates=3070, lr=2.60462e-05, gnorm=2.832, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=493
2022-02-28 00:10:50 | INFO | train_inner | epoch 052:     21 / 60 loss=2.421, ppl=5.35, wps=10015.4, ups=15.25, wpb=656.7, bsz=32, num_updates=3080, lr=2.60308e-05, gnorm=2.796, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=494
2022-02-28 00:10:51 | INFO | train_inner | epoch 052:     31 / 60 loss=2.594, ppl=6.04, wps=9820.9, ups=14.55, wpb=675, bsz=32, num_updates=3090, lr=2.60154e-05, gnorm=2.862, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=495
2022-02-28 00:10:52 | INFO | train_inner | epoch 052:     41 / 60 loss=2.522, ppl=5.75, wps=9616.8, ups=15.5, wpb=620.6, bsz=32, num_updates=3100, lr=2.6e-05, gnorm=2.695, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=495
2022-02-28 00:10:52 | INFO | train_inner | epoch 052:     51 / 60 loss=2.32, ppl=4.99, wps=8596.7, ups=14.02, wpb=613, bsz=32, num_updates=3110, lr=2.59846e-05, gnorm=2.634, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=496
2022-02-28 00:10:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:10:53 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.806 | ppl 13.98 | wps 31391.7 | wpb 653.8 | bsz 31.5 | num_updates 3119 | best_loss 3.806
2022-02-28 00:10:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 3119 updates
2022-02-28 00:10:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:10:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 52 @ 3119 updates, score 3.806) (writing took 3.909944977000123 seconds)
2022-02-28 00:10:57 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-02-28 00:10:57 | INFO | train | epoch 052 | loss 2.648 | ppl 6.27 | wps 4580.8 | ups 6.88 | wpb 666.1 | bsz 31.8 | num_updates 3119 | lr 2.59708e-05 | gnorm 2.774 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 19.2 | wall 501
2022-02-28 00:10:57 | INFO | fairseq.trainer | begin training epoch 53
2022-02-28 00:10:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:10:58 | INFO | train_inner | epoch 053:      1 / 60 loss=2.907, ppl=7.5, wps=1202.7, ups=1.9, wpb=632.2, bsz=31, num_updates=3120, lr=2.59692e-05, gnorm=2.87, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=501
2022-02-28 00:10:58 | INFO | train_inner | epoch 053:     11 / 60 loss=2.242, ppl=4.73, wps=7740.5, ups=13.78, wpb=561.6, bsz=32, num_updates=3130, lr=2.59538e-05, gnorm=2.659, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=502
2022-02-28 00:10:59 | INFO | train_inner | epoch 053:     21 / 60 loss=2.765, ppl=6.8, wps=9765.5, ups=15.4, wpb=634, bsz=32, num_updates=3140, lr=2.59385e-05, gnorm=2.747, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=503
2022-02-28 00:11:00 | INFO | train_inner | epoch 053:     31 / 60 loss=2.891, ppl=7.42, wps=10406.6, ups=14.72, wpb=706.8, bsz=31, num_updates=3150, lr=2.59231e-05, gnorm=2.7, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=503
2022-02-28 00:11:00 | INFO | train_inner | epoch 053:     41 / 60 loss=2.577, ppl=5.97, wps=10597.6, ups=13.91, wpb=762, bsz=32, num_updates=3160, lr=2.59077e-05, gnorm=2.745, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=504
2022-02-28 00:11:01 | INFO | train_inner | epoch 053:     51 / 60 loss=2.351, ppl=5.1, wps=8674.3, ups=13.05, wpb=664.8, bsz=32, num_updates=3170, lr=2.58923e-05, gnorm=2.584, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=505
2022-02-28 00:11:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:11:02 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.735 | ppl 13.32 | wps 26898.1 | wpb 653.8 | bsz 31.5 | num_updates 3179 | best_loss 3.735
2022-02-28 00:11:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 3179 updates
2022-02-28 00:11:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:11:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:11:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 53 @ 3179 updates, score 3.735) (writing took 3.9000338629994076 seconds)
2022-02-28 00:11:06 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-02-28 00:11:06 | INFO | train | epoch 053 | loss 2.581 | ppl 5.98 | wps 4477.3 | ups 6.72 | wpb 666.1 | bsz 31.8 | num_updates 3179 | lr 2.58785e-05 | gnorm 2.709 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 510
2022-02-28 00:11:06 | INFO | fairseq.trainer | begin training epoch 54
2022-02-28 00:11:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:11:07 | INFO | train_inner | epoch 054:      1 / 60 loss=2.559, ppl=5.89, wps=1183.1, ups=1.85, wpb=641.2, bsz=32, num_updates=3180, lr=2.58769e-05, gnorm=2.783, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=510
2022-02-28 00:11:07 | INFO | train_inner | epoch 054:     11 / 60 loss=2.102, ppl=4.29, wps=8154.2, ups=15.46, wpb=527.5, bsz=32, num_updates=3190, lr=2.58615e-05, gnorm=3.195, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=511
2022-02-28 00:11:08 | INFO | train_inner | epoch 054:     21 / 60 loss=2.711, ppl=6.55, wps=11218.3, ups=14.71, wpb=762.5, bsz=32, num_updates=3200, lr=2.58462e-05, gnorm=2.936, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=512
2022-02-28 00:11:09 | INFO | train_inner | epoch 054:     31 / 60 loss=2.297, ppl=4.91, wps=10892.3, ups=15.7, wpb=693.6, bsz=32, num_updates=3210, lr=2.58308e-05, gnorm=2.975, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=512
2022-02-28 00:11:09 | INFO | train_inner | epoch 054:     41 / 60 loss=2.714, ppl=6.56, wps=7274.2, ups=13.27, wpb=548.1, bsz=31, num_updates=3220, lr=2.58154e-05, gnorm=2.747, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=513
2022-02-28 00:11:10 | INFO | train_inner | epoch 054:     51 / 60 loss=2.844, ppl=7.18, wps=9719.5, ups=11.86, wpb=819.4, bsz=32, num_updates=3230, lr=2.58e-05, gnorm=2.584, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=514
2022-02-28 00:11:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:11:11 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.721 | ppl 13.19 | wps 27385.1 | wpb 653.8 | bsz 31.5 | num_updates 3239 | best_loss 3.721
2022-02-28 00:11:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 3239 updates
2022-02-28 00:11:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:11:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 54 @ 3239 updates, score 3.721) (writing took 3.8555050730064977 seconds)
2022-02-28 00:11:15 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-02-28 00:11:15 | INFO | train | epoch 054 | loss 2.523 | ppl 5.75 | wps 4488.2 | ups 6.74 | wpb 666.1 | bsz 31.8 | num_updates 3239 | lr 2.57862e-05 | gnorm 2.84 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 519
2022-02-28 00:11:15 | INFO | fairseq.trainer | begin training epoch 55
2022-02-28 00:11:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:11:15 | INFO | train_inner | epoch 055:      1 / 60 loss=2.327, ppl=5.02, wps=1192.1, ups=1.87, wpb=636.2, bsz=32, num_updates=3240, lr=2.57846e-05, gnorm=2.622, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=519
2022-02-28 00:11:16 | INFO | train_inner | epoch 055:     11 / 60 loss=2.688, ppl=6.44, wps=9145.5, ups=14, wpb=653.3, bsz=31, num_updates=3250, lr=2.57692e-05, gnorm=2.664, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=520
2022-02-28 00:11:17 | INFO | train_inner | epoch 055:     21 / 60 loss=2.218, ppl=4.65, wps=9498.4, ups=16.31, wpb=582.4, bsz=32, num_updates=3260, lr=2.57538e-05, gnorm=2.756, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=521
2022-02-28 00:11:17 | INFO | train_inner | epoch 055:     31 / 60 loss=2.438, ppl=5.42, wps=10996.5, ups=15.63, wpb=703.6, bsz=32, num_updates=3270, lr=2.57385e-05, gnorm=2.63, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=521
2022-02-28 00:11:18 | INFO | train_inner | epoch 055:     41 / 60 loss=2.186, ppl=4.55, wps=9705.4, ups=16.03, wpb=605.5, bsz=32, num_updates=3280, lr=2.57231e-05, gnorm=2.684, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=522
2022-02-28 00:11:19 | INFO | train_inner | epoch 055:     51 / 60 loss=2.668, ppl=6.36, wps=9712.6, ups=12.94, wpb=750.5, bsz=32, num_updates=3290, lr=2.57077e-05, gnorm=2.527, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=523
2022-02-28 00:11:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:11:20 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.737 | ppl 13.33 | wps 26404.1 | wpb 653.8 | bsz 31.5 | num_updates 3299 | best_loss 3.721
2022-02-28 00:11:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 3299 updates
2022-02-28 00:11:20 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:11:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 55 @ 3299 updates, score 3.737) (writing took 2.748850862000836 seconds)
2022-02-28 00:11:23 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-02-28 00:11:23 | INFO | train | epoch 055 | loss 2.459 | ppl 5.5 | wps 5257.5 | ups 7.89 | wpb 666.1 | bsz 31.8 | num_updates 3299 | lr 2.56938e-05 | gnorm 2.666 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 527
2022-02-28 00:11:23 | INFO | fairseq.trainer | begin training epoch 56
2022-02-28 00:11:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:11:23 | INFO | train_inner | epoch 056:      1 / 60 loss=2.463, ppl=5.51, wps=1689.5, ups=2.38, wpb=710.8, bsz=32, num_updates=3300, lr=2.56923e-05, gnorm=2.733, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=527
2022-02-28 00:11:24 | INFO | train_inner | epoch 056:     11 / 60 loss=2.204, ppl=4.61, wps=9308.3, ups=15.38, wpb=605.4, bsz=32, num_updates=3310, lr=2.56769e-05, gnorm=2.923, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=527
2022-02-28 00:11:24 | INFO | train_inner | epoch 056:     21 / 60 loss=2.456, ppl=5.49, wps=10173.2, ups=13.69, wpb=742.9, bsz=32, num_updates=3320, lr=2.56615e-05, gnorm=2.493, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=528
2022-02-28 00:11:25 | INFO | train_inner | epoch 056:     31 / 60 loss=2.468, ppl=5.53, wps=9884.4, ups=14.86, wpb=665.3, bsz=32, num_updates=3330, lr=2.56462e-05, gnorm=2.571, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=529
2022-02-28 00:11:26 | INFO | train_inner | epoch 056:     41 / 60 loss=2.367, ppl=5.16, wps=10551.9, ups=15.7, wpb=672, bsz=32, num_updates=3340, lr=2.56308e-05, gnorm=2.687, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=529
2022-02-28 00:11:27 | INFO | train_inner | epoch 056:     51 / 60 loss=2.689, ppl=6.45, wps=9719.7, ups=12.78, wpb=760.7, bsz=31, num_updates=3350, lr=2.56154e-05, gnorm=2.841, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=530
2022-02-28 00:11:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:11:28 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.654 | ppl 12.59 | wps 24953.3 | wpb 653.8 | bsz 31.5 | num_updates 3359 | best_loss 3.654
2022-02-28 00:11:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 3359 updates
2022-02-28 00:11:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:11:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:11:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 56 @ 3359 updates, score 3.654) (writing took 4.164867270999821 seconds)
2022-02-28 00:11:32 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-02-28 00:11:32 | INFO | train | epoch 056 | loss 2.399 | ppl 5.27 | wps 4368.3 | ups 6.56 | wpb 666.1 | bsz 31.8 | num_updates 3359 | lr 2.56015e-05 | gnorm 2.687 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 536
2022-02-28 00:11:32 | INFO | fairseq.trainer | begin training epoch 57
2022-02-28 00:11:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:11:32 | INFO | train_inner | epoch 057:      1 / 60 loss=2.214, ppl=4.64, wps=1076.9, ups=1.75, wpb=614, bsz=32, num_updates=3360, lr=2.56e-05, gnorm=2.618, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=536
2022-02-28 00:11:33 | INFO | train_inner | epoch 057:     11 / 60 loss=1.862, ppl=3.63, wps=7473.4, ups=14.38, wpb=519.6, bsz=32, num_updates=3370, lr=2.55846e-05, gnorm=2.68, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=537
2022-02-28 00:11:34 | INFO | train_inner | epoch 057:     21 / 60 loss=2.086, ppl=4.25, wps=10398.8, ups=14.93, wpb=696.5, bsz=32, num_updates=3380, lr=2.55692e-05, gnorm=2.705, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=537
2022-02-28 00:11:34 | INFO | train_inner | epoch 057:     31 / 60 loss=2.269, ppl=4.82, wps=10121.7, ups=14.09, wpb=718.5, bsz=32, num_updates=3390, lr=2.55538e-05, gnorm=2.666, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=538
2022-02-28 00:11:35 | INFO | train_inner | epoch 057:     41 / 60 loss=2.147, ppl=4.43, wps=7019.1, ups=12.35, wpb=568.3, bsz=32, num_updates=3400, lr=2.55385e-05, gnorm=2.635, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=539
2022-02-28 00:11:36 | INFO | train_inner | epoch 057:     51 / 60 loss=2.938, ppl=7.67, wps=9761.9, ups=12.06, wpb=809.5, bsz=31, num_updates=3410, lr=2.55231e-05, gnorm=2.779, clip=100, loss_scale=64, train_wall=1, gb_free=19.2, wall=540
2022-02-28 00:11:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:11:37 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 3.62 | ppl 12.3 | wps 28046.4 | wpb 653.8 | bsz 31.5 | num_updates 3419 | best_loss 3.62
2022-02-28 00:11:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 3419 updates
2022-02-28 00:11:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:11:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:11:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 57 @ 3419 updates, score 3.62) (writing took 4.231816499988781 seconds)
2022-02-28 00:11:41 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-02-28 00:11:41 | INFO | train | epoch 057 | loss 2.354 | ppl 5.11 | wps 4283.2 | ups 6.43 | wpb 666.1 | bsz 31.8 | num_updates 3419 | lr 2.55092e-05 | gnorm 2.706 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 545
2022-02-28 00:11:41 | INFO | fairseq.trainer | begin training epoch 58
2022-02-28 00:11:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:11:42 | INFO | train_inner | epoch 058:      1 / 60 loss=2.521, ppl=5.74, wps=1097.9, ups=1.79, wpb=613.2, bsz=32, num_updates=3420, lr=2.55077e-05, gnorm=2.762, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=545
2022-02-28 00:11:42 | INFO | train_inner | epoch 058:     11 / 60 loss=2.476, ppl=5.57, wps=10062.9, ups=13.73, wpb=732.7, bsz=31, num_updates=3430, lr=2.54923e-05, gnorm=2.74, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=546
2022-02-28 00:11:43 | INFO | train_inner | epoch 058:     21 / 60 loss=2.413, ppl=5.33, wps=10269.7, ups=14.83, wpb=692.3, bsz=32, num_updates=3440, lr=2.54769e-05, gnorm=2.951, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=547
2022-02-28 00:11:44 | INFO | train_inner | epoch 058:     31 / 60 loss=2.312, ppl=4.96, wps=11908.6, ups=15.73, wpb=756.9, bsz=32, num_updates=3450, lr=2.54615e-05, gnorm=2.798, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=547
2022-02-28 00:11:44 | INFO | train_inner | epoch 058:     41 / 60 loss=1.926, ppl=3.8, wps=8635.8, ups=15.92, wpb=542.3, bsz=32, num_updates=3460, lr=2.54462e-05, gnorm=2.785, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=548
2022-02-28 00:11:45 | INFO | train_inner | epoch 058:     51 / 60 loss=2.544, ppl=5.83, wps=10044, ups=13.09, wpb=767.1, bsz=32, num_updates=3470, lr=2.54308e-05, gnorm=2.767, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=549
2022-02-28 00:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:11:46 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 3.6 | ppl 12.12 | wps 27179.7 | wpb 653.8 | bsz 31.5 | num_updates 3479 | best_loss 3.6
2022-02-28 00:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 3479 updates
2022-02-28 00:11:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:11:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:11:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 58 @ 3479 updates, score 3.6) (writing took 4.11903214402264 seconds)
2022-02-28 00:11:50 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-02-28 00:11:51 | INFO | train | epoch 058 | loss 2.29 | ppl 4.89 | wps 4469.1 | ups 6.71 | wpb 666.1 | bsz 31.8 | num_updates 3479 | lr 2.54169e-05 | gnorm 2.778 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 554
2022-02-28 00:11:51 | INFO | fairseq.trainer | begin training epoch 59
2022-02-28 00:11:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:11:51 | INFO | train_inner | epoch 059:      1 / 60 loss=1.829, ppl=3.55, wps=878.9, ups=1.58, wpb=556, bsz=32, num_updates=3480, lr=2.54154e-05, gnorm=2.628, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=555
2022-02-28 00:11:52 | INFO | train_inner | epoch 059:     11 / 60 loss=2.22, ppl=4.66, wps=10372.9, ups=15.47, wpb=670.4, bsz=32, num_updates=3490, lr=2.54e-05, gnorm=2.645, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=556
2022-02-28 00:11:53 | INFO | train_inner | epoch 059:     21 / 60 loss=2.223, ppl=4.67, wps=10728.7, ups=15.47, wpb=693.4, bsz=32, num_updates=3500, lr=2.53846e-05, gnorm=2.533, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=556
2022-02-28 00:11:53 | INFO | train_inner | epoch 059:     31 / 60 loss=1.99, ppl=3.97, wps=10193.5, ups=16.08, wpb=633.9, bsz=32, num_updates=3510, lr=2.53692e-05, gnorm=2.645, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=557
2022-02-28 00:11:54 | INFO | train_inner | epoch 059:     41 / 60 loss=2.536, ppl=5.8, wps=12458.8, ups=15.27, wpb=815.7, bsz=31, num_updates=3520, lr=2.53538e-05, gnorm=2.59, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=558
2022-02-28 00:11:55 | INFO | train_inner | epoch 059:     51 / 60 loss=1.981, ppl=3.95, wps=8322, ups=15.26, wpb=545.4, bsz=32, num_updates=3530, lr=2.53385e-05, gnorm=2.741, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=558
2022-02-28 00:11:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:11:56 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 3.622 | ppl 12.31 | wps 27969.9 | wpb 653.8 | bsz 31.5 | num_updates 3539 | best_loss 3.6
2022-02-28 00:11:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 3539 updates
2022-02-28 00:11:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:11:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 59 @ 3539 updates, score 3.622) (writing took 2.6021365610067733 seconds)
2022-02-28 00:11:58 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-02-28 00:11:58 | INFO | train | epoch 059 | loss 2.234 | ppl 4.71 | wps 5566.2 | ups 8.36 | wpb 666.1 | bsz 31.8 | num_updates 3539 | lr 2.53246e-05 | gnorm 2.659 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 562
2022-02-28 00:11:58 | INFO | fairseq.trainer | begin training epoch 60
2022-02-28 00:11:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:11:58 | INFO | train_inner | epoch 060:      1 / 60 loss=2.343, ppl=5.08, wps=1548.7, ups=2.54, wpb=609.9, bsz=32, num_updates=3540, lr=2.53231e-05, gnorm=2.79, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=562
2022-02-28 00:11:59 | INFO | train_inner | epoch 060:     11 / 60 loss=1.839, ppl=3.58, wps=8771.2, ups=15.29, wpb=573.6, bsz=32, num_updates=3550, lr=2.53077e-05, gnorm=2.664, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=563
2022-02-28 00:12:00 | INFO | train_inner | epoch 060:     21 / 60 loss=1.946, ppl=3.85, wps=9442.1, ups=15.59, wpb=605.7, bsz=32, num_updates=3560, lr=2.52923e-05, gnorm=2.762, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=564
2022-02-28 00:12:00 | INFO | train_inner | epoch 060:     31 / 60 loss=2.048, ppl=4.13, wps=10439.9, ups=15.19, wpb=687.5, bsz=32, num_updates=3570, lr=2.52769e-05, gnorm=2.969, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=564
2022-02-28 00:12:01 | INFO | train_inner | epoch 060:     41 / 60 loss=2.478, ppl=5.57, wps=12087.5, ups=15.19, wpb=796, bsz=31, num_updates=3580, lr=2.52615e-05, gnorm=2.396, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=565
2022-02-28 00:12:02 | INFO | train_inner | epoch 060:     51 / 60 loss=2.523, ppl=5.75, wps=9752.3, ups=15.09, wpb=646.4, bsz=32, num_updates=3590, lr=2.52462e-05, gnorm=2.777, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=566
2022-02-28 00:12:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:12:03 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 3.58 | ppl 11.96 | wps 24785.5 | wpb 653.8 | bsz 31.5 | num_updates 3599 | best_loss 3.58
2022-02-28 00:12:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 3599 updates
2022-02-28 00:12:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:12:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:12:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 60 @ 3599 updates, score 3.58) (writing took 3.90143022799748 seconds)
2022-02-28 00:12:07 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-02-28 00:12:07 | INFO | train | epoch 060 | loss 2.202 | ppl 4.6 | wps 4651.5 | ups 6.98 | wpb 666.1 | bsz 31.8 | num_updates 3599 | lr 2.52323e-05 | gnorm 2.757 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 571
2022-02-28 00:12:07 | INFO | fairseq.trainer | begin training epoch 61
2022-02-28 00:12:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:12:07 | INFO | train_inner | epoch 061:      1 / 60 loss=2.315, ppl=4.98, wps=1372.4, ups=1.86, wpb=736, bsz=32, num_updates=3600, lr=2.52308e-05, gnorm=3.034, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=571
2022-02-28 00:12:08 | INFO | train_inner | epoch 061:     11 / 60 loss=2.099, ppl=4.28, wps=10088.5, ups=14.85, wpb=679.3, bsz=32, num_updates=3610, lr=2.52154e-05, gnorm=2.613, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=572
2022-02-28 00:12:08 | INFO | train_inner | epoch 061:     21 / 60 loss=2.254, ppl=4.77, wps=10014.9, ups=16.14, wpb=620.6, bsz=32, num_updates=3620, lr=2.52e-05, gnorm=2.603, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=572
2022-02-28 00:12:09 | INFO | train_inner | epoch 061:     31 / 60 loss=2.465, ppl=5.52, wps=12843.9, ups=15.2, wpb=844.9, bsz=31, num_updates=3630, lr=2.51846e-05, gnorm=2.712, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=573
2022-02-28 00:12:10 | INFO | train_inner | epoch 061:     41 / 60 loss=2.11, ppl=4.32, wps=9414.7, ups=15.59, wpb=603.8, bsz=32, num_updates=3640, lr=2.51692e-05, gnorm=2.634, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=573
2022-02-28 00:12:10 | INFO | train_inner | epoch 061:     51 / 60 loss=1.99, ppl=3.97, wps=9125.2, ups=14.04, wpb=649.9, bsz=32, num_updates=3650, lr=2.51538e-05, gnorm=2.804, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=574
2022-02-28 00:12:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:12:12 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 3.595 | ppl 12.08 | wps 25643.6 | wpb 653.8 | bsz 31.5 | num_updates 3659 | best_loss 3.58
2022-02-28 00:12:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3659 updates
2022-02-28 00:12:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:12:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:12:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 61 @ 3659 updates, score 3.595) (writing took 2.6053760270006023 seconds)
2022-02-28 00:12:14 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-02-28 00:12:14 | INFO | train | epoch 061 | loss 2.143 | ppl 4.42 | wps 5446.5 | ups 8.18 | wpb 666.1 | bsz 31.8 | num_updates 3659 | lr 2.514e-05 | gnorm 2.622 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 578
2022-02-28 00:12:14 | INFO | fairseq.trainer | begin training epoch 62
2022-02-28 00:12:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:12:14 | INFO | train_inner | epoch 062:      1 / 60 loss=1.696, ppl=3.24, wps=1306.8, ups=2.5, wpb=522.5, bsz=32, num_updates=3660, lr=2.51385e-05, gnorm=2.287, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=578
2022-02-28 00:12:15 | INFO | train_inner | epoch 062:     11 / 60 loss=1.856, ppl=3.62, wps=9037.7, ups=14.75, wpb=612.7, bsz=32, num_updates=3670, lr=2.51231e-05, gnorm=2.906, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=579
2022-02-28 00:12:16 | INFO | train_inner | epoch 062:     21 / 60 loss=1.942, ppl=3.84, wps=9843.5, ups=16.25, wpb=605.6, bsz=32, num_updates=3680, lr=2.51077e-05, gnorm=2.687, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=580
2022-02-28 00:12:16 | INFO | train_inner | epoch 062:     31 / 60 loss=2.16, ppl=4.47, wps=10403.5, ups=14.93, wpb=696.9, bsz=32, num_updates=3690, lr=2.50923e-05, gnorm=2.554, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=580
2022-02-28 00:12:17 | INFO | train_inner | epoch 062:     41 / 60 loss=1.96, ppl=3.89, wps=11495.7, ups=15.45, wpb=744.3, bsz=32, num_updates=3700, lr=2.50769e-05, gnorm=2.59, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=581
2022-02-28 00:12:18 | INFO | train_inner | epoch 062:     51 / 60 loss=2.656, ppl=6.3, wps=10420, ups=12.43, wpb=838.6, bsz=31, num_updates=3710, lr=2.50615e-05, gnorm=2.998, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=582
2022-02-28 00:12:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:12:19 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 3.513 | ppl 11.42 | wps 24583.8 | wpb 653.8 | bsz 31.5 | num_updates 3719 | best_loss 3.513
2022-02-28 00:12:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 3719 updates
2022-02-28 00:12:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:12:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:12:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 62 @ 3719 updates, score 3.513) (writing took 5.384485079994192 seconds)
2022-02-28 00:12:25 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-02-28 00:12:25 | INFO | train | epoch 062 | loss 2.093 | ppl 4.27 | wps 3856.1 | ups 5.79 | wpb 666.1 | bsz 31.8 | num_updates 3719 | lr 2.50477e-05 | gnorm 2.745 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 588
2022-02-28 00:12:25 | INFO | fairseq.trainer | begin training epoch 63
2022-02-28 00:12:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:12:25 | INFO | train_inner | epoch 063:      1 / 60 loss=1.743, ppl=3.35, wps=723, ups=1.44, wpb=503.2, bsz=32, num_updates=3720, lr=2.50462e-05, gnorm=2.773, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=589
2022-02-28 00:12:26 | INFO | train_inner | epoch 063:     11 / 60 loss=1.775, ppl=3.42, wps=8325.9, ups=14.6, wpb=570.1, bsz=32, num_updates=3730, lr=2.50308e-05, gnorm=2.559, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=589
2022-02-28 00:12:26 | INFO | train_inner | epoch 063:     21 / 60 loss=2.223, ppl=4.67, wps=11887.8, ups=15.34, wpb=774.8, bsz=32, num_updates=3740, lr=2.50154e-05, gnorm=2.414, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=590
2022-02-28 00:12:27 | INFO | train_inner | epoch 063:     31 / 60 loss=1.573, ppl=2.98, wps=7034.3, ups=15.59, wpb=451.3, bsz=32, num_updates=3750, lr=2.5e-05, gnorm=2.366, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=591
2022-02-28 00:12:27 | INFO | train_inner | epoch 063:     41 / 60 loss=2.058, ppl=4.16, wps=12625.5, ups=15.6, wpb=809.3, bsz=32, num_updates=3760, lr=2.49846e-05, gnorm=2.72, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=591
2022-02-28 00:12:28 | INFO | train_inner | epoch 063:     51 / 60 loss=1.93, ppl=3.81, wps=9034, ups=13.98, wpb=646.2, bsz=32, num_updates=3770, lr=2.49692e-05, gnorm=2.504, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=592
2022-02-28 00:12:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:12:30 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 3.482 | ppl 11.17 | wps 28259.8 | wpb 653.8 | bsz 31.5 | num_updates 3779 | best_loss 3.482
2022-02-28 00:12:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 3779 updates
2022-02-28 00:12:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:12:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-02-28 00:12:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 63 @ 3779 updates, score 3.482) (writing took 4.0407666970277205 seconds)
2022-02-28 00:12:34 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-02-28 00:12:34 | INFO | train | epoch 063 | loss 2.047 | ppl 4.13 | wps 4523.7 | ups 6.79 | wpb 666.1 | bsz 31.8 | num_updates 3779 | lr 2.49554e-05 | gnorm 2.562 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.8 | wall 597
2022-02-28 00:12:34 | INFO | fairseq.trainer | begin training epoch 64
2022-02-28 00:12:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:12:34 | INFO | train_inner | epoch 064:      1 / 60 loss=2.434, ppl=5.4, wps=1360.9, ups=1.82, wpb=746.8, bsz=31, num_updates=3780, lr=2.49538e-05, gnorm=2.824, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=597
2022-02-28 00:12:34 | INFO | train_inner | epoch 064:     11 / 60 loss=2.339, ppl=5.06, wps=10790.9, ups=14.84, wpb=727.3, bsz=32, num_updates=3790, lr=2.49385e-05, gnorm=2.681, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=598
2022-02-28 00:12:35 | INFO | train_inner | epoch 064:     21 / 60 loss=2.292, ppl=4.9, wps=10643.3, ups=13.88, wpb=766.6, bsz=31, num_updates=3800, lr=2.49231e-05, gnorm=2.482, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=599
2022-02-28 00:12:36 | INFO | train_inner | epoch 064:     31 / 60 loss=1.976, ppl=3.93, wps=9999.4, ups=14.98, wpb=667.3, bsz=32, num_updates=3810, lr=2.49077e-05, gnorm=2.516, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=600
2022-02-28 00:12:36 | INFO | train_inner | epoch 064:     41 / 60 loss=1.684, ppl=3.21, wps=9309.4, ups=16.37, wpb=568.8, bsz=32, num_updates=3820, lr=2.48923e-05, gnorm=2.614, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=600
2022-02-28 00:12:37 | INFO | train_inner | epoch 064:     51 / 60 loss=1.626, ppl=3.09, wps=7981.5, ups=13.3, wpb=600.2, bsz=32, num_updates=3830, lr=2.48769e-05, gnorm=2.66, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=601
2022-02-28 00:12:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:12:38 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 3.498 | ppl 11.3 | wps 28711.7 | wpb 653.8 | bsz 31.5 | num_updates 3839 | best_loss 3.482
2022-02-28 00:12:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 3839 updates
2022-02-28 00:12:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:12:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:12:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 64 @ 3839 updates, score 3.498) (writing took 2.6058876200113446 seconds)
2022-02-28 00:12:41 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-02-28 00:12:41 | INFO | train | epoch 064 | loss 1.985 | ppl 3.96 | wps 5339.3 | ups 8.02 | wpb 666.1 | bsz 31.8 | num_updates 3839 | lr 2.48631e-05 | gnorm 2.6 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.7 | wall 605
2022-02-28 00:12:41 | INFO | fairseq.trainer | begin training epoch 65
2022-02-28 00:12:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 00:12:41 | INFO | train_inner | epoch 065:      1 / 60 loss=2.236, ppl=4.71, wps=1902, ups=2.46, wpb=773.8, bsz=31, num_updates=3840, lr=2.48615e-05, gnorm=2.672, clip=100, loss_scale=64, train_wall=1, gb_free=19.2, wall=605
2022-02-28 00:12:42 | INFO | train_inner | epoch 065:     11 / 60 loss=1.591, ppl=3.01, wps=8587.7, ups=15.96, wpb=538.2, bsz=32, num_updates=3850, lr=2.48462e-05, gnorm=2.574, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=606
2022-02-28 00:12:42 | INFO | train_inner | epoch 065:     21 / 60 loss=1.669, ppl=3.18, wps=9001.1, ups=16.22, wpb=554.9, bsz=32, num_updates=3860, lr=2.48308e-05, gnorm=2.465, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=606
2022-02-28 00:12:43 | INFO | train_inner | epoch 065:     31 / 60 loss=2.213, ppl=4.64, wps=13147.2, ups=14.95, wpb=879.4, bsz=32, num_updates=3870, lr=2.48154e-05, gnorm=2.479, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=607
2022-02-28 00:12:44 | INFO | train_inner | epoch 065:     41 / 60 loss=2.074, ppl=4.21, wps=10588.6, ups=15.73, wpb=673, bsz=32, num_updates=3880, lr=2.48e-05, gnorm=2.442, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=608
2022-02-28 00:12:45 | INFO | train_inner | epoch 065:     51 / 60 loss=1.751, ppl=3.37, wps=8488.3, ups=13.18, wpb=643.9, bsz=32, num_updates=3890, lr=2.47846e-05, gnorm=2.579, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=608
2022-02-28 00:12:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 00:12:46 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 3.551 | ppl 11.72 | wps 25396.8 | wpb 653.8 | bsz 31.5 | num_updates 3899 | best_loss 3.482
2022-02-28 00:12:46 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 2 runs
2022-02-28 00:12:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 3899 updates
2022-02-28 00:12:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:12:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-02-28 00:12:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 65 @ 3899 updates, score 3.551) (writing took 2.6780345250153914 seconds)
2022-02-28 00:12:49 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-02-28 00:12:49 | INFO | train | epoch 065 | loss 1.947 | ppl 3.85 | wps 5299 | ups 7.95 | wpb 666.1 | bsz 31.8 | num_updates 3899 | lr 2.47708e-05 | gnorm 2.522 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.6 | wall 612
2022-02-28 00:12:49 | INFO | fairseq_cli.train | done training in 611.0 seconds
