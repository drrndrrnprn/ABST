2022-01-09 20:49:53 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 10, 'log_format': None, 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 8192, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 5000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 12288, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.abst', 'restore_file': 'bartabst/checkpoints/bart.mlm/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 10, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.1, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=10, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=3.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='aspect_base_denoising', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.1, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=10, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=3.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 20000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-09 20:49:53 | INFO | bartabst.tasks.aspect_base_denoising | dictionary: 51200 types
2022-01-09 20:49:56 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-01-09 20:49:56 | INFO | fairseq_cli.train | task: AspectBaseDenoisingTask
2022-01-09 20:49:56 | INFO | fairseq_cli.train | model: BARTMLModel
2022-01-09 20:49:56 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-01-09 20:49:56 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-01-09 20:49:56 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-09 20:49:56 | INFO | bartabst.data.data_utils | loaded 907 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-bin/valid
2022-01-09 20:49:56 | INFO | bartabst.tasks.aspect_base_denoising | Split: valid, Loaded 907 samples of denoising_dataset
2022-01-09 20:49:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-01-09 20:49:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-09 20:49:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-01-09 20:49:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-09 20:49:59 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-01-09 20:49:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-09 20:49:59 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-09 20:49:59 | INFO | fairseq_cli.train | max tokens per device = 8192 and max sentences per device = 32
2022-01-09 20:49:59 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.mlm/checkpoint_best.pt
2022-01-09 20:50:01 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.mlm/checkpoint_best.pt (epoch 2 @ 0 updates)
2022-01-09 20:50:01 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-09 20:50:03 | INFO | bartabst.data.data_utils | loaded 3,163 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-bin/train
2022-01-09 20:50:03 | INFO | bartabst.tasks.aspect_base_denoising | Split: train, Loaded 3163 samples of denoising_dataset
2022-01-09 20:50:03 | INFO | fairseq.trainer | begin training epoch 1
2022-01-09 20:50:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:50:04 | INFO | train_inner | epoch 001:     10 / 99 loss=13.467, ppl=11320.9, wps=10349.3, ups=11.46, wpb=868.8, bsz=32, num_updates=10, lr=6e-07, gnorm=3.693, clip=100, loss_scale=128, train_wall=1, gb_free=20.1, wall=5
2022-01-09 20:50:05 | INFO | train_inner | epoch 001:     20 / 99 loss=13.528, ppl=11815.4, wps=12392.8, ups=13.39, wpb=925.3, bsz=32, num_updates=20, lr=1.2e-06, gnorm=4.104, clip=100, loss_scale=128, train_wall=1, gb_free=20, wall=5
2022-01-09 20:50:06 | INFO | train_inner | epoch 001:     30 / 99 loss=13.682, ppl=13144.1, wps=15582, ups=12.62, wpb=1234.8, bsz=32, num_updates=30, lr=1.8e-06, gnorm=3.056, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=6
2022-01-09 20:50:07 | INFO | train_inner | epoch 001:     40 / 99 loss=13.289, ppl=10008.2, wps=8895.3, ups=11.1, wpb=801.6, bsz=32, num_updates=40, lr=2.4e-06, gnorm=3.578, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=7
2022-01-09 20:50:08 | INFO | train_inner | epoch 001:     50 / 99 loss=13.338, ppl=10355.9, wps=10106.8, ups=11.3, wpb=894.6, bsz=32, num_updates=50, lr=3e-06, gnorm=3.293, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=8
2022-01-09 20:50:08 | INFO | train_inner | epoch 001:     60 / 99 loss=13.405, ppl=10845.5, wps=14483.7, ups=13.6, wpb=1065.2, bsz=32, num_updates=60, lr=3.6e-06, gnorm=3.079, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=9
2022-01-09 20:50:09 | INFO | train_inner | epoch 001:     70 / 99 loss=13.327, ppl=10274.8, wps=13012.7, ups=12, wpb=1084.5, bsz=31.5, num_updates=70, lr=4.2e-06, gnorm=3.368, clip=100, loss_scale=128, train_wall=1, gb_free=16.9, wall=10
2022-01-09 20:50:10 | INFO | train_inner | epoch 001:     80 / 99 loss=13.135, ppl=8998.36, wps=12958.5, ups=12.98, wpb=998.6, bsz=32, num_updates=80, lr=4.8e-06, gnorm=3.144, clip=100, loss_scale=128, train_wall=1, gb_free=19.7, wall=10
2022-01-09 20:50:11 | INFO | train_inner | epoch 001:     90 / 99 loss=13.19, ppl=9346.28, wps=16128.8, ups=13.15, wpb=1226.8, bsz=32, num_updates=90, lr=5.4e-06, gnorm=2.915, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=11
2022-01-09 20:50:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:50:12 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.599 | ppl 6203.34 | wps 31718.8 | wpb 930.4 | bsz 31.3 | num_updates 99
2022-01-09 20:50:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 99 updates
2022-01-09 20:50:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:50:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:50:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 1 @ 99 updates, score 12.599) (writing took 5.801760675036348 seconds)
2022-01-09 20:50:18 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-09 20:50:18 | INFO | train | epoch 001 | loss 13.328 | ppl 10285.2 | wps 6635.4 | ups 6.62 | wpb 997.9 | bsz 31.9 | num_updates 99 | lr 5.94e-06 | gnorm 3.347 | clip 100 | loss_scale 128 | train_wall 8 | gb_free 20.4 | wall 19
2022-01-09 20:50:18 | INFO | fairseq.trainer | begin training epoch 2
2022-01-09 20:50:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:50:18 | INFO | train_inner | epoch 002:      1 / 99 loss=12.781, ppl=7039.71, wps=1117.2, ups=1.3, wpb=861.6, bsz=32, num_updates=100, lr=6e-06, gnorm=3.192, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=19
2022-01-09 20:50:19 | INFO | train_inner | epoch 002:     11 / 99 loss=12.751, ppl=6891.1, wps=13323.1, ups=13.8, wpb=965.6, bsz=32, num_updates=110, lr=6.6e-06, gnorm=3.154, clip=100, loss_scale=128, train_wall=1, gb_free=19.8, wall=20
2022-01-09 20:50:20 | INFO | train_inner | epoch 002:     21 / 99 loss=12.913, ppl=7713.1, wps=16129.6, ups=12.88, wpb=1251.9, bsz=31.5, num_updates=120, lr=7.2e-06, gnorm=2.932, clip=100, loss_scale=128, train_wall=1, gb_free=20.1, wall=20
2022-01-09 20:50:21 | INFO | train_inner | epoch 002:     31 / 99 loss=12.697, ppl=6640.89, wps=17591.1, ups=13.82, wpb=1272.7, bsz=32, num_updates=130, lr=7.8e-06, gnorm=3.007, clip=100, loss_scale=128, train_wall=1, gb_free=20, wall=21
2022-01-09 20:50:21 | INFO | train_inner | epoch 002:     41 / 99 loss=12.471, ppl=5677.23, wps=14616.7, ups=14.3, wpb=1022, bsz=32, num_updates=140, lr=8.4e-06, gnorm=3.42, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=22
2022-01-09 20:50:22 | INFO | train_inner | epoch 002:     51 / 99 loss=11.85, ppl=3690.34, wps=11473.2, ups=15.86, wpb=723.6, bsz=32, num_updates=150, lr=9e-06, gnorm=3.255, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=22
2022-01-09 20:50:23 | INFO | train_inner | epoch 002:     61 / 99 loss=11.749, ppl=3442.67, wps=11488.4, ups=15.35, wpb=748.3, bsz=32, num_updates=160, lr=9.6e-06, gnorm=3.28, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=23
2022-01-09 20:50:23 | INFO | train_inner | epoch 002:     71 / 99 loss=12.085, ppl=4345.35, wps=14080, ups=13.25, wpb=1062.7, bsz=32, num_updates=170, lr=1.02e-05, gnorm=3.221, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=24
2022-01-09 20:50:24 | INFO | train_inner | epoch 002:     81 / 99 loss=11.874, ppl=3752.46, wps=15070, ups=14.42, wpb=1045, bsz=32, num_updates=180, lr=1.08e-05, gnorm=2.806, clip=100, loss_scale=128, train_wall=1, gb_free=20, wall=25
2022-01-09 20:50:25 | INFO | train_inner | epoch 002:     91 / 99 loss=11.563, ppl=3026.5, wps=11614.5, ups=12.42, wpb=935, bsz=32, num_updates=190, lr=1.14e-05, gnorm=2.947, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=25
2022-01-09 20:50:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:50:26 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.179 | ppl 2319.18 | wps 31591.6 | wpb 930.4 | bsz 31.3 | num_updates 198 | best_loss 11.179
2022-01-09 20:50:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 198 updates
2022-01-09 20:50:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:50:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:50:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 2 @ 198 updates, score 11.179) (writing took 5.089604103937745 seconds)
2022-01-09 20:50:32 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-09 20:50:32 | INFO | train | epoch 002 | loss 12.219 | ppl 4768.73 | wps 7408.8 | ups 7.42 | wpb 997.9 | bsz 31.9 | num_updates 198 | lr 1.188e-05 | gnorm 3.084 | clip 100 | loss_scale 128 | train_wall 7 | gb_free 20.8 | wall 32
2022-01-09 20:50:32 | INFO | fairseq.trainer | begin training epoch 3
2022-01-09 20:50:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:50:32 | INFO | train_inner | epoch 003:      2 / 99 loss=11.344, ppl=2599.53, wps=1189.3, ups=1.44, wpb=825.1, bsz=32, num_updates=200, lr=1.2e-05, gnorm=3.152, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=32
2022-01-09 20:50:33 | INFO | train_inner | epoch 003:     12 / 99 loss=11.51, ppl=2916.82, wps=14617.4, ups=12.72, wpb=1149.6, bsz=31.5, num_updates=210, lr=1.26e-05, gnorm=2.689, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=33
2022-01-09 20:50:33 | INFO | train_inner | epoch 003:     22 / 99 loss=10.925, ppl=1944.26, wps=12549.4, ups=14.85, wpb=845.1, bsz=32, num_updates=220, lr=1.32e-05, gnorm=2.736, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=34
2022-01-09 20:50:34 | INFO | train_inner | epoch 003:     32 / 99 loss=11.016, ppl=2070.12, wps=14149.8, ups=14.02, wpb=1009.4, bsz=32, num_updates=230, lr=1.38e-05, gnorm=2.533, clip=100, loss_scale=128, train_wall=1, gb_free=19.6, wall=34
2022-01-09 20:50:35 | INFO | train_inner | epoch 003:     42 / 99 loss=10.685, ppl=1645.74, wps=13188.4, ups=14.56, wpb=905.6, bsz=32, num_updates=240, lr=1.44e-05, gnorm=2.47, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=35
2022-01-09 20:50:35 | INFO | train_inner | epoch 003:     52 / 99 loss=10.74, ppl=1710.64, wps=14786, ups=14.13, wpb=1046.1, bsz=32, num_updates=250, lr=1.5e-05, gnorm=2.387, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=36
2022-01-09 20:50:36 | INFO | train_inner | epoch 003:     62 / 99 loss=10.436, ppl=1385.07, wps=14661.5, ups=14.86, wpb=986.4, bsz=32, num_updates=260, lr=1.56e-05, gnorm=2.428, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=37
2022-01-09 20:50:37 | INFO | train_inner | epoch 003:     72 / 99 loss=10.507, ppl=1454.71, wps=14488.3, ups=12.61, wpb=1149.2, bsz=32, num_updates=270, lr=1.62e-05, gnorm=2.37, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=37
2022-01-09 20:50:38 | INFO | train_inner | epoch 003:     82 / 99 loss=10.314, ppl=1273.23, wps=16197.7, ups=13.94, wpb=1161.7, bsz=32, num_updates=280, lr=1.68e-05, gnorm=2.344, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=38
2022-01-09 20:50:38 | INFO | train_inner | epoch 003:     92 / 99 loss=9.879, ppl=941.71, wps=12218.9, ups=13.12, wpb=931, bsz=32, num_updates=290, lr=1.74e-05, gnorm=2.314, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=39
2022-01-09 20:50:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:50:40 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.631 | ppl 792.99 | wps 31933.3 | wpb 930.4 | bsz 31.3 | num_updates 297 | best_loss 9.631
2022-01-09 20:50:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 297 updates
2022-01-09 20:50:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:50:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:50:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 3 @ 297 updates, score 9.631) (writing took 4.7539901540149 seconds)
2022-01-09 20:50:45 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-09 20:50:45 | INFO | train | epoch 003 | loss 10.613 | ppl 1566.49 | wps 7574.4 | ups 7.59 | wpb 997.9 | bsz 31.9 | num_updates 297 | lr 1.782e-05 | gnorm 2.506 | clip 100 | loss_scale 128 | train_wall 7 | gb_free 20.2 | wall 45
2022-01-09 20:50:45 | INFO | fairseq.trainer | begin training epoch 4
2022-01-09 20:50:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:50:45 | INFO | train_inner | epoch 004:      3 / 99 loss=9.772, ppl=874.48, wps=1389.2, ups=1.53, wpb=910.6, bsz=32, num_updates=300, lr=1.8e-05, gnorm=2.242, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=45
2022-01-09 20:50:46 | INFO | train_inner | epoch 004:     13 / 99 loss=9.58, ppl=765.32, wps=13405.5, ups=13.26, wpb=1011.1, bsz=32, num_updates=310, lr=1.86e-05, gnorm=2.28, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=46
2022-01-09 20:50:46 | INFO | train_inner | epoch 004:     23 / 99 loss=9.649, ppl=802.95, wps=14443.7, ups=12.63, wpb=1143.8, bsz=32, num_updates=320, lr=1.92e-05, gnorm=2.143, clip=100, loss_scale=128, train_wall=1, gb_free=19.1, wall=47
2022-01-09 20:50:47 | INFO | train_inner | epoch 004:     33 / 99 loss=8.781, ppl=439.79, wps=9834.4, ups=14.2, wpb=692.6, bsz=32, num_updates=330, lr=1.98e-05, gnorm=2.222, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=48
2022-01-09 20:50:48 | INFO | train_inner | epoch 004:     43 / 99 loss=8.892, ppl=475.05, wps=11865.3, ups=14.07, wpb=843.3, bsz=32, num_updates=340, lr=2.04e-05, gnorm=2.166, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=48
2022-01-09 20:50:49 | INFO | train_inner | epoch 004:     53 / 99 loss=8.801, ppl=446.07, wps=12050.5, ups=13.38, wpb=900.6, bsz=32, num_updates=350, lr=2.1e-05, gnorm=2.227, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=49
2022-01-09 20:50:49 | INFO | train_inner | epoch 004:     63 / 99 loss=8.946, ppl=493.31, wps=14721.1, ups=13.26, wpb=1109.8, bsz=32, num_updates=360, lr=2.16e-05, gnorm=2.224, clip=100, loss_scale=128, train_wall=1, gb_free=20.1, wall=50
2022-01-09 20:50:50 | INFO | train_inner | epoch 004:     73 / 99 loss=8.86, ppl=464.64, wps=13729.3, ups=12.24, wpb=1121.6, bsz=31.5, num_updates=370, lr=2.22e-05, gnorm=2.13, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=51
2022-01-09 20:50:51 | INFO | train_inner | epoch 004:     83 / 99 loss=8.83, ppl=455.02, wps=17450.2, ups=12.65, wpb=1379.9, bsz=32, num_updates=380, lr=2.28e-05, gnorm=1.842, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=51
2022-01-09 20:50:52 | INFO | train_inner | epoch 004:     93 / 99 loss=8.176, ppl=289.17, wps=11466, ups=12.89, wpb=889.6, bsz=32, num_updates=390, lr=2.34e-05, gnorm=2.009, clip=100, loss_scale=128, train_wall=1, gb_free=20.2, wall=52
2022-01-09 20:50:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:50:53 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.014 | ppl 258.57 | wps 32526.8 | wpb 930.4 | bsz 31.3 | num_updates 396 | best_loss 8.014
2022-01-09 20:50:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 396 updates
2022-01-09 20:50:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:50:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:50:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 4 @ 396 updates, score 8.014) (writing took 4.312024782062508 seconds)
2022-01-09 20:50:57 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-09 20:50:57 | INFO | train | epoch 004 | loss 8.931 | ppl 488.16 | wps 7691.5 | ups 7.71 | wpb 997.9 | bsz 31.9 | num_updates 396 | lr 2.376e-05 | gnorm 2.127 | clip 100 | loss_scale 128 | train_wall 7 | gb_free 20.5 | wall 58
2022-01-09 20:50:57 | INFO | fairseq.trainer | begin training epoch 5
2022-01-09 20:50:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:50:58 | INFO | train_inner | epoch 005:      4 / 99 loss=8.142, ppl=282.47, wps=1721.8, ups=1.64, wpb=1049.4, bsz=32, num_updates=400, lr=2.4e-05, gnorm=1.853, clip=100, loss_scale=128, train_wall=1, gb_free=19.8, wall=58
2022-01-09 20:50:59 | INFO | train_inner | epoch 005:     14 / 99 loss=7.934, ppl=244.47, wps=13268, ups=14.13, wpb=939.2, bsz=32, num_updates=410, lr=2.46e-05, gnorm=2.145, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=59
2022-01-09 20:50:59 | INFO | train_inner | epoch 005:     24 / 99 loss=8.082, ppl=270.95, wps=15977.4, ups=13.64, wpb=1171.6, bsz=32, num_updates=420, lr=2.52e-05, gnorm=2.04, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=60
2022-01-09 20:51:00 | INFO | train_inner | epoch 005:     34 / 99 loss=7.711, ppl=209.51, wps=12819.3, ups=13.2, wpb=970.8, bsz=32, num_updates=430, lr=2.58e-05, gnorm=1.814, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=61
2022-01-09 20:51:01 | INFO | train_inner | epoch 005:     44 / 99 loss=7.327, ppl=160.58, wps=11590.2, ups=13.16, wpb=880.6, bsz=32, num_updates=440, lr=2.64e-05, gnorm=2.098, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=61
2022-01-09 20:51:01 | INFO | train_inner | epoch 005:     54 / 99 loss=7.101, ppl=137.32, wps=11578, ups=15.11, wpb=766.4, bsz=32, num_updates=450, lr=2.7e-05, gnorm=2.046, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=62
2022-01-09 20:51:02 | INFO | train_inner | epoch 005:     64 / 99 loss=7.423, ppl=171.65, wps=13961.2, ups=14.01, wpb=996.5, bsz=32, num_updates=460, lr=2.76e-05, gnorm=1.718, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=63
2022-01-09 20:51:03 | INFO | train_inner | epoch 005:     74 / 99 loss=6.973, ppl=125.6, wps=11195.3, ups=14.49, wpb=772.6, bsz=32, num_updates=470, lr=2.82e-05, gnorm=1.825, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=63
2022-01-09 20:51:04 | INFO | train_inner | epoch 005:     84 / 99 loss=7.194, ppl=146.41, wps=15052.1, ups=13.48, wpb=1116.3, bsz=32, num_updates=480, lr=2.88e-05, gnorm=1.489, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=64
2022-01-09 20:51:05 | INFO | train_inner | epoch 005:     94 / 99 loss=7.509, ppl=182.1, wps=14798, ups=10.85, wpb=1363.6, bsz=31.5, num_updates=490, lr=2.94e-05, gnorm=1.634, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=65
2022-01-09 20:51:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:51:06 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.906 | ppl 119.94 | wps 30704.2 | wpb 930.4 | bsz 31.3 | num_updates 495 | best_loss 6.906
2022-01-09 20:51:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 495 updates
2022-01-09 20:51:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:51:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:51:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 5 @ 495 updates, score 6.906) (writing took 4.669567574048415 seconds)
2022-01-09 20:51:11 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-09 20:51:11 | INFO | train | epoch 005 | loss 7.521 | ppl 183.73 | wps 7471.1 | ups 7.49 | wpb 997.9 | bsz 31.9 | num_updates 495 | lr 2.97e-05 | gnorm 1.883 | clip 100 | loss_scale 128 | train_wall 7 | gb_free 20.8 | wall 71
2022-01-09 20:51:11 | INFO | fairseq.trainer | begin training epoch 6
2022-01-09 20:51:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:51:11 | INFO | train_inner | epoch 006:      5 / 99 loss=6.739, ppl=106.82, wps=1221.9, ups=1.53, wpb=799.5, bsz=32, num_updates=500, lr=3e-05, gnorm=2.127, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=72
2022-01-09 20:51:12 | INFO | train_inner | epoch 006:     15 / 99 loss=6.889, ppl=118.53, wps=15144.2, ups=13.94, wpb=1086.7, bsz=32, num_updates=510, lr=2.99846e-05, gnorm=1.553, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=72
2022-01-09 20:51:13 | INFO | train_inner | epoch 006:     25 / 99 loss=6.93, ppl=121.95, wps=13898, ups=13.09, wpb=1062, bsz=32, num_updates=520, lr=2.99692e-05, gnorm=1.712, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=73
2022-01-09 20:51:13 | INFO | train_inner | epoch 006:     35 / 99 loss=6.75, ppl=107.64, wps=13028.3, ups=13.53, wpb=962.7, bsz=32, num_updates=530, lr=2.99538e-05, gnorm=1.943, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=74
2022-01-09 20:51:14 | INFO | train_inner | epoch 006:     45 / 99 loss=7.059, ppl=133.34, wps=13779.9, ups=11.52, wpb=1195.8, bsz=31.5, num_updates=540, lr=2.99385e-05, gnorm=1.979, clip=100, loss_scale=128, train_wall=1, gb_free=16.9, wall=75
2022-01-09 20:51:15 | INFO | train_inner | epoch 006:     55 / 99 loss=6.363, ppl=82.31, wps=10443.5, ups=12.3, wpb=849, bsz=32, num_updates=550, lr=2.99231e-05, gnorm=1.829, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=76
2022-01-09 20:51:16 | INFO | train_inner | epoch 006:     65 / 99 loss=6.408, ppl=84.92, wps=11680.6, ups=12.99, wpb=899.2, bsz=32, num_updates=560, lr=2.99077e-05, gnorm=1.805, clip=100, loss_scale=128, train_wall=1, gb_free=19.8, wall=76
2022-01-09 20:51:17 | INFO | train_inner | epoch 006:     75 / 99 loss=6.552, ppl=93.86, wps=13112, ups=13.56, wpb=967, bsz=32, num_updates=570, lr=2.98923e-05, gnorm=1.635, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=77
2022-01-09 20:51:17 | INFO | train_inner | epoch 006:     85 / 99 loss=6.411, ppl=85.09, wps=12807.5, ups=13.33, wpb=960.5, bsz=32, num_updates=580, lr=2.98769e-05, gnorm=1.649, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=78
2022-01-09 20:51:18 | INFO | train_inner | epoch 006:     95 / 99 loss=6.512, ppl=91.24, wps=11118, ups=10.23, wpb=1086.9, bsz=32, num_updates=590, lr=2.98615e-05, gnorm=1.671, clip=100, loss_scale=128, train_wall=1, gb_free=19.6, wall=79
2022-01-09 20:51:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:51:20 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.294 | ppl 78.48 | wps 31938.5 | wpb 930.4 | bsz 31.3 | num_updates 594 | best_loss 6.294
2022-01-09 20:51:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 594 updates
2022-01-09 20:51:20 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:51:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:51:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 6 @ 594 updates, score 6.294) (writing took 4.629685007035732 seconds)
2022-01-09 20:51:24 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-09 20:51:24 | INFO | train | epoch 006 | loss 6.662 | ppl 101.3 | wps 7296.9 | ups 7.31 | wpb 997.9 | bsz 31.9 | num_updates 594 | lr 2.98554e-05 | gnorm 1.78 | clip 100 | loss_scale 128 | train_wall 8 | gb_free 20.8 | wall 85
2022-01-09 20:51:24 | INFO | fairseq.trainer | begin training epoch 7
2022-01-09 20:51:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:51:25 | INFO | train_inner | epoch 007:      6 / 99 loss=6.305, ppl=79.07, wps=1530.8, ups=1.54, wpb=992, bsz=32, num_updates=600, lr=2.98462e-05, gnorm=1.871, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=85
2022-01-09 20:51:26 | INFO | train_inner | epoch 007:     16 / 99 loss=6.03, ppl=65.35, wps=9907.9, ups=11.97, wpb=827.8, bsz=32, num_updates=610, lr=2.98308e-05, gnorm=2.145, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=86
2022-01-09 20:51:27 | INFO | train_inner | epoch 007:     26 / 99 loss=6.462, ppl=88.17, wps=12159.3, ups=10.66, wpb=1140.2, bsz=31.5, num_updates=620, lr=2.98154e-05, gnorm=1.792, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=87
2022-01-09 20:51:27 | INFO | train_inner | epoch 007:     36 / 99 loss=6.256, ppl=76.42, wps=11271.7, ups=10.73, wpb=1050.4, bsz=32, num_updates=630, lr=2.98e-05, gnorm=1.762, clip=100, loss_scale=128, train_wall=1, gb_free=20.3, wall=88
2022-01-09 20:51:28 | INFO | train_inner | epoch 007:     46 / 99 loss=6.402, ppl=84.58, wps=13152.9, ups=11.45, wpb=1148.4, bsz=32, num_updates=640, lr=2.97846e-05, gnorm=1.735, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=89
2022-01-09 20:51:29 | INFO | train_inner | epoch 007:     56 / 99 loss=6.06, ppl=66.74, wps=9581.7, ups=11.27, wpb=850.3, bsz=32, num_updates=650, lr=2.97692e-05, gnorm=2.275, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=90
2022-01-09 20:51:30 | INFO | train_inner | epoch 007:     66 / 99 loss=6.014, ppl=64.62, wps=10722.9, ups=11.97, wpb=895.9, bsz=32, num_updates=660, lr=2.97538e-05, gnorm=1.991, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=91
2022-01-09 20:51:31 | INFO | train_inner | epoch 007:     76 / 99 loss=6.133, ppl=70.17, wps=12050.7, ups=11.15, wpb=1080.4, bsz=32, num_updates=670, lr=2.97385e-05, gnorm=1.713, clip=100, loss_scale=128, train_wall=1, gb_free=20.3, wall=91
2022-01-09 20:51:32 | INFO | train_inner | epoch 007:     86 / 99 loss=6.196, ppl=73.3, wps=12072.5, ups=11.07, wpb=1090.8, bsz=32, num_updates=680, lr=2.97231e-05, gnorm=1.674, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=92
2022-01-09 20:51:33 | INFO | train_inner | epoch 007:     96 / 99 loss=5.733, ppl=53.2, wps=10329.3, ups=11.11, wpb=930.1, bsz=32, num_updates=690, lr=2.97077e-05, gnorm=1.842, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=93
2022-01-09 20:51:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:51:34 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.885 | ppl 59.11 | wps 30891.1 | wpb 930.4 | bsz 31.3 | num_updates 693 | best_loss 5.885
2022-01-09 20:51:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 693 updates
2022-01-09 20:51:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:51:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:51:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 7 @ 693 updates, score 5.885) (writing took 4.665770085994154 seconds)
2022-01-09 20:51:39 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-09 20:51:39 | INFO | train | epoch 007 | loss 6.164 | ppl 71.73 | wps 6822.1 | ups 6.84 | wpb 997.9 | bsz 31.9 | num_updates 693 | lr 2.97031e-05 | gnorm 1.863 | clip 100 | loss_scale 128 | train_wall 9 | gb_free 20.7 | wall 99
2022-01-09 20:51:39 | INFO | fairseq.trainer | begin training epoch 8
2022-01-09 20:51:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:51:39 | INFO | train_inner | epoch 008:      7 / 99 loss=5.865, ppl=58.27, wps=1460.2, ups=1.53, wpb=955.9, bsz=32, num_updates=700, lr=2.96923e-05, gnorm=1.733, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=100
2022-01-09 20:51:40 | INFO | train_inner | epoch 008:     17 / 99 loss=6.082, ppl=67.75, wps=13609.8, ups=12.79, wpb=1064.4, bsz=31.5, num_updates=710, lr=2.96769e-05, gnorm=2.022, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=101
2022-01-09 20:51:41 | INFO | train_inner | epoch 008:     27 / 99 loss=5.897, ppl=59.57, wps=12869.2, ups=13.65, wpb=942.7, bsz=32, num_updates=720, lr=2.96615e-05, gnorm=1.804, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=101
2022-01-09 20:51:42 | INFO | train_inner | epoch 008:     37 / 99 loss=6.019, ppl=64.86, wps=16322.8, ups=14.26, wpb=1144.8, bsz=32, num_updates=730, lr=2.96462e-05, gnorm=1.726, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=102
2022-01-09 20:51:42 | INFO | train_inner | epoch 008:     47 / 99 loss=5.671, ppl=50.96, wps=12697.3, ups=14.47, wpb=877.7, bsz=32, num_updates=740, lr=2.96308e-05, gnorm=1.926, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=103
2022-01-09 20:51:43 | INFO | train_inner | epoch 008:     57 / 99 loss=5.517, ppl=45.78, wps=11778.3, ups=13.74, wpb=857.1, bsz=32, num_updates=750, lr=2.96154e-05, gnorm=1.821, clip=100, loss_scale=128, train_wall=1, gb_free=19.6, wall=103
2022-01-09 20:51:44 | INFO | train_inner | epoch 008:     67 / 99 loss=5.85, ppl=57.68, wps=13307.6, ups=12.47, wpb=1067.4, bsz=32, num_updates=760, lr=2.96e-05, gnorm=2.072, clip=100, loss_scale=128, train_wall=1, gb_free=20.3, wall=104
2022-01-09 20:51:45 | INFO | train_inner | epoch 008:     77 / 99 loss=5.91, ppl=60.11, wps=16069, ups=13.75, wpb=1169, bsz=32, num_updates=770, lr=2.95846e-05, gnorm=1.851, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=105
2022-01-09 20:51:45 | INFO | train_inner | epoch 008:     87 / 99 loss=5.439, ppl=43.38, wps=10927, ups=13.45, wpb=812.7, bsz=32, num_updates=780, lr=2.95692e-05, gnorm=1.981, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=106
2022-01-09 20:51:46 | INFO | train_inner | epoch 008:     97 / 99 loss=5.591, ppl=48.21, wps=10925.3, ups=11.99, wpb=911.1, bsz=32, num_updates=790, lr=2.95538e-05, gnorm=2.27, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=107
2022-01-09 20:51:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:51:47 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.602 | ppl 48.56 | wps 33602.3 | wpb 930.4 | bsz 31.3 | num_updates 792 | best_loss 5.602
2022-01-09 20:51:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 792 updates
2022-01-09 20:51:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:51:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:51:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 8 @ 792 updates, score 5.602) (writing took 4.447673984104767 seconds)
2022-01-09 20:51:52 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-09 20:51:52 | INFO | train | epoch 008 | loss 5.821 | ppl 56.52 | wps 7629.6 | ups 7.65 | wpb 997.9 | bsz 31.9 | num_updates 792 | lr 2.95508e-05 | gnorm 1.914 | clip 100 | loss_scale 128 | train_wall 7 | gb_free 20.7 | wall 112
2022-01-09 20:51:52 | INFO | fairseq.trainer | begin training epoch 9
2022-01-09 20:51:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:51:52 | INFO | train_inner | epoch 009:      8 / 99 loss=5.776, ppl=54.81, wps=1723.1, ups=1.6, wpb=1077.1, bsz=32, num_updates=800, lr=2.95385e-05, gnorm=1.685, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=113
2022-01-09 20:51:53 | INFO | train_inner | epoch 009:     18 / 99 loss=5.652, ppl=50.29, wps=14495.4, ups=14.14, wpb=1025.3, bsz=32, num_updates=810, lr=2.95231e-05, gnorm=1.731, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=114
2022-01-09 20:51:54 | INFO | train_inner | epoch 009:     28 / 99 loss=5.615, ppl=49.01, wps=14161.5, ups=14.24, wpb=994.6, bsz=32, num_updates=820, lr=2.95077e-05, gnorm=1.861, clip=100, loss_scale=128, train_wall=1, gb_free=19.6, wall=114
2022-01-09 20:51:55 | INFO | train_inner | epoch 009:     38 / 99 loss=5.792, ppl=55.41, wps=14876.5, ups=12.35, wpb=1205, bsz=32, num_updates=830, lr=2.94923e-05, gnorm=1.686, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=115
2022-01-09 20:51:55 | INFO | train_inner | epoch 009:     48 / 99 loss=5.694, ppl=51.76, wps=13993, ups=12.14, wpb=1152.2, bsz=32, num_updates=840, lr=2.94769e-05, gnorm=1.592, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=116
2022-01-09 20:51:56 | INFO | train_inner | epoch 009:     58 / 99 loss=4.919, ppl=30.25, wps=8442.8, ups=12.42, wpb=679.5, bsz=32, num_updates=850, lr=2.94615e-05, gnorm=2.06, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=117
2022-01-09 20:51:57 | INFO | train_inner | epoch 009:     68 / 99 loss=5.043, ppl=32.97, wps=10003.8, ups=12.7, wpb=787.9, bsz=32, num_updates=860, lr=2.94462e-05, gnorm=1.933, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=118
2022-01-09 20:51:58 | INFO | train_inner | epoch 009:     78 / 99 loss=5.78, ppl=54.96, wps=13207, ups=10.68, wpb=1236.1, bsz=31.5, num_updates=870, lr=2.94308e-05, gnorm=1.79, clip=100, loss_scale=128, train_wall=1, gb_free=20.2, wall=118
2022-01-09 20:51:59 | INFO | train_inner | epoch 009:     88 / 99 loss=5.29, ppl=39.12, wps=9888, ups=11.76, wpb=841.1, bsz=32, num_updates=880, lr=2.94154e-05, gnorm=2.362, clip=100, loss_scale=128, train_wall=1, gb_free=19.9, wall=119
2022-01-09 20:52:00 | INFO | train_inner | epoch 009:     98 / 99 loss=5.583, ppl=47.92, wps=12086.2, ups=10.94, wpb=1105.1, bsz=32, num_updates=890, lr=2.94e-05, gnorm=1.904, clip=100, loss_scale=128, train_wall=1, gb_free=18.8, wall=120
2022-01-09 20:52:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:52:01 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.373 | ppl 41.43 | wps 30719.6 | wpb 930.4 | bsz 31.3 | num_updates 891 | best_loss 5.373
2022-01-09 20:52:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 891 updates
2022-01-09 20:52:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:52:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:52:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 9 @ 891 updates, score 5.373) (writing took 4.350197091000155 seconds)
2022-01-09 20:52:05 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-09 20:52:05 | INFO | train | epoch 009 | loss 5.537 | ppl 46.45 | wps 7335 | ups 7.35 | wpb 997.9 | bsz 31.9 | num_updates 891 | lr 2.93985e-05 | gnorm 1.868 | clip 100 | loss_scale 128 | train_wall 8 | gb_free 20.5 | wall 126
2022-01-09 20:52:05 | INFO | fairseq.trainer | begin training epoch 10
2022-01-09 20:52:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:52:06 | INFO | train_inner | epoch 010:      9 / 99 loss=5.041, ppl=32.92, wps=1380.9, ups=1.64, wpb=843.4, bsz=32, num_updates=900, lr=2.93846e-05, gnorm=2.472, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=126
2022-01-09 20:52:07 | INFO | train_inner | epoch 010:     19 / 99 loss=5.238, ppl=37.75, wps=12446.8, ups=14.12, wpb=881.8, bsz=32, num_updates=910, lr=2.93692e-05, gnorm=2.566, clip=100, loss_scale=128, train_wall=1, gb_free=20.3, wall=127
2022-01-09 20:52:07 | INFO | train_inner | epoch 010:     29 / 99 loss=5.581, ppl=47.88, wps=17171.8, ups=13.42, wpb=1279.5, bsz=32, num_updates=920, lr=2.93538e-05, gnorm=2.08, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=128
2022-01-09 20:52:08 | INFO | train_inner | epoch 010:     39 / 99 loss=5.648, ppl=50.15, wps=16559.1, ups=12.74, wpb=1299.4, bsz=32, num_updates=930, lr=2.93385e-05, gnorm=1.795, clip=100, loss_scale=128, train_wall=1, gb_free=20.4, wall=129
2022-01-09 20:52:09 | INFO | train_inner | epoch 010:     49 / 99 loss=5.513, ppl=45.67, wps=15054.8, ups=14.13, wpb=1065.7, bsz=32, num_updates=940, lr=2.93231e-05, gnorm=1.831, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=129
2022-01-09 20:52:09 | INFO | train_inner | epoch 010:     59 / 99 loss=4.707, ppl=26.13, wps=10463.6, ups=14.89, wpb=702.9, bsz=32, num_updates=950, lr=2.93077e-05, gnorm=2.402, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=130
2022-01-09 20:52:10 | INFO | train_inner | epoch 010:     69 / 99 loss=4.905, ppl=29.96, wps=11835.1, ups=15.15, wpb=781.2, bsz=32, num_updates=960, lr=2.92923e-05, gnorm=2.031, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=131
2022-01-09 20:52:11 | INFO | train_inner | epoch 010:     79 / 99 loss=5.243, ppl=37.86, wps=14793.4, ups=14.72, wpb=1004.9, bsz=32, num_updates=970, lr=2.92769e-05, gnorm=1.993, clip=100, loss_scale=128, train_wall=1, gb_free=19.9, wall=131
2022-01-09 20:52:12 | INFO | train_inner | epoch 010:     89 / 99 loss=5.194, ppl=36.61, wps=12178.3, ups=13.08, wpb=931, bsz=32, num_updates=980, lr=2.92615e-05, gnorm=2.18, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=132
2022-01-09 20:52:13 | INFO | train_inner | epoch 010:     99 / 99 loss=5.584, ppl=47.97, wps=11572.3, ups=9.74, wpb=1188.2, bsz=31.5, num_updates=990, lr=2.92462e-05, gnorm=2.194, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=133
2022-01-09 20:52:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:52:14 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.263 | ppl 38.39 | wps 30526.7 | wpb 930.4 | bsz 31.3 | num_updates 990 | best_loss 5.263
2022-01-09 20:52:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 990 updates
2022-01-09 20:52:14 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:52:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:52:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 10 @ 990 updates, score 5.263) (writing took 4.391276523005217 seconds)
2022-01-09 20:52:18 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-09 20:52:18 | INFO | train | epoch 010 | loss 5.32 | ppl 39.96 | wps 7698.3 | ups 7.71 | wpb 997.9 | bsz 31.9 | num_updates 990 | lr 2.92462e-05 | gnorm 2.159 | clip 100 | loss_scale 128 | train_wall 7 | gb_free 20.8 | wall 138
2022-01-09 20:52:18 | INFO | fairseq.trainer | begin training epoch 11
2022-01-09 20:52:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:52:19 | INFO | train_inner | epoch 011:     10 / 99 loss=5.391, ppl=41.95, wps=1696.9, ups=1.61, wpb=1056.9, bsz=32, num_updates=1000, lr=2.92308e-05, gnorm=2.511, clip=100, loss_scale=128, train_wall=1, gb_free=18.8, wall=139
2022-01-09 20:52:20 | INFO | train_inner | epoch 011:     20 / 99 loss=5.156, ppl=35.64, wps=15494.2, ups=14.7, wpb=1053.7, bsz=32, num_updates=1010, lr=2.92154e-05, gnorm=1.792, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=140
2022-01-09 20:52:20 | INFO | train_inner | epoch 011:     30 / 99 loss=4.835, ppl=28.54, wps=11267.4, ups=13.43, wpb=838.7, bsz=32, num_updates=1020, lr=2.92e-05, gnorm=2.132, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=141
2022-01-09 20:52:21 | INFO | train_inner | epoch 011:     40 / 99 loss=5.151, ppl=35.52, wps=11099.1, ups=11.59, wpb=957.4, bsz=31.5, num_updates=1030, lr=2.91846e-05, gnorm=2.277, clip=100, loss_scale=128, train_wall=1, gb_free=20.6, wall=142
2022-01-09 20:52:22 | INFO | train_inner | epoch 011:     50 / 99 loss=5.131, ppl=35.04, wps=15454.3, ups=14.27, wpb=1083.3, bsz=32, num_updates=1040, lr=2.91692e-05, gnorm=1.971, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=142
2022-01-09 20:52:23 | INFO | train_inner | epoch 011:     60 / 99 loss=5.509, ppl=45.54, wps=16368.7, ups=13.01, wpb=1258.3, bsz=32, num_updates=1050, lr=2.91538e-05, gnorm=2.092, clip=100, loss_scale=128, train_wall=1, gb_free=19.1, wall=143
2022-01-09 20:52:23 | INFO | train_inner | epoch 011:     70 / 99 loss=5.057, ppl=33.28, wps=14155.5, ups=14.87, wpb=951.8, bsz=32, num_updates=1060, lr=2.91385e-05, gnorm=2.048, clip=100, loss_scale=128, train_wall=1, gb_free=20.3, wall=144
2022-01-09 20:52:24 | INFO | train_inner | epoch 011:     80 / 99 loss=4.795, ppl=27.76, wps=12833.6, ups=14.56, wpb=881.2, bsz=32, num_updates=1070, lr=2.91231e-05, gnorm=2.178, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=144
2022-01-09 20:52:25 | INFO | train_inner | epoch 011:     90 / 99 loss=5.021, ppl=32.47, wps=12833.1, ups=13.35, wpb=961.5, bsz=32, num_updates=1080, lr=2.91077e-05, gnorm=1.974, clip=100, loss_scale=128, train_wall=1, gb_free=20.8, wall=145
2022-01-09 20:52:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:52:26 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.958 | ppl 31.08 | wps 31786.3 | wpb 930.4 | bsz 31.3 | num_updates 1089 | best_loss 4.958
2022-01-09 20:52:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1089 updates
2022-01-09 20:52:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:52:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:52:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 11 @ 1089 updates, score 4.958) (writing took 4.154843179974705 seconds)
2022-01-09 20:52:31 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-09 20:52:31 | INFO | train | epoch 011 | loss 5.108 | ppl 34.49 | wps 7817.5 | ups 7.83 | wpb 997.9 | bsz 31.9 | num_updates 1089 | lr 2.90938e-05 | gnorm 2.088 | clip 100 | loss_scale 128 | train_wall 7 | gb_free 20.8 | wall 151
2022-01-09 20:52:31 | INFO | fairseq.trainer | begin training epoch 12
2022-01-09 20:52:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:52:31 | INFO | train_inner | epoch 012:      1 / 99 loss=4.722, ppl=26.4, wps=1508.4, ups=1.65, wpb=912.8, bsz=32, num_updates=1090, lr=2.90923e-05, gnorm=1.901, clip=100, loss_scale=128, train_wall=1, gb_free=20.5, wall=151
2022-01-09 20:52:32 | INFO | train_inner | epoch 012:     11 / 99 loss=4.896, ppl=29.77, wps=14187.4, ups=13.5, wpb=1051.1, bsz=32, num_updates=1100, lr=2.90769e-05, gnorm=2.477, clip=100, loss_scale=128, train_wall=1, gb_free=20, wall=152
2022-01-09 20:52:32 | INFO | train_inner | epoch 012:     21 / 99 loss=5.281, ppl=38.89, wps=15520.9, ups=13.49, wpb=1150.6, bsz=32, num_updates=1110, lr=2.90615e-05, gnorm=3.501, clip=100, loss_scale=128, train_wall=1, gb_free=19.7, wall=153
2022-01-09 20:52:33 | INFO | train_inner | epoch 012:     31 / 99 loss=5.249, ppl=38.04, wps=17185.7, ups=13.88, wpb=1238.6, bsz=32, num_updates=1120, lr=2.90462e-05, gnorm=2.08, clip=100, loss_scale=128, train_wall=1, gb_free=20.7, wall=154
2022-01-09 20:52:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-01-09 20:52:34 | INFO | train_inner | epoch 012:     42 / 99 loss=4.819, ppl=28.23, wps=11147.4, ups=12.23, wpb=911.8, bsz=32, num_updates=1130, lr=2.90308e-05, gnorm=2.228, clip=100, loss_scale=64, train_wall=1, gb_free=16.9, wall=154
2022-01-09 20:52:35 | INFO | train_inner | epoch 012:     52 / 99 loss=5.185, ppl=36.37, wps=16390.5, ups=12.99, wpb=1261.4, bsz=32, num_updates=1140, lr=2.90154e-05, gnorm=2.008, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=155
2022-01-09 20:52:35 | INFO | train_inner | epoch 012:     62 / 99 loss=4.727, ppl=26.49, wps=12101.5, ups=13.47, wpb=898.4, bsz=32, num_updates=1150, lr=2.9e-05, gnorm=2.306, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=156
2022-01-09 20:52:36 | INFO | train_inner | epoch 012:     72 / 99 loss=4.666, ppl=25.39, wps=12605.9, ups=14.61, wpb=862.7, bsz=32, num_updates=1160, lr=2.89846e-05, gnorm=2.244, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=157
2022-01-09 20:52:37 | INFO | train_inner | epoch 012:     82 / 99 loss=4.721, ppl=26.37, wps=11144.1, ups=12.96, wpb=860.1, bsz=32, num_updates=1170, lr=2.89692e-05, gnorm=2.319, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=157
2022-01-09 20:52:38 | INFO | train_inner | epoch 012:     92 / 99 loss=4.374, ppl=20.73, wps=10390.4, ups=13.33, wpb=779.5, bsz=32, num_updates=1180, lr=2.89538e-05, gnorm=2.512, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=158
2022-01-09 20:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:52:39 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.791 | ppl 27.68 | wps 31779.8 | wpb 930.4 | bsz 31.3 | num_updates 1187 | best_loss 4.791
2022-01-09 20:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1187 updates
2022-01-09 20:52:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:52:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:52:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 12 @ 1187 updates, score 4.791) (writing took 4.175252444925718 seconds)
2022-01-09 20:52:43 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-09 20:52:43 | INFO | train | epoch 012 | loss 4.881 | ppl 29.47 | wps 7624.7 | ups 7.78 | wpb 979.7 | bsz 32 | num_updates 1187 | lr 2.89431e-05 | gnorm 2.433 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.4 | wall 164
2022-01-09 20:52:43 | INFO | fairseq.trainer | begin training epoch 13
2022-01-09 20:52:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:52:44 | INFO | train_inner | epoch 013:      3 / 99 loss=4.911, ppl=30.08, wps=1686.7, ups=1.66, wpb=1017.5, bsz=32, num_updates=1190, lr=2.89385e-05, gnorm=2.714, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=164
2022-01-09 20:52:44 | INFO | train_inner | epoch 013:     13 / 99 loss=4.871, ppl=29.27, wps=14461.5, ups=13.52, wpb=1069.8, bsz=32, num_updates=1200, lr=2.89231e-05, gnorm=2.419, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=165
2022-01-09 20:52:45 | INFO | train_inner | epoch 013:     23 / 99 loss=4.535, ppl=23.18, wps=11829.3, ups=13.51, wpb=875.7, bsz=32, num_updates=1210, lr=2.89077e-05, gnorm=2.156, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=166
2022-01-09 20:52:46 | INFO | train_inner | epoch 013:     33 / 99 loss=4.879, ppl=29.43, wps=13623.6, ups=12.62, wpb=1079.3, bsz=32, num_updates=1220, lr=2.88923e-05, gnorm=1.938, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=166
2022-01-09 20:52:47 | INFO | train_inner | epoch 013:     43 / 99 loss=4.513, ppl=22.83, wps=11821.5, ups=13.29, wpb=889.6, bsz=32, num_updates=1230, lr=2.88769e-05, gnorm=2.286, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=167
2022-01-09 20:52:47 | INFO | train_inner | epoch 013:     53 / 99 loss=4.417, ppl=21.37, wps=11077.6, ups=14.11, wpb=785, bsz=32, num_updates=1240, lr=2.88615e-05, gnorm=2.236, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=168
2022-01-09 20:52:48 | INFO | train_inner | epoch 013:     63 / 99 loss=4.593, ppl=24.13, wps=12694.1, ups=12.97, wpb=978.6, bsz=32, num_updates=1250, lr=2.88462e-05, gnorm=2.11, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=169
2022-01-09 20:52:49 | INFO | train_inner | epoch 013:     73 / 99 loss=4.487, ppl=22.43, wps=13219.8, ups=14.2, wpb=930.8, bsz=32, num_updates=1260, lr=2.88308e-05, gnorm=2.219, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=169
2022-01-09 20:52:50 | INFO | train_inner | epoch 013:     83 / 99 loss=4.887, ppl=29.58, wps=14568.2, ups=13.15, wpb=1108.1, bsz=32, num_updates=1270, lr=2.88154e-05, gnorm=2.469, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=170
2022-01-09 20:52:50 | INFO | train_inner | epoch 013:     93 / 99 loss=4.858, ppl=29, wps=13132, ups=12.11, wpb=1084.1, bsz=31.5, num_updates=1280, lr=2.88e-05, gnorm=2.688, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=171
2022-01-09 20:52:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:52:52 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.495 | ppl 22.54 | wps 28141.8 | wpb 930.4 | bsz 31.3 | num_updates 1286 | best_loss 4.495
2022-01-09 20:52:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1286 updates
2022-01-09 20:52:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:52:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:52:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 13 @ 1286 updates, score 4.495) (writing took 3.8605204590130597 seconds)
2022-01-09 20:52:56 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-09 20:52:56 | INFO | train | epoch 013 | loss 4.719 | ppl 26.34 | wps 7827.3 | ups 7.84 | wpb 997.9 | bsz 31.9 | num_updates 1286 | lr 2.87908e-05 | gnorm 2.276 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.2 | wall 176
2022-01-09 20:52:56 | INFO | fairseq.trainer | begin training epoch 14
2022-01-09 20:52:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:52:56 | INFO | train_inner | epoch 014:      4 / 99 loss=4.42, ppl=21.41, wps=1631.7, ups=1.71, wpb=956.4, bsz=32, num_updates=1290, lr=2.87846e-05, gnorm=2.071, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=177
2022-01-09 20:52:57 | INFO | train_inner | epoch 014:     14 / 99 loss=4.419, ppl=21.39, wps=12488.1, ups=13.28, wpb=940.2, bsz=32, num_updates=1300, lr=2.87692e-05, gnorm=2.429, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=178
2022-01-09 20:52:58 | INFO | train_inner | epoch 014:     24 / 99 loss=3.981, ppl=15.79, wps=10988.8, ups=13.92, wpb=789.4, bsz=32, num_updates=1310, lr=2.87538e-05, gnorm=2.558, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=178
2022-01-09 20:52:58 | INFO | train_inner | epoch 014:     34 / 99 loss=4.598, ppl=24.21, wps=13335.7, ups=13.71, wpb=972.8, bsz=32, num_updates=1320, lr=2.87385e-05, gnorm=2.516, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=179
2022-01-09 20:52:59 | INFO | train_inner | epoch 014:     44 / 99 loss=5.251, ppl=38.09, wps=16759.9, ups=11.37, wpb=1473.6, bsz=31.5, num_updates=1330, lr=2.87231e-05, gnorm=2.16, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=180
2022-01-09 20:53:00 | INFO | train_inner | epoch 014:     54 / 99 loss=4.207, ppl=18.47, wps=12704.8, ups=13.93, wpb=912.3, bsz=32, num_updates=1340, lr=2.87077e-05, gnorm=2.09, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=181
2022-01-09 20:53:01 | INFO | train_inner | epoch 014:     64 / 99 loss=4.422, ppl=21.43, wps=12731.5, ups=12.09, wpb=1053, bsz=32, num_updates=1350, lr=2.86923e-05, gnorm=1.991, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=181
2022-01-09 20:53:02 | INFO | train_inner | epoch 014:     74 / 99 loss=4.113, ppl=17.31, wps=9560.8, ups=11.6, wpb=823.9, bsz=32, num_updates=1360, lr=2.86769e-05, gnorm=2.261, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=182
2022-01-09 20:53:03 | INFO | train_inner | epoch 014:     84 / 99 loss=4.832, ppl=28.49, wps=13426.3, ups=11.74, wpb=1144, bsz=32, num_updates=1370, lr=2.86615e-05, gnorm=2.312, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=183
2022-01-09 20:53:04 | INFO | train_inner | epoch 014:     94 / 99 loss=4.374, ppl=20.73, wps=9932.1, ups=10.52, wpb=944.5, bsz=32, num_updates=1380, lr=2.86462e-05, gnorm=2.454, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=184
2022-01-09 20:53:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:53:05 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.275 | ppl 19.36 | wps 26449.2 | wpb 930.4 | bsz 31.3 | num_updates 1385 | best_loss 4.275
2022-01-09 20:53:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1385 updates
2022-01-09 20:53:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:53:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:53:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 14 @ 1385 updates, score 4.275) (writing took 5.500411833054386 seconds)
2022-01-09 20:53:11 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-09 20:53:11 | INFO | train | epoch 014 | loss 4.517 | ppl 22.9 | wps 6661 | ups 6.68 | wpb 997.9 | bsz 31.9 | num_updates 1385 | lr 2.86385e-05 | gnorm 2.311 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.9 | wall 191
2022-01-09 20:53:11 | INFO | fairseq.trainer | begin training epoch 15
2022-01-09 20:53:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:53:11 | INFO | train_inner | epoch 015:      5 / 99 loss=3.816, ppl=14.09, wps=989.5, ups=1.33, wpb=744.8, bsz=32, num_updates=1390, lr=2.86308e-05, gnorm=2.614, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=192
2022-01-09 20:53:12 | INFO | train_inner | epoch 015:     15 / 99 loss=4.635, ppl=24.85, wps=15234.8, ups=12.62, wpb=1207.3, bsz=32, num_updates=1400, lr=2.86154e-05, gnorm=2.391, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=192
2022-01-09 20:53:13 | INFO | train_inner | epoch 015:     25 / 99 loss=4.321, ppl=19.98, wps=15118.7, ups=13.88, wpb=1089.1, bsz=32, num_updates=1410, lr=2.86e-05, gnorm=2.311, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=193
2022-01-09 20:53:13 | INFO | train_inner | epoch 015:     35 / 99 loss=4.018, ppl=16.21, wps=10776.4, ups=13.03, wpb=826.9, bsz=32, num_updates=1420, lr=2.85846e-05, gnorm=2.865, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=194
2022-01-09 20:53:14 | INFO | train_inner | epoch 015:     45 / 99 loss=3.937, ppl=15.32, wps=10132.9, ups=13.12, wpb=772.3, bsz=32, num_updates=1430, lr=2.85692e-05, gnorm=2.342, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=195
2022-01-09 20:53:15 | INFO | train_inner | epoch 015:     55 / 99 loss=4.596, ppl=24.19, wps=15817.8, ups=13.29, wpb=1190.2, bsz=32, num_updates=1440, lr=2.85538e-05, gnorm=2.432, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=195
2022-01-09 20:53:16 | INFO | train_inner | epoch 015:     65 / 99 loss=4.3, ppl=19.7, wps=13234.9, ups=13.15, wpb=1006.2, bsz=32, num_updates=1450, lr=2.85385e-05, gnorm=2.417, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=196
2022-01-09 20:53:16 | INFO | train_inner | epoch 015:     75 / 99 loss=4.062, ppl=16.7, wps=10955.8, ups=13.36, wpb=820.1, bsz=32, num_updates=1460, lr=2.85231e-05, gnorm=3.317, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=197
2022-01-09 20:53:17 | INFO | train_inner | epoch 015:     85 / 99 loss=4.431, ppl=21.57, wps=15570.9, ups=13.96, wpb=1115.7, bsz=32, num_updates=1470, lr=2.85077e-05, gnorm=2.721, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=198
2022-01-09 20:53:18 | INFO | train_inner | epoch 015:     95 / 99 loss=4.706, ppl=26.09, wps=10927.8, ups=9.48, wpb=1152.4, bsz=31.5, num_updates=1480, lr=2.84923e-05, gnorm=2.409, clip=100, loss_scale=64, train_wall=1, gb_free=16.9, wall=199
2022-01-09 20:53:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:53:20 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.092 | ppl 17.06 | wps 30211.4 | wpb 930.4 | bsz 31.3 | num_updates 1484 | best_loss 4.092
2022-01-09 20:53:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1484 updates
2022-01-09 20:53:20 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:53:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:53:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 15 @ 1484 updates, score 4.092) (writing took 4.263503408059478 seconds)
2022-01-09 20:53:24 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-09 20:53:24 | INFO | train | epoch 015 | loss 4.343 | ppl 20.3 | wps 7486.3 | ups 7.5 | wpb 997.9 | bsz 31.9 | num_updates 1484 | lr 2.84862e-05 | gnorm 2.591 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.1 | wall 204
2022-01-09 20:53:24 | INFO | fairseq.trainer | begin training epoch 16
2022-01-09 20:53:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:53:24 | INFO | train_inner | epoch 016:      6 / 99 loss=4.11, ppl=17.26, wps=1442, ups=1.61, wpb=893.3, bsz=32, num_updates=1490, lr=2.84769e-05, gnorm=2.807, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=205
2022-01-09 20:53:25 | INFO | train_inner | epoch 016:     16 / 99 loss=4.071, ppl=16.81, wps=14619.7, ups=14.79, wpb=988.4, bsz=32, num_updates=1500, lr=2.84615e-05, gnorm=2.712, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=206
2022-01-09 20:53:26 | INFO | train_inner | epoch 016:     26 / 99 loss=4.124, ppl=17.44, wps=14521, ups=14.54, wpb=998.4, bsz=32, num_updates=1510, lr=2.84462e-05, gnorm=2.383, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=206
2022-01-09 20:53:27 | INFO | train_inner | epoch 016:     36 / 99 loss=4.351, ppl=20.4, wps=12306.6, ups=12.49, wpb=985.6, bsz=32, num_updates=1520, lr=2.84308e-05, gnorm=2.466, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=207
2022-01-09 20:53:27 | INFO | train_inner | epoch 016:     46 / 99 loss=3.918, ppl=15.12, wps=11656.3, ups=13.39, wpb=870.8, bsz=32, num_updates=1530, lr=2.84154e-05, gnorm=2.6, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=208
2022-01-09 20:53:28 | INFO | train_inner | epoch 016:     56 / 99 loss=4.587, ppl=24.04, wps=16619.6, ups=13.07, wpb=1271.3, bsz=32, num_updates=1540, lr=2.84e-05, gnorm=2.21, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=209
2022-01-09 20:53:29 | INFO | train_inner | epoch 016:     66 / 99 loss=4.257, ppl=19.13, wps=11847.5, ups=11.65, wpb=1017, bsz=32, num_updates=1550, lr=2.83846e-05, gnorm=2.595, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=209
2022-01-09 20:53:30 | INFO | train_inner | epoch 016:     76 / 99 loss=3.96, ppl=15.56, wps=14124.2, ups=13.55, wpb=1042.3, bsz=32, num_updates=1560, lr=2.83692e-05, gnorm=2.857, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=210
2022-01-09 20:53:30 | INFO | train_inner | epoch 016:     86 / 99 loss=3.833, ppl=14.25, wps=12267.9, ups=13.69, wpb=895.8, bsz=32, num_updates=1570, lr=2.83538e-05, gnorm=3.225, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=211
2022-01-09 20:53:31 | INFO | train_inner | epoch 016:     96 / 99 loss=4.623, ppl=24.65, wps=12438.2, ups=11.32, wpb=1098.9, bsz=31.5, num_updates=1580, lr=2.83385e-05, gnorm=3.752, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=212
2022-01-09 20:53:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:53:33 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.949 | ppl 15.45 | wps 30430.7 | wpb 930.4 | bsz 31.3 | num_updates 1583 | best_loss 3.949
2022-01-09 20:53:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1583 updates
2022-01-09 20:53:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:53:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:53:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 16 @ 1583 updates, score 3.949) (writing took 4.300934329046868 seconds)
2022-01-09 20:53:37 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-09 20:53:37 | INFO | train | epoch 016 | loss 4.182 | ppl 18.15 | wps 7628.7 | ups 7.65 | wpb 997.9 | bsz 31.9 | num_updates 1583 | lr 2.83338e-05 | gnorm 2.784 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.8 | wall 217
2022-01-09 20:53:37 | INFO | fairseq.trainer | begin training epoch 17
2022-01-09 20:53:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:53:37 | INFO | train_inner | epoch 017:      7 / 99 loss=4.031, ppl=16.35, wps=1556.2, ups=1.64, wpb=949.2, bsz=32, num_updates=1590, lr=2.83231e-05, gnorm=3.269, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=218
2022-01-09 20:53:38 | INFO | train_inner | epoch 017:     17 / 99 loss=4.035, ppl=16.39, wps=14410.5, ups=14.42, wpb=999.2, bsz=32, num_updates=1600, lr=2.83077e-05, gnorm=2.408, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=219
2022-01-09 20:53:39 | INFO | train_inner | epoch 017:     27 / 99 loss=4.763, ppl=27.15, wps=15824, ups=12.55, wpb=1261.3, bsz=31.5, num_updates=1610, lr=2.82923e-05, gnorm=2.532, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=219
2022-01-09 20:53:40 | INFO | train_inner | epoch 017:     37 / 99 loss=3.961, ppl=15.57, wps=13370.9, ups=14.29, wpb=935.4, bsz=32, num_updates=1620, lr=2.82769e-05, gnorm=2.815, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=220
2022-01-09 20:53:40 | INFO | train_inner | epoch 017:     47 / 99 loss=3.929, ppl=15.23, wps=12400, ups=13.93, wpb=890, bsz=32, num_updates=1630, lr=2.82615e-05, gnorm=3.026, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=221
2022-01-09 20:53:41 | INFO | train_inner | epoch 017:     57 / 99 loss=4.078, ppl=16.89, wps=13681.6, ups=13.24, wpb=1033.7, bsz=32, num_updates=1640, lr=2.82462e-05, gnorm=3.603, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=222
2022-01-09 20:53:42 | INFO | train_inner | epoch 017:     67 / 99 loss=3.703, ppl=13.03, wps=12378, ups=13.42, wpb=922.4, bsz=32, num_updates=1650, lr=2.82308e-05, gnorm=3.158, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=222
2022-01-09 20:53:43 | INFO | train_inner | epoch 017:     77 / 99 loss=4.082, ppl=16.93, wps=11553.1, ups=11.5, wpb=1004.8, bsz=32, num_updates=1660, lr=2.82154e-05, gnorm=2.545, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=223
2022-01-09 20:53:44 | INFO | train_inner | epoch 017:     87 / 99 loss=3.766, ppl=13.61, wps=11059.2, ups=12.04, wpb=918.3, bsz=32, num_updates=1670, lr=2.82e-05, gnorm=2.59, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=224
2022-01-09 20:53:45 | INFO | train_inner | epoch 017:     97 / 99 loss=3.913, ppl=15.06, wps=9873.8, ups=9.85, wpb=1002, bsz=32, num_updates=1680, lr=2.81846e-05, gnorm=2.427, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=225
2022-01-09 20:53:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:53:46 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.822 | ppl 14.14 | wps 30118.3 | wpb 930.4 | bsz 31.3 | num_updates 1682 | best_loss 3.822
2022-01-09 20:53:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1682 updates
2022-01-09 20:53:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:53:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:53:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 17 @ 1682 updates, score 3.822) (writing took 4.335332729038782 seconds)
2022-01-09 20:53:50 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-09 20:53:50 | INFO | train | epoch 017 | loss 4.057 | ppl 16.65 | wps 7457 | ups 7.47 | wpb 997.9 | bsz 31.9 | num_updates 1682 | lr 2.81815e-05 | gnorm 2.801 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.7 | wall 231
2022-01-09 20:53:50 | INFO | fairseq.trainer | begin training epoch 18
2022-01-09 20:53:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:53:51 | INFO | train_inner | epoch 018:      8 / 99 loss=4.218, ppl=18.61, wps=1740.7, ups=1.6, wpb=1088, bsz=32, num_updates=1690, lr=2.81692e-05, gnorm=2.545, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=231
2022-01-09 20:53:52 | INFO | train_inner | epoch 018:     18 / 99 loss=3.859, ppl=14.51, wps=14141, ups=14.12, wpb=1001.4, bsz=32, num_updates=1700, lr=2.81538e-05, gnorm=2.56, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=232
2022-01-09 20:53:52 | INFO | train_inner | epoch 018:     28 / 99 loss=3.754, ppl=13.49, wps=13207.8, ups=13.54, wpb=975.3, bsz=32, num_updates=1710, lr=2.81385e-05, gnorm=2.785, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=233
2022-01-09 20:53:53 | INFO | train_inner | epoch 018:     38 / 99 loss=4.426, ppl=21.49, wps=16228.9, ups=12.73, wpb=1274.8, bsz=32, num_updates=1720, lr=2.81231e-05, gnorm=3.609, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=234
2022-01-09 20:53:54 | INFO | train_inner | epoch 018:     48 / 99 loss=3.378, ppl=10.4, wps=11938.9, ups=15.02, wpb=794.8, bsz=32, num_updates=1730, lr=2.81077e-05, gnorm=2.722, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=234
2022-01-09 20:53:54 | INFO | train_inner | epoch 018:     58 / 99 loss=3.444, ppl=10.88, wps=11063.5, ups=14, wpb=790.3, bsz=32, num_updates=1740, lr=2.80923e-05, gnorm=4.069, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=235
2022-01-09 20:53:55 | INFO | train_inner | epoch 018:     68 / 99 loss=4.237, ppl=18.85, wps=13745.7, ups=12.24, wpb=1123, bsz=31.5, num_updates=1750, lr=2.80769e-05, gnorm=3.017, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=236
2022-01-09 20:53:56 | INFO | train_inner | epoch 018:     78 / 99 loss=3.923, ppl=15.17, wps=12829.1, ups=13.41, wpb=957, bsz=32, num_updates=1760, lr=2.80615e-05, gnorm=3.919, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=237
2022-01-09 20:53:57 | INFO | train_inner | epoch 018:     88 / 99 loss=3.761, ppl=13.56, wps=13895.3, ups=13.53, wpb=1027.2, bsz=32, num_updates=1770, lr=2.80462e-05, gnorm=2.498, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=237
2022-01-09 20:53:58 | INFO | train_inner | epoch 018:     98 / 99 loss=3.546, ppl=11.68, wps=10260.5, ups=11.55, wpb=888.6, bsz=32, num_updates=1780, lr=2.80308e-05, gnorm=2.65, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=238
2022-01-09 20:53:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:53:59 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.667 | ppl 12.71 | wps 31061.2 | wpb 930.4 | bsz 31.3 | num_updates 1781 | best_loss 3.667
2022-01-09 20:53:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1781 updates
2022-01-09 20:53:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:54:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:54:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 18 @ 1781 updates, score 3.667) (writing took 4.210233945050277 seconds)
2022-01-09 20:54:03 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-09 20:54:03 | INFO | train | epoch 018 | loss 3.908 | ppl 15.01 | wps 7761.1 | ups 7.78 | wpb 997.9 | bsz 31.9 | num_updates 1781 | lr 2.80292e-05 | gnorm 3.046 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.2 | wall 243
2022-01-09 20:54:03 | INFO | fairseq.trainer | begin training epoch 19
2022-01-09 20:54:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:54:04 | INFO | train_inner | epoch 019:      9 / 99 loss=4.042, ppl=16.48, wps=1747, ups=1.67, wpb=1048.1, bsz=32, num_updates=1790, lr=2.80154e-05, gnorm=2.982, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=244
2022-01-09 20:54:04 | INFO | train_inner | epoch 019:     19 / 99 loss=3.833, ppl=14.25, wps=13380.6, ups=13.13, wpb=1018.7, bsz=32, num_updates=1800, lr=2.8e-05, gnorm=2.812, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=245
2022-01-09 20:54:05 | INFO | train_inner | epoch 019:     29 / 99 loss=3.508, ppl=11.38, wps=11085.5, ups=14.67, wpb=755.5, bsz=32, num_updates=1810, lr=2.79846e-05, gnorm=2.751, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=246
2022-01-09 20:54:06 | INFO | train_inner | epoch 019:     39 / 99 loss=3.708, ppl=13.07, wps=13520.6, ups=13.75, wpb=983.1, bsz=32, num_updates=1820, lr=2.79692e-05, gnorm=2.479, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=246
2022-01-09 20:54:07 | INFO | train_inner | epoch 019:     49 / 99 loss=2.999, ppl=7.99, wps=9940.5, ups=13.91, wpb=714.8, bsz=32, num_updates=1830, lr=2.79538e-05, gnorm=2.557, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=247
2022-01-09 20:54:07 | INFO | train_inner | epoch 019:     59 / 99 loss=3.66, ppl=12.64, wps=14374.1, ups=14.33, wpb=1003.1, bsz=32, num_updates=1840, lr=2.79385e-05, gnorm=3.054, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=248
2022-01-09 20:54:08 | INFO | train_inner | epoch 019:     69 / 99 loss=4.235, ppl=18.83, wps=14363, ups=11.1, wpb=1293.9, bsz=31.5, num_updates=1850, lr=2.79231e-05, gnorm=2.793, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=249
2022-01-09 20:54:09 | INFO | train_inner | epoch 019:     79 / 99 loss=3.737, ppl=13.33, wps=13864.8, ups=13.4, wpb=1034.6, bsz=32, num_updates=1860, lr=2.79077e-05, gnorm=4.075, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=249
2022-01-09 20:54:10 | INFO | train_inner | epoch 019:     89 / 99 loss=3.947, ppl=15.42, wps=12602.2, ups=11.06, wpb=1139.2, bsz=32, num_updates=1870, lr=2.78923e-05, gnorm=3.878, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=250
2022-01-09 20:54:11 | INFO | train_inner | epoch 019:     99 / 99 loss=3.753, ppl=13.48, wps=11452.5, ups=11.24, wpb=1019, bsz=32, num_updates=1880, lr=2.78769e-05, gnorm=3.741, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=251
2022-01-09 20:54:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:54:12 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.588 | ppl 12.03 | wps 29485 | wpb 930.4 | bsz 31.3 | num_updates 1880 | best_loss 3.588
2022-01-09 20:54:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1880 updates
2022-01-09 20:54:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:54:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:54:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 19 @ 1880 updates, score 3.588) (writing took 4.145958445966244 seconds)
2022-01-09 20:54:16 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-09 20:54:16 | INFO | train | epoch 019 | loss 3.784 | ppl 13.77 | wps 7626.7 | ups 7.64 | wpb 997.9 | bsz 31.9 | num_updates 1880 | lr 2.78769e-05 | gnorm 3.121 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 256
2022-01-09 20:54:16 | INFO | fairseq.trainer | begin training epoch 20
2022-01-09 20:54:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:54:17 | INFO | train_inner | epoch 020:     10 / 99 loss=4.065, ppl=16.73, wps=2086.3, ups=1.64, wpb=1273.2, bsz=31.5, num_updates=1890, lr=2.78615e-05, gnorm=2.688, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=257
2022-01-09 20:54:18 | INFO | train_inner | epoch 020:     20 / 99 loss=3.691, ppl=12.92, wps=14491.9, ups=13.99, wpb=1035.8, bsz=32, num_updates=1900, lr=2.78462e-05, gnorm=2.574, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=258
2022-01-09 20:54:18 | INFO | train_inner | epoch 020:     30 / 99 loss=3.558, ppl=11.78, wps=12631.8, ups=13.03, wpb=969.2, bsz=32, num_updates=1910, lr=2.78308e-05, gnorm=2.882, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=259
2022-01-09 20:54:19 | INFO | train_inner | epoch 020:     40 / 99 loss=3.969, ppl=15.66, wps=15007.1, ups=12.57, wpb=1194.1, bsz=32, num_updates=1920, lr=2.78154e-05, gnorm=2.395, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=260
2022-01-09 20:54:20 | INFO | train_inner | epoch 020:     50 / 99 loss=2.977, ppl=7.87, wps=9457.2, ups=13.55, wpb=697.7, bsz=32, num_updates=1930, lr=2.78e-05, gnorm=3.06, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=260
2022-01-09 20:54:21 | INFO | train_inner | epoch 020:     60 / 99 loss=3.998, ppl=15.98, wps=17366.7, ups=13.59, wpb=1277.8, bsz=32, num_updates=1940, lr=2.77846e-05, gnorm=2.666, clip=100, loss_scale=64, train_wall=1, gb_free=18.8, wall=261
2022-01-09 20:54:21 | INFO | train_inner | epoch 020:     70 / 99 loss=3.483, ppl=11.18, wps=12401.1, ups=13.15, wpb=943, bsz=32, num_updates=1950, lr=2.77692e-05, gnorm=3.267, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=262
2022-01-09 20:54:22 | INFO | train_inner | epoch 020:     80 / 99 loss=3.428, ppl=10.77, wps=11263.6, ups=12.18, wpb=925, bsz=32, num_updates=1960, lr=2.77538e-05, gnorm=2.969, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=263
2022-01-09 20:54:23 | INFO | train_inner | epoch 020:     90 / 99 loss=3.481, ppl=11.16, wps=11616.7, ups=13.07, wpb=888.9, bsz=32, num_updates=1970, lr=2.77385e-05, gnorm=3.376, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=263
2022-01-09 20:54:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:54:25 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.425 | ppl 10.74 | wps 31733.4 | wpb 930.4 | bsz 31.3 | num_updates 1979 | best_loss 3.425
2022-01-09 20:54:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1979 updates
2022-01-09 20:54:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:54:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:54:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 20 @ 1979 updates, score 3.425) (writing took 4.050786159001291 seconds)
2022-01-09 20:54:29 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-09 20:54:29 | INFO | train | epoch 020 | loss 3.644 | ppl 12.5 | wps 7776.7 | ups 7.79 | wpb 997.9 | bsz 31.9 | num_updates 1979 | lr 2.77246e-05 | gnorm 3.01 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.8 | wall 269
2022-01-09 20:54:29 | INFO | fairseq.trainer | begin training epoch 21
2022-01-09 20:54:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:54:29 | INFO | train_inner | epoch 021:      1 / 99 loss=3.122, ppl=8.71, wps=1336.5, ups=1.71, wpb=782.9, bsz=32, num_updates=1980, lr=2.77231e-05, gnorm=4.129, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=269
2022-01-09 20:54:30 | INFO | train_inner | epoch 021:     11 / 99 loss=3.311, ppl=9.92, wps=12269.5, ups=12.67, wpb=968.7, bsz=32, num_updates=1990, lr=2.77077e-05, gnorm=3.331, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=270
2022-01-09 20:54:30 | INFO | train_inner | epoch 021:     21 / 99 loss=3.893, ppl=14.85, wps=13769.7, ups=11.66, wpb=1180.8, bsz=32, num_updates=2000, lr=2.76923e-05, gnorm=4.12, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=271
2022-01-09 20:54:31 | INFO | train_inner | epoch 021:     31 / 99 loss=3.87, ppl=14.63, wps=13905.9, ups=12.53, wpb=1109.8, bsz=32, num_updates=2010, lr=2.76769e-05, gnorm=4.548, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=272
2022-01-09 20:54:32 | INFO | train_inner | epoch 021:     41 / 99 loss=3.175, ppl=9.03, wps=9126.4, ups=11.15, wpb=818.4, bsz=32, num_updates=2020, lr=2.76615e-05, gnorm=3.497, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=273
2022-01-09 20:54:33 | INFO | train_inner | epoch 021:     51 / 99 loss=3.793, ppl=13.86, wps=11007.8, ups=10.02, wpb=1098.3, bsz=32, num_updates=2030, lr=2.76462e-05, gnorm=3.613, clip=100, loss_scale=64, train_wall=1, gb_free=18.8, wall=274
2022-01-09 20:54:34 | INFO | train_inner | epoch 021:     61 / 99 loss=3.447, ppl=10.91, wps=13182.6, ups=12.23, wpb=1077.9, bsz=32, num_updates=2040, lr=2.76308e-05, gnorm=2.549, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=274
2022-01-09 20:54:35 | INFO | train_inner | epoch 021:     71 / 99 loss=3.628, ppl=12.36, wps=12524.5, ups=12.13, wpb=1032.7, bsz=32, num_updates=2050, lr=2.76154e-05, gnorm=2.514, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=275
2022-01-09 20:54:36 | INFO | train_inner | epoch 021:     81 / 99 loss=3.058, ppl=8.33, wps=9242, ups=11.98, wpb=771.3, bsz=32, num_updates=2060, lr=2.76e-05, gnorm=2.932, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=276
2022-01-09 20:54:37 | INFO | train_inner | epoch 021:     91 / 99 loss=3.664, ppl=12.67, wps=9101.3, ups=9.2, wpb=989.3, bsz=31.5, num_updates=2070, lr=2.75846e-05, gnorm=3.708, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=277
2022-01-09 20:54:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:54:38 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.395 | ppl 10.52 | wps 31447.3 | wpb 930.4 | bsz 31.3 | num_updates 2078 | best_loss 3.395
2022-01-09 20:54:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2078 updates
2022-01-09 20:54:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:54:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:54:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 21 @ 2078 updates, score 3.395) (writing took 4.5017275349237025 seconds)
2022-01-09 20:54:43 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-09 20:54:43 | INFO | train | epoch 021 | loss 3.572 | ppl 11.89 | wps 6924.1 | ups 6.94 | wpb 997.9 | bsz 31.9 | num_updates 2078 | lr 2.75723e-05 | gnorm 3.519 | clip 100 | loss_scale 64 | train_wall 9 | gb_free 20.7 | wall 283
2022-01-09 20:54:43 | INFO | fairseq.trainer | begin training epoch 22
2022-01-09 20:54:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:54:43 | INFO | train_inner | epoch 022:      2 / 99 loss=3.438, ppl=10.84, wps=1302.1, ups=1.55, wpb=838, bsz=32, num_updates=2080, lr=2.75692e-05, gnorm=4.336, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=284
2022-01-09 20:54:44 | INFO | train_inner | epoch 022:     12 / 99 loss=3.599, ppl=12.12, wps=13740.9, ups=12.66, wpb=1085.4, bsz=32, num_updates=2090, lr=2.75538e-05, gnorm=3.393, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=284
2022-01-09 20:54:45 | INFO | train_inner | epoch 022:     22 / 99 loss=3.384, ppl=10.44, wps=12262.3, ups=12.86, wpb=953.7, bsz=32, num_updates=2100, lr=2.75385e-05, gnorm=2.888, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=285
2022-01-09 20:54:46 | INFO | train_inner | epoch 022:     32 / 99 loss=4.091, ppl=17.04, wps=12902.9, ups=12.31, wpb=1048, bsz=31.5, num_updates=2110, lr=2.75231e-05, gnorm=4.314, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=286
2022-01-09 20:54:46 | INFO | train_inner | epoch 022:     42 / 99 loss=3.338, ppl=10.11, wps=14354.4, ups=13.76, wpb=1043.4, bsz=32, num_updates=2120, lr=2.75077e-05, gnorm=4.158, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=287
2022-01-09 20:54:47 | INFO | train_inner | epoch 022:     52 / 99 loss=3.665, ppl=12.68, wps=15664, ups=13.14, wpb=1192.4, bsz=32, num_updates=2130, lr=2.74923e-05, gnorm=3.262, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=288
2022-01-09 20:54:48 | INFO | train_inner | epoch 022:     62 / 99 loss=3.72, ppl=13.18, wps=15429.9, ups=13.48, wpb=1144.8, bsz=32, num_updates=2140, lr=2.74769e-05, gnorm=2.901, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=288
2022-01-09 20:54:49 | INFO | train_inner | epoch 022:     72 / 99 loss=3.479, ppl=11.15, wps=13280.8, ups=13.16, wpb=1009.1, bsz=32, num_updates=2150, lr=2.74615e-05, gnorm=3.062, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=289
2022-01-09 20:54:49 | INFO | train_inner | epoch 022:     82 / 99 loss=2.908, ppl=7.5, wps=10277.9, ups=12.39, wpb=829.2, bsz=32, num_updates=2160, lr=2.74462e-05, gnorm=2.803, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=290
2022-01-09 20:54:50 | INFO | train_inner | epoch 022:     92 / 99 loss=2.858, ppl=7.25, wps=8750.5, ups=11.46, wpb=763.4, bsz=32, num_updates=2170, lr=2.74308e-05, gnorm=2.527, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=291
2022-01-09 20:54:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:54:52 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.231 | ppl 9.39 | wps 30997.9 | wpb 930.4 | bsz 31.3 | num_updates 2177 | best_loss 3.231
2022-01-09 20:54:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2177 updates
2022-01-09 20:54:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:54:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:54:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 22 @ 2177 updates, score 3.231) (writing took 4.210079133976251 seconds)
2022-01-09 20:54:56 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-09 20:54:56 | INFO | train | epoch 022 | loss 3.442 | ppl 10.86 | wps 7542 | ups 7.56 | wpb 997.9 | bsz 31.9 | num_updates 2177 | lr 2.742e-05 | gnorm 3.234 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.4 | wall 297
2022-01-09 20:54:56 | INFO | fairseq.trainer | begin training epoch 23
2022-01-09 20:54:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:54:56 | INFO | train_inner | epoch 023:      3 / 99 loss=3.043, ppl=8.24, wps=1637.8, ups=1.65, wpb=993.2, bsz=32, num_updates=2180, lr=2.74154e-05, gnorm=2.969, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=297
2022-01-09 20:54:57 | INFO | train_inner | epoch 023:     13 / 99 loss=3.202, ppl=9.21, wps=14345.9, ups=13.28, wpb=1080.3, bsz=32, num_updates=2190, lr=2.74e-05, gnorm=2.51, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=298
2022-01-09 20:54:58 | INFO | train_inner | epoch 023:     23 / 99 loss=3.202, ppl=9.2, wps=13415.3, ups=13, wpb=1032.2, bsz=32, num_updates=2200, lr=2.73846e-05, gnorm=2.337, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=298
2022-01-09 20:54:59 | INFO | train_inner | epoch 023:     33 / 99 loss=3.182, ppl=9.08, wps=13714, ups=13.16, wpb=1042.3, bsz=32, num_updates=2210, lr=2.73692e-05, gnorm=2.573, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=299
2022-01-09 20:54:59 | INFO | train_inner | epoch 023:     43 / 99 loss=3.203, ppl=9.21, wps=11402.1, ups=12.06, wpb=945.2, bsz=32, num_updates=2220, lr=2.73538e-05, gnorm=3.115, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=300
2022-01-09 20:55:00 | INFO | train_inner | epoch 023:     53 / 99 loss=3.478, ppl=11.14, wps=12380.4, ups=11.68, wpb=1060.1, bsz=32, num_updates=2230, lr=2.73385e-05, gnorm=3.041, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=301
2022-01-09 20:55:01 | INFO | train_inner | epoch 023:     63 / 99 loss=3.641, ppl=12.47, wps=12478, ups=12.39, wpb=1007.3, bsz=31.5, num_updates=2240, lr=2.73231e-05, gnorm=2.994, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=302
2022-01-09 20:55:02 | INFO | train_inner | epoch 023:     73 / 99 loss=3.228, ppl=9.37, wps=12690.3, ups=12.95, wpb=979.8, bsz=32, num_updates=2250, lr=2.73077e-05, gnorm=2.497, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=302
2022-01-09 20:55:03 | INFO | train_inner | epoch 023:     83 / 99 loss=2.753, ppl=6.74, wps=10294.6, ups=13.61, wpb=756.2, bsz=32, num_updates=2260, lr=2.72923e-05, gnorm=2.809, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=303
2022-01-09 20:55:04 | INFO | train_inner | epoch 023:     93 / 99 loss=3.631, ppl=12.39, wps=11462, ups=11.11, wpb=1031.6, bsz=32, num_updates=2270, lr=2.72769e-05, gnorm=4.022, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=304
2022-01-09 20:55:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:55:05 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.127 | ppl 8.74 | wps 30588.9 | wpb 930.4 | bsz 31.3 | num_updates 2276 | best_loss 3.127
2022-01-09 20:55:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2276 updates
2022-01-09 20:55:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:55:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:55:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 23 @ 2276 updates, score 3.127) (writing took 4.166034167981707 seconds)
2022-01-09 20:55:09 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-09 20:55:10 | INFO | train | epoch 023 | loss 3.277 | ppl 9.69 | wps 7517.8 | ups 7.53 | wpb 997.9 | bsz 31.9 | num_updates 2276 | lr 2.72677e-05 | gnorm 2.9 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.7 | wall 310
2022-01-09 20:55:10 | INFO | fairseq.trainer | begin training epoch 24
2022-01-09 20:55:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:55:10 | INFO | train_inner | epoch 024:      4 / 99 loss=3.282, ppl=9.73, wps=1585.2, ups=1.45, wpb=1092.5, bsz=32, num_updates=2280, lr=2.72615e-05, gnorm=3.329, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=311
2022-01-09 20:55:11 | INFO | train_inner | epoch 024:     14 / 99 loss=2.923, ppl=7.58, wps=12365.1, ups=13.9, wpb=889.5, bsz=32, num_updates=2290, lr=2.72462e-05, gnorm=2.492, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=312
2022-01-09 20:55:12 | INFO | train_inner | epoch 024:     24 / 99 loss=3.085, ppl=8.48, wps=13200.9, ups=13.47, wpb=979.7, bsz=32, num_updates=2300, lr=2.72308e-05, gnorm=2.395, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=312
2022-01-09 20:55:13 | INFO | train_inner | epoch 024:     34 / 99 loss=3.437, ppl=10.83, wps=14238.7, ups=12.65, wpb=1125.3, bsz=31.5, num_updates=2310, lr=2.72154e-05, gnorm=2.925, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=313
2022-01-09 20:55:13 | INFO | train_inner | epoch 024:     44 / 99 loss=2.981, ppl=7.89, wps=12574.9, ups=12.21, wpb=1030.1, bsz=32, num_updates=2320, lr=2.72e-05, gnorm=2.529, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=314
2022-01-09 20:55:14 | INFO | train_inner | epoch 024:     54 / 99 loss=2.447, ppl=5.45, wps=10583.9, ups=14.1, wpb=750.5, bsz=32, num_updates=2330, lr=2.71846e-05, gnorm=2.949, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=315
2022-01-09 20:55:15 | INFO | train_inner | epoch 024:     64 / 99 loss=3.067, ppl=8.38, wps=13792.2, ups=13.71, wpb=1005.8, bsz=32, num_updates=2340, lr=2.71692e-05, gnorm=2.78, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=315
2022-01-09 20:55:16 | INFO | train_inner | epoch 024:     74 / 99 loss=3.344, ppl=10.16, wps=13576.1, ups=12.81, wpb=1059.6, bsz=32, num_updates=2350, lr=2.71538e-05, gnorm=2.819, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=316
2022-01-09 20:55:17 | INFO | train_inner | epoch 024:     84 / 99 loss=2.938, ppl=7.66, wps=11107.3, ups=12.6, wpb=881.8, bsz=32, num_updates=2360, lr=2.71385e-05, gnorm=3.076, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=317
2022-01-09 20:55:17 | INFO | train_inner | epoch 024:     94 / 99 loss=3.687, ppl=12.88, wps=13356.8, ups=11.25, wpb=1187, bsz=32, num_updates=2370, lr=2.71231e-05, gnorm=2.948, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=318
2022-01-09 20:55:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:55:19 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.022 | ppl 8.12 | wps 30412 | wpb 930.4 | bsz 31.3 | num_updates 2375 | best_loss 3.022
2022-01-09 20:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2375 updates
2022-01-09 20:55:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:55:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:55:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 24 @ 2375 updates, score 3.022) (writing took 4.169149835011922 seconds)
2022-01-09 20:55:23 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-09 20:55:23 | INFO | train | epoch 024 | loss 3.155 | ppl 8.91 | wps 7568.5 | ups 7.58 | wpb 997.9 | bsz 31.9 | num_updates 2375 | lr 2.71154e-05 | gnorm 2.784 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 324
2022-01-09 20:55:23 | INFO | fairseq.trainer | begin training epoch 25
2022-01-09 20:55:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:55:24 | INFO | train_inner | epoch 025:      5 / 99 loss=3.466, ppl=11.05, wps=1901.9, ups=1.63, wpb=1167.1, bsz=32, num_updates=2380, lr=2.71077e-05, gnorm=3.119, clip=100, loss_scale=64, train_wall=1, gb_free=18.8, wall=324
2022-01-09 20:55:24 | INFO | train_inner | epoch 025:     15 / 99 loss=2.765, ppl=6.8, wps=11090.8, ups=12.11, wpb=915.9, bsz=32, num_updates=2390, lr=2.70923e-05, gnorm=2.977, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=325
2022-01-09 20:55:25 | INFO | train_inner | epoch 025:     25 / 99 loss=3.411, ppl=10.64, wps=11629.6, ups=12.2, wpb=953.1, bsz=31.5, num_updates=2400, lr=2.70769e-05, gnorm=2.663, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=326
2022-01-09 20:55:26 | INFO | train_inner | epoch 025:     35 / 99 loss=2.479, ppl=5.58, wps=9380.8, ups=13.86, wpb=676.9, bsz=32, num_updates=2410, lr=2.70615e-05, gnorm=2.84, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=326
2022-01-09 20:55:27 | INFO | train_inner | epoch 025:     45 / 99 loss=2.735, ppl=6.66, wps=11558.7, ups=12.75, wpb=906.4, bsz=32, num_updates=2420, lr=2.70462e-05, gnorm=2.792, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=327
2022-01-09 20:55:27 | INFO | train_inner | epoch 025:     55 / 99 loss=3.07, ppl=8.4, wps=14406.6, ups=13.8, wpb=1044.3, bsz=32, num_updates=2430, lr=2.70308e-05, gnorm=2.502, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=328
2022-01-09 20:55:28 | INFO | train_inner | epoch 025:     65 / 99 loss=2.796, ppl=6.95, wps=14430, ups=14.62, wpb=987, bsz=32, num_updates=2440, lr=2.70154e-05, gnorm=2.47, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=329
2022-01-09 20:55:29 | INFO | train_inner | epoch 025:     75 / 99 loss=3.232, ppl=9.39, wps=14730.6, ups=12.41, wpb=1186.6, bsz=32, num_updates=2450, lr=2.7e-05, gnorm=2.729, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=329
2022-01-09 20:55:30 | INFO | train_inner | epoch 025:     85 / 99 loss=2.779, ppl=6.86, wps=14311.2, ups=14.8, wpb=967.3, bsz=32, num_updates=2460, lr=2.69846e-05, gnorm=2.438, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=330
2022-01-09 20:55:31 | INFO | train_inner | epoch 025:     95 / 99 loss=3.492, ppl=11.25, wps=12435.1, ups=10.8, wpb=1151, bsz=32, num_updates=2470, lr=2.69692e-05, gnorm=2.273, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=331
2022-01-09 20:55:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:55:32 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 2.887 | ppl 7.4 | wps 31680.3 | wpb 930.4 | bsz 31.3 | num_updates 2474 | best_loss 2.887
2022-01-09 20:55:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2474 updates
2022-01-09 20:55:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:55:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:55:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 25 @ 2474 updates, score 2.887) (writing took 4.09792405506596 seconds)
2022-01-09 20:55:36 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-09 20:55:36 | INFO | train | epoch 025 | loss 3.045 | ppl 8.26 | wps 7713.8 | ups 7.73 | wpb 997.9 | bsz 31.9 | num_updates 2474 | lr 2.69631e-05 | gnorm 2.664 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.4 | wall 336
2022-01-09 20:55:36 | INFO | fairseq.trainer | begin training epoch 26
2022-01-09 20:55:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:55:37 | INFO | train_inner | epoch 026:      6 / 99 loss=2.917, ppl=7.55, wps=1863.6, ups=1.66, wpb=1125, bsz=32, num_updates=2480, lr=2.69538e-05, gnorm=2.269, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=337
2022-01-09 20:55:37 | INFO | train_inner | epoch 026:     16 / 99 loss=3.655, ppl=12.6, wps=15251.4, ups=12.03, wpb=1268.1, bsz=31.5, num_updates=2490, lr=2.69385e-05, gnorm=2.414, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=338
2022-01-09 20:55:38 | INFO | train_inner | epoch 026:     26 / 99 loss=3.485, ppl=11.2, wps=15787.3, ups=11.77, wpb=1341.1, bsz=32, num_updates=2500, lr=2.69231e-05, gnorm=2.75, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=339
2022-01-09 20:55:39 | INFO | train_inner | epoch 026:     36 / 99 loss=2.469, ppl=5.54, wps=10306.5, ups=13.18, wpb=781.9, bsz=32, num_updates=2510, lr=2.69077e-05, gnorm=2.918, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=340
2022-01-09 20:55:40 | INFO | train_inner | epoch 026:     46 / 99 loss=2.499, ppl=5.65, wps=9933.6, ups=13.45, wpb=738.4, bsz=32, num_updates=2520, lr=2.68923e-05, gnorm=2.963, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=340
2022-01-09 20:55:41 | INFO | train_inner | epoch 026:     56 / 99 loss=2.991, ppl=7.95, wps=13212.5, ups=13.25, wpb=997.4, bsz=32, num_updates=2530, lr=2.68769e-05, gnorm=2.555, clip=100, loss_scale=64, train_wall=1, gb_free=19.1, wall=341
2022-01-09 20:55:41 | INFO | train_inner | epoch 026:     66 / 99 loss=2.602, ppl=6.07, wps=11159.6, ups=11.78, wpb=947.7, bsz=32, num_updates=2540, lr=2.68615e-05, gnorm=2.631, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=342
2022-01-09 20:55:42 | INFO | train_inner | epoch 026:     76 / 99 loss=2.946, ppl=7.71, wps=12519, ups=11.44, wpb=1094.1, bsz=32, num_updates=2550, lr=2.68462e-05, gnorm=2.812, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=343
2022-01-09 20:55:43 | INFO | train_inner | epoch 026:     86 / 99 loss=2.52, ppl=5.73, wps=10098.4, ups=11.41, wpb=885.4, bsz=32, num_updates=2560, lr=2.68308e-05, gnorm=2.921, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=344
2022-01-09 20:55:44 | INFO | train_inner | epoch 026:     96 / 99 loss=2.676, ppl=6.39, wps=8988.7, ups=10.21, wpb=880.6, bsz=32, num_updates=2570, lr=2.68154e-05, gnorm=2.867, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=345
2022-01-09 20:55:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:55:45 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 2.808 | ppl 7 | wps 33009.6 | wpb 930.4 | bsz 31.3 | num_updates 2573 | best_loss 2.808
2022-01-09 20:55:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2573 updates
2022-01-09 20:55:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:55:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:55:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 26 @ 2573 updates, score 2.808) (writing took 4.272732938989066 seconds)
2022-01-09 20:55:50 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-09 20:55:50 | INFO | train | epoch 026 | loss 2.941 | ppl 7.68 | wps 7255.6 | ups 7.27 | wpb 997.9 | bsz 31.9 | num_updates 2573 | lr 2.68108e-05 | gnorm 2.715 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.4 | wall 350
2022-01-09 20:55:50 | INFO | fairseq.trainer | begin training epoch 27
2022-01-09 20:55:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:55:50 | INFO | train_inner | epoch 027:      7 / 99 loss=2.393, ppl=5.25, wps=1365.7, ups=1.63, wpb=835.6, bsz=32, num_updates=2580, lr=2.68e-05, gnorm=2.55, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=351
2022-01-09 20:55:51 | INFO | train_inner | epoch 027:     17 / 99 loss=3.449, ppl=10.92, wps=17121.4, ups=12.62, wpb=1356.7, bsz=32, num_updates=2590, lr=2.67846e-05, gnorm=2.381, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=352
2022-01-09 20:55:52 | INFO | train_inner | epoch 027:     27 / 99 loss=2.807, ppl=7, wps=14500.8, ups=14.65, wpb=989.8, bsz=32, num_updates=2600, lr=2.67692e-05, gnorm=2.591, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=352
2022-01-09 20:55:52 | INFO | train_inner | epoch 027:     37 / 99 loss=2.446, ppl=5.45, wps=13479.8, ups=14.5, wpb=929.8, bsz=32, num_updates=2610, lr=2.67538e-05, gnorm=2.506, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=353
2022-01-09 20:55:53 | INFO | train_inner | epoch 027:     47 / 99 loss=2.757, ppl=6.76, wps=12301.4, ups=14.78, wpb=832.5, bsz=32, num_updates=2620, lr=2.67385e-05, gnorm=2.871, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=354
2022-01-09 20:55:54 | INFO | train_inner | epoch 027:     57 / 99 loss=3.251, ppl=9.52, wps=14831, ups=12.14, wpb=1221.5, bsz=31.5, num_updates=2630, lr=2.67231e-05, gnorm=2.573, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=354
2022-01-09 20:55:55 | INFO | train_inner | epoch 027:     67 / 99 loss=2.335, ppl=5.04, wps=12781.2, ups=14.87, wpb=859.3, bsz=32, num_updates=2640, lr=2.67077e-05, gnorm=2.41, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=355
2022-01-09 20:55:55 | INFO | train_inner | epoch 027:     77 / 99 loss=2.705, ppl=6.52, wps=14260.4, ups=14.89, wpb=957.4, bsz=32, num_updates=2650, lr=2.66923e-05, gnorm=2.604, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=356
2022-01-09 20:55:56 | INFO | train_inner | epoch 027:     87 / 99 loss=3.032, ppl=8.18, wps=14464.2, ups=13.53, wpb=1068.9, bsz=32, num_updates=2660, lr=2.66769e-05, gnorm=2.767, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=357
2022-01-09 20:55:57 | INFO | train_inner | epoch 027:     97 / 99 loss=2.554, ppl=5.87, wps=8572.3, ups=9.87, wpb=868.3, bsz=32, num_updates=2670, lr=2.66615e-05, gnorm=2.753, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=358
2022-01-09 20:55:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:55:58 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 2.714 | ppl 6.56 | wps 31645.4 | wpb 930.4 | bsz 31.3 | num_updates 2672 | best_loss 2.714
2022-01-09 20:55:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2672 updates
2022-01-09 20:55:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:56:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:56:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 27 @ 2672 updates, score 2.714) (writing took 4.098734008031897 seconds)
2022-01-09 20:56:02 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-09 20:56:02 | INFO | train | epoch 027 | loss 2.84 | ppl 7.16 | wps 7847.4 | ups 7.86 | wpb 997.9 | bsz 31.9 | num_updates 2672 | lr 2.66585e-05 | gnorm 2.615 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.6 | wall 363
2022-01-09 20:56:02 | INFO | fairseq.trainer | begin training epoch 28
2022-01-09 20:56:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:56:03 | INFO | train_inner | epoch 028:      8 / 99 loss=2.389, ppl=5.24, wps=1588.9, ups=1.71, wpb=930.5, bsz=32, num_updates=2680, lr=2.66462e-05, gnorm=2.711, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=363
2022-01-09 20:56:04 | INFO | train_inner | epoch 028:     18 / 99 loss=2.921, ppl=7.57, wps=15991.3, ups=14.07, wpb=1136.7, bsz=32, num_updates=2690, lr=2.66308e-05, gnorm=2.551, clip=100, loss_scale=64, train_wall=1, gb_free=18.8, wall=364
2022-01-09 20:56:04 | INFO | train_inner | epoch 028:     28 / 99 loss=2.719, ppl=6.59, wps=13042.9, ups=14.02, wpb=930.5, bsz=32, num_updates=2700, lr=2.66154e-05, gnorm=2.465, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=365
2022-01-09 20:56:05 | INFO | train_inner | epoch 028:     38 / 99 loss=2.677, ppl=6.4, wps=14157.4, ups=13.93, wpb=1016, bsz=32, num_updates=2710, lr=2.66e-05, gnorm=2.579, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=366
2022-01-09 20:56:06 | INFO | train_inner | epoch 028:     48 / 99 loss=2.911, ppl=7.52, wps=14174.1, ups=14.25, wpb=995, bsz=32, num_updates=2720, lr=2.65846e-05, gnorm=2.495, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=366
2022-01-09 20:56:06 | INFO | train_inner | epoch 028:     58 / 99 loss=2.284, ppl=4.87, wps=12690.3, ups=14.88, wpb=852.7, bsz=32, num_updates=2730, lr=2.65692e-05, gnorm=2.685, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=367
2022-01-09 20:56:07 | INFO | train_inner | epoch 028:     68 / 99 loss=3.104, ppl=8.6, wps=13737.7, ups=12.37, wpb=1110.2, bsz=31.5, num_updates=2740, lr=2.65538e-05, gnorm=2.458, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=368
2022-01-09 20:56:08 | INFO | train_inner | epoch 028:     78 / 99 loss=2.804, ppl=6.98, wps=12047.1, ups=12.96, wpb=929.5, bsz=32, num_updates=2750, lr=2.65385e-05, gnorm=2.628, clip=100, loss_scale=64, train_wall=1, gb_free=19.1, wall=369
2022-01-09 20:56:09 | INFO | train_inner | epoch 028:     88 / 99 loss=2.807, ppl=7, wps=13062.2, ups=12.63, wpb=1034, bsz=32, num_updates=2760, lr=2.65231e-05, gnorm=2.627, clip=100, loss_scale=64, train_wall=1, gb_free=19.5, wall=369
2022-01-09 20:56:10 | INFO | train_inner | epoch 028:     98 / 99 loss=2.587, ppl=6.01, wps=10564.4, ups=10.23, wpb=1032.5, bsz=32, num_updates=2770, lr=2.65077e-05, gnorm=2.562, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=370
2022-01-09 20:56:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:56:11 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 2.628 | ppl 6.18 | wps 29510.5 | wpb 930.4 | bsz 31.3 | num_updates 2771 | best_loss 2.628
2022-01-09 20:56:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2771 updates
2022-01-09 20:56:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:56:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:56:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 28 @ 2771 updates, score 2.628) (writing took 4.26338456897065 seconds)
2022-01-09 20:56:15 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-09 20:56:15 | INFO | train | epoch 028 | loss 2.738 | ppl 6.67 | wps 7660.9 | ups 7.68 | wpb 997.9 | bsz 31.9 | num_updates 2771 | lr 2.65062e-05 | gnorm 2.568 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.1 | wall 376
2022-01-09 20:56:15 | INFO | fairseq.trainer | begin training epoch 29
2022-01-09 20:56:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:56:16 | INFO | train_inner | epoch 029:      9 / 99 loss=2.903, ppl=7.48, wps=1874.2, ups=1.63, wpb=1150.6, bsz=32, num_updates=2780, lr=2.64923e-05, gnorm=2.793, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=376
2022-01-09 20:56:17 | INFO | train_inner | epoch 029:     19 / 99 loss=2.706, ppl=6.52, wps=13106.7, ups=13.01, wpb=1007.4, bsz=32, num_updates=2790, lr=2.64769e-05, gnorm=2.672, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=377
2022-01-09 20:56:18 | INFO | train_inner | epoch 029:     29 / 99 loss=2.948, ppl=7.72, wps=15530.9, ups=12.21, wpb=1272.3, bsz=32, num_updates=2800, lr=2.64615e-05, gnorm=2.515, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=378
2022-01-09 20:56:18 | INFO | train_inner | epoch 029:     39 / 99 loss=2.406, ppl=5.3, wps=11018.5, ups=12.69, wpb=868.5, bsz=32, num_updates=2810, lr=2.64462e-05, gnorm=2.721, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=379
2022-01-09 20:56:19 | INFO | train_inner | epoch 029:     49 / 99 loss=2.345, ppl=5.08, wps=11447.2, ups=12.51, wpb=915.3, bsz=32, num_updates=2820, lr=2.64308e-05, gnorm=3.133, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=380
2022-01-09 20:56:20 | INFO | train_inner | epoch 029:     59 / 99 loss=2.259, ppl=4.79, wps=9838.7, ups=11, wpb=894.3, bsz=32, num_updates=2830, lr=2.64154e-05, gnorm=2.837, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=381
2022-01-09 20:56:21 | INFO | train_inner | epoch 029:     69 / 99 loss=3.147, ppl=8.86, wps=12633.7, ups=11.38, wpb=1110.3, bsz=31.5, num_updates=2840, lr=2.64e-05, gnorm=2.964, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=381
2022-01-09 20:56:22 | INFO | train_inner | epoch 029:     79 / 99 loss=2.674, ppl=6.38, wps=12925.8, ups=12, wpb=1077.3, bsz=32, num_updates=2850, lr=2.63846e-05, gnorm=2.585, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=382
2022-01-09 20:56:23 | INFO | train_inner | epoch 029:     89 / 99 loss=2.33, ppl=5.03, wps=9829.8, ups=12.66, wpb=776.3, bsz=32, num_updates=2860, lr=2.63692e-05, gnorm=2.621, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=383
2022-01-09 20:56:24 | INFO | train_inner | epoch 029:     99 / 99 loss=2.594, ppl=6.04, wps=9014.1, ups=9.58, wpb=940.8, bsz=32, num_updates=2870, lr=2.63538e-05, gnorm=2.456, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=384
2022-01-09 20:56:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:56:25 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 2.59 | ppl 6.02 | wps 31662.4 | wpb 930.4 | bsz 31.3 | num_updates 2870 | best_loss 2.59
2022-01-09 20:56:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2870 updates
2022-01-09 20:56:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:56:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 29 @ 2870 updates, score 2.59) (writing took 4.409028484951705 seconds)
2022-01-09 20:56:29 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-09 20:56:30 | INFO | train | epoch 029 | loss 2.663 | ppl 6.33 | wps 7153.2 | ups 7.17 | wpb 997.9 | bsz 31.9 | num_updates 2870 | lr 2.63538e-05 | gnorm 2.731 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.7 | wall 389
2022-01-09 20:56:30 | INFO | fairseq.trainer | begin training epoch 30
2022-01-09 20:56:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:56:30 | INFO | train_inner | epoch 030:     10 / 99 loss=2.501, ppl=5.66, wps=1452.1, ups=1.46, wpb=997.3, bsz=32, num_updates=2880, lr=2.63385e-05, gnorm=2.481, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=391
2022-01-09 20:56:31 | INFO | train_inner | epoch 030:     20 / 99 loss=2.736, ppl=6.66, wps=11682.4, ups=11.17, wpb=1045.9, bsz=32, num_updates=2890, lr=2.63231e-05, gnorm=2.814, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=392
2022-01-09 20:56:32 | INFO | train_inner | epoch 030:     30 / 99 loss=2.237, ppl=4.71, wps=9840, ups=12.29, wpb=800.6, bsz=32, num_updates=2900, lr=2.63077e-05, gnorm=2.61, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=393
2022-01-09 20:56:33 | INFO | train_inner | epoch 030:     40 / 99 loss=2.735, ppl=6.66, wps=13395.1, ups=12.35, wpb=1084.7, bsz=32, num_updates=2910, lr=2.62923e-05, gnorm=2.631, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=393
2022-01-09 20:56:34 | INFO | train_inner | epoch 030:     50 / 99 loss=2.516, ppl=5.72, wps=15068.9, ups=13.22, wpb=1139.6, bsz=32, num_updates=2920, lr=2.62769e-05, gnorm=2.563, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=394
2022-01-09 20:56:35 | INFO | train_inner | epoch 030:     60 / 99 loss=2.466, ppl=5.53, wps=9880.6, ups=10.55, wpb=936.7, bsz=32, num_updates=2930, lr=2.62615e-05, gnorm=2.46, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=395
2022-01-09 20:56:36 | INFO | train_inner | epoch 030:     70 / 99 loss=3.215, ppl=9.29, wps=11754.5, ups=11.19, wpb=1050.2, bsz=31.5, num_updates=2940, lr=2.62462e-05, gnorm=2.402, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=396
2022-01-09 20:56:36 | INFO | train_inner | epoch 030:     80 / 99 loss=2.104, ppl=4.3, wps=10830.7, ups=12.58, wpb=861, bsz=32, num_updates=2950, lr=2.62308e-05, gnorm=2.546, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=397
2022-01-09 20:56:37 | INFO | train_inner | epoch 030:     90 / 99 loss=2.413, ppl=5.32, wps=11760.1, ups=12.12, wpb=970.6, bsz=32, num_updates=2960, lr=2.62154e-05, gnorm=2.912, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=398
2022-01-09 20:56:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:56:39 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 2.485 | ppl 5.6 | wps 31665.5 | wpb 930.4 | bsz 31.3 | num_updates 2969 | best_loss 2.485
2022-01-09 20:56:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2969 updates
2022-01-09 20:56:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:56:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:56:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 30 @ 2969 updates, score 2.485) (writing took 4.486738384934142 seconds)
2022-01-09 20:56:43 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-09 20:56:43 | INFO | train | epoch 030 | loss 2.567 | ppl 5.93 | wps 7083.7 | ups 7.1 | wpb 997.9 | bsz 31.9 | num_updates 2969 | lr 2.62015e-05 | gnorm 2.613 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.8 | wall 404
2022-01-09 20:56:44 | INFO | fairseq.trainer | begin training epoch 31
2022-01-09 20:56:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:56:44 | INFO | train_inner | epoch 031:      1 / 99 loss=2.516, ppl=5.72, wps=1680.8, ups=1.55, wpb=1081.8, bsz=32, num_updates=2970, lr=2.62e-05, gnorm=2.672, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=404
2022-01-09 20:56:44 | INFO | train_inner | epoch 031:     11 / 99 loss=2.414, ppl=5.33, wps=12983.6, ups=13.69, wpb=948.5, bsz=32, num_updates=2980, lr=2.61846e-05, gnorm=2.658, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=405
2022-01-09 20:56:45 | INFO | train_inner | epoch 031:     21 / 99 loss=2.243, ppl=4.73, wps=13279.7, ups=14.36, wpb=925, bsz=32, num_updates=2990, lr=2.61692e-05, gnorm=2.731, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=406
2022-01-09 20:56:46 | INFO | train_inner | epoch 031:     31 / 99 loss=2.23, ppl=4.69, wps=11127.9, ups=12.49, wpb=891.1, bsz=32, num_updates=3000, lr=2.61538e-05, gnorm=2.548, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=406
2022-01-09 20:56:47 | INFO | train_inner | epoch 031:     41 / 99 loss=2.623, ppl=6.16, wps=10953.3, ups=11.49, wpb=953.7, bsz=32, num_updates=3010, lr=2.61385e-05, gnorm=2.511, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=407
2022-01-09 20:56:48 | INFO | train_inner | epoch 031:     51 / 99 loss=2.635, ppl=6.21, wps=11777, ups=11.84, wpb=994.7, bsz=32, num_updates=3020, lr=2.61231e-05, gnorm=2.959, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=408
2022-01-09 20:56:48 | INFO | train_inner | epoch 031:     61 / 99 loss=2.533, ppl=5.79, wps=14770.9, ups=12.48, wpb=1183.1, bsz=32, num_updates=3030, lr=2.61077e-05, gnorm=2.582, clip=100, loss_scale=64, train_wall=1, gb_free=19.4, wall=409
2022-01-09 20:56:49 | INFO | train_inner | epoch 031:     71 / 99 loss=3.027, ppl=8.15, wps=15703.4, ups=12, wpb=1308.7, bsz=31.5, num_updates=3040, lr=2.60923e-05, gnorm=2.404, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=410
2022-01-09 20:56:50 | INFO | train_inner | epoch 031:     81 / 99 loss=2.089, ppl=4.26, wps=12038.9, ups=14.38, wpb=837.3, bsz=32, num_updates=3050, lr=2.60769e-05, gnorm=2.627, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=410
2022-01-09 20:56:51 | INFO | train_inner | epoch 031:     91 / 99 loss=2.704, ppl=6.52, wps=15133.9, ups=12.84, wpb=1178.7, bsz=32, num_updates=3060, lr=2.60615e-05, gnorm=2.426, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=411
2022-01-09 20:56:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:56:52 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 2.418 | ppl 5.35 | wps 30561.6 | wpb 930.4 | bsz 31.3 | num_updates 3068 | best_loss 2.418
2022-01-09 20:56:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 3068 updates
2022-01-09 20:56:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:56:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:56:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 31 @ 3068 updates, score 2.418) (writing took 4.192032146966085 seconds)
2022-01-09 20:56:57 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-09 20:56:57 | INFO | train | epoch 031 | loss 2.494 | ppl 5.63 | wps 7576.4 | ups 7.59 | wpb 997.9 | bsz 31.9 | num_updates 3068 | lr 2.60492e-05 | gnorm 2.599 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 417
2022-01-09 20:56:57 | INFO | fairseq.trainer | begin training epoch 32
2022-01-09 20:56:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:56:57 | INFO | train_inner | epoch 032:      2 / 99 loss=2.084, ppl=4.24, wps=1374.5, ups=1.66, wpb=827, bsz=32, num_updates=3070, lr=2.60462e-05, gnorm=2.61, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=417
2022-01-09 20:56:57 | INFO | train_inner | epoch 032:     12 / 99 loss=2.04, ppl=4.11, wps=10533.7, ups=14.79, wpb=712.3, bsz=32, num_updates=3080, lr=2.60308e-05, gnorm=2.556, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=418
2022-01-09 20:56:58 | INFO | train_inner | epoch 032:     22 / 99 loss=2.278, ppl=4.85, wps=14476.1, ups=14.05, wpb=1030.1, bsz=32, num_updates=3090, lr=2.60154e-05, gnorm=2.342, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=419
2022-01-09 20:56:59 | INFO | train_inner | epoch 032:     32 / 99 loss=2.804, ppl=6.98, wps=14020.1, ups=13.18, wpb=1063.7, bsz=31.5, num_updates=3100, lr=2.6e-05, gnorm=2.708, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=419
2022-01-09 20:57:00 | INFO | train_inner | epoch 032:     42 / 99 loss=2.198, ppl=4.59, wps=13162.4, ups=13.92, wpb=945.9, bsz=32, num_updates=3110, lr=2.59846e-05, gnorm=2.461, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=420
2022-01-09 20:57:00 | INFO | train_inner | epoch 032:     52 / 99 loss=2.56, ppl=5.9, wps=11931.4, ups=12.23, wpb=975.3, bsz=32, num_updates=3120, lr=2.59692e-05, gnorm=2.474, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=421
2022-01-09 20:57:01 | INFO | train_inner | epoch 032:     62 / 99 loss=2.016, ppl=4.05, wps=12868, ups=14.21, wpb=905.7, bsz=32, num_updates=3130, lr=2.59538e-05, gnorm=2.367, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=422
2022-01-09 20:57:02 | INFO | train_inner | epoch 032:     72 / 99 loss=2.598, ppl=6.05, wps=15523.1, ups=12.59, wpb=1233.3, bsz=32, num_updates=3140, lr=2.59385e-05, gnorm=2.321, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=422
2022-01-09 20:57:03 | INFO | train_inner | epoch 032:     82 / 99 loss=2.036, ppl=4.1, wps=13755, ups=15.21, wpb=904.1, bsz=32, num_updates=3150, lr=2.59231e-05, gnorm=2.398, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=423
2022-01-09 20:57:03 | INFO | train_inner | epoch 032:     92 / 99 loss=2.428, ppl=5.38, wps=13195.5, ups=12.51, wpb=1055.1, bsz=32, num_updates=3160, lr=2.59077e-05, gnorm=2.35, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=424
2022-01-09 20:57:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:57:05 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 2.34 | ppl 5.06 | wps 30806.4 | wpb 930.4 | bsz 31.3 | num_updates 3167 | best_loss 2.34
2022-01-09 20:57:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3167 updates
2022-01-09 20:57:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:57:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:57:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 32 @ 3167 updates, score 2.34) (writing took 4.225409358041361 seconds)
2022-01-09 20:57:09 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-09 20:57:09 | INFO | train | epoch 032 | loss 2.402 | ppl 5.28 | wps 7781 | ups 7.8 | wpb 997.9 | bsz 31.9 | num_updates 3167 | lr 2.58969e-05 | gnorm 2.455 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.8 | wall 430
2022-01-09 20:57:09 | INFO | fairseq.trainer | begin training epoch 33
2022-01-09 20:57:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:57:10 | INFO | train_inner | epoch 033:      3 / 99 loss=2.662, ppl=6.33, wps=1522.5, ups=1.6, wpb=954.4, bsz=32, num_updates=3170, lr=2.58923e-05, gnorm=2.515, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=430
2022-01-09 20:57:10 | INFO | train_inner | epoch 033:     13 / 99 loss=1.969, ppl=3.91, wps=13210.8, ups=14.46, wpb=913.5, bsz=32, num_updates=3180, lr=2.58769e-05, gnorm=2.365, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=431
2022-01-09 20:57:11 | INFO | train_inner | epoch 033:     23 / 99 loss=2.675, ppl=6.38, wps=15262.8, ups=12.69, wpb=1202.7, bsz=31.5, num_updates=3190, lr=2.58615e-05, gnorm=2.394, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=432
2022-01-09 20:57:12 | INFO | train_inner | epoch 033:     33 / 99 loss=2.361, ppl=5.14, wps=12636.5, ups=13.14, wpb=961.5, bsz=32, num_updates=3200, lr=2.58462e-05, gnorm=2.399, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=432
2022-01-09 20:57:13 | INFO | train_inner | epoch 033:     43 / 99 loss=2.381, ppl=5.21, wps=14134.2, ups=13.53, wpb=1044.8, bsz=32, num_updates=3210, lr=2.58308e-05, gnorm=2.294, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=433
2022-01-09 20:57:13 | INFO | train_inner | epoch 033:     53 / 99 loss=2.173, ppl=4.51, wps=12550.6, ups=14.14, wpb=887.9, bsz=32, num_updates=3220, lr=2.58154e-05, gnorm=2.428, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=434
2022-01-09 20:57:14 | INFO | train_inner | epoch 033:     63 / 99 loss=2.192, ppl=4.57, wps=13086.6, ups=14.84, wpb=881.7, bsz=32, num_updates=3230, lr=2.58e-05, gnorm=2.666, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=435
2022-01-09 20:57:15 | INFO | train_inner | epoch 033:     73 / 99 loss=2.316, ppl=4.98, wps=15299.8, ups=12.91, wpb=1185.3, bsz=32, num_updates=3240, lr=2.57846e-05, gnorm=2.487, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=435
2022-01-09 20:57:16 | INFO | train_inner | epoch 033:     83 / 99 loss=2.328, ppl=5.02, wps=13218, ups=13.85, wpb=954.1, bsz=32, num_updates=3250, lr=2.57692e-05, gnorm=2.453, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=436
2022-01-09 20:57:16 | INFO | train_inner | epoch 033:     93 / 99 loss=2.536, ppl=5.8, wps=13509.6, ups=11.72, wpb=1153.1, bsz=32, num_updates=3260, lr=2.57538e-05, gnorm=2.286, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=437
2022-01-09 20:57:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:57:18 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 2.321 | ppl 5 | wps 29011.7 | wpb 930.4 | bsz 31.3 | num_updates 3266 | best_loss 2.321
2022-01-09 20:57:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3266 updates
2022-01-09 20:57:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:57:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:57:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 33 @ 3266 updates, score 2.321) (writing took 4.253874043934047 seconds)
2022-01-09 20:57:22 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-09 20:57:22 | INFO | train | epoch 033 | loss 2.314 | ppl 4.97 | wps 7704.8 | ups 7.72 | wpb 997.9 | bsz 31.9 | num_updates 3266 | lr 2.57446e-05 | gnorm 2.427 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.8 | wall 443
2022-01-09 20:57:22 | INFO | fairseq.trainer | begin training epoch 34
2022-01-09 20:57:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:57:23 | INFO | train_inner | epoch 034:      4 / 99 loss=1.932, ppl=3.82, wps=1379.8, ups=1.62, wpb=850.3, bsz=32, num_updates=3270, lr=2.57385e-05, gnorm=2.437, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=443
2022-01-09 20:57:23 | INFO | train_inner | epoch 034:     14 / 99 loss=2.221, ppl=4.66, wps=13240, ups=14.03, wpb=944, bsz=32, num_updates=3280, lr=2.57231e-05, gnorm=2.334, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=444
2022-01-09 20:57:24 | INFO | train_inner | epoch 034:     24 / 99 loss=2.655, ppl=6.3, wps=13713, ups=11.14, wpb=1230.8, bsz=31.5, num_updates=3290, lr=2.57077e-05, gnorm=2.258, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=445
2022-01-09 20:57:25 | INFO | train_inner | epoch 034:     34 / 99 loss=2.514, ppl=5.71, wps=13745.8, ups=11.99, wpb=1146.8, bsz=32, num_updates=3300, lr=2.56923e-05, gnorm=2.392, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=446
2022-01-09 20:57:26 | INFO | train_inner | epoch 034:     44 / 99 loss=2.029, ppl=4.08, wps=11024.1, ups=11.27, wpb=978.5, bsz=32, num_updates=3310, lr=2.56769e-05, gnorm=2.441, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=446
2022-01-09 20:57:27 | INFO | train_inner | epoch 034:     54 / 99 loss=1.844, ppl=3.59, wps=10813.4, ups=13.84, wpb=781.4, bsz=32, num_updates=3320, lr=2.56615e-05, gnorm=2.48, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=447
2022-01-09 20:57:28 | INFO | train_inner | epoch 034:     64 / 99 loss=2.096, ppl=4.28, wps=10739.6, ups=10.5, wpb=1022.4, bsz=32, num_updates=3330, lr=2.56462e-05, gnorm=2.705, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=448
2022-01-09 20:57:28 | INFO | train_inner | epoch 034:     74 / 99 loss=2.334, ppl=5.04, wps=13009.5, ups=12.01, wpb=1083.3, bsz=32, num_updates=3340, lr=2.56308e-05, gnorm=3.072, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=449
2022-01-09 20:57:29 | INFO | train_inner | epoch 034:     84 / 99 loss=2.275, ppl=4.84, wps=12742.8, ups=13.52, wpb=942.6, bsz=32, num_updates=3350, lr=2.56154e-05, gnorm=2.531, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=450
2022-01-09 20:57:30 | INFO | train_inner | epoch 034:     94 / 99 loss=2.411, ppl=5.32, wps=10918.2, ups=9.54, wpb=1144.2, bsz=32, num_updates=3360, lr=2.56e-05, gnorm=2.257, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=451
2022-01-09 20:57:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:57:32 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 2.232 | ppl 4.7 | wps 30959.2 | wpb 930.4 | bsz 31.3 | num_updates 3365 | best_loss 2.232
2022-01-09 20:57:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3365 updates
2022-01-09 20:57:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:57:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:57:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 34 @ 3365 updates, score 2.232) (writing took 4.258639981970191 seconds)
2022-01-09 20:57:36 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-09 20:57:36 | INFO | train | epoch 034 | loss 2.256 | ppl 4.78 | wps 7233.6 | ups 7.25 | wpb 997.9 | bsz 31.9 | num_updates 3365 | lr 2.55923e-05 | gnorm 2.481 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 456
2022-01-09 20:57:36 | INFO | fairseq.trainer | begin training epoch 35
2022-01-09 20:57:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:57:36 | INFO | train_inner | epoch 035:      5 / 99 loss=2.174, ppl=4.51, wps=1404.1, ups=1.63, wpb=860.3, bsz=32, num_updates=3370, lr=2.55846e-05, gnorm=2.359, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=457
2022-01-09 20:57:37 | INFO | train_inner | epoch 035:     15 / 99 loss=2.094, ppl=4.27, wps=13182.1, ups=13.73, wpb=959.8, bsz=32, num_updates=3380, lr=2.55692e-05, gnorm=2.285, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=458
2022-01-09 20:57:38 | INFO | train_inner | epoch 035:     25 / 99 loss=1.911, ppl=3.76, wps=12987.6, ups=12.76, wpb=1018.1, bsz=32, num_updates=3390, lr=2.55538e-05, gnorm=2.355, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=458
2022-01-09 20:57:39 | INFO | train_inner | epoch 035:     35 / 99 loss=1.841, ppl=3.58, wps=12910.4, ups=15.25, wpb=846.5, bsz=32, num_updates=3400, lr=2.55385e-05, gnorm=2.571, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=459
2022-01-09 20:57:39 | INFO | train_inner | epoch 035:     45 / 99 loss=2.669, ppl=6.36, wps=16362.2, ups=12.66, wpb=1292.4, bsz=31.5, num_updates=3410, lr=2.55231e-05, gnorm=2.33, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=460
2022-01-09 20:57:40 | INFO | train_inner | epoch 035:     55 / 99 loss=2.419, ppl=5.35, wps=12622.7, ups=12.46, wpb=1012.9, bsz=32, num_updates=3420, lr=2.55077e-05, gnorm=2.375, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=461
2022-01-09 20:57:41 | INFO | train_inner | epoch 035:     65 / 99 loss=2.119, ppl=4.35, wps=12434.2, ups=13.59, wpb=914.8, bsz=32, num_updates=3430, lr=2.54923e-05, gnorm=2.401, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=461
2022-01-09 20:57:42 | INFO | train_inner | epoch 035:     75 / 99 loss=1.857, ppl=3.62, wps=12646.4, ups=14.43, wpb=876.4, bsz=32, num_updates=3440, lr=2.54769e-05, gnorm=2.645, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=462
2022-01-09 20:57:42 | INFO | train_inner | epoch 035:     85 / 99 loss=1.969, ppl=3.91, wps=10556.1, ups=12.8, wpb=824.8, bsz=32, num_updates=3450, lr=2.54615e-05, gnorm=2.542, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=463
2022-01-09 20:57:43 | INFO | train_inner | epoch 035:     95 / 99 loss=1.77, ppl=3.41, wps=9659, ups=12.04, wpb=802.2, bsz=32, num_updates=3460, lr=2.54462e-05, gnorm=2.373, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=464
2022-01-09 20:57:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:57:44 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 2.171 | ppl 4.5 | wps 32482.1 | wpb 930.4 | bsz 31.3 | num_updates 3464 | best_loss 2.171
2022-01-09 20:57:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3464 updates
2022-01-09 20:57:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:57:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:57:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 35 @ 3464 updates, score 2.171) (writing took 5.416822765022516 seconds)
2022-01-09 20:57:50 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-09 20:57:50 | INFO | train | epoch 035 | loss 2.202 | ppl 4.6 | wps 7051.5 | ups 7.07 | wpb 997.9 | bsz 31.9 | num_updates 3464 | lr 2.544e-05 | gnorm 2.418 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 19.8 | wall 470
2022-01-09 20:57:50 | INFO | fairseq.trainer | begin training epoch 36
2022-01-09 20:57:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:57:50 | INFO | train_inner | epoch 036:      6 / 99 loss=2.612, ppl=6.12, wps=1566.2, ups=1.39, wpb=1123.7, bsz=32, num_updates=3470, lr=2.54308e-05, gnorm=2.289, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=471
2022-01-09 20:57:51 | INFO | train_inner | epoch 036:     16 / 99 loss=1.729, ppl=3.31, wps=11391.6, ups=14.14, wpb=805.6, bsz=32, num_updates=3480, lr=2.54154e-05, gnorm=2.303, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=472
2022-01-09 20:57:52 | INFO | train_inner | epoch 036:     26 / 99 loss=2.208, ppl=4.62, wps=14303.8, ups=12.92, wpb=1107, bsz=32, num_updates=3490, lr=2.54e-05, gnorm=2.4, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=472
2022-01-09 20:57:53 | INFO | train_inner | epoch 036:     36 / 99 loss=1.88, ppl=3.68, wps=13155.9, ups=14.93, wpb=881.1, bsz=32, num_updates=3500, lr=2.53846e-05, gnorm=2.479, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=473
2022-01-09 20:57:53 | INFO | train_inner | epoch 036:     46 / 99 loss=1.856, ppl=3.62, wps=12370.9, ups=13.45, wpb=919.9, bsz=32, num_updates=3510, lr=2.53692e-05, gnorm=2.309, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=474
2022-01-09 20:57:54 | INFO | train_inner | epoch 036:     56 / 99 loss=2.082, ppl=4.23, wps=13391.8, ups=13.11, wpb=1021.3, bsz=32, num_updates=3520, lr=2.53538e-05, gnorm=2.237, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=475
2022-01-09 20:57:55 | INFO | train_inner | epoch 036:     66 / 99 loss=2.527, ppl=5.76, wps=13544.3, ups=12.82, wpb=1056.2, bsz=31.5, num_updates=3530, lr=2.53385e-05, gnorm=2.472, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=475
2022-01-09 20:57:56 | INFO | train_inner | epoch 036:     76 / 99 loss=2.196, ppl=4.58, wps=15519.8, ups=13.05, wpb=1189.4, bsz=32, num_updates=3540, lr=2.53231e-05, gnorm=2.412, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=476
2022-01-09 20:57:56 | INFO | train_inner | epoch 036:     86 / 99 loss=2.743, ppl=6.7, wps=16725.9, ups=12.11, wpb=1381.7, bsz=32, num_updates=3550, lr=2.53077e-05, gnorm=2.295, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=477
2022-01-09 20:57:57 | INFO | train_inner | epoch 036:     96 / 99 loss=1.746, ppl=3.36, wps=9582.9, ups=11.72, wpb=817.4, bsz=32, num_updates=3560, lr=2.52923e-05, gnorm=2.344, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=478
2022-01-09 20:57:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:57:58 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 2.158 | ppl 4.46 | wps 32498.5 | wpb 930.4 | bsz 31.3 | num_updates 3563 | best_loss 2.158
2022-01-09 20:57:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3563 updates
2022-01-09 20:57:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:58:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:58:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 36 @ 3563 updates, score 2.158) (writing took 4.192025514086708 seconds)
2022-01-09 20:58:03 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-09 20:58:03 | INFO | train | epoch 036 | loss 2.131 | ppl 4.38 | wps 7730.7 | ups 7.75 | wpb 997.9 | bsz 31.9 | num_updates 3563 | lr 2.52877e-05 | gnorm 2.359 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.4 | wall 483
2022-01-09 20:58:03 | INFO | fairseq.trainer | begin training epoch 37
2022-01-09 20:58:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:58:03 | INFO | train_inner | epoch 037:      7 / 99 loss=1.703, ppl=3.25, wps=1537.1, ups=1.68, wpb=916.1, bsz=32, num_updates=3570, lr=2.52769e-05, gnorm=2.267, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=484
2022-01-09 20:58:04 | INFO | train_inner | epoch 037:     17 / 99 loss=2.106, ppl=4.31, wps=13285.5, ups=14.33, wpb=927.2, bsz=32, num_updates=3580, lr=2.52615e-05, gnorm=2.404, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=484
2022-01-09 20:58:05 | INFO | train_inner | epoch 037:     27 / 99 loss=1.797, ppl=3.47, wps=11674, ups=12.78, wpb=913.2, bsz=32, num_updates=3590, lr=2.52462e-05, gnorm=2.235, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=485
2022-01-09 20:58:05 | INFO | train_inner | epoch 037:     37 / 99 loss=1.808, ppl=3.5, wps=11616.3, ups=13.59, wpb=855, bsz=32, num_updates=3600, lr=2.52308e-05, gnorm=2.197, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=486
2022-01-09 20:58:06 | INFO | train_inner | epoch 037:     47 / 99 loss=2.106, ppl=4.31, wps=13156.6, ups=13.9, wpb=946.8, bsz=32, num_updates=3610, lr=2.52154e-05, gnorm=2.357, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=487
2022-01-09 20:58:07 | INFO | train_inner | epoch 037:     57 / 99 loss=2.372, ppl=5.18, wps=13094.8, ups=12.28, wpb=1066.7, bsz=32, num_updates=3620, lr=2.52e-05, gnorm=2.325, clip=100, loss_scale=64, train_wall=1, gb_free=19.4, wall=488
2022-01-09 20:58:08 | INFO | train_inner | epoch 037:     67 / 99 loss=2.36, ppl=5.13, wps=14546, ups=13.34, wpb=1090.1, bsz=32, num_updates=3630, lr=2.51846e-05, gnorm=2.624, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=488
2022-01-09 20:58:09 | INFO | train_inner | epoch 037:     77 / 99 loss=1.8, ppl=3.48, wps=13446.7, ups=13.72, wpb=979.8, bsz=32, num_updates=3640, lr=2.51692e-05, gnorm=2.471, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=489
2022-01-09 20:58:09 | INFO | train_inner | epoch 037:     87 / 99 loss=1.938, ppl=3.83, wps=13588.3, ups=13.45, wpb=1010.2, bsz=32, num_updates=3650, lr=2.51538e-05, gnorm=2.374, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=490
2022-01-09 20:58:10 | INFO | train_inner | epoch 037:     97 / 99 loss=2.491, ppl=5.62, wps=14445.7, ups=11.61, wpb=1243.9, bsz=31.5, num_updates=3660, lr=2.51385e-05, gnorm=2.518, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=491
2022-01-09 20:58:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:58:11 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 2.093 | ppl 4.27 | wps 32333.1 | wpb 930.4 | bsz 31.3 | num_updates 3662 | best_loss 2.093
2022-01-09 20:58:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3662 updates
2022-01-09 20:58:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:58:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:58:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 37 @ 3662 updates, score 2.093) (writing took 4.350287061999552 seconds)
2022-01-09 20:58:16 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-09 20:58:16 | INFO | train | epoch 037 | loss 2.079 | ppl 4.22 | wps 7653.8 | ups 7.67 | wpb 997.9 | bsz 31.9 | num_updates 3662 | lr 2.51354e-05 | gnorm 2.383 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 19.8 | wall 496
2022-01-09 20:58:16 | INFO | fairseq.trainer | begin training epoch 38
2022-01-09 20:58:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:58:16 | INFO | train_inner | epoch 038:      8 / 99 loss=2.518, ppl=5.73, wps=2153.9, ups=1.59, wpb=1350.8, bsz=31.5, num_updates=3670, lr=2.51231e-05, gnorm=2.61, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=497
2022-01-09 20:58:17 | INFO | train_inner | epoch 038:     18 / 99 loss=1.971, ppl=3.92, wps=13596.4, ups=13.63, wpb=997.3, bsz=32, num_updates=3680, lr=2.51077e-05, gnorm=2.497, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=498
2022-01-09 20:58:18 | INFO | train_inner | epoch 038:     28 / 99 loss=2.137, ppl=4.4, wps=11930.4, ups=12.81, wpb=931, bsz=32, num_updates=3690, lr=2.50923e-05, gnorm=2.421, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=498
2022-01-09 20:58:19 | INFO | train_inner | epoch 038:     38 / 99 loss=1.85, ppl=3.6, wps=12592.8, ups=14.2, wpb=887.1, bsz=32, num_updates=3700, lr=2.50769e-05, gnorm=2.385, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=499
2022-01-09 20:58:19 | INFO | train_inner | epoch 038:     48 / 99 loss=2.038, ppl=4.11, wps=12709.4, ups=12.84, wpb=989.8, bsz=32, num_updates=3710, lr=2.50615e-05, gnorm=2.254, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=500
2022-01-09 20:58:20 | INFO | train_inner | epoch 038:     58 / 99 loss=1.781, ppl=3.44, wps=12260.1, ups=13.26, wpb=924.6, bsz=32, num_updates=3720, lr=2.50462e-05, gnorm=2.193, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=501
2022-01-09 20:58:21 | INFO | train_inner | epoch 038:     68 / 99 loss=2.066, ppl=4.19, wps=14192.9, ups=14.47, wpb=981.1, bsz=32, num_updates=3730, lr=2.50308e-05, gnorm=2.325, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=501
2022-01-09 20:58:22 | INFO | train_inner | epoch 038:     78 / 99 loss=1.955, ppl=3.88, wps=16412.2, ups=13.29, wpb=1235.3, bsz=32, num_updates=3740, lr=2.50154e-05, gnorm=2.131, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=502
2022-01-09 20:58:22 | INFO | train_inner | epoch 038:     88 / 99 loss=1.875, ppl=3.67, wps=10438.5, ups=12.29, wpb=849.2, bsz=32, num_updates=3750, lr=2.5e-05, gnorm=2.373, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=503
2022-01-09 20:58:23 | INFO | train_inner | epoch 038:     98 / 99 loss=1.781, ppl=3.44, wps=11259.6, ups=12.36, wpb=911.1, bsz=32, num_updates=3760, lr=2.49846e-05, gnorm=2.307, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=504
2022-01-09 20:58:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:58:24 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 2.069 | ppl 4.2 | wps 30719.6 | wpb 930.4 | bsz 31.3 | num_updates 3761 | best_loss 2.069
2022-01-09 20:58:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3761 updates
2022-01-09 20:58:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:58:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:58:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 38 @ 3761 updates, score 2.069) (writing took 4.0545507109491155 seconds)
2022-01-09 20:58:28 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-09 20:58:28 | INFO | train | epoch 038 | loss 2.015 | ppl 4.04 | wps 7740.5 | ups 7.76 | wpb 997.9 | bsz 31.9 | num_updates 3761 | lr 2.49831e-05 | gnorm 2.344 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.7 | wall 509
2022-01-09 20:58:28 | INFO | fairseq.trainer | begin training epoch 39
2022-01-09 20:58:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:58:29 | INFO | train_inner | epoch 039:      9 / 99 loss=2.017, ppl=4.05, wps=1913.7, ups=1.69, wpb=1133, bsz=32, num_updates=3770, lr=2.49692e-05, gnorm=2.28, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=510
2022-01-09 20:58:30 | INFO | train_inner | epoch 039:     19 / 99 loss=1.526, ppl=2.88, wps=10016.6, ups=13.52, wpb=740.8, bsz=32, num_updates=3780, lr=2.49538e-05, gnorm=2.161, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=510
2022-01-09 20:58:31 | INFO | train_inner | epoch 039:     29 / 99 loss=1.716, ppl=3.29, wps=10464.1, ups=11.89, wpb=880.3, bsz=32, num_updates=3790, lr=2.49385e-05, gnorm=2.318, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=511
2022-01-09 20:58:32 | INFO | train_inner | epoch 039:     39 / 99 loss=1.798, ppl=3.48, wps=10653.9, ups=11.14, wpb=956.3, bsz=32, num_updates=3800, lr=2.49231e-05, gnorm=2.231, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=512
2022-01-09 20:58:33 | INFO | train_inner | epoch 039:     49 / 99 loss=1.84, ppl=3.58, wps=12081.8, ups=10.7, wpb=1129.5, bsz=32, num_updates=3810, lr=2.49077e-05, gnorm=2.191, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=513
2022-01-09 20:58:34 | INFO | train_inner | epoch 039:     59 / 99 loss=2.053, ppl=4.15, wps=11751.5, ups=10.88, wpb=1080.5, bsz=32, num_updates=3820, lr=2.48923e-05, gnorm=2.384, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=514
2022-01-09 20:58:34 | INFO | train_inner | epoch 039:     69 / 99 loss=1.61, ppl=3.05, wps=8753.1, ups=11.75, wpb=745.1, bsz=32, num_updates=3830, lr=2.48769e-05, gnorm=2.352, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=515
2022-01-09 20:58:35 | INFO | train_inner | epoch 039:     79 / 99 loss=1.759, ppl=3.38, wps=11264.2, ups=13.64, wpb=825.9, bsz=32, num_updates=3840, lr=2.48615e-05, gnorm=2.535, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=516
2022-01-09 20:58:36 | INFO | train_inner | epoch 039:     89 / 99 loss=2.475, ppl=5.56, wps=12974.2, ups=10.66, wpb=1217.5, bsz=32, num_updates=3850, lr=2.48462e-05, gnorm=2.344, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=517
2022-01-09 20:58:37 | INFO | train_inner | epoch 039:     99 / 99 loss=2.431, ppl=5.39, wps=13216.4, ups=10.52, wpb=1256.3, bsz=31.5, num_updates=3860, lr=2.48308e-05, gnorm=2.405, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=518
2022-01-09 20:58:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:58:38 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 2.005 | ppl 4.01 | wps 32184.1 | wpb 930.4 | bsz 31.3 | num_updates 3860 | best_loss 2.005
2022-01-09 20:58:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3860 updates
2022-01-09 20:58:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:58:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:58:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 39 @ 3860 updates, score 2.005) (writing took 4.753628809005022 seconds)
2022-01-09 20:58:43 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-09 20:58:43 | INFO | train | epoch 039 | loss 1.976 | ppl 3.93 | wps 6898.3 | ups 6.91 | wpb 997.9 | bsz 31.9 | num_updates 3860 | lr 2.48308e-05 | gnorm 2.322 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.4 | wall 523
2022-01-09 20:58:43 | INFO | fairseq.trainer | begin training epoch 40
2022-01-09 20:58:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:58:43 | INFO | train_inner | epoch 040:     10 / 99 loss=1.624, ppl=3.08, wps=1393.5, ups=1.54, wpb=904.1, bsz=32, num_updates=3870, lr=2.48154e-05, gnorm=2.27, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=524
2022-01-09 20:58:44 | INFO | train_inner | epoch 040:     20 / 99 loss=1.592, ppl=3.01, wps=10564.2, ups=13.66, wpb=773.6, bsz=32, num_updates=3880, lr=2.48e-05, gnorm=2.316, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=525
2022-01-09 20:58:45 | INFO | train_inner | epoch 040:     30 / 99 loss=1.951, ppl=3.87, wps=13472, ups=12.29, wpb=1096.6, bsz=32, num_updates=3890, lr=2.47846e-05, gnorm=2.291, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=526
2022-01-09 20:58:46 | INFO | train_inner | epoch 040:     40 / 99 loss=1.95, ppl=3.86, wps=14024.7, ups=13.64, wpb=1028.3, bsz=32, num_updates=3900, lr=2.47692e-05, gnorm=2.265, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=526
2022-01-09 20:58:47 | INFO | train_inner | epoch 040:     50 / 99 loss=1.794, ppl=3.47, wps=11791, ups=13.63, wpb=865.1, bsz=32, num_updates=3910, lr=2.47538e-05, gnorm=2.351, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=527
2022-01-09 20:58:47 | INFO | train_inner | epoch 040:     60 / 99 loss=2.122, ppl=4.35, wps=12612.2, ups=13.01, wpb=969.5, bsz=32, num_updates=3920, lr=2.47385e-05, gnorm=2.267, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=528
2022-01-09 20:58:48 | INFO | train_inner | epoch 040:     70 / 99 loss=2.212, ppl=4.63, wps=16183.4, ups=12.74, wpb=1270.3, bsz=32, num_updates=3930, lr=2.47231e-05, gnorm=2.353, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=529
2022-01-09 20:58:49 | INFO | train_inner | epoch 040:     80 / 99 loss=1.695, ppl=3.24, wps=11105.8, ups=12.49, wpb=889.5, bsz=32, num_updates=3940, lr=2.47077e-05, gnorm=2.532, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=529
2022-01-09 20:58:50 | INFO | train_inner | epoch 040:     90 / 99 loss=1.568, ppl=2.97, wps=10037.4, ups=11.14, wpb=900.7, bsz=32, num_updates=3950, lr=2.46923e-05, gnorm=2.254, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=530
2022-01-09 20:58:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:58:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 1.982 | ppl 3.95 | wps 32032.1 | wpb 930.4 | bsz 31.3 | num_updates 3959 | best_loss 1.982
2022-01-09 20:58:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3959 updates
2022-01-09 20:58:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:58:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:58:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 40 @ 3959 updates, score 1.982) (writing took 4.2341897380538285 seconds)
2022-01-09 20:58:56 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-09 20:58:56 | INFO | train | epoch 040 | loss 1.93 | ppl 3.81 | wps 7473.8 | ups 7.49 | wpb 997.9 | bsz 31.9 | num_updates 3959 | lr 2.46785e-05 | gnorm 2.322 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 536
2022-01-09 20:58:56 | INFO | fairseq.trainer | begin training epoch 41
2022-01-09 20:58:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:58:56 | INFO | train_inner | epoch 041:      1 / 99 loss=2.394, ppl=5.26, wps=1885.2, ups=1.54, wpb=1222.4, bsz=31.5, num_updates=3960, lr=2.46769e-05, gnorm=2.331, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=537
2022-01-09 20:58:57 | INFO | train_inner | epoch 041:     11 / 99 loss=1.89, ppl=3.71, wps=14837.4, ups=13.1, wpb=1132.8, bsz=32, num_updates=3970, lr=2.46615e-05, gnorm=2.301, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=538
2022-01-09 20:58:58 | INFO | train_inner | epoch 041:     21 / 99 loss=1.801, ppl=3.49, wps=15312.4, ups=13.03, wpb=1175.1, bsz=32, num_updates=3980, lr=2.46462e-05, gnorm=2.29, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=538
2022-01-09 20:58:58 | INFO | train_inner | epoch 041:     31 / 99 loss=1.554, ppl=2.94, wps=12402.8, ups=14.84, wpb=835.7, bsz=32, num_updates=3990, lr=2.46308e-05, gnorm=2.283, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=539
2022-01-09 20:58:59 | INFO | train_inner | epoch 041:     41 / 99 loss=2.024, ppl=4.07, wps=13073.7, ups=13.88, wpb=941.6, bsz=32, num_updates=4000, lr=2.46154e-05, gnorm=2.252, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=540
2022-01-09 20:59:00 | INFO | train_inner | epoch 041:     51 / 99 loss=1.71, ppl=3.27, wps=12761.4, ups=13.25, wpb=963, bsz=32, num_updates=4010, lr=2.46e-05, gnorm=2.274, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=540
2022-01-09 20:59:01 | INFO | train_inner | epoch 041:     61 / 99 loss=1.789, ppl=3.46, wps=13217.5, ups=14.38, wpb=919, bsz=32, num_updates=4020, lr=2.45846e-05, gnorm=2.304, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=541
2022-01-09 20:59:01 | INFO | train_inner | epoch 041:     71 / 99 loss=2.287, ppl=4.88, wps=14792.7, ups=12.19, wpb=1213.5, bsz=31.5, num_updates=4030, lr=2.45692e-05, gnorm=2.184, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=542
2022-01-09 20:59:02 | INFO | train_inner | epoch 041:     81 / 99 loss=2.048, ppl=4.13, wps=14540.4, ups=12.17, wpb=1194.3, bsz=32, num_updates=4040, lr=2.45538e-05, gnorm=2.229, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=543
2022-01-09 20:59:03 | INFO | train_inner | epoch 041:     91 / 99 loss=1.455, ppl=2.74, wps=8896.4, ups=12.39, wpb=718.3, bsz=32, num_updates=4050, lr=2.45385e-05, gnorm=2.226, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=544
2022-01-09 20:59:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:59:05 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 1.956 | ppl 3.88 | wps 30948.4 | wpb 930.4 | bsz 31.3 | num_updates 4058 | best_loss 1.956
2022-01-09 20:59:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 4058 updates
2022-01-09 20:59:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:59:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:59:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 41 @ 4058 updates, score 1.956) (writing took 4.165692146052606 seconds)
2022-01-09 20:59:09 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-09 20:59:09 | INFO | train | epoch 041 | loss 1.879 | ppl 3.68 | wps 7629.4 | ups 7.65 | wpb 997.9 | bsz 31.9 | num_updates 4058 | lr 2.45262e-05 | gnorm 2.275 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20 | wall 550
2022-01-09 20:59:09 | INFO | fairseq.trainer | begin training epoch 42
2022-01-09 20:59:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:59:09 | INFO | train_inner | epoch 042:      2 / 99 loss=1.916, ppl=3.77, wps=1555.8, ups=1.62, wpb=963, bsz=32, num_updates=4060, lr=2.45231e-05, gnorm=2.387, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=550
2022-01-09 20:59:10 | INFO | train_inner | epoch 042:     12 / 99 loss=1.621, ppl=3.08, wps=11711.2, ups=13.73, wpb=853, bsz=32, num_updates=4070, lr=2.45077e-05, gnorm=2.4, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=551
2022-01-09 20:59:11 | INFO | train_inner | epoch 042:     22 / 99 loss=1.806, ppl=3.5, wps=15439.4, ups=14.26, wpb=1082.9, bsz=32, num_updates=4080, lr=2.44923e-05, gnorm=2.18, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=551
2022-01-09 20:59:11 | INFO | train_inner | epoch 042:     32 / 99 loss=1.962, ppl=3.9, wps=13861.8, ups=13.95, wpb=993.7, bsz=32, num_updates=4090, lr=2.44769e-05, gnorm=2.2, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=552
2022-01-09 20:59:12 | INFO | train_inner | epoch 042:     42 / 99 loss=1.822, ppl=3.54, wps=14250.7, ups=13.97, wpb=1020.1, bsz=32, num_updates=4100, lr=2.44615e-05, gnorm=2.28, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=553
2022-01-09 20:59:13 | INFO | train_inner | epoch 042:     52 / 99 loss=2.208, ppl=4.62, wps=13346.9, ups=12.64, wpb=1056.2, bsz=31.5, num_updates=4110, lr=2.44462e-05, gnorm=2.179, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=554
2022-01-09 20:59:14 | INFO | train_inner | epoch 042:     62 / 99 loss=1.827, ppl=3.55, wps=14678, ups=14.41, wpb=1018.4, bsz=32, num_updates=4120, lr=2.44308e-05, gnorm=2.198, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=554
2022-01-09 20:59:14 | INFO | train_inner | epoch 042:     72 / 99 loss=1.397, ppl=2.63, wps=10193.3, ups=14.95, wpb=681.9, bsz=32, num_updates=4130, lr=2.44154e-05, gnorm=2.316, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=555
2022-01-09 20:59:15 | INFO | train_inner | epoch 042:     82 / 99 loss=1.904, ppl=3.74, wps=12459.8, ups=13.65, wpb=912.8, bsz=32, num_updates=4140, lr=2.44e-05, gnorm=2.241, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=556
2022-01-09 20:59:16 | INFO | train_inner | epoch 042:     92 / 99 loss=1.833, ppl=3.56, wps=13975.5, ups=11.57, wpb=1208.4, bsz=32, num_updates=4150, lr=2.43846e-05, gnorm=2.188, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=556
2022-01-09 20:59:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:59:18 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 1.859 | ppl 3.63 | wps 31623.2 | wpb 930.4 | bsz 31.3 | num_updates 4157 | best_loss 1.859
2022-01-09 20:59:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4157 updates
2022-01-09 20:59:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:59:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:59:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 42 @ 4157 updates, score 1.859) (writing took 5.313288499019109 seconds)
2022-01-09 20:59:23 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-09 20:59:23 | INFO | train | epoch 042 | loss 1.828 | ppl 3.55 | wps 7178.1 | ups 7.19 | wpb 997.9 | bsz 31.9 | num_updates 4157 | lr 2.43738e-05 | gnorm 2.255 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.7 | wall 563
2022-01-09 20:59:23 | INFO | fairseq.trainer | begin training epoch 43
2022-01-09 20:59:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:59:23 | INFO | train_inner | epoch 043:      3 / 99 loss=1.711, ppl=3.27, wps=1553.8, ups=1.39, wpb=1116.1, bsz=32, num_updates=4160, lr=2.43692e-05, gnorm=2.337, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=564
2022-01-09 20:59:24 | INFO | train_inner | epoch 043:     13 / 99 loss=1.736, ppl=3.33, wps=15076, ups=13.34, wpb=1130.2, bsz=32, num_updates=4170, lr=2.43538e-05, gnorm=2.141, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=564
2022-01-09 20:59:25 | INFO | train_inner | epoch 043:     23 / 99 loss=1.908, ppl=3.75, wps=13695.6, ups=12.71, wpb=1077.8, bsz=32, num_updates=4180, lr=2.43385e-05, gnorm=2.217, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=565
2022-01-09 20:59:26 | INFO | train_inner | epoch 043:     33 / 99 loss=2.192, ppl=4.57, wps=14439.2, ups=11.06, wpb=1305.8, bsz=31.5, num_updates=4190, lr=2.43231e-05, gnorm=2.216, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=566
2022-01-09 20:59:26 | INFO | train_inner | epoch 043:     43 / 99 loss=1.518, ppl=2.86, wps=11691.3, ups=13.99, wpb=835.4, bsz=32, num_updates=4200, lr=2.43077e-05, gnorm=2.231, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=567
2022-01-09 20:59:27 | INFO | train_inner | epoch 043:     53 / 99 loss=1.513, ppl=2.85, wps=12192, ups=14.41, wpb=846.1, bsz=32, num_updates=4210, lr=2.42923e-05, gnorm=2.26, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=568
2022-01-09 20:59:28 | INFO | train_inner | epoch 043:     63 / 99 loss=1.976, ppl=3.93, wps=12954.5, ups=12.59, wpb=1029, bsz=32, num_updates=4220, lr=2.42769e-05, gnorm=2.397, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=568
2022-01-09 20:59:29 | INFO | train_inner | epoch 043:     73 / 99 loss=1.46, ppl=2.75, wps=10506.9, ups=12.81, wpb=819.9, bsz=32, num_updates=4230, lr=2.42615e-05, gnorm=2.278, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=569
2022-01-09 20:59:29 | INFO | train_inner | epoch 043:     83 / 99 loss=1.695, ppl=3.24, wps=12288.6, ups=13.18, wpb=932.5, bsz=32, num_updates=4240, lr=2.42462e-05, gnorm=2.422, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=570
2022-01-09 20:59:30 | INFO | train_inner | epoch 043:     93 / 99 loss=1.617, ppl=3.07, wps=9712.6, ups=10.42, wpb=932.3, bsz=32, num_updates=4250, lr=2.42308e-05, gnorm=2.467, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=571
2022-01-09 20:59:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:59:32 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 1.872 | ppl 3.66 | wps 31236.7 | wpb 930.4 | bsz 31.3 | num_updates 4256 | best_loss 1.859
2022-01-09 20:59:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4256 updates
2022-01-09 20:59:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 20:59:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 20:59:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 43 @ 4256 updates, score 1.872) (writing took 2.9218982768943533 seconds)
2022-01-09 20:59:35 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-09 20:59:35 | INFO | train | epoch 043 | loss 1.788 | ppl 3.45 | wps 8234.8 | ups 8.25 | wpb 997.9 | bsz 31.9 | num_updates 4256 | lr 2.42215e-05 | gnorm 2.28 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 575
2022-01-09 20:59:35 | INFO | fairseq.trainer | begin training epoch 44
2022-01-09 20:59:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:59:35 | INFO | train_inner | epoch 044:      4 / 99 loss=1.953, ppl=3.87, wps=2193.1, ups=2.05, wpb=1070.9, bsz=32, num_updates=4260, lr=2.42154e-05, gnorm=2.183, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=576
2022-01-09 20:59:36 | INFO | train_inner | epoch 044:     14 / 99 loss=1.375, ppl=2.59, wps=10182.5, ups=13.96, wpb=729.6, bsz=32, num_updates=4270, lr=2.42e-05, gnorm=2.224, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=576
2022-01-09 20:59:37 | INFO | train_inner | epoch 044:     24 / 99 loss=2.136, ppl=4.4, wps=16762, ups=12.5, wpb=1340.9, bsz=31.5, num_updates=4280, lr=2.41846e-05, gnorm=2.206, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=577
2022-01-09 20:59:37 | INFO | train_inner | epoch 044:     34 / 99 loss=1.717, ppl=3.29, wps=13179, ups=13.69, wpb=962.6, bsz=32, num_updates=4290, lr=2.41692e-05, gnorm=2.381, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=578
2022-01-09 20:59:38 | INFO | train_inner | epoch 044:     44 / 99 loss=1.589, ppl=3.01, wps=14357.4, ups=13.89, wpb=1033.5, bsz=32, num_updates=4300, lr=2.41538e-05, gnorm=2.299, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=579
2022-01-09 20:59:39 | INFO | train_inner | epoch 044:     54 / 99 loss=1.892, ppl=3.71, wps=15065.6, ups=12.11, wpb=1243.9, bsz=32, num_updates=4310, lr=2.41385e-05, gnorm=2.405, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=580
2022-01-09 20:59:40 | INFO | train_inner | epoch 044:     64 / 99 loss=1.661, ppl=3.16, wps=14499.3, ups=13.84, wpb=1047.7, bsz=32, num_updates=4320, lr=2.41231e-05, gnorm=2.279, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=580
2022-01-09 20:59:40 | INFO | train_inner | epoch 044:     74 / 99 loss=1.526, ppl=2.88, wps=13225.7, ups=14.76, wpb=895.8, bsz=32, num_updates=4330, lr=2.41077e-05, gnorm=2.235, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=581
2022-01-09 20:59:41 | INFO | train_inner | epoch 044:     84 / 99 loss=1.347, ppl=2.54, wps=9183.1, ups=11.89, wpb=772.3, bsz=32, num_updates=4340, lr=2.40923e-05, gnorm=2.152, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=582
2022-01-09 20:59:42 | INFO | train_inner | epoch 044:     94 / 99 loss=1.919, ppl=3.78, wps=10050.2, ups=10.65, wpb=943.8, bsz=32, num_updates=4350, lr=2.40769e-05, gnorm=2.384, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=583
2022-01-09 20:59:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:59:44 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 1.865 | ppl 3.64 | wps 31925.9 | wpb 930.4 | bsz 31.3 | num_updates 4355 | best_loss 1.859
2022-01-09 20:59:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4355 updates
2022-01-09 20:59:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 20:59:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 20:59:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 44 @ 4355 updates, score 1.865) (writing took 2.6821089549921453 seconds)
2022-01-09 20:59:46 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-09 20:59:46 | INFO | train | epoch 044 | loss 1.744 | ppl 3.35 | wps 8674.6 | ups 8.69 | wpb 997.9 | bsz 31.9 | num_updates 4355 | lr 2.40692e-05 | gnorm 2.278 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.8 | wall 587
2022-01-09 20:59:46 | INFO | fairseq.trainer | begin training epoch 45
2022-01-09 20:59:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 20:59:47 | INFO | train_inner | epoch 045:      5 / 99 loss=1.851, ppl=3.61, wps=2159.4, ups=2.23, wpb=969.4, bsz=32, num_updates=4360, lr=2.40615e-05, gnorm=2.199, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=587
2022-01-09 20:59:47 | INFO | train_inner | epoch 045:     15 / 99 loss=1.436, ppl=2.71, wps=13620.1, ups=14.29, wpb=953.2, bsz=32, num_updates=4370, lr=2.40462e-05, gnorm=2.205, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=588
2022-01-09 20:59:48 | INFO | train_inner | epoch 045:     25 / 99 loss=1.646, ppl=3.13, wps=11974.6, ups=13.57, wpb=882.4, bsz=32, num_updates=4380, lr=2.40308e-05, gnorm=2.314, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=589
2022-01-09 20:59:49 | INFO | train_inner | epoch 045:     35 / 99 loss=1.474, ppl=2.78, wps=11886.9, ups=14.32, wpb=830.1, bsz=32, num_updates=4390, lr=2.40154e-05, gnorm=2.295, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=589
2022-01-09 20:59:50 | INFO | train_inner | epoch 045:     45 / 99 loss=1.944, ppl=3.85, wps=16915.8, ups=13.35, wpb=1267.5, bsz=32, num_updates=4400, lr=2.4e-05, gnorm=2.184, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=590
2022-01-09 20:59:50 | INFO | train_inner | epoch 045:     55 / 99 loss=1.572, ppl=2.97, wps=12358.6, ups=14.51, wpb=851.8, bsz=32, num_updates=4410, lr=2.39846e-05, gnorm=2.278, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=591
2022-01-09 20:59:51 | INFO | train_inner | epoch 045:     65 / 99 loss=1.548, ppl=2.92, wps=13848.1, ups=13.54, wpb=1023.1, bsz=32, num_updates=4420, lr=2.39692e-05, gnorm=2.117, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=592
2022-01-09 20:59:52 | INFO | train_inner | epoch 045:     75 / 99 loss=2.139, ppl=4.41, wps=13664, ups=12.59, wpb=1085.1, bsz=31.5, num_updates=4430, lr=2.39538e-05, gnorm=2.315, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=592
2022-01-09 20:59:53 | INFO | train_inner | epoch 045:     85 / 99 loss=1.942, ppl=3.84, wps=14975.9, ups=12.1, wpb=1237.9, bsz=32, num_updates=4440, lr=2.39385e-05, gnorm=2.425, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=593
2022-01-09 20:59:53 | INFO | train_inner | epoch 045:     95 / 99 loss=1.727, ppl=3.31, wps=11640.3, ups=11.93, wpb=975.5, bsz=32, num_updates=4450, lr=2.39231e-05, gnorm=2.253, clip=100, loss_scale=64, train_wall=1, gb_free=19.1, wall=594
2022-01-09 20:59:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 20:59:55 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 1.789 | ppl 3.46 | wps 30456.6 | wpb 930.4 | bsz 31.3 | num_updates 4454 | best_loss 1.789
2022-01-09 20:59:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4454 updates
2022-01-09 20:59:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 20:59:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:00:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 45 @ 4454 updates, score 1.789) (writing took 5.684929746086709 seconds)
2022-01-09 21:00:01 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-09 21:00:01 | INFO | train | epoch 045 | loss 1.72 | ppl 3.29 | wps 6901.4 | ups 6.92 | wpb 997.9 | bsz 31.9 | num_updates 4454 | lr 2.39169e-05 | gnorm 2.267 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.2 | wall 601
2022-01-09 21:00:01 | INFO | fairseq.trainer | begin training epoch 46
2022-01-09 21:00:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:00:01 | INFO | train_inner | epoch 046:      6 / 99 loss=1.609, ppl=3.05, wps=1330.5, ups=1.32, wpb=1006.9, bsz=32, num_updates=4460, lr=2.39077e-05, gnorm=2.272, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=602
2022-01-09 21:00:02 | INFO | train_inner | epoch 046:     16 / 99 loss=1.566, ppl=2.96, wps=13712.7, ups=12.63, wpb=1085.5, bsz=32, num_updates=4470, lr=2.38923e-05, gnorm=2.11, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=602
2022-01-09 21:00:03 | INFO | train_inner | epoch 046:     26 / 99 loss=1.575, ppl=2.98, wps=12970.6, ups=13.23, wpb=980.3, bsz=32, num_updates=4480, lr=2.38769e-05, gnorm=2.225, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=603
2022-01-09 21:00:03 | INFO | train_inner | epoch 046:     36 / 99 loss=1.733, ppl=3.32, wps=14095.8, ups=13.25, wpb=1064.2, bsz=32, num_updates=4490, lr=2.38615e-05, gnorm=2.229, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=604
2022-01-09 21:00:04 | INFO | train_inner | epoch 046:     46 / 99 loss=2.333, ppl=5.04, wps=11798.8, ups=10.77, wpb=1096, bsz=31.5, num_updates=4500, lr=2.38462e-05, gnorm=2.138, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=605
2022-01-09 21:00:05 | INFO | train_inner | epoch 046:     56 / 99 loss=1.485, ppl=2.8, wps=12469.9, ups=12.97, wpb=961.5, bsz=32, num_updates=4510, lr=2.38308e-05, gnorm=2.253, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=606
2022-01-09 21:00:06 | INFO | train_inner | epoch 046:     66 / 99 loss=1.623, ppl=3.08, wps=14393.6, ups=13.17, wpb=1092.9, bsz=32, num_updates=4520, lr=2.38154e-05, gnorm=2.171, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=606
2022-01-09 21:00:07 | INFO | train_inner | epoch 046:     76 / 99 loss=1.782, ppl=3.44, wps=13460, ups=13.75, wpb=979.2, bsz=32, num_updates=4530, lr=2.38e-05, gnorm=2.32, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=607
2022-01-09 21:00:07 | INFO | train_inner | epoch 046:     86 / 99 loss=1.358, ppl=2.56, wps=10671.8, ups=15.09, wpb=707.1, bsz=32, num_updates=4540, lr=2.37846e-05, gnorm=2.401, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=608
2022-01-09 21:00:08 | INFO | train_inner | epoch 046:     96 / 99 loss=1.389, ppl=2.62, wps=9223.3, ups=10.75, wpb=857.9, bsz=32, num_updates=4550, lr=2.37692e-05, gnorm=2.223, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=609
2022-01-09 21:00:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:00:09 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 1.817 | ppl 3.52 | wps 31711.3 | wpb 930.4 | bsz 31.3 | num_updates 4553 | best_loss 1.789
2022-01-09 21:00:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4553 updates
2022-01-09 21:00:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:00:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:00:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 46 @ 4553 updates, score 1.817) (writing took 3.1048038109438494 seconds)
2022-01-09 21:00:13 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-09 21:00:13 | INFO | train | epoch 046 | loss 1.673 | ppl 3.19 | wps 8254.5 | ups 8.27 | wpb 997.9 | bsz 31.9 | num_updates 4553 | lr 2.37646e-05 | gnorm 2.227 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.8 | wall 613
2022-01-09 21:00:13 | INFO | fairseq.trainer | begin training epoch 47
2022-01-09 21:00:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:00:13 | INFO | train_inner | epoch 047:      7 / 99 loss=1.559, ppl=2.95, wps=2093.5, ups=2.01, wpb=1040.4, bsz=32, num_updates=4560, lr=2.37538e-05, gnorm=2.314, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=614
2022-01-09 21:00:14 | INFO | train_inner | epoch 047:     17 / 99 loss=1.695, ppl=3.24, wps=14800.5, ups=13.87, wpb=1066.9, bsz=32, num_updates=4570, lr=2.37385e-05, gnorm=2.262, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=614
2022-01-09 21:00:15 | INFO | train_inner | epoch 047:     27 / 99 loss=1.611, ppl=3.05, wps=14453.4, ups=13.9, wpb=1040.1, bsz=32, num_updates=4580, lr=2.37231e-05, gnorm=2.509, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=615
2022-01-09 21:00:15 | INFO | train_inner | epoch 047:     37 / 99 loss=1.878, ppl=3.68, wps=16452.1, ups=12.46, wpb=1320, bsz=32, num_updates=4590, lr=2.37077e-05, gnorm=2.354, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=616
2022-01-09 21:00:16 | INFO | train_inner | epoch 047:     47 / 99 loss=1.677, ppl=3.2, wps=13640.9, ups=13.89, wpb=982.2, bsz=32, num_updates=4600, lr=2.36923e-05, gnorm=2.334, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=617
2022-01-09 21:00:17 | INFO | train_inner | epoch 047:     57 / 99 loss=1.422, ppl=2.68, wps=9144.9, ups=12.39, wpb=737.8, bsz=32, num_updates=4610, lr=2.36769e-05, gnorm=2.421, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=617
2022-01-09 21:00:18 | INFO | train_inner | epoch 047:     67 / 99 loss=1.554, ppl=2.94, wps=12115.8, ups=12.55, wpb=965.7, bsz=32, num_updates=4620, lr=2.36615e-05, gnorm=2.307, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=618
2022-01-09 21:00:18 | INFO | train_inner | epoch 047:     77 / 99 loss=1.49, ppl=2.81, wps=13164, ups=13.96, wpb=943.2, bsz=32, num_updates=4630, lr=2.36462e-05, gnorm=2.185, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=619
2022-01-09 21:00:19 | INFO | train_inner | epoch 047:     87 / 99 loss=2.14, ppl=4.41, wps=14199.3, ups=11.7, wpb=1213.8, bsz=31.5, num_updates=4640, lr=2.36308e-05, gnorm=2.241, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=620
2022-01-09 21:00:20 | INFO | train_inner | epoch 047:     97 / 99 loss=1.31, ppl=2.48, wps=9561.3, ups=11.73, wpb=814.9, bsz=32, num_updates=4650, lr=2.36154e-05, gnorm=2.114, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=621
2022-01-09 21:00:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:00:21 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 1.779 | ppl 3.43 | wps 30620.4 | wpb 930.4 | bsz 31.3 | num_updates 4652 | best_loss 1.779
2022-01-09 21:00:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4652 updates
2022-01-09 21:00:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:00:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:00:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 47 @ 4652 updates, score 1.779) (writing took 4.1060814609518275 seconds)
2022-01-09 21:00:25 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-09 21:00:25 | INFO | train | epoch 047 | loss 1.657 | ppl 3.15 | wps 7646.5 | ups 7.66 | wpb 997.9 | bsz 31.9 | num_updates 4652 | lr 2.36123e-05 | gnorm 2.305 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 626
2022-01-09 21:00:25 | INFO | fairseq.trainer | begin training epoch 48
2022-01-09 21:00:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:00:26 | INFO | train_inner | epoch 048:      8 / 99 loss=1.601, ppl=3.03, wps=1740.4, ups=1.68, wpb=1038, bsz=32, num_updates=4660, lr=2.36e-05, gnorm=2.1, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=627
2022-01-09 21:00:27 | INFO | train_inner | epoch 048:     18 / 99 loss=2.232, ppl=4.7, wps=15381.9, ups=11.99, wpb=1282.5, bsz=31.5, num_updates=4670, lr=2.35846e-05, gnorm=2.249, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=627
2022-01-09 21:00:28 | INFO | train_inner | epoch 048:     28 / 99 loss=1.629, ppl=3.09, wps=14422.8, ups=13.66, wpb=1056, bsz=32, num_updates=4680, lr=2.35692e-05, gnorm=2.114, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=628
2022-01-09 21:00:28 | INFO | train_inner | epoch 048:     38 / 99 loss=1.347, ppl=2.54, wps=12693.8, ups=15.24, wpb=832.7, bsz=32, num_updates=4690, lr=2.35538e-05, gnorm=2.148, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=629
2022-01-09 21:00:29 | INFO | train_inner | epoch 048:     48 / 99 loss=1.422, ppl=2.68, wps=13591.3, ups=14.57, wpb=932.7, bsz=32, num_updates=4700, lr=2.35385e-05, gnorm=2.201, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=630
2022-01-09 21:00:30 | INFO | train_inner | epoch 048:     58 / 99 loss=1.535, ppl=2.9, wps=12560.5, ups=13.08, wpb=960.3, bsz=32, num_updates=4710, lr=2.35231e-05, gnorm=2.399, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=630
2022-01-09 21:00:31 | INFO | train_inner | epoch 048:     68 / 99 loss=1.37, ppl=2.59, wps=11002.1, ups=13.48, wpb=816, bsz=32, num_updates=4720, lr=2.35077e-05, gnorm=2.284, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=631
2022-01-09 21:00:31 | INFO | train_inner | epoch 048:     78 / 99 loss=1.794, ppl=3.47, wps=14415.2, ups=13.74, wpb=1048.8, bsz=32, num_updates=4730, lr=2.34923e-05, gnorm=2.308, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=632
2022-01-09 21:00:32 | INFO | train_inner | epoch 048:     88 / 99 loss=1.597, ppl=3.03, wps=12768.4, ups=13.24, wpb=964.2, bsz=32, num_updates=4740, lr=2.34769e-05, gnorm=2.357, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=633
2022-01-09 21:00:33 | INFO | train_inner | epoch 048:     98 / 99 loss=1.329, ppl=2.51, wps=11021.8, ups=12.27, wpb=898, bsz=32, num_updates=4750, lr=2.34615e-05, gnorm=2.15, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=633
2022-01-09 21:00:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:00:34 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 1.73 | ppl 3.32 | wps 30184.8 | wpb 930.4 | bsz 31.3 | num_updates 4751 | best_loss 1.73
2022-01-09 21:00:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4751 updates
2022-01-09 21:00:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:00:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:00:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 48 @ 4751 updates, score 1.73) (writing took 4.306438014958985 seconds)
2022-01-09 21:00:38 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-09 21:00:38 | INFO | train | epoch 048 | loss 1.632 | ppl 3.1 | wps 7701.2 | ups 7.72 | wpb 997.9 | bsz 31.9 | num_updates 4751 | lr 2.346e-05 | gnorm 2.233 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 19.6 | wall 639
2022-01-09 21:00:38 | INFO | fairseq.trainer | begin training epoch 49
2022-01-09 21:00:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:00:39 | INFO | train_inner | epoch 049:      9 / 99 loss=1.422, ppl=2.68, wps=1622.9, ups=1.63, wpb=994.8, bsz=32, num_updates=4760, lr=2.34462e-05, gnorm=2.087, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=640
2022-01-09 21:00:40 | INFO | train_inner | epoch 049:     19 / 99 loss=1.411, ppl=2.66, wps=11190.2, ups=13.23, wpb=845.5, bsz=32, num_updates=4770, lr=2.34308e-05, gnorm=2.198, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=640
2022-01-09 21:00:41 | INFO | train_inner | epoch 049:     29 / 99 loss=1.594, ppl=3.02, wps=12874.8, ups=13.14, wpb=979.9, bsz=32, num_updates=4780, lr=2.34154e-05, gnorm=2.322, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=641
2022-01-09 21:00:41 | INFO | train_inner | epoch 049:     39 / 99 loss=1.701, ppl=3.25, wps=11507.3, ups=13.73, wpb=838, bsz=32, num_updates=4790, lr=2.34e-05, gnorm=2.33, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=642
2022-01-09 21:00:42 | INFO | train_inner | epoch 049:     49 / 99 loss=1.731, ppl=3.32, wps=15103.2, ups=13.43, wpb=1124.9, bsz=32, num_updates=4800, lr=2.33846e-05, gnorm=2.147, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=643
2022-01-09 21:00:43 | INFO | train_inner | epoch 049:     59 / 99 loss=2.081, ppl=4.23, wps=15629.6, ups=11.4, wpb=1371.1, bsz=31.5, num_updates=4810, lr=2.33692e-05, gnorm=2.224, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=643
2022-01-09 21:00:44 | INFO | train_inner | epoch 049:     69 / 99 loss=1.447, ppl=2.73, wps=12020.5, ups=12.76, wpb=942.1, bsz=32, num_updates=4820, lr=2.33538e-05, gnorm=2.145, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=644
2022-01-09 21:00:44 | INFO | train_inner | epoch 049:     79 / 99 loss=1.392, ppl=2.62, wps=12403.6, ups=12.59, wpb=985.3, bsz=32, num_updates=4830, lr=2.33385e-05, gnorm=2.17, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=645
2022-01-09 21:00:45 | INFO | train_inner | epoch 049:     89 / 99 loss=1.554, ppl=2.94, wps=12744.1, ups=11.75, wpb=1085, bsz=32, num_updates=4840, lr=2.33231e-05, gnorm=2.297, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=646
2022-01-09 21:00:46 | INFO | train_inner | epoch 049:     99 / 99 loss=1.373, ppl=2.59, wps=9195.5, ups=10.33, wpb=889.8, bsz=32, num_updates=4850, lr=2.33077e-05, gnorm=2.183, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=647
2022-01-09 21:00:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:00:47 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 1.711 | ppl 3.27 | wps 32862.4 | wpb 930.4 | bsz 31.3 | num_updates 4850 | best_loss 1.711
2022-01-09 21:00:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4850 updates
2022-01-09 21:00:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:00:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:00:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 49 @ 4850 updates, score 1.711) (writing took 4.492174294078723 seconds)
2022-01-09 21:00:52 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-09 21:00:52 | INFO | train | epoch 049 | loss 1.586 | ppl 3 | wps 7351.2 | ups 7.37 | wpb 997.9 | bsz 31.9 | num_updates 4850 | lr 2.33077e-05 | gnorm 2.209 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.7 | wall 652
2022-01-09 21:00:52 | INFO | fairseq.trainer | begin training epoch 50
2022-01-09 21:00:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:00:53 | INFO | train_inner | epoch 050:     10 / 99 loss=1.909, ppl=3.76, wps=1813.5, ups=1.6, wpb=1136.4, bsz=31.5, num_updates=4860, lr=2.32923e-05, gnorm=2.156, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=653
2022-01-09 21:00:53 | INFO | train_inner | epoch 050:     20 / 99 loss=1.394, ppl=2.63, wps=11832.8, ups=13.67, wpb=865.5, bsz=32, num_updates=4870, lr=2.32769e-05, gnorm=2.216, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=654
2022-01-09 21:00:54 | INFO | train_inner | epoch 050:     30 / 99 loss=1.756, ppl=3.38, wps=14154, ups=12.34, wpb=1146.8, bsz=32, num_updates=4880, lr=2.32615e-05, gnorm=2.133, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=655
2022-01-09 21:00:55 | INFO | train_inner | epoch 050:     40 / 99 loss=1.254, ppl=2.38, wps=11005.1, ups=14.53, wpb=757.6, bsz=32, num_updates=4890, lr=2.32462e-05, gnorm=2.114, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=655
2022-01-09 21:00:56 | INFO | train_inner | epoch 050:     50 / 99 loss=1.704, ppl=3.26, wps=16176.6, ups=12.28, wpb=1317.7, bsz=32, num_updates=4900, lr=2.32308e-05, gnorm=2.112, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=656
2022-01-09 21:00:56 | INFO | train_inner | epoch 050:     60 / 99 loss=1.581, ppl=2.99, wps=12690.1, ups=13.58, wpb=934.7, bsz=32, num_updates=4910, lr=2.32154e-05, gnorm=2.209, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=657
2022-01-09 21:00:57 | INFO | train_inner | epoch 050:     70 / 99 loss=1.501, ppl=2.83, wps=14035.6, ups=12.93, wpb=1085.5, bsz=32, num_updates=4920, lr=2.32e-05, gnorm=2.163, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=658
2022-01-09 21:00:58 | INFO | train_inner | epoch 050:     80 / 99 loss=1.294, ppl=2.45, wps=12899.6, ups=15.22, wpb=847.4, bsz=32, num_updates=4930, lr=2.31846e-05, gnorm=2.148, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=658
2022-01-09 21:00:59 | INFO | train_inner | epoch 050:     90 / 99 loss=1.555, ppl=2.94, wps=14143.3, ups=13.93, wpb=1015.2, bsz=32, num_updates=4940, lr=2.31692e-05, gnorm=2.176, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=659
2022-01-09 21:00:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:01:00 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 1.739 | ppl 3.34 | wps 30720.7 | wpb 930.4 | bsz 31.3 | num_updates 4949 | best_loss 1.711
2022-01-09 21:01:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4949 updates
2022-01-09 21:01:00 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:01:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:01:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 50 @ 4949 updates, score 1.739) (writing took 2.665169554995373 seconds)
2022-01-09 21:01:03 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-09 21:01:03 | INFO | train | epoch 050 | loss 1.557 | ppl 2.94 | wps 8824.8 | ups 8.84 | wpb 997.9 | bsz 31.9 | num_updates 4949 | lr 2.31554e-05 | gnorm 2.157 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.7 | wall 663
2022-01-09 21:01:03 | INFO | fairseq.trainer | begin training epoch 51
2022-01-09 21:01:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:01:03 | INFO | train_inner | epoch 051:      1 / 99 loss=1.316, ppl=2.49, wps=1869, ups=2.21, wpb=847, bsz=32, num_updates=4950, lr=2.31538e-05, gnorm=2.146, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=664
2022-01-09 21:01:04 | INFO | train_inner | epoch 051:     11 / 99 loss=1.792, ppl=3.46, wps=15810, ups=13.13, wpb=1203.9, bsz=32, num_updates=4960, lr=2.31385e-05, gnorm=2.287, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=664
2022-01-09 21:01:04 | INFO | train_inner | epoch 051:     21 / 99 loss=1.375, ppl=2.59, wps=13982.5, ups=14.84, wpb=942.3, bsz=32, num_updates=4970, lr=2.31231e-05, gnorm=2.449, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=665
2022-01-09 21:01:05 | INFO | train_inner | epoch 051:     31 / 99 loss=1.47, ppl=2.77, wps=14823.5, ups=13.92, wpb=1065, bsz=32, num_updates=4980, lr=2.31077e-05, gnorm=2.356, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=666
2022-01-09 21:01:06 | INFO | train_inner | epoch 051:     41 / 99 loss=1.93, ppl=3.81, wps=12400.8, ups=12.29, wpb=1009.4, bsz=31.5, num_updates=4990, lr=2.30923e-05, gnorm=2.326, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=667
2022-01-09 21:01:07 | INFO | train_inner | epoch 051:     51 / 99 loss=1.522, ppl=2.87, wps=13692.1, ups=14.56, wpb=940.6, bsz=32, num_updates=5000, lr=2.30769e-05, gnorm=2.195, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=667
2022-01-09 21:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:01:08 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 1.708 | ppl 3.27 | wps 30696.7 | wpb 930.4 | bsz 31.3 | num_updates 5000 | best_loss 1.708
2022-01-09 21:01:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 5000 updates
2022-01-09 21:01:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_51_5000.pt
2022-01-09 21:01:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_51_5000.pt
2022-01-09 21:01:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_51_5000.pt (epoch 51 @ 5000 updates, score 1.708) (writing took 12.559509656042792 seconds)
2022-01-09 21:01:21 | INFO | train_inner | epoch 051:     61 / 99 loss=1.339, ppl=2.53, wps=670.7, ups=0.7, wpb=957.6, bsz=32, num_updates=5010, lr=2.30615e-05, gnorm=2.069, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=682
2022-01-09 21:01:22 | INFO | train_inner | epoch 051:     71 / 99 loss=1.322, ppl=2.5, wps=13337.8, ups=14.16, wpb=941.9, bsz=32, num_updates=5020, lr=2.30462e-05, gnorm=2.065, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=682
2022-01-09 21:01:22 | INFO | train_inner | epoch 051:     81 / 99 loss=1.662, ppl=3.17, wps=12020.2, ups=13.87, wpb=866.8, bsz=32, num_updates=5030, lr=2.30308e-05, gnorm=2.085, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=683
2022-01-09 21:01:23 | INFO | train_inner | epoch 051:     91 / 99 loss=1.369, ppl=2.58, wps=12582.7, ups=12.48, wpb=1008.3, bsz=32, num_updates=5040, lr=2.30154e-05, gnorm=2.109, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=684
2022-01-09 21:01:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:01:25 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 1.682 | ppl 3.21 | wps 32424.8 | wpb 930.4 | bsz 31.3 | num_updates 5048 | best_loss 1.682
2022-01-09 21:01:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 5048 updates
2022-01-09 21:01:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:01:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:01:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 51 @ 5048 updates, score 1.682) (writing took 5.742760158958845 seconds)
2022-01-09 21:01:31 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-09 21:01:31 | INFO | train | epoch 051 | loss 1.535 | ppl 2.9 | wps 3548 | ups 3.56 | wpb 997.9 | bsz 31.9 | num_updates 5048 | lr 2.30031e-05 | gnorm 2.209 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.8 | wall 691
2022-01-09 21:01:31 | INFO | fairseq.trainer | begin training epoch 52
2022-01-09 21:01:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:01:31 | INFO | train_inner | epoch 052:      2 / 99 loss=1.492, ppl=2.81, wps=1302.2, ups=1.25, wpb=1041.6, bsz=32, num_updates=5050, lr=2.3e-05, gnorm=2.172, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=692
2022-01-09 21:01:32 | INFO | train_inner | epoch 052:     12 / 99 loss=1.852, ppl=3.61, wps=9946.5, ups=11.76, wpb=846.1, bsz=31.5, num_updates=5060, lr=2.29846e-05, gnorm=1.994, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=693
2022-01-09 21:01:33 | INFO | train_inner | epoch 052:     22 / 99 loss=1.403, ppl=2.64, wps=13955.7, ups=13.12, wpb=1063.3, bsz=32, num_updates=5070, lr=2.29692e-05, gnorm=2.103, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=693
2022-01-09 21:01:34 | INFO | train_inner | epoch 052:     32 / 99 loss=1.357, ppl=2.56, wps=11566.8, ups=12.15, wpb=952.2, bsz=32, num_updates=5080, lr=2.29538e-05, gnorm=2.233, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=694
2022-01-09 21:01:35 | INFO | train_inner | epoch 052:     42 / 99 loss=1.197, ppl=2.29, wps=9605, ups=12.46, wpb=771, bsz=32, num_updates=5090, lr=2.29385e-05, gnorm=2.166, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=695
2022-01-09 21:01:35 | INFO | train_inner | epoch 052:     52 / 99 loss=1.592, ppl=3.02, wps=15694.3, ups=12.84, wpb=1222, bsz=32, num_updates=5100, lr=2.29231e-05, gnorm=2.227, clip=100, loss_scale=64, train_wall=1, gb_free=19.4, wall=696
2022-01-09 21:01:36 | INFO | train_inner | epoch 052:     62 / 99 loss=1.707, ppl=3.26, wps=15273.3, ups=12.67, wpb=1205.9, bsz=32, num_updates=5110, lr=2.29077e-05, gnorm=2.076, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=697
2022-01-09 21:01:37 | INFO | train_inner | epoch 052:     72 / 99 loss=1.343, ppl=2.54, wps=13726.5, ups=13.16, wpb=1043.1, bsz=32, num_updates=5120, lr=2.28923e-05, gnorm=2.102, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=697
2022-01-09 21:01:38 | INFO | train_inner | epoch 052:     82 / 99 loss=1.783, ppl=3.44, wps=13046.5, ups=12.77, wpb=1021.5, bsz=32, num_updates=5130, lr=2.28769e-05, gnorm=2.447, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=698
2022-01-09 21:01:38 | INFO | train_inner | epoch 052:     92 / 99 loss=1.418, ppl=2.67, wps=10548.2, ups=12.21, wpb=864.2, bsz=32, num_updates=5140, lr=2.28615e-05, gnorm=2.251, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=699
2022-01-09 21:01:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:01:40 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 1.67 | ppl 3.18 | wps 31788.7 | wpb 930.4 | bsz 31.3 | num_updates 5147 | best_loss 1.67
2022-01-09 21:01:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5147 updates
2022-01-09 21:01:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:01:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:01:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 52 @ 5147 updates, score 1.67) (writing took 4.420872093993239 seconds)
2022-01-09 21:01:44 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-09 21:01:44 | INFO | train | epoch 052 | loss 1.504 | ppl 2.84 | wps 7304.9 | ups 7.32 | wpb 997.9 | bsz 31.9 | num_updates 5147 | lr 2.28508e-05 | gnorm 2.178 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.5 | wall 705
2022-01-09 21:01:44 | INFO | fairseq.trainer | begin training epoch 53
2022-01-09 21:01:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:01:45 | INFO | train_inner | epoch 053:      3 / 99 loss=1.395, ppl=2.63, wps=1664.7, ups=1.6, wpb=1042.9, bsz=32, num_updates=5150, lr=2.28462e-05, gnorm=2.175, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=705
2022-01-09 21:01:45 | INFO | train_inner | epoch 053:     13 / 99 loss=1.468, ppl=2.77, wps=13288.6, ups=13.57, wpb=979.1, bsz=32, num_updates=5160, lr=2.28308e-05, gnorm=2.53, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=706
2022-01-09 21:01:46 | INFO | train_inner | epoch 053:     23 / 99 loss=1.277, ppl=2.42, wps=12933.5, ups=13.53, wpb=955.8, bsz=32, num_updates=5170, lr=2.28154e-05, gnorm=2.154, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=707
2022-01-09 21:01:47 | INFO | train_inner | epoch 053:     33 / 99 loss=1.27, ppl=2.41, wps=12416.5, ups=14.3, wpb=868.2, bsz=32, num_updates=5180, lr=2.28e-05, gnorm=2.067, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=707
2022-01-09 21:01:48 | INFO | train_inner | epoch 053:     43 / 99 loss=1.576, ppl=2.98, wps=13912.9, ups=12.56, wpb=1107.3, bsz=32, num_updates=5190, lr=2.27846e-05, gnorm=2.124, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=708
2022-01-09 21:01:48 | INFO | train_inner | epoch 053:     53 / 99 loss=1.242, ppl=2.37, wps=14020.4, ups=14.28, wpb=981.5, bsz=32, num_updates=5200, lr=2.27692e-05, gnorm=2.019, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=709
2022-01-09 21:01:49 | INFO | train_inner | epoch 053:     63 / 99 loss=1.808, ppl=3.5, wps=14589.6, ups=12.48, wpb=1169.1, bsz=31.5, num_updates=5210, lr=2.27538e-05, gnorm=2.23, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=710
2022-01-09 21:01:50 | INFO | train_inner | epoch 053:     73 / 99 loss=1.223, ppl=2.33, wps=11341.9, ups=15.09, wpb=751.8, bsz=32, num_updates=5220, lr=2.27385e-05, gnorm=2.326, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=710
2022-01-09 21:01:51 | INFO | train_inner | epoch 053:     83 / 99 loss=1.702, ppl=3.25, wps=15657.6, ups=13.49, wpb=1160.9, bsz=32, num_updates=5230, lr=2.27231e-05, gnorm=2.28, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=711
2022-01-09 21:01:52 | INFO | train_inner | epoch 053:     93 / 99 loss=1.617, ppl=3.07, wps=12971.5, ups=10.78, wpb=1203.7, bsz=32, num_updates=5240, lr=2.27077e-05, gnorm=2.213, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=712
2022-01-09 21:01:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:01:53 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 1.621 | ppl 3.08 | wps 30510.1 | wpb 930.4 | bsz 31.3 | num_updates 5246 | best_loss 1.621
2022-01-09 21:01:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5246 updates
2022-01-09 21:01:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:01:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:01:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 53 @ 5246 updates, score 1.621) (writing took 4.178062847000547 seconds)
2022-01-09 21:01:57 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-09 21:01:57 | INFO | train | epoch 053 | loss 1.483 | ppl 2.8 | wps 7695.5 | ups 7.71 | wpb 997.9 | bsz 31.9 | num_updates 5246 | lr 2.26985e-05 | gnorm 2.216 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.8 | wall 718
2022-01-09 21:01:57 | INFO | fairseq.trainer | begin training epoch 54
2022-01-09 21:01:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:01:58 | INFO | train_inner | epoch 054:      4 / 99 loss=1.836, ppl=3.57, wps=1449, ups=1.62, wpb=895, bsz=31.5, num_updates=5250, lr=2.26923e-05, gnorm=2.131, clip=100, loss_scale=64, train_wall=1, gb_free=16.9, wall=718
2022-01-09 21:01:59 | INFO | train_inner | epoch 054:     14 / 99 loss=1.575, ppl=2.98, wps=14039.4, ups=12.72, wpb=1103.8, bsz=32, num_updates=5260, lr=2.26769e-05, gnorm=2.333, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=719
2022-01-09 21:01:59 | INFO | train_inner | epoch 054:     24 / 99 loss=1.177, ppl=2.26, wps=10091.5, ups=12.34, wpb=817.9, bsz=32, num_updates=5270, lr=2.26615e-05, gnorm=2.28, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=720
2022-01-09 21:02:00 | INFO | train_inner | epoch 054:     34 / 99 loss=1.34, ppl=2.53, wps=11234.6, ups=11.22, wpb=1001.4, bsz=32, num_updates=5280, lr=2.26462e-05, gnorm=2.187, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=721
2022-01-09 21:02:01 | INFO | train_inner | epoch 054:     44 / 99 loss=1.267, ppl=2.41, wps=10277, ups=12.06, wpb=852.3, bsz=32, num_updates=5290, lr=2.26308e-05, gnorm=2.148, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=722
2022-01-09 21:02:02 | INFO | train_inner | epoch 054:     54 / 99 loss=1.137, ppl=2.2, wps=10995.6, ups=12.95, wpb=849, bsz=32, num_updates=5300, lr=2.26154e-05, gnorm=2.066, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=722
2022-01-09 21:02:03 | INFO | train_inner | epoch 054:     64 / 99 loss=1.824, ppl=3.54, wps=11324.2, ups=9.87, wpb=1147.3, bsz=32, num_updates=5310, lr=2.26e-05, gnorm=2.165, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=723
2022-01-09 21:02:04 | INFO | train_inner | epoch 054:     74 / 99 loss=1.413, ppl=2.66, wps=11769.1, ups=10.69, wpb=1101, bsz=32, num_updates=5320, lr=2.25846e-05, gnorm=2.285, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=724
2022-01-09 21:02:05 | INFO | train_inner | epoch 054:     84 / 99 loss=1.256, ppl=2.39, wps=10313.6, ups=11.12, wpb=927.7, bsz=32, num_updates=5330, lr=2.25692e-05, gnorm=2.226, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=725
2022-01-09 21:02:06 | INFO | train_inner | epoch 054:     94 / 99 loss=1.364, ppl=2.57, wps=9705.2, ups=10.21, wpb=950.5, bsz=32, num_updates=5340, lr=2.25538e-05, gnorm=2.131, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=726
2022-01-09 21:02:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:02:07 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 1.626 | ppl 3.09 | wps 30870.3 | wpb 930.4 | bsz 31.3 | num_updates 5345 | best_loss 1.621
2022-01-09 21:02:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5345 updates
2022-01-09 21:02:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:02:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:02:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 54 @ 5345 updates, score 1.626) (writing took 2.8306938749738038 seconds)
2022-01-09 21:02:10 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-09 21:02:10 | INFO | train | epoch 054 | loss 1.45 | ppl 2.73 | wps 7755.2 | ups 7.77 | wpb 997.9 | bsz 31.9 | num_updates 5345 | lr 2.25462e-05 | gnorm 2.193 | clip 100 | loss_scale 64 | train_wall 9 | gb_free 19.9 | wall 731
2022-01-09 21:02:10 | INFO | fairseq.trainer | begin training epoch 55
2022-01-09 21:02:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:02:10 | INFO | train_inner | epoch 055:      5 / 99 loss=1.391, ppl=2.62, wps=2490.6, ups=2.1, wpb=1186.6, bsz=32, num_updates=5350, lr=2.25385e-05, gnorm=2.048, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=731
2022-01-09 21:02:11 | INFO | train_inner | epoch 055:     15 / 99 loss=1.344, ppl=2.54, wps=14274.7, ups=14.53, wpb=982.4, bsz=32, num_updates=5360, lr=2.25231e-05, gnorm=2.121, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=732
2022-01-09 21:02:12 | INFO | train_inner | epoch 055:     25 / 99 loss=1.435, ppl=2.7, wps=13645.4, ups=14.32, wpb=952.7, bsz=32, num_updates=5370, lr=2.25077e-05, gnorm=2.087, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=732
2022-01-09 21:02:13 | INFO | train_inner | epoch 055:     35 / 99 loss=1.341, ppl=2.53, wps=13877.1, ups=13.82, wpb=1004.2, bsz=32, num_updates=5380, lr=2.24923e-05, gnorm=2.146, clip=100, loss_scale=64, train_wall=1, gb_free=19.4, wall=733
2022-01-09 21:02:13 | INFO | train_inner | epoch 055:     45 / 99 loss=1.097, ppl=2.14, wps=10974.8, ups=14.9, wpb=736.5, bsz=32, num_updates=5390, lr=2.24769e-05, gnorm=2.16, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=734
2022-01-09 21:02:14 | INFO | train_inner | epoch 055:     55 / 99 loss=1.214, ppl=2.32, wps=11671.2, ups=13.06, wpb=893.8, bsz=32, num_updates=5400, lr=2.24615e-05, gnorm=2.017, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=735
2022-01-09 21:02:15 | INFO | train_inner | epoch 055:     65 / 99 loss=1.367, ppl=2.58, wps=11030.4, ups=13.45, wpb=820.2, bsz=32, num_updates=5410, lr=2.24462e-05, gnorm=2.128, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=735
2022-01-09 21:02:16 | INFO | train_inner | epoch 055:     75 / 99 loss=1.255, ppl=2.39, wps=11530.7, ups=12.2, wpb=945, bsz=32, num_updates=5420, lr=2.24308e-05, gnorm=2.055, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=736
2022-01-09 21:02:16 | INFO | train_inner | epoch 055:     85 / 99 loss=1.624, ppl=3.08, wps=14861.6, ups=11.81, wpb=1258.2, bsz=32, num_updates=5430, lr=2.24154e-05, gnorm=2.218, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=737
2022-01-09 21:02:17 | INFO | train_inner | epoch 055:     95 / 99 loss=1.98, ppl=3.94, wps=13914.1, ups=10.93, wpb=1273.2, bsz=31.5, num_updates=5440, lr=2.24e-05, gnorm=2.227, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=738
2022-01-09 21:02:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:02:19 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 1.611 | ppl 3.05 | wps 30515.8 | wpb 930.4 | bsz 31.3 | num_updates 5444 | best_loss 1.611
2022-01-09 21:02:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5444 updates
2022-01-09 21:02:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:02:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:02:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 55 @ 5444 updates, score 1.611) (writing took 5.160474676056765 seconds)
2022-01-09 21:02:24 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-09 21:02:24 | INFO | train | epoch 055 | loss 1.43 | ppl 2.69 | wps 7152.7 | ups 7.17 | wpb 997.9 | bsz 31.9 | num_updates 5444 | lr 2.23938e-05 | gnorm 2.119 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 19.6 | wall 744
2022-01-09 21:02:24 | INFO | fairseq.trainer | begin training epoch 56
2022-01-09 21:02:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:02:24 | INFO | train_inner | epoch 056:      6 / 99 loss=1.531, ppl=2.89, wps=1411.4, ups=1.42, wpb=994.4, bsz=32, num_updates=5450, lr=2.23846e-05, gnorm=2.16, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=745
2022-01-09 21:02:25 | INFO | train_inner | epoch 056:     16 / 99 loss=1.282, ppl=2.43, wps=12620.9, ups=13.04, wpb=967.8, bsz=32, num_updates=5460, lr=2.23692e-05, gnorm=2.122, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=746
2022-01-09 21:02:26 | INFO | train_inner | epoch 056:     26 / 99 loss=1.185, ppl=2.27, wps=11349.9, ups=12.75, wpb=890.2, bsz=32, num_updates=5470, lr=2.23538e-05, gnorm=2.147, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=746
2022-01-09 21:02:27 | INFO | train_inner | epoch 056:     36 / 99 loss=1.209, ppl=2.31, wps=12432.7, ups=13.58, wpb=915.7, bsz=32, num_updates=5480, lr=2.23385e-05, gnorm=2.091, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=747
2022-01-09 21:02:28 | INFO | train_inner | epoch 056:     46 / 99 loss=1.729, ppl=3.32, wps=11857.5, ups=11.65, wpb=1017.8, bsz=31.5, num_updates=5490, lr=2.23231e-05, gnorm=2.248, clip=100, loss_scale=64, train_wall=1, gb_free=16.9, wall=748
2022-01-09 21:02:28 | INFO | train_inner | epoch 056:     56 / 99 loss=1.193, ppl=2.29, wps=11371.1, ups=12.86, wpb=884.1, bsz=32, num_updates=5500, lr=2.23077e-05, gnorm=2.15, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=749
2022-01-09 21:02:29 | INFO | train_inner | epoch 056:     66 / 99 loss=1.799, ppl=3.48, wps=15554.1, ups=12.85, wpb=1210.7, bsz=32, num_updates=5510, lr=2.22923e-05, gnorm=2.304, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=750
2022-01-09 21:02:30 | INFO | train_inner | epoch 056:     76 / 99 loss=1.108, ppl=2.16, wps=12090.2, ups=14.48, wpb=835.2, bsz=32, num_updates=5520, lr=2.22769e-05, gnorm=2.121, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=750
2022-01-09 21:02:31 | INFO | train_inner | epoch 056:     86 / 99 loss=1.446, ppl=2.73, wps=14989.7, ups=13.44, wpb=1115, bsz=32, num_updates=5530, lr=2.22615e-05, gnorm=2.269, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=751
2022-01-09 21:02:31 | INFO | train_inner | epoch 056:     96 / 99 loss=1.359, ppl=2.57, wps=13454.7, ups=11.88, wpb=1132.7, bsz=32, num_updates=5540, lr=2.22462e-05, gnorm=2.03, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=752
2022-01-09 21:02:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:02:33 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 1.58 | ppl 2.99 | wps 31122.9 | wpb 930.4 | bsz 31.3 | num_updates 5543 | best_loss 1.58
2022-01-09 21:02:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5543 updates
2022-01-09 21:02:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:02:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:02:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 56 @ 5543 updates, score 1.58) (writing took 4.455292856087908 seconds)
2022-01-09 21:02:37 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-09 21:02:37 | INFO | train | epoch 056 | loss 1.407 | ppl 2.65 | wps 7444.1 | ups 7.46 | wpb 997.9 | bsz 31.9 | num_updates 5543 | lr 2.22415e-05 | gnorm 2.163 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.6 | wall 758
2022-01-09 21:02:37 | INFO | fairseq.trainer | begin training epoch 57
2022-01-09 21:02:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:02:38 | INFO | train_inner | epoch 057:      7 / 99 loss=1.168, ppl=2.25, wps=1388.4, ups=1.59, wpb=874, bsz=32, num_updates=5550, lr=2.22308e-05, gnorm=2.027, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=758
2022-01-09 21:02:38 | INFO | train_inner | epoch 057:     17 / 99 loss=1.194, ppl=2.29, wps=10961.3, ups=12.65, wpb=866.2, bsz=32, num_updates=5560, lr=2.22154e-05, gnorm=2.15, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=759
2022-01-09 21:02:39 | INFO | train_inner | epoch 057:     27 / 99 loss=1.295, ppl=2.45, wps=12900.9, ups=12.03, wpb=1072, bsz=32, num_updates=5570, lr=2.22e-05, gnorm=2.257, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=760
2022-01-09 21:02:40 | INFO | train_inner | epoch 057:     37 / 99 loss=1.62, ppl=3.07, wps=11748.6, ups=10.75, wpb=1093, bsz=32, num_updates=5580, lr=2.21846e-05, gnorm=2.27, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=761
2022-01-09 21:02:41 | INFO | train_inner | epoch 057:     47 / 99 loss=1.354, ppl=2.56, wps=12754.6, ups=12.64, wpb=1009, bsz=32, num_updates=5590, lr=2.21692e-05, gnorm=2.23, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=762
2022-01-09 21:02:42 | INFO | train_inner | epoch 057:     57 / 99 loss=1.149, ppl=2.22, wps=13394.1, ups=14.6, wpb=917.1, bsz=32, num_updates=5600, lr=2.21538e-05, gnorm=2.012, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=762
2022-01-09 21:02:43 | INFO | train_inner | epoch 057:     67 / 99 loss=1.354, ppl=2.56, wps=12536, ups=11.93, wpb=1050.9, bsz=32, num_updates=5610, lr=2.21385e-05, gnorm=2.028, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=763
2022-01-09 21:02:43 | INFO | train_inner | epoch 057:     77 / 99 loss=1.209, ppl=2.31, wps=13271.7, ups=14.94, wpb=888.5, bsz=32, num_updates=5620, lr=2.21231e-05, gnorm=2.152, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=764
2022-01-09 21:02:44 | INFO | train_inner | epoch 057:     87 / 99 loss=1.328, ppl=2.51, wps=12670.9, ups=12.93, wpb=979.8, bsz=32, num_updates=5630, lr=2.21077e-05, gnorm=2.125, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=765
2022-01-09 21:02:45 | INFO | train_inner | epoch 057:     97 / 99 loss=1.942, ppl=3.84, wps=14657.2, ups=10.69, wpb=1371.4, bsz=31.5, num_updates=5640, lr=2.20923e-05, gnorm=2.18, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=765
2022-01-09 21:02:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:02:46 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 1.621 | ppl 3.08 | wps 31272.5 | wpb 930.4 | bsz 31.3 | num_updates 5642 | best_loss 1.58
2022-01-09 21:02:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5642 updates
2022-01-09 21:02:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:02:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:02:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 57 @ 5642 updates, score 1.621) (writing took 3.35253460588865 seconds)
2022-01-09 21:02:49 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-09 21:02:49 | INFO | train | epoch 057 | loss 1.389 | ppl 2.62 | wps 8018.8 | ups 8.04 | wpb 997.9 | bsz 31.9 | num_updates 5642 | lr 2.20892e-05 | gnorm 2.155 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 770
2022-01-09 21:02:49 | INFO | fairseq.trainer | begin training epoch 58
2022-01-09 21:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:02:50 | INFO | train_inner | epoch 058:      8 / 99 loss=1.187, ppl=2.28, wps=1690.5, ups=1.95, wpb=866, bsz=32, num_updates=5650, lr=2.20769e-05, gnorm=2.353, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=771
2022-01-09 21:02:51 | INFO | train_inner | epoch 058:     18 / 99 loss=1.253, ppl=2.38, wps=13062.6, ups=13.88, wpb=940.9, bsz=32, num_updates=5660, lr=2.20615e-05, gnorm=2.075, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=771
2022-01-09 21:02:52 | INFO | train_inner | epoch 058:     28 / 99 loss=1.147, ppl=2.21, wps=12379, ups=14.23, wpb=869.8, bsz=32, num_updates=5670, lr=2.20462e-05, gnorm=2.121, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=772
2022-01-09 21:02:52 | INFO | train_inner | epoch 058:     38 / 99 loss=1.19, ppl=2.28, wps=12815.7, ups=12.84, wpb=998.2, bsz=32, num_updates=5680, lr=2.20308e-05, gnorm=2.049, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=773
2022-01-09 21:02:53 | INFO | train_inner | epoch 058:     48 / 99 loss=1.798, ppl=3.48, wps=14254.9, ups=12.28, wpb=1160.8, bsz=31.5, num_updates=5690, lr=2.20154e-05, gnorm=2.157, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=774
2022-01-09 21:02:54 | INFO | train_inner | epoch 058:     58 / 99 loss=1.304, ppl=2.47, wps=15767.1, ups=13.45, wpb=1172.4, bsz=32, num_updates=5700, lr=2.2e-05, gnorm=2.005, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=774
2022-01-09 21:02:55 | INFO | train_inner | epoch 058:     68 / 99 loss=1.422, ppl=2.68, wps=16161.3, ups=13.48, wpb=1198.5, bsz=32, num_updates=5710, lr=2.19846e-05, gnorm=2.085, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=775
2022-01-09 21:02:55 | INFO | train_inner | epoch 058:     78 / 99 loss=1.404, ppl=2.65, wps=11541.5, ups=12.15, wpb=950.1, bsz=32, num_updates=5720, lr=2.19692e-05, gnorm=2.163, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=776
2022-01-09 21:02:56 | INFO | train_inner | epoch 058:     88 / 99 loss=1.532, ppl=2.89, wps=10423.9, ups=12.6, wpb=827.3, bsz=32, num_updates=5730, lr=2.19538e-05, gnorm=2.232, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=777
2022-01-09 21:02:57 | INFO | train_inner | epoch 058:     98 / 99 loss=1.196, ppl=2.29, wps=10367.8, ups=10.83, wpb=957.4, bsz=32, num_updates=5740, lr=2.19385e-05, gnorm=2.136, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=778
2022-01-09 21:02:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:02:58 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 1.582 | ppl 2.99 | wps 34392.5 | wpb 930.4 | bsz 31.3 | num_updates 5741 | best_loss 1.58
2022-01-09 21:02:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5741 updates
2022-01-09 21:02:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:03:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:03:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 58 @ 5741 updates, score 1.582) (writing took 2.7015002500265837 seconds)
2022-01-09 21:03:01 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-09 21:03:01 | INFO | train | epoch 058 | loss 1.355 | ppl 2.56 | wps 8666.9 | ups 8.69 | wpb 997.9 | bsz 31.9 | num_updates 5741 | lr 2.19369e-05 | gnorm 2.131 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 781
2022-01-09 21:03:01 | INFO | fairseq.trainer | begin training epoch 59
2022-01-09 21:03:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:03:02 | INFO | train_inner | epoch 059:      9 / 99 loss=1.035, ppl=2.05, wps=1796.1, ups=2.25, wpb=797.9, bsz=32, num_updates=5750, lr=2.19231e-05, gnorm=2.096, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=782
2022-01-09 21:03:02 | INFO | train_inner | epoch 059:     19 / 99 loss=1.112, ppl=2.16, wps=12200.1, ups=14.14, wpb=862.9, bsz=32, num_updates=5760, lr=2.19077e-05, gnorm=2.048, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=783
2022-01-09 21:03:03 | INFO | train_inner | epoch 059:     29 / 99 loss=1.171, ppl=2.25, wps=9953, ups=9.94, wpb=1001.2, bsz=32, num_updates=5770, lr=2.18923e-05, gnorm=2.101, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=784
2022-01-09 21:03:04 | INFO | train_inner | epoch 059:     39 / 99 loss=1.898, ppl=3.73, wps=11965.2, ups=10.9, wpb=1098.2, bsz=31.5, num_updates=5780, lr=2.18769e-05, gnorm=2.051, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=785
2022-01-09 21:03:05 | INFO | train_inner | epoch 059:     49 / 99 loss=1.537, ppl=2.9, wps=13086.7, ups=11.43, wpb=1144.7, bsz=32, num_updates=5790, lr=2.18615e-05, gnorm=2.174, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=786
2022-01-09 21:03:06 | INFO | train_inner | epoch 059:     59 / 99 loss=1.338, ppl=2.53, wps=13101.4, ups=11.99, wpb=1093.1, bsz=32, num_updates=5800, lr=2.18462e-05, gnorm=2.107, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=786
2022-01-09 21:03:07 | INFO | train_inner | epoch 059:     69 / 99 loss=1.123, ppl=2.18, wps=13287, ups=14.2, wpb=935.6, bsz=32, num_updates=5810, lr=2.18308e-05, gnorm=1.941, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=787
2022-01-09 21:03:07 | INFO | train_inner | epoch 059:     79 / 99 loss=1.132, ppl=2.19, wps=11889.1, ups=14.48, wpb=820.9, bsz=32, num_updates=5820, lr=2.18154e-05, gnorm=2.276, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=788
2022-01-09 21:03:08 | INFO | train_inner | epoch 059:     89 / 99 loss=1.308, ppl=2.48, wps=11756.7, ups=11.83, wpb=993.8, bsz=32, num_updates=5830, lr=2.18e-05, gnorm=2.146, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=789
2022-01-09 21:03:09 | INFO | train_inner | epoch 059:     99 / 99 loss=1.402, ppl=2.64, wps=11634.4, ups=9.92, wpb=1173.3, bsz=32, num_updates=5840, lr=2.17846e-05, gnorm=2.171, clip=100, loss_scale=64, train_wall=1, gb_free=19.1, wall=790
2022-01-09 21:03:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:03:10 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 1.568 | ppl 2.96 | wps 31408.9 | wpb 930.4 | bsz 31.3 | num_updates 5840 | best_loss 1.568
2022-01-09 21:03:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 5840 updates
2022-01-09 21:03:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:03:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:03:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 59 @ 5840 updates, score 1.568) (writing took 5.296515204012394 seconds)
2022-01-09 21:03:15 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-09 21:03:15 | INFO | train | epoch 059 | loss 1.331 | ppl 2.52 | wps 6742.5 | ups 6.76 | wpb 997.9 | bsz 31.9 | num_updates 5840 | lr 2.17846e-05 | gnorm 2.107 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.1 | wall 796
2022-01-09 21:03:16 | INFO | fairseq.trainer | begin training epoch 60
2022-01-09 21:03:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:03:16 | INFO | train_inner | epoch 060:     10 / 99 loss=1.455, ppl=2.74, wps=1483, ups=1.4, wpb=1055.7, bsz=32, num_updates=5850, lr=2.17692e-05, gnorm=2.205, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=797
2022-01-09 21:03:17 | INFO | train_inner | epoch 060:     20 / 99 loss=1.209, ppl=2.31, wps=12246.2, ups=12.42, wpb=985.8, bsz=32, num_updates=5860, lr=2.17538e-05, gnorm=2.18, clip=100, loss_scale=64, train_wall=1, gb_free=19.4, wall=798
2022-01-09 21:03:18 | INFO | train_inner | epoch 060:     30 / 99 loss=0.955, ppl=1.94, wps=9410.5, ups=13.23, wpb=711.3, bsz=32, num_updates=5870, lr=2.17385e-05, gnorm=2.036, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=798
2022-01-09 21:03:19 | INFO | train_inner | epoch 060:     40 / 99 loss=1.187, ppl=2.28, wps=12405.8, ups=12.85, wpb=965.7, bsz=32, num_updates=5880, lr=2.17231e-05, gnorm=2.061, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=799
2022-01-09 21:03:20 | INFO | train_inner | epoch 060:     50 / 99 loss=1.175, ppl=2.26, wps=12275.6, ups=12.08, wpb=1016, bsz=32, num_updates=5890, lr=2.17077e-05, gnorm=2.081, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=800
2022-01-09 21:03:20 | INFO | train_inner | epoch 060:     60 / 99 loss=1.525, ppl=2.88, wps=13124.3, ups=12.75, wpb=1029.1, bsz=32, num_updates=5900, lr=2.16923e-05, gnorm=2.224, clip=100, loss_scale=64, train_wall=1, gb_free=19.5, wall=801
2022-01-09 21:03:21 | INFO | train_inner | epoch 060:     70 / 99 loss=1.218, ppl=2.33, wps=11761.3, ups=12.49, wpb=941.6, bsz=32, num_updates=5910, lr=2.16769e-05, gnorm=2.274, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=802
2022-01-09 21:03:22 | INFO | train_inner | epoch 060:     80 / 99 loss=1.156, ppl=2.23, wps=12472.7, ups=12.61, wpb=989.1, bsz=32, num_updates=5920, lr=2.16615e-05, gnorm=2.044, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=802
2022-01-09 21:03:23 | INFO | train_inner | epoch 060:     90 / 99 loss=1.728, ppl=3.31, wps=11973.4, ups=11.6, wpb=1031.9, bsz=31.5, num_updates=5930, lr=2.16462e-05, gnorm=2.108, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=803
2022-01-09 21:03:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:03:25 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 1.544 | ppl 2.92 | wps 31430.2 | wpb 930.4 | bsz 31.3 | num_updates 5939 | best_loss 1.544
2022-01-09 21:03:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 5939 updates
2022-01-09 21:03:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:03:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:03:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 60 @ 5939 updates, score 1.544) (writing took 5.064204827067442 seconds)
2022-01-09 21:03:30 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-09 21:03:30 | INFO | train | epoch 060 | loss 1.327 | ppl 2.51 | wps 6944.2 | ups 6.96 | wpb 997.9 | bsz 31.9 | num_updates 5939 | lr 2.16323e-05 | gnorm 2.141 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 810
2022-01-09 21:03:30 | INFO | fairseq.trainer | begin training epoch 61
2022-01-09 21:03:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:03:30 | INFO | train_inner | epoch 061:      1 / 99 loss=1.468, ppl=2.77, wps=1728.1, ups=1.41, wpb=1226.2, bsz=32, num_updates=5940, lr=2.16308e-05, gnorm=2.19, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=810
2022-01-09 21:03:31 | INFO | train_inner | epoch 061:     11 / 99 loss=1.77, ppl=3.41, wps=16514.8, ups=11.68, wpb=1414, bsz=31.5, num_updates=5950, lr=2.16154e-05, gnorm=2.086, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=811
2022-01-09 21:03:32 | INFO | train_inner | epoch 061:     21 / 99 loss=1.589, ppl=3.01, wps=18104.6, ups=11.77, wpb=1538.1, bsz=32, num_updates=5960, lr=2.16e-05, gnorm=2.147, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=812
2022-01-09 21:03:32 | INFO | train_inner | epoch 061:     31 / 99 loss=1.185, ppl=2.27, wps=11647.9, ups=13.31, wpb=875, bsz=32, num_updates=5970, lr=2.15846e-05, gnorm=2.18, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=813
2022-01-09 21:03:33 | INFO | train_inner | epoch 061:     41 / 99 loss=1.105, ppl=2.15, wps=10800.8, ups=12.33, wpb=875.9, bsz=32, num_updates=5980, lr=2.15692e-05, gnorm=2.054, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=814
2022-01-09 21:03:34 | INFO | train_inner | epoch 061:     51 / 99 loss=1.087, ppl=2.12, wps=12257.6, ups=13.92, wpb=880.5, bsz=32, num_updates=5990, lr=2.15538e-05, gnorm=2.011, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=814
2022-01-09 21:03:35 | INFO | train_inner | epoch 061:     61 / 99 loss=1.13, ppl=2.19, wps=13579.2, ups=13.55, wpb=1002.5, bsz=32, num_updates=6000, lr=2.15385e-05, gnorm=2.111, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=815
2022-01-09 21:03:35 | INFO | train_inner | epoch 061:     71 / 99 loss=1.003, ppl=2, wps=10105.2, ups=14.23, wpb=710.1, bsz=32, num_updates=6010, lr=2.15231e-05, gnorm=2.099, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=816
2022-01-09 21:03:36 | INFO | train_inner | epoch 061:     81 / 99 loss=1.089, ppl=2.13, wps=13221.4, ups=14.64, wpb=902.8, bsz=32, num_updates=6020, lr=2.15077e-05, gnorm=2.198, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=817
2022-01-09 21:03:37 | INFO | train_inner | epoch 061:     91 / 99 loss=1.186, ppl=2.27, wps=11761.4, ups=13.65, wpb=861.5, bsz=32, num_updates=6030, lr=2.14923e-05, gnorm=2.156, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=817
2022-01-09 21:03:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:03:38 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 1.532 | ppl 2.89 | wps 32035.5 | wpb 930.4 | bsz 31.3 | num_updates 6038 | best_loss 1.532
2022-01-09 21:03:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 6038 updates
2022-01-09 21:03:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:03:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:03:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 61 @ 6038 updates, score 1.532) (writing took 5.47041543002706 seconds)
2022-01-09 21:03:44 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-09 21:03:44 | INFO | train | epoch 061 | loss 1.295 | ppl 2.45 | wps 7009.2 | ups 7.02 | wpb 997.9 | bsz 31.9 | num_updates 6038 | lr 2.148e-05 | gnorm 2.118 | clip 100 | loss_scale 64 | train_wall 7 | gb_free 20.8 | wall 824
2022-01-09 21:03:44 | INFO | fairseq.trainer | begin training epoch 62
2022-01-09 21:03:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:03:44 | INFO | train_inner | epoch 062:      2 / 99 loss=1.215, ppl=2.32, wps=1151.2, ups=1.37, wpb=839.5, bsz=32, num_updates=6040, lr=2.14769e-05, gnorm=2.09, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=825
2022-01-09 21:03:45 | INFO | train_inner | epoch 062:     12 / 99 loss=1.106, ppl=2.15, wps=14251.8, ups=14.15, wpb=1007.2, bsz=32, num_updates=6050, lr=2.14615e-05, gnorm=2.031, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=825
2022-01-09 21:03:46 | INFO | train_inner | epoch 062:     22 / 99 loss=1.028, ppl=2.04, wps=11284.2, ups=12.29, wpb=918.2, bsz=32, num_updates=6060, lr=2.14462e-05, gnorm=2.038, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=826
2022-01-09 21:03:46 | INFO | train_inner | epoch 062:     32 / 99 loss=1.44, ppl=2.71, wps=12187, ups=12.96, wpb=940.6, bsz=32, num_updates=6070, lr=2.14308e-05, gnorm=2.162, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=827
2022-01-09 21:03:47 | INFO | train_inner | epoch 062:     42 / 99 loss=0.954, ppl=1.94, wps=10345.7, ups=13.42, wpb=771, bsz=32, num_updates=6080, lr=2.14154e-05, gnorm=1.994, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=828
2022-01-09 21:03:48 | INFO | train_inner | epoch 062:     52 / 99 loss=1.523, ppl=2.87, wps=14533.6, ups=11.73, wpb=1239.4, bsz=32, num_updates=6090, lr=2.14e-05, gnorm=2.176, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=828
2022-01-09 21:03:49 | INFO | train_inner | epoch 062:     62 / 99 loss=1.064, ppl=2.09, wps=10773.2, ups=13.84, wpb=778.6, bsz=32, num_updates=6100, lr=2.13846e-05, gnorm=2.005, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=829
2022-01-09 21:03:49 | INFO | train_inner | epoch 062:     72 / 99 loss=1.274, ppl=2.42, wps=13978.4, ups=12.6, wpb=1109.6, bsz=32, num_updates=6110, lr=2.13692e-05, gnorm=2.125, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=830
2022-01-09 21:03:50 | INFO | train_inner | epoch 062:     82 / 99 loss=1.668, ppl=3.18, wps=13947, ups=11.35, wpb=1229, bsz=31.5, num_updates=6120, lr=2.13538e-05, gnorm=2.131, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=831
2022-01-09 21:03:51 | INFO | train_inner | epoch 062:     92 / 99 loss=1.341, ppl=2.53, wps=11284.2, ups=10.62, wpb=1062.4, bsz=32, num_updates=6130, lr=2.13385e-05, gnorm=2.18, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=832
2022-01-09 21:03:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:03:53 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 1.482 | ppl 2.79 | wps 32490.6 | wpb 930.4 | bsz 31.3 | num_updates 6137 | best_loss 1.482
2022-01-09 21:03:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 6137 updates
2022-01-09 21:03:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:03:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:03:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 62 @ 6137 updates, score 1.482) (writing took 5.451159326010384 seconds)
2022-01-09 21:03:58 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-09 21:03:58 | INFO | train | epoch 062 | loss 1.281 | ppl 2.43 | wps 6798 | ups 6.81 | wpb 997.9 | bsz 31.9 | num_updates 6137 | lr 2.13277e-05 | gnorm 2.086 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.4 | wall 839
2022-01-09 21:03:58 | INFO | fairseq.trainer | begin training epoch 63
2022-01-09 21:03:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:03:59 | INFO | train_inner | epoch 063:      3 / 99 loss=1.113, ppl=2.16, wps=1331, ups=1.36, wpb=975.6, bsz=32, num_updates=6140, lr=2.13231e-05, gnorm=2.059, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=839
2022-01-09 21:04:00 | INFO | train_inner | epoch 063:     13 / 99 loss=1.218, ppl=2.33, wps=11197.1, ups=10.7, wpb=1046.1, bsz=32, num_updates=6150, lr=2.13077e-05, gnorm=2.127, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=840
2022-01-09 21:04:00 | INFO | train_inner | epoch 063:     23 / 99 loss=1.242, ppl=2.36, wps=13402.9, ups=12.33, wpb=1087.1, bsz=32, num_updates=6160, lr=2.12923e-05, gnorm=2.044, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=841
2022-01-09 21:04:01 | INFO | train_inner | epoch 063:     33 / 99 loss=1.246, ppl=2.37, wps=12443.8, ups=12.09, wpb=1028.9, bsz=32, num_updates=6170, lr=2.12769e-05, gnorm=2.131, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=842
2022-01-09 21:04:02 | INFO | train_inner | epoch 063:     43 / 99 loss=1.222, ppl=2.33, wps=13044.7, ups=12.89, wpb=1011.9, bsz=32, num_updates=6180, lr=2.12615e-05, gnorm=2.089, clip=100, loss_scale=64, train_wall=1, gb_free=19.5, wall=843
2022-01-09 21:04:03 | INFO | train_inner | epoch 063:     53 / 99 loss=1.116, ppl=2.17, wps=12528.3, ups=12.84, wpb=975.6, bsz=32, num_updates=6190, lr=2.12462e-05, gnorm=2.149, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=843
2022-01-09 21:04:03 | INFO | train_inner | epoch 063:     63 / 99 loss=1.131, ppl=2.19, wps=11847.3, ups=14.63, wpb=809.8, bsz=32, num_updates=6200, lr=2.12308e-05, gnorm=2.315, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=844
2022-01-09 21:04:04 | INFO | train_inner | epoch 063:     73 / 99 loss=1.047, ppl=2.07, wps=11675.2, ups=15.04, wpb=776.2, bsz=32, num_updates=6210, lr=2.12154e-05, gnorm=2.045, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=845
2022-01-09 21:04:05 | INFO | train_inner | epoch 063:     83 / 99 loss=1.414, ppl=2.66, wps=11362, ups=9.88, wpb=1150.2, bsz=32, num_updates=6220, lr=2.12e-05, gnorm=2.115, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=846
2022-01-09 21:04:06 | INFO | train_inner | epoch 063:     93 / 99 loss=1.361, ppl=2.57, wps=12137.8, ups=11.83, wpb=1026.4, bsz=32, num_updates=6230, lr=2.11846e-05, gnorm=2.166, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=847
2022-01-09 21:04:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:04:08 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 1.501 | ppl 2.83 | wps 32307 | wpb 930.4 | bsz 31.3 | num_updates 6236 | best_loss 1.482
2022-01-09 21:04:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 6236 updates
2022-01-09 21:04:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:04:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:04:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 63 @ 6236 updates, score 1.501) (writing took 3.2525369611103088 seconds)
2022-01-09 21:04:11 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-09 21:04:11 | INFO | train | epoch 063 | loss 1.274 | ppl 2.42 | wps 7928.9 | ups 7.95 | wpb 997.9 | bsz 31.9 | num_updates 6236 | lr 2.11754e-05 | gnorm 2.124 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.4 | wall 851
2022-01-09 21:04:11 | INFO | fairseq.trainer | begin training epoch 64
2022-01-09 21:04:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:04:11 | INFO | train_inner | epoch 064:      4 / 99 loss=1.837, ppl=3.57, wps=2257.4, ups=1.92, wpb=1176.5, bsz=31.5, num_updates=6240, lr=2.11692e-05, gnorm=2.098, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=852
2022-01-09 21:04:12 | INFO | train_inner | epoch 064:     14 / 99 loss=0.959, ppl=1.94, wps=11132, ups=14.59, wpb=763.2, bsz=32, num_updates=6250, lr=2.11538e-05, gnorm=2.073, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=852
2022-01-09 21:04:13 | INFO | train_inner | epoch 064:     24 / 99 loss=1.304, ppl=2.47, wps=16756.2, ups=12.91, wpb=1297.5, bsz=32, num_updates=6260, lr=2.11385e-05, gnorm=2.006, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=853
2022-01-09 21:04:13 | INFO | train_inner | epoch 064:     34 / 99 loss=1.22, ppl=2.33, wps=12704.8, ups=13.4, wpb=948.4, bsz=32, num_updates=6270, lr=2.11231e-05, gnorm=2.048, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=854
2022-01-09 21:04:14 | INFO | train_inner | epoch 064:     44 / 99 loss=1.598, ppl=3.03, wps=14094.3, ups=11.41, wpb=1234.9, bsz=31.5, num_updates=6280, lr=2.11077e-05, gnorm=2.094, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=855
2022-01-09 21:04:15 | INFO | train_inner | epoch 064:     54 / 99 loss=0.987, ppl=1.98, wps=9904.3, ups=11.13, wpb=889.8, bsz=32, num_updates=6290, lr=2.10923e-05, gnorm=1.956, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=856
2022-01-09 21:04:16 | INFO | train_inner | epoch 064:     64 / 99 loss=1.065, ppl=2.09, wps=11414.8, ups=12.01, wpb=950.5, bsz=32, num_updates=6300, lr=2.10769e-05, gnorm=2.034, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=857
2022-01-09 21:04:17 | INFO | train_inner | epoch 064:     74 / 99 loss=1.438, ppl=2.71, wps=14437.5, ups=11.57, wpb=1248, bsz=32, num_updates=6310, lr=2.10615e-05, gnorm=2.056, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=857
2022-01-09 21:04:18 | INFO | train_inner | epoch 064:     84 / 99 loss=0.965, ppl=1.95, wps=8059.5, ups=12.38, wpb=651, bsz=32, num_updates=6320, lr=2.10462e-05, gnorm=2.153, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=858
2022-01-09 21:04:19 | INFO | train_inner | epoch 064:     94 / 99 loss=1.133, ppl=2.19, wps=12047, ups=12.61, wpb=955.5, bsz=32, num_updates=6330, lr=2.10308e-05, gnorm=2.149, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=859
2022-01-09 21:04:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:04:20 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 1.504 | ppl 2.84 | wps 31662 | wpb 930.4 | bsz 31.3 | num_updates 6335 | best_loss 1.482
2022-01-09 21:04:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 6335 updates
2022-01-09 21:04:20 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:04:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:04:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 64 @ 6335 updates, score 1.504) (writing took 2.8752164570614696 seconds)
2022-01-09 21:04:23 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-09 21:04:23 | INFO | train | epoch 064 | loss 1.247 | ppl 2.37 | wps 8220.3 | ups 8.24 | wpb 997.9 | bsz 31.9 | num_updates 6335 | lr 2.10231e-05 | gnorm 2.064 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.6 | wall 863
2022-01-09 21:04:23 | INFO | fairseq.trainer | begin training epoch 65
2022-01-09 21:04:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:04:23 | INFO | train_inner | epoch 065:      5 / 99 loss=1.538, ppl=2.9, wps=2523.9, ups=2.07, wpb=1218.8, bsz=32, num_updates=6340, lr=2.10154e-05, gnorm=2.09, clip=100, loss_scale=64, train_wall=1, gb_free=18.8, wall=864
2022-01-09 21:04:24 | INFO | train_inner | epoch 065:     15 / 99 loss=1.273, ppl=2.42, wps=13613.5, ups=11.56, wpb=1177.9, bsz=32, num_updates=6350, lr=2.1e-05, gnorm=2.156, clip=100, loss_scale=64, train_wall=1, gb_free=19.4, wall=865
2022-01-09 21:04:25 | INFO | train_inner | epoch 065:     25 / 99 loss=0.982, ppl=1.98, wps=10034.4, ups=11.76, wpb=853.4, bsz=32, num_updates=6360, lr=2.09846e-05, gnorm=2.11, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=866
2022-01-09 21:04:26 | INFO | train_inner | epoch 065:     35 / 99 loss=1.034, ppl=2.05, wps=10684.4, ups=10.94, wpb=976.7, bsz=32, num_updates=6370, lr=2.09692e-05, gnorm=2.144, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=867
2022-01-09 21:04:27 | INFO | train_inner | epoch 065:     45 / 99 loss=1.143, ppl=2.21, wps=9806.4, ups=10.33, wpb=949.3, bsz=32, num_updates=6380, lr=2.09538e-05, gnorm=2.11, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=867
2022-01-09 21:04:28 | INFO | train_inner | epoch 065:     55 / 99 loss=1.311, ppl=2.48, wps=12265.9, ups=12.04, wpb=1018.6, bsz=32, num_updates=6390, lr=2.09385e-05, gnorm=1.975, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=868
2022-01-09 21:04:29 | INFO | train_inner | epoch 065:     65 / 99 loss=1.085, ppl=2.12, wps=9727, ups=10.97, wpb=886.6, bsz=32, num_updates=6400, lr=2.09231e-05, gnorm=2.133, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=869
2022-01-09 21:04:30 | INFO | train_inner | epoch 065:     75 / 99 loss=1.628, ppl=3.09, wps=10518.7, ups=11.43, wpb=920.2, bsz=31.5, num_updates=6410, lr=2.09077e-05, gnorm=2.166, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=870
2022-01-09 21:04:30 | INFO | train_inner | epoch 065:     85 / 99 loss=0.992, ppl=1.99, wps=13224.6, ups=13.35, wpb=990.7, bsz=32, num_updates=6420, lr=2.08923e-05, gnorm=1.958, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=871
2022-01-09 21:04:31 | INFO | train_inner | epoch 065:     95 / 99 loss=1.142, ppl=2.21, wps=9803.5, ups=11.04, wpb=888.1, bsz=32, num_updates=6430, lr=2.08769e-05, gnorm=2.124, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=872
2022-01-09 21:04:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:04:33 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 1.467 | ppl 2.77 | wps 30765.7 | wpb 930.4 | bsz 31.3 | num_updates 6434 | best_loss 1.467
2022-01-09 21:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 6434 updates
2022-01-09 21:04:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:04:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:04:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 65 @ 6434 updates, score 1.467) (writing took 4.009191660094075 seconds)
2022-01-09 21:04:37 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-01-09 21:04:37 | INFO | train | epoch 065 | loss 1.225 | ppl 2.34 | wps 7168.9 | ups 7.18 | wpb 997.9 | bsz 31.9 | num_updates 6434 | lr 2.08708e-05 | gnorm 2.111 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.5 | wall 877
2022-01-09 21:04:37 | INFO | fairseq.trainer | begin training epoch 66
2022-01-09 21:04:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:04:37 | INFO | train_inner | epoch 066:      6 / 99 loss=1.025, ppl=2.03, wps=1588.9, ups=1.7, wpb=936.9, bsz=32, num_updates=6440, lr=2.08615e-05, gnorm=2.159, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=878
2022-01-09 21:04:38 | INFO | train_inner | epoch 066:     16 / 99 loss=1.156, ppl=2.23, wps=11487.3, ups=13.83, wpb=830.8, bsz=32, num_updates=6450, lr=2.08462e-05, gnorm=2.102, clip=100, loss_scale=64, train_wall=1, gb_free=19.1, wall=878
2022-01-09 21:04:39 | INFO | train_inner | epoch 066:     26 / 99 loss=1.18, ppl=2.27, wps=10832.4, ups=9.93, wpb=1091.2, bsz=32, num_updates=6460, lr=2.08308e-05, gnorm=2.201, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=879
2022-01-09 21:04:40 | INFO | train_inner | epoch 066:     36 / 99 loss=1.212, ppl=2.32, wps=12464, ups=11.76, wpb=1059.7, bsz=32, num_updates=6470, lr=2.08154e-05, gnorm=2.225, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=880
2022-01-09 21:04:41 | INFO | train_inner | epoch 066:     46 / 99 loss=1.099, ppl=2.14, wps=12447.6, ups=13.13, wpb=947.9, bsz=32, num_updates=6480, lr=2.08e-05, gnorm=2.337, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=881
2022-01-09 21:04:41 | INFO | train_inner | epoch 066:     56 / 99 loss=1.35, ppl=2.55, wps=13902.4, ups=12.01, wpb=1158, bsz=32, num_updates=6490, lr=2.07846e-05, gnorm=2.03, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=882
2022-01-09 21:04:42 | INFO | train_inner | epoch 066:     66 / 99 loss=0.915, ppl=1.89, wps=10926.7, ups=14.08, wpb=776.2, bsz=32, num_updates=6500, lr=2.07692e-05, gnorm=1.982, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=883
2022-01-09 21:04:43 | INFO | train_inner | epoch 066:     76 / 99 loss=1.506, ppl=2.84, wps=15583.1, ups=11.94, wpb=1305.2, bsz=32, num_updates=6510, lr=2.07538e-05, gnorm=2.152, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=883
2022-01-09 21:04:44 | INFO | train_inner | epoch 066:     86 / 99 loss=0.973, ppl=1.96, wps=12632.4, ups=14.43, wpb=875.7, bsz=32, num_updates=6520, lr=2.07385e-05, gnorm=1.958, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=884
2022-01-09 21:04:45 | INFO | train_inner | epoch 066:     96 / 99 loss=1.608, ppl=3.05, wps=10870.1, ups=9.58, wpb=1134.1, bsz=31.5, num_updates=6530, lr=2.07231e-05, gnorm=2.214, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=885
2022-01-09 21:04:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:04:46 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 1.483 | ppl 2.8 | wps 32700.3 | wpb 930.4 | bsz 31.3 | num_updates 6533 | best_loss 1.467
2022-01-09 21:04:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 6533 updates
2022-01-09 21:04:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:04:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:04:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 66 @ 6533 updates, score 1.483) (writing took 3.523494171909988 seconds)
2022-01-09 21:04:49 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-01-09 21:04:49 | INFO | train | epoch 066 | loss 1.224 | ppl 2.34 | wps 7737.6 | ups 7.75 | wpb 997.9 | bsz 31.9 | num_updates 6533 | lr 2.07185e-05 | gnorm 2.118 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 890
2022-01-09 21:04:49 | INFO | fairseq.trainer | begin training epoch 67
2022-01-09 21:04:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:04:50 | INFO | train_inner | epoch 067:      7 / 99 loss=1.042, ppl=2.06, wps=1891.2, ups=1.85, wpb=1022.2, bsz=32, num_updates=6540, lr=2.07077e-05, gnorm=1.937, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=891
2022-01-09 21:04:51 | INFO | train_inner | epoch 067:     17 / 99 loss=1.106, ppl=2.15, wps=12008.2, ups=12.53, wpb=958.2, bsz=32, num_updates=6550, lr=2.06923e-05, gnorm=2.079, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=891
2022-01-09 21:04:52 | INFO | train_inner | epoch 067:     27 / 99 loss=1.021, ppl=2.03, wps=12712.1, ups=13.56, wpb=937.7, bsz=32, num_updates=6560, lr=2.06769e-05, gnorm=2.093, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=892
2022-01-09 21:04:52 | INFO | train_inner | epoch 067:     37 / 99 loss=1.293, ppl=2.45, wps=14374, ups=12.15, wpb=1182.6, bsz=32, num_updates=6570, lr=2.06615e-05, gnorm=2.177, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=893
2022-01-09 21:04:53 | INFO | train_inner | epoch 067:     47 / 99 loss=0.977, ppl=1.97, wps=9834.6, ups=12.38, wpb=794.5, bsz=32, num_updates=6580, lr=2.06462e-05, gnorm=2.105, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=894
2022-01-09 21:04:54 | INFO | train_inner | epoch 067:     57 / 99 loss=1.265, ppl=2.4, wps=12244.4, ups=10.85, wpb=1128.4, bsz=32, num_updates=6590, lr=2.06308e-05, gnorm=2.079, clip=100, loss_scale=64, train_wall=1, gb_free=19.4, wall=895
2022-01-09 21:04:55 | INFO | train_inner | epoch 067:     67 / 99 loss=1.704, ppl=3.26, wps=12018, ups=10.64, wpb=1130, bsz=31.5, num_updates=6600, lr=2.06154e-05, gnorm=2.128, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=896
2022-01-09 21:04:56 | INFO | train_inner | epoch 067:     77 / 99 loss=1.063, ppl=2.09, wps=12389.6, ups=12.62, wpb=981.8, bsz=32, num_updates=6610, lr=2.06e-05, gnorm=2.083, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=896
2022-01-09 21:04:57 | INFO | train_inner | epoch 067:     87 / 99 loss=1.313, ppl=2.48, wps=13047.5, ups=11.55, wpb=1129.4, bsz=32, num_updates=6620, lr=2.05846e-05, gnorm=2.214, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=897
2022-01-09 21:04:58 | INFO | train_inner | epoch 067:     97 / 99 loss=0.959, ppl=1.94, wps=8455.5, ups=11.04, wpb=766.1, bsz=32, num_updates=6630, lr=2.05692e-05, gnorm=2.063, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=898
2022-01-09 21:04:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:04:59 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 1.45 | ppl 2.73 | wps 32538.8 | wpb 930.4 | bsz 31.3 | num_updates 6632 | best_loss 1.45
2022-01-09 21:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 6632 updates
2022-01-09 21:04:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:05:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:05:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 67 @ 6632 updates, score 1.45) (writing took 4.475622760946862 seconds)
2022-01-09 21:05:03 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-01-09 21:05:03 | INFO | train | epoch 067 | loss 1.199 | ppl 2.3 | wps 7125.9 | ups 7.14 | wpb 997.9 | bsz 31.9 | num_updates 6632 | lr 2.05662e-05 | gnorm 2.106 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 904
2022-01-09 21:05:03 | INFO | fairseq.trainer | begin training epoch 68
2022-01-09 21:05:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:05:04 | INFO | train_inner | epoch 068:      8 / 99 loss=0.978, ppl=1.97, wps=1452.7, ups=1.59, wpb=911, bsz=32, num_updates=6640, lr=2.05538e-05, gnorm=1.98, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=904
2022-01-09 21:05:05 | INFO | train_inner | epoch 068:     18 / 99 loss=1.236, ppl=2.36, wps=11209.5, ups=13.22, wpb=847.7, bsz=32, num_updates=6650, lr=2.05385e-05, gnorm=1.98, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=905
2022-01-09 21:05:05 | INFO | train_inner | epoch 068:     28 / 99 loss=1.112, ppl=2.16, wps=13561.9, ups=13.6, wpb=997.1, bsz=32, num_updates=6660, lr=2.05231e-05, gnorm=2.063, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=906
2022-01-09 21:05:06 | INFO | train_inner | epoch 068:     38 / 99 loss=1.286, ppl=2.44, wps=14269.9, ups=13.58, wpb=1050.6, bsz=32, num_updates=6670, lr=2.05077e-05, gnorm=2.157, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=907
2022-01-09 21:05:07 | INFO | train_inner | epoch 068:     48 / 99 loss=1.235, ppl=2.35, wps=15373.4, ups=13.15, wpb=1169, bsz=32, num_updates=6680, lr=2.04923e-05, gnorm=2.032, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=907
2022-01-09 21:05:08 | INFO | train_inner | epoch 068:     58 / 99 loss=1.12, ppl=2.17, wps=14326.3, ups=12.91, wpb=1109.4, bsz=32, num_updates=6690, lr=2.04769e-05, gnorm=2.065, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=908
2022-01-09 21:05:09 | INFO | train_inner | epoch 068:     68 / 99 loss=1.051, ppl=2.07, wps=10238.3, ups=11.55, wpb=886.7, bsz=32, num_updates=6700, lr=2.04615e-05, gnorm=2.058, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=909
2022-01-09 21:05:09 | INFO | train_inner | epoch 068:     78 / 99 loss=1.026, ppl=2.04, wps=11256.1, ups=12.08, wpb=932.1, bsz=32, num_updates=6710, lr=2.04462e-05, gnorm=1.999, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=910
2022-01-09 21:05:10 | INFO | train_inner | epoch 068:     88 / 99 loss=0.962, ppl=1.95, wps=11135.9, ups=13.39, wpb=831.6, bsz=32, num_updates=6720, lr=2.04308e-05, gnorm=1.984, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=911
2022-01-09 21:05:11 | INFO | train_inner | epoch 068:     98 / 99 loss=1.567, ppl=2.96, wps=10575.1, ups=8.74, wpb=1210, bsz=31.5, num_updates=6730, lr=2.04154e-05, gnorm=2.107, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=912
2022-01-09 21:05:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:05:12 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 1.464 | ppl 2.76 | wps 31087.8 | wpb 930.4 | bsz 31.3 | num_updates 6731 | best_loss 1.45
2022-01-09 21:05:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 6731 updates
2022-01-09 21:05:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:05:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:05:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 68 @ 6731 updates, score 1.464) (writing took 3.982397965970449 seconds)
2022-01-09 21:05:16 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-01-09 21:05:16 | INFO | train | epoch 068 | loss 1.177 | ppl 2.26 | wps 7529.6 | ups 7.55 | wpb 997.9 | bsz 31.9 | num_updates 6731 | lr 2.04138e-05 | gnorm 2.048 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 917
2022-01-09 21:05:16 | INFO | fairseq.trainer | begin training epoch 69
2022-01-09 21:05:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:05:17 | INFO | train_inner | epoch 069:      9 / 99 loss=1.102, ppl=2.15, wps=1647.7, ups=1.68, wpb=980.7, bsz=32, num_updates=6740, lr=2.04e-05, gnorm=2.187, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=918
2022-01-09 21:05:18 | INFO | train_inner | epoch 069:     19 / 99 loss=0.905, ppl=1.87, wps=12066.8, ups=13.31, wpb=906.4, bsz=32, num_updates=6750, lr=2.03846e-05, gnorm=1.928, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=919
2022-01-09 21:05:19 | INFO | train_inner | epoch 069:     29 / 99 loss=1.219, ppl=2.33, wps=13718.9, ups=11.35, wpb=1209, bsz=32, num_updates=6760, lr=2.03692e-05, gnorm=2.024, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=919
2022-01-09 21:05:20 | INFO | train_inner | epoch 069:     39 / 99 loss=1.039, ppl=2.06, wps=10027.8, ups=11.27, wpb=889.8, bsz=32, num_updates=6770, lr=2.03538e-05, gnorm=1.973, clip=100, loss_scale=64, train_wall=1, gb_free=19.5, wall=920
2022-01-09 21:05:21 | INFO | train_inner | epoch 069:     49 / 99 loss=1.084, ppl=2.12, wps=10526.9, ups=12.57, wpb=837.5, bsz=32, num_updates=6780, lr=2.03385e-05, gnorm=2.043, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=921
2022-01-09 21:05:22 | INFO | train_inner | epoch 069:     59 / 99 loss=1.036, ppl=2.05, wps=10204.4, ups=10.93, wpb=933.7, bsz=32, num_updates=6790, lr=2.03231e-05, gnorm=2.016, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=922
2022-01-09 21:05:22 | INFO | train_inner | epoch 069:     69 / 99 loss=0.997, ppl=2, wps=11571.9, ups=12.39, wpb=933.9, bsz=32, num_updates=6800, lr=2.03077e-05, gnorm=1.94, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=923
2022-01-09 21:05:23 | INFO | train_inner | epoch 069:     79 / 99 loss=1.177, ppl=2.26, wps=11917, ups=11.01, wpb=1082, bsz=32, num_updates=6810, lr=2.02923e-05, gnorm=2.127, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=924
2022-01-09 21:05:24 | INFO | train_inner | epoch 069:     89 / 99 loss=0.853, ppl=1.81, wps=8297.5, ups=11.98, wpb=692.4, bsz=32, num_updates=6820, lr=2.02769e-05, gnorm=1.999, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=925
2022-01-09 21:05:25 | INFO | train_inner | epoch 069:     99 / 99 loss=1.733, ppl=3.32, wps=14450.2, ups=10.04, wpb=1439.1, bsz=31.5, num_updates=6830, lr=2.02615e-05, gnorm=2.119, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=926
2022-01-09 21:05:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:05:26 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 1.424 | ppl 2.68 | wps 32254.3 | wpb 930.4 | bsz 31.3 | num_updates 6830 | best_loss 1.424
2022-01-09 21:05:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 6830 updates
2022-01-09 21:05:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:05:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:05:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 69 @ 6830 updates, score 1.424) (writing took 5.118204541038722 seconds)
2022-01-09 21:05:31 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-01-09 21:05:31 | INFO | train | epoch 069 | loss 1.159 | ppl 2.23 | wps 6699.3 | ups 6.71 | wpb 997.9 | bsz 31.9 | num_updates 6830 | lr 2.02615e-05 | gnorm 2.026 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.9 | wall 932
2022-01-09 21:05:31 | INFO | fairseq.trainer | begin training epoch 70
2022-01-09 21:05:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:05:32 | INFO | train_inner | epoch 070:     10 / 99 loss=1.494, ppl=2.82, wps=1647.1, ups=1.43, wpb=1153.6, bsz=31.5, num_updates=6840, lr=2.02462e-05, gnorm=2.211, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=933
2022-01-09 21:05:33 | INFO | train_inner | epoch 070:     20 / 99 loss=0.967, ppl=1.95, wps=12141, ups=13.23, wpb=917.5, bsz=32, num_updates=6850, lr=2.02308e-05, gnorm=2.044, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=933
2022-01-09 21:05:34 | INFO | train_inner | epoch 070:     30 / 99 loss=1.447, ppl=2.73, wps=14504.4, ups=12.37, wpb=1172.7, bsz=32, num_updates=6860, lr=2.02154e-05, gnorm=2.047, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=934
2022-01-09 21:05:35 | INFO | train_inner | epoch 070:     40 / 99 loss=1.134, ppl=2.19, wps=13088.8, ups=11.52, wpb=1136.6, bsz=32, num_updates=6870, lr=2.02e-05, gnorm=2.049, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=935
2022-01-09 21:05:35 | INFO | train_inner | epoch 070:     50 / 99 loss=1.297, ppl=2.46, wps=11747.6, ups=12.2, wpb=963.3, bsz=32, num_updates=6880, lr=2.01846e-05, gnorm=2.041, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=936
2022-01-09 21:05:36 | INFO | train_inner | epoch 070:     60 / 99 loss=0.965, ppl=1.95, wps=9335.3, ups=10.18, wpb=916.7, bsz=32, num_updates=6890, lr=2.01692e-05, gnorm=1.915, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=937
2022-01-09 21:05:37 | INFO | train_inner | epoch 070:     70 / 99 loss=1.008, ppl=2.01, wps=12149.6, ups=12.62, wpb=962.4, bsz=32, num_updates=6900, lr=2.01538e-05, gnorm=1.949, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=938
2022-01-09 21:05:38 | INFO | train_inner | epoch 070:     80 / 99 loss=0.927, ppl=1.9, wps=9319.5, ups=11.19, wpb=833, bsz=32, num_updates=6910, lr=2.01385e-05, gnorm=1.974, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=939
2022-01-09 21:05:39 | INFO | train_inner | epoch 070:     90 / 99 loss=0.985, ppl=1.98, wps=10921.9, ups=11.37, wpb=961, bsz=32, num_updates=6920, lr=2.01231e-05, gnorm=1.956, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=939
2022-01-09 21:05:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:05:41 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 1.393 | ppl 2.63 | wps 32311.6 | wpb 930.4 | bsz 31.3 | num_updates 6929 | best_loss 1.393
2022-01-09 21:05:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 6929 updates
2022-01-09 21:05:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:05:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:05:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 70 @ 6929 updates, score 1.393) (writing took 4.456077184062451 seconds)
2022-01-09 21:05:45 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-01-09 21:05:45 | INFO | train | epoch 070 | loss 1.143 | ppl 2.21 | wps 7046.3 | ups 7.06 | wpb 997.9 | bsz 31.9 | num_updates 6929 | lr 2.01092e-05 | gnorm 2.019 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.7 | wall 946
2022-01-09 21:05:45 | INFO | fairseq.trainer | begin training epoch 71
2022-01-09 21:05:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:05:45 | INFO | train_inner | epoch 071:      1 / 99 loss=1.011, ppl=2.01, wps=1512.2, ups=1.56, wpb=970.8, bsz=32, num_updates=6930, lr=2.01077e-05, gnorm=2.009, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=946
2022-01-09 21:05:46 | INFO | train_inner | epoch 071:     11 / 99 loss=1.092, ppl=2.13, wps=14008, ups=12.59, wpb=1112.9, bsz=32, num_updates=6940, lr=2.00923e-05, gnorm=1.974, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=947
2022-01-09 21:05:47 | INFO | train_inner | epoch 071:     21 / 99 loss=1.486, ppl=2.8, wps=13953.9, ups=12.29, wpb=1135.3, bsz=31.5, num_updates=6950, lr=2.00769e-05, gnorm=1.922, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=947
2022-01-09 21:05:48 | INFO | train_inner | epoch 071:     31 / 99 loss=1.28, ppl=2.43, wps=14522.5, ups=13.21, wpb=1099, bsz=32, num_updates=6960, lr=2.00615e-05, gnorm=2.066, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=948
2022-01-09 21:05:49 | INFO | train_inner | epoch 071:     41 / 99 loss=0.819, ppl=1.76, wps=8818.3, ups=11.66, wpb=756, bsz=32, num_updates=6970, lr=2.00462e-05, gnorm=1.933, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=949
2022-01-09 21:05:49 | INFO | train_inner | epoch 071:     51 / 99 loss=0.965, ppl=1.95, wps=11814.4, ups=12.96, wpb=911.4, bsz=32, num_updates=6980, lr=2.00308e-05, gnorm=2.005, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=950
2022-01-09 21:05:50 | INFO | train_inner | epoch 071:     61 / 99 loss=0.951, ppl=1.93, wps=12782.4, ups=14.25, wpb=896.8, bsz=32, num_updates=6990, lr=2.00154e-05, gnorm=2.016, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=951
2022-01-09 21:05:51 | INFO | train_inner | epoch 071:     71 / 99 loss=1.139, ppl=2.2, wps=12385.8, ups=13.35, wpb=927.8, bsz=32, num_updates=7000, lr=2e-05, gnorm=2.179, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=951
2022-01-09 21:05:52 | INFO | train_inner | epoch 071:     81 / 99 loss=1.334, ppl=2.52, wps=17137.3, ups=12.61, wpb=1358.9, bsz=32, num_updates=7010, lr=1.99846e-05, gnorm=2.205, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=952
2022-01-09 21:05:52 | INFO | train_inner | epoch 071:     91 / 99 loss=0.931, ppl=1.91, wps=10797.4, ups=12.39, wpb=871.8, bsz=32, num_updates=7020, lr=1.99692e-05, gnorm=1.934, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=953
2022-01-09 21:05:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:05:54 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 1.402 | ppl 2.64 | wps 31183.8 | wpb 930.4 | bsz 31.3 | num_updates 7028 | best_loss 1.393
2022-01-09 21:05:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 7028 updates
2022-01-09 21:05:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:05:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:05:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 71 @ 7028 updates, score 1.402) (writing took 3.631355528952554 seconds)
2022-01-09 21:05:58 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-01-09 21:05:58 | INFO | train | epoch 071 | loss 1.129 | ppl 2.19 | wps 7858.2 | ups 7.88 | wpb 997.9 | bsz 31.9 | num_updates 7028 | lr 1.99569e-05 | gnorm 2.024 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 958
2022-01-09 21:05:58 | INFO | fairseq.trainer | begin training epoch 72
2022-01-09 21:05:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:05:58 | INFO | train_inner | epoch 072:      2 / 99 loss=0.959, ppl=1.94, wps=1513.2, ups=1.8, wpb=838.9, bsz=32, num_updates=7030, lr=1.99538e-05, gnorm=1.98, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=958
2022-01-09 21:05:59 | INFO | train_inner | epoch 072:     12 / 99 loss=1.262, ppl=2.4, wps=11577.1, ups=11.24, wpb=1030, bsz=32, num_updates=7040, lr=1.99385e-05, gnorm=1.957, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=959
2022-01-09 21:06:00 | INFO | train_inner | epoch 072:     22 / 99 loss=1.021, ppl=2.03, wps=10839.7, ups=11.77, wpb=920.7, bsz=32, num_updates=7050, lr=1.99231e-05, gnorm=1.992, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=960
2022-01-09 21:06:01 | INFO | train_inner | epoch 072:     32 / 99 loss=0.933, ppl=1.91, wps=12389.5, ups=11.7, wpb=1059.1, bsz=32, num_updates=7060, lr=1.99077e-05, gnorm=1.963, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=961
2022-01-09 21:06:01 | INFO | train_inner | epoch 072:     42 / 99 loss=1.042, ppl=2.06, wps=10193.9, ups=11.38, wpb=895.6, bsz=32, num_updates=7070, lr=1.98923e-05, gnorm=2.053, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=962
2022-01-09 21:06:02 | INFO | train_inner | epoch 072:     52 / 99 loss=1.013, ppl=2.02, wps=10969.8, ups=12.56, wpb=873.7, bsz=32, num_updates=7080, lr=1.98769e-05, gnorm=2.065, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=963
2022-01-09 21:06:03 | INFO | train_inner | epoch 072:     62 / 99 loss=1.032, ppl=2.05, wps=11864.6, ups=11.78, wpb=1006.8, bsz=32, num_updates=7090, lr=1.98615e-05, gnorm=2.011, clip=100, loss_scale=64, train_wall=1, gb_free=19.4, wall=964
2022-01-09 21:06:04 | INFO | train_inner | epoch 072:     72 / 99 loss=1.446, ppl=2.72, wps=13867.1, ups=11.81, wpb=1174.6, bsz=31.5, num_updates=7100, lr=1.98462e-05, gnorm=2.043, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=964
2022-01-09 21:06:05 | INFO | train_inner | epoch 072:     82 / 99 loss=1.248, ppl=2.37, wps=13846.2, ups=12.15, wpb=1139.3, bsz=32, num_updates=7110, lr=1.98308e-05, gnorm=2.11, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=965
2022-01-09 21:06:06 | INFO | train_inner | epoch 072:     92 / 99 loss=0.93, ppl=1.91, wps=11315.7, ups=12.21, wpb=926.9, bsz=32, num_updates=7120, lr=1.98154e-05, gnorm=1.997, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=966
2022-01-09 21:06:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:06:07 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 1.409 | ppl 2.66 | wps 31495.7 | wpb 930.4 | bsz 31.3 | num_updates 7127 | best_loss 1.393
2022-01-09 21:06:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 7127 updates
2022-01-09 21:06:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:06:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:06:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 72 @ 7127 updates, score 1.409) (writing took 2.6802173390751705 seconds)
2022-01-09 21:06:10 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-01-09 21:06:10 | INFO | train | epoch 072 | loss 1.119 | ppl 2.17 | wps 8198.7 | ups 8.22 | wpb 997.9 | bsz 31.9 | num_updates 7127 | lr 1.98046e-05 | gnorm 2.027 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.3 | wall 970
2022-01-09 21:06:10 | INFO | fairseq.trainer | begin training epoch 73
2022-01-09 21:06:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:06:10 | INFO | train_inner | epoch 073:      3 / 99 loss=1.284, ppl=2.44, wps=2458.5, ups=2.19, wpb=1122.6, bsz=32, num_updates=7130, lr=1.98e-05, gnorm=2.163, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=971
2022-01-09 21:06:11 | INFO | train_inner | epoch 073:     13 / 99 loss=0.9, ppl=1.87, wps=12946.6, ups=13.85, wpb=934.8, bsz=32, num_updates=7140, lr=1.97846e-05, gnorm=1.924, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=971
2022-01-09 21:06:12 | INFO | train_inner | epoch 073:     23 / 99 loss=0.958, ppl=1.94, wps=13119.1, ups=12.3, wpb=1066.9, bsz=32, num_updates=7150, lr=1.97692e-05, gnorm=1.924, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=972
2022-01-09 21:06:13 | INFO | train_inner | epoch 073:     33 / 99 loss=0.909, ppl=1.88, wps=8309.8, ups=9.82, wpb=846.1, bsz=32, num_updates=7160, lr=1.97538e-05, gnorm=2.025, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=973
2022-01-09 21:06:14 | INFO | train_inner | epoch 073:     43 / 99 loss=0.849, ppl=1.8, wps=10407.6, ups=11.81, wpb=881.5, bsz=32, num_updates=7170, lr=1.97385e-05, gnorm=1.959, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=974
2022-01-09 21:06:14 | INFO | train_inner | epoch 073:     53 / 99 loss=1.006, ppl=2.01, wps=12842.3, ups=12.78, wpb=1004.7, bsz=32, num_updates=7180, lr=1.97231e-05, gnorm=2.018, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=975
2022-01-09 21:06:15 | INFO | train_inner | epoch 073:     63 / 99 loss=1.646, ppl=3.13, wps=12488, ups=10.71, wpb=1166.5, bsz=31.5, num_updates=7190, lr=1.97077e-05, gnorm=2.16, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=976
2022-01-09 21:06:16 | INFO | train_inner | epoch 073:     73 / 99 loss=1.049, ppl=2.07, wps=14189.3, ups=13.13, wpb=1080.9, bsz=32, num_updates=7200, lr=1.96923e-05, gnorm=2.072, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=977
2022-01-09 21:06:17 | INFO | train_inner | epoch 073:     83 / 99 loss=1.284, ppl=2.43, wps=13191.2, ups=12.44, wpb=1060.5, bsz=32, num_updates=7210, lr=1.96769e-05, gnorm=2.129, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=977
2022-01-09 21:06:18 | INFO | train_inner | epoch 073:     93 / 99 loss=1.069, ppl=2.1, wps=11859.8, ups=12.18, wpb=973.4, bsz=32, num_updates=7220, lr=1.96615e-05, gnorm=2.32, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=978
2022-01-09 21:06:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:06:19 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 1.343 | ppl 2.54 | wps 30632.2 | wpb 930.4 | bsz 31.3 | num_updates 7226 | best_loss 1.343
2022-01-09 21:06:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 7226 updates
2022-01-09 21:06:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:06:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:06:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 73 @ 7226 updates, score 1.343) (writing took 4.773773288005032 seconds)
2022-01-09 21:06:24 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-01-09 21:06:24 | INFO | train | epoch 073 | loss 1.103 | ppl 2.15 | wps 6962.8 | ups 6.98 | wpb 997.9 | bsz 31.9 | num_updates 7226 | lr 1.96523e-05 | gnorm 2.069 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 984
2022-01-09 21:06:24 | INFO | fairseq.trainer | begin training epoch 74
2022-01-09 21:06:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:06:24 | INFO | train_inner | epoch 074:      4 / 99 loss=0.936, ppl=1.91, wps=1130.9, ups=1.49, wpb=761.4, bsz=32, num_updates=7230, lr=1.96462e-05, gnorm=2.094, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=985
2022-01-09 21:06:25 | INFO | train_inner | epoch 074:     14 / 99 loss=1.226, ppl=2.34, wps=13417.9, ups=12.21, wpb=1099, bsz=32, num_updates=7240, lr=1.96308e-05, gnorm=2.073, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=986
2022-01-09 21:06:26 | INFO | train_inner | epoch 074:     24 / 99 loss=0.958, ppl=1.94, wps=12087, ups=12.93, wpb=934.6, bsz=32, num_updates=7250, lr=1.96154e-05, gnorm=2.079, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=987
2022-01-09 21:06:27 | INFO | train_inner | epoch 074:     34 / 99 loss=1.028, ppl=2.04, wps=10937.9, ups=13.2, wpb=828.5, bsz=32, num_updates=7260, lr=1.96e-05, gnorm=1.897, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=987
2022-01-09 21:06:28 | INFO | train_inner | epoch 074:     44 / 99 loss=1.465, ppl=2.76, wps=13091.7, ups=10.98, wpb=1191.9, bsz=31.5, num_updates=7270, lr=1.95846e-05, gnorm=2.062, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=988
2022-01-09 21:06:29 | INFO | train_inner | epoch 074:     54 / 99 loss=1.207, ppl=2.31, wps=14557.6, ups=12.41, wpb=1173, bsz=32, num_updates=7280, lr=1.95692e-05, gnorm=1.982, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=989
2022-01-09 21:06:29 | INFO | train_inner | epoch 074:     64 / 99 loss=1.145, ppl=2.21, wps=15473.7, ups=12.34, wpb=1253.5, bsz=32, num_updates=7290, lr=1.95538e-05, gnorm=2.152, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=990
2022-01-09 21:06:30 | INFO | train_inner | epoch 074:     74 / 99 loss=1.045, ppl=2.06, wps=12652.9, ups=11.62, wpb=1088.6, bsz=32, num_updates=7300, lr=1.95385e-05, gnorm=2.029, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=991
2022-01-09 21:06:31 | INFO | train_inner | epoch 074:     84 / 99 loss=0.74, ppl=1.67, wps=8670.4, ups=12, wpb=722.4, bsz=32, num_updates=7310, lr=1.95231e-05, gnorm=1.909, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=992
2022-01-09 21:06:32 | INFO | train_inner | epoch 074:     94 / 99 loss=0.906, ppl=1.87, wps=10368.3, ups=11.83, wpb=876.8, bsz=32, num_updates=7320, lr=1.95077e-05, gnorm=2.041, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=992
2022-01-09 21:06:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:06:33 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 1.413 | ppl 2.66 | wps 31830.1 | wpb 930.4 | bsz 31.3 | num_updates 7325 | best_loss 1.343
2022-01-09 21:06:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 7325 updates
2022-01-09 21:06:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:06:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:06:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 74 @ 7325 updates, score 1.413) (writing took 3.543051777058281 seconds)
2022-01-09 21:06:37 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-01-09 21:06:37 | INFO | train | epoch 074 | loss 1.083 | ppl 2.12 | wps 7688.2 | ups 7.7 | wpb 997.9 | bsz 31.9 | num_updates 7325 | lr 1.95e-05 | gnorm 2.02 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.6 | wall 997
2022-01-09 21:06:37 | INFO | fairseq.trainer | begin training epoch 75
2022-01-09 21:06:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:06:37 | INFO | train_inner | epoch 075:      5 / 99 loss=0.829, ppl=1.78, wps=1694, ups=1.85, wpb=917.3, bsz=32, num_updates=7330, lr=1.94923e-05, gnorm=1.891, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=998
2022-01-09 21:06:38 | INFO | train_inner | epoch 075:     15 / 99 loss=0.742, ppl=1.67, wps=10586.3, ups=14.53, wpb=728.5, bsz=32, num_updates=7340, lr=1.94769e-05, gnorm=1.884, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=999
2022-01-09 21:06:39 | INFO | train_inner | epoch 075:     25 / 99 loss=0.822, ppl=1.77, wps=11374.1, ups=13.23, wpb=859.8, bsz=32, num_updates=7350, lr=1.94615e-05, gnorm=2.059, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=999
2022-01-09 21:06:40 | INFO | train_inner | epoch 075:     35 / 99 loss=1.188, ppl=2.28, wps=14892.3, ups=12.66, wpb=1176.5, bsz=32, num_updates=7360, lr=1.94462e-05, gnorm=2.069, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=1000
2022-01-09 21:06:40 | INFO | train_inner | epoch 075:     45 / 99 loss=1.61, ppl=3.05, wps=14629.6, ups=11.68, wpb=1252.1, bsz=31.5, num_updates=7370, lr=1.94308e-05, gnorm=2.206, clip=100, loss_scale=64, train_wall=1, gb_free=16.9, wall=1001
2022-01-09 21:06:41 | INFO | train_inner | epoch 075:     55 / 99 loss=0.901, ppl=1.87, wps=11572.7, ups=13.68, wpb=845.8, bsz=32, num_updates=7380, lr=1.94154e-05, gnorm=2.276, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1002
2022-01-09 21:06:42 | INFO | train_inner | epoch 075:     65 / 99 loss=0.971, ppl=1.96, wps=12700.7, ups=13.25, wpb=958.8, bsz=32, num_updates=7390, lr=1.94e-05, gnorm=2.172, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=1002
2022-01-09 21:06:43 | INFO | train_inner | epoch 075:     75 / 99 loss=1.056, ppl=2.08, wps=13072.1, ups=13.33, wpb=980.8, bsz=32, num_updates=7400, lr=1.93846e-05, gnorm=2.178, clip=100, loss_scale=64, train_wall=1, gb_free=19.1, wall=1003
2022-01-09 21:06:43 | INFO | train_inner | epoch 075:     85 / 99 loss=0.977, ppl=1.97, wps=13426, ups=13.13, wpb=1022.7, bsz=32, num_updates=7410, lr=1.93692e-05, gnorm=2.1, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1004
2022-01-09 21:06:44 | INFO | train_inner | epoch 075:     95 / 99 loss=1.254, ppl=2.39, wps=11375.8, ups=10.94, wpb=1039.9, bsz=32, num_updates=7420, lr=1.93538e-05, gnorm=2.082, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=1005
2022-01-09 21:06:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:06:46 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 1.357 | ppl 2.56 | wps 31601.6 | wpb 930.4 | bsz 31.3 | num_updates 7424 | best_loss 1.343
2022-01-09 21:06:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 7424 updates
2022-01-09 21:06:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:06:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:06:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 75 @ 7424 updates, score 1.357) (writing took 3.6315314900130033 seconds)
2022-01-09 21:06:49 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-01-09 21:06:49 | INFO | train | epoch 075 | loss 1.083 | ppl 2.12 | wps 7939.9 | ups 7.96 | wpb 997.9 | bsz 31.9 | num_updates 7424 | lr 1.93477e-05 | gnorm 2.102 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.1 | wall 1010
2022-01-09 21:06:49 | INFO | fairseq.trainer | begin training epoch 76
2022-01-09 21:06:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:06:50 | INFO | train_inner | epoch 076:      6 / 99 loss=0.998, ppl=2, wps=1879.7, ups=1.84, wpb=1023.6, bsz=32, num_updates=7430, lr=1.93385e-05, gnorm=2.036, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1010
2022-01-09 21:06:51 | INFO | train_inner | epoch 076:     16 / 99 loss=0.998, ppl=2, wps=12928.1, ups=13.76, wpb=939.6, bsz=32, num_updates=7440, lr=1.93231e-05, gnorm=2.135, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1011
2022-01-09 21:06:51 | INFO | train_inner | epoch 076:     26 / 99 loss=0.912, ppl=1.88, wps=11583.7, ups=13.6, wpb=852, bsz=32, num_updates=7450, lr=1.93077e-05, gnorm=1.976, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1012
2022-01-09 21:06:52 | INFO | train_inner | epoch 076:     36 / 99 loss=0.885, ppl=1.85, wps=12966.8, ups=14.35, wpb=903.8, bsz=32, num_updates=7460, lr=1.92923e-05, gnorm=1.978, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1012
2022-01-09 21:06:53 | INFO | train_inner | epoch 076:     46 / 99 loss=0.855, ppl=1.81, wps=10986.7, ups=12.9, wpb=851.7, bsz=32, num_updates=7470, lr=1.92769e-05, gnorm=2.002, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1013
2022-01-09 21:06:54 | INFO | train_inner | epoch 076:     56 / 99 loss=1.387, ppl=2.61, wps=13482.2, ups=11.03, wpb=1222.4, bsz=31.5, num_updates=7480, lr=1.92615e-05, gnorm=2.088, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1014
2022-01-09 21:06:54 | INFO | train_inner | epoch 076:     66 / 99 loss=1.213, ppl=2.32, wps=13903, ups=12.14, wpb=1144.8, bsz=32, num_updates=7490, lr=1.92462e-05, gnorm=2.011, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1015
2022-01-09 21:06:55 | INFO | train_inner | epoch 076:     76 / 99 loss=1.202, ppl=2.3, wps=14281.2, ups=11.19, wpb=1276, bsz=32, num_updates=7500, lr=1.92308e-05, gnorm=2.013, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=1016
2022-01-09 21:06:56 | INFO | train_inner | epoch 076:     86 / 99 loss=1.131, ppl=2.19, wps=12309, ups=11.12, wpb=1106.6, bsz=32, num_updates=7510, lr=1.92154e-05, gnorm=2.078, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1017
2022-01-09 21:06:57 | INFO | train_inner | epoch 076:     96 / 99 loss=0.921, ppl=1.89, wps=9376, ups=10.67, wpb=878.9, bsz=32, num_updates=7520, lr=1.92e-05, gnorm=2.037, clip=100, loss_scale=64, train_wall=1, gb_free=19.4, wall=1018
2022-01-09 21:06:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:06:58 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 1.337 | ppl 2.53 | wps 31191.1 | wpb 930.4 | bsz 31.3 | num_updates 7523 | best_loss 1.337
2022-01-09 21:06:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 7523 updates
2022-01-09 21:06:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:07:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:07:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 76 @ 7523 updates, score 1.337) (writing took 4.858278854982927 seconds)
2022-01-09 21:07:03 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-01-09 21:07:03 | INFO | train | epoch 076 | loss 1.064 | ppl 2.09 | wps 7046.9 | ups 7.06 | wpb 997.9 | bsz 31.9 | num_updates 7523 | lr 1.91954e-05 | gnorm 2.031 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 1024
2022-01-09 21:07:03 | INFO | fairseq.trainer | begin training epoch 77
2022-01-09 21:07:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:07:04 | INFO | train_inner | epoch 077:      7 / 99 loss=0.626, ppl=1.54, wps=917.7, ups=1.5, wpb=612, bsz=32, num_updates=7530, lr=1.91846e-05, gnorm=1.946, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1024
2022-01-09 21:07:05 | INFO | train_inner | epoch 077:     17 / 99 loss=0.957, ppl=1.94, wps=11611.7, ups=11.76, wpb=987.7, bsz=32, num_updates=7540, lr=1.91692e-05, gnorm=1.982, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1025
2022-01-09 21:07:05 | INFO | train_inner | epoch 077:     27 / 99 loss=0.926, ppl=1.9, wps=13010.9, ups=13.63, wpb=954.3, bsz=32, num_updates=7550, lr=1.91538e-05, gnorm=2.142, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1026
2022-01-09 21:07:06 | INFO | train_inner | epoch 077:     37 / 99 loss=1.438, ppl=2.71, wps=10351.6, ups=11.51, wpb=899, bsz=31.5, num_updates=7560, lr=1.91385e-05, gnorm=1.924, clip=100, loss_scale=64, train_wall=1, gb_free=16.9, wall=1027
2022-01-09 21:07:07 | INFO | train_inner | epoch 077:     47 / 99 loss=1.145, ppl=2.21, wps=14302.2, ups=12.42, wpb=1151.7, bsz=32, num_updates=7570, lr=1.91231e-05, gnorm=2.119, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1028
2022-01-09 21:07:08 | INFO | train_inner | epoch 077:     57 / 99 loss=1.172, ppl=2.25, wps=15092.2, ups=13.4, wpb=1126.5, bsz=32, num_updates=7580, lr=1.91077e-05, gnorm=2, clip=100, loss_scale=64, train_wall=1, gb_free=18.8, wall=1028
2022-01-09 21:07:09 | INFO | train_inner | epoch 077:     67 / 99 loss=0.803, ppl=1.75, wps=12996.7, ups=14.51, wpb=895.8, bsz=32, num_updates=7590, lr=1.90923e-05, gnorm=2.023, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1029
2022-01-09 21:07:09 | INFO | train_inner | epoch 077:     77 / 99 loss=0.985, ppl=1.98, wps=14910.8, ups=13.87, wpb=1075.3, bsz=32, num_updates=7600, lr=1.90769e-05, gnorm=1.954, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=1030
2022-01-09 21:07:10 | INFO | train_inner | epoch 077:     87 / 99 loss=1.066, ppl=2.09, wps=12915.7, ups=12.78, wpb=1011, bsz=32, num_updates=7610, lr=1.90615e-05, gnorm=1.944, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=1031
2022-01-09 21:07:11 | INFO | train_inner | epoch 077:     97 / 99 loss=1.163, ppl=2.24, wps=11211.2, ups=9.67, wpb=1159.2, bsz=32, num_updates=7620, lr=1.90462e-05, gnorm=1.938, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=1032
2022-01-09 21:07:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:07:12 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 1.319 | ppl 2.49 | wps 30438.8 | wpb 930.4 | bsz 31.3 | num_updates 7622 | best_loss 1.319
2022-01-09 21:07:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 7622 updates
2022-01-09 21:07:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:07:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:07:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 77 @ 7622 updates, score 1.319) (writing took 5.180825400049798 seconds)
2022-01-09 21:07:18 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-01-09 21:07:18 | INFO | train | epoch 077 | loss 1.055 | ppl 2.08 | wps 6954.5 | ups 6.97 | wpb 997.9 | bsz 31.9 | num_updates 7622 | lr 1.90431e-05 | gnorm 1.998 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.4 | wall 1038
2022-01-09 21:07:18 | INFO | fairseq.trainer | begin training epoch 78
2022-01-09 21:07:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:07:18 | INFO | train_inner | epoch 078:      8 / 99 loss=0.92, ppl=1.89, wps=1305, ups=1.4, wpb=934, bsz=32, num_updates=7630, lr=1.90308e-05, gnorm=1.968, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1039
2022-01-09 21:07:19 | INFO | train_inner | epoch 078:     18 / 99 loss=0.857, ppl=1.81, wps=11128.2, ups=12.72, wpb=875.1, bsz=32, num_updates=7640, lr=1.90154e-05, gnorm=1.937, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=1040
2022-01-09 21:07:20 | INFO | train_inner | epoch 078:     28 / 99 loss=0.914, ppl=1.88, wps=12187.2, ups=11.65, wpb=1046.3, bsz=32, num_updates=7650, lr=1.9e-05, gnorm=1.818, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1040
2022-01-09 21:07:21 | INFO | train_inner | epoch 078:     38 / 99 loss=0.998, ppl=2, wps=12257.3, ups=12.64, wpb=969.9, bsz=32, num_updates=7660, lr=1.89846e-05, gnorm=1.912, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1041
2022-01-09 21:07:22 | INFO | train_inner | epoch 078:     48 / 99 loss=1.019, ppl=2.03, wps=14013.8, ups=12.51, wpb=1120.3, bsz=32, num_updates=7670, lr=1.89692e-05, gnorm=1.964, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1042
2022-01-09 21:07:22 | INFO | train_inner | epoch 078:     58 / 99 loss=1.032, ppl=2.05, wps=13608.4, ups=14.02, wpb=970.8, bsz=32, num_updates=7680, lr=1.89538e-05, gnorm=1.915, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1043
2022-01-09 21:07:23 | INFO | train_inner | epoch 078:     68 / 99 loss=1.537, ppl=2.9, wps=16613, ups=11.44, wpb=1452.2, bsz=31.5, num_updates=7690, lr=1.89385e-05, gnorm=2.039, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1044
2022-01-09 21:07:24 | INFO | train_inner | epoch 078:     78 / 99 loss=0.789, ppl=1.73, wps=11475.3, ups=14.52, wpb=790.3, bsz=32, num_updates=7700, lr=1.89231e-05, gnorm=1.965, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1044
2022-01-09 21:07:25 | INFO | train_inner | epoch 078:     88 / 99 loss=0.95, ppl=1.93, wps=11088.4, ups=12.32, wpb=899.7, bsz=32, num_updates=7710, lr=1.89077e-05, gnorm=2.036, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1045
2022-01-09 21:07:26 | INFO | train_inner | epoch 078:     98 / 99 loss=1.049, ppl=2.07, wps=10412.1, ups=11.25, wpb=925.8, bsz=32, num_updates=7720, lr=1.88923e-05, gnorm=2.151, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1046
2022-01-09 21:07:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:07:27 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 1.348 | ppl 2.55 | wps 32703.4 | wpb 930.4 | bsz 31.3 | num_updates 7721 | best_loss 1.319
2022-01-09 21:07:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 7721 updates
2022-01-09 21:07:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:07:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:07:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 78 @ 7721 updates, score 1.348) (writing took 3.00418108003214 seconds)
2022-01-09 21:07:30 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-01-09 21:07:30 | INFO | train | epoch 078 | loss 1.033 | ppl 2.05 | wps 8209.3 | ups 8.23 | wpb 997.9 | bsz 31.9 | num_updates 7721 | lr 1.88908e-05 | gnorm 1.965 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 1050
2022-01-09 21:07:30 | INFO | fairseq.trainer | begin training epoch 79
2022-01-09 21:07:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:07:30 | INFO | train_inner | epoch 079:      9 / 99 loss=1.08, ppl=2.11, wps=1998.3, ups=2.05, wpb=975.1, bsz=32, num_updates=7730, lr=1.88769e-05, gnorm=1.843, clip=100, loss_scale=64, train_wall=1, gb_free=18.8, wall=1051
2022-01-09 21:07:31 | INFO | train_inner | epoch 079:     19 / 99 loss=1.025, ppl=2.03, wps=11121.4, ups=11.43, wpb=973.2, bsz=32, num_updates=7740, lr=1.88615e-05, gnorm=1.953, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1052
2022-01-09 21:07:32 | INFO | train_inner | epoch 079:     29 / 99 loss=0.777, ppl=1.71, wps=8944.7, ups=10.7, wpb=835.6, bsz=32, num_updates=7750, lr=1.88462e-05, gnorm=1.937, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1053
2022-01-09 21:07:33 | INFO | train_inner | epoch 079:     39 / 99 loss=1.099, ppl=2.14, wps=12858.2, ups=10.87, wpb=1183.1, bsz=32, num_updates=7760, lr=1.88308e-05, gnorm=1.839, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1054
2022-01-09 21:07:34 | INFO | train_inner | epoch 079:     49 / 99 loss=0.828, ppl=1.77, wps=10228.8, ups=11.28, wpb=907, bsz=32, num_updates=7770, lr=1.88154e-05, gnorm=1.92, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1055
2022-01-09 21:07:35 | INFO | train_inner | epoch 079:     59 / 99 loss=1.387, ppl=2.62, wps=11543.1, ups=9.78, wpb=1180.2, bsz=31.5, num_updates=7780, lr=1.88e-05, gnorm=2.067, clip=100, loss_scale=64, train_wall=1, gb_free=16.9, wall=1056
2022-01-09 21:07:36 | INFO | train_inner | epoch 079:     69 / 99 loss=0.898, ppl=1.86, wps=10648.7, ups=11.85, wpb=898.7, bsz=32, num_updates=7790, lr=1.87846e-05, gnorm=1.963, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1056
2022-01-09 21:07:37 | INFO | train_inner | epoch 079:     79 / 99 loss=0.839, ppl=1.79, wps=9681, ups=11.55, wpb=837.9, bsz=32, num_updates=7800, lr=1.87692e-05, gnorm=1.989, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1057
2022-01-09 21:07:38 | INFO | train_inner | epoch 079:     89 / 99 loss=0.904, ppl=1.87, wps=10560.6, ups=11.17, wpb=945.1, bsz=32, num_updates=7810, lr=1.87538e-05, gnorm=1.997, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1058
2022-01-09 21:07:39 | INFO | train_inner | epoch 079:     99 / 99 loss=1.108, ppl=2.16, wps=12152.9, ups=9.94, wpb=1222.9, bsz=32, num_updates=7820, lr=1.87385e-05, gnorm=1.992, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1059
2022-01-09 21:07:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:07:40 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 1.318 | ppl 2.49 | wps 30539.7 | wpb 930.4 | bsz 31.3 | num_updates 7820 | best_loss 1.318
2022-01-09 21:07:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 7820 updates
2022-01-09 21:07:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:07:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:07:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 79 @ 7820 updates, score 1.318) (writing took 3.944638068904169 seconds)
2022-01-09 21:07:44 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-01-09 21:07:44 | INFO | train | epoch 079 | loss 1.018 | ppl 2.03 | wps 7024.3 | ups 7.04 | wpb 997.9 | bsz 31.9 | num_updates 7820 | lr 1.87385e-05 | gnorm 1.952 | clip 100 | loss_scale 64 | train_wall 9 | gb_free 20.4 | wall 1064
2022-01-09 21:07:44 | INFO | fairseq.trainer | begin training epoch 80
2022-01-09 21:07:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:07:44 | INFO | train_inner | epoch 080:     10 / 99 loss=0.882, ppl=1.84, wps=1648.6, ups=1.73, wpb=953.5, bsz=32, num_updates=7830, lr=1.87231e-05, gnorm=1.85, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1065
2022-01-09 21:07:45 | INFO | train_inner | epoch 080:     20 / 99 loss=0.735, ppl=1.66, wps=11309.7, ups=14.51, wpb=779.2, bsz=32, num_updates=7840, lr=1.87077e-05, gnorm=1.865, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1066
2022-01-09 21:07:46 | INFO | train_inner | epoch 080:     30 / 99 loss=1.074, ppl=2.11, wps=15850.5, ups=13.14, wpb=1206, bsz=32, num_updates=7850, lr=1.86923e-05, gnorm=1.971, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1066
2022-01-09 21:07:47 | INFO | train_inner | epoch 080:     40 / 99 loss=0.785, ppl=1.72, wps=13111, ups=14.35, wpb=913.7, bsz=32, num_updates=7860, lr=1.86769e-05, gnorm=1.861, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1067
2022-01-09 21:07:47 | INFO | train_inner | epoch 080:     50 / 99 loss=1.417, ppl=2.67, wps=16175.7, ups=12.3, wpb=1314.7, bsz=31.5, num_updates=7870, lr=1.86615e-05, gnorm=2.176, clip=100, loss_scale=64, train_wall=1, gb_free=19.4, wall=1068
2022-01-09 21:07:48 | INFO | train_inner | epoch 080:     60 / 99 loss=1.024, ppl=2.03, wps=12210.4, ups=13.35, wpb=914.9, bsz=32, num_updates=7880, lr=1.86462e-05, gnorm=1.989, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1069
2022-01-09 21:07:49 | INFO | train_inner | epoch 080:     70 / 99 loss=0.91, ppl=1.88, wps=12567.5, ups=13.36, wpb=940.5, bsz=32, num_updates=7890, lr=1.86308e-05, gnorm=2.071, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1069
2022-01-09 21:07:50 | INFO | train_inner | epoch 080:     80 / 99 loss=0.933, ppl=1.91, wps=10301.9, ups=11.2, wpb=919.5, bsz=32, num_updates=7900, lr=1.86154e-05, gnorm=2.075, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1070
2022-01-09 21:07:51 | INFO | train_inner | epoch 080:     90 / 99 loss=1.109, ppl=2.16, wps=12093.8, ups=10.52, wpb=1149.2, bsz=32, num_updates=7910, lr=1.86e-05, gnorm=2.098, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=1071
2022-01-09 21:07:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:07:52 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 1.326 | ppl 2.51 | wps 31574 | wpb 930.4 | bsz 31.3 | num_updates 7919 | best_loss 1.318
2022-01-09 21:07:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 7919 updates
2022-01-09 21:07:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:07:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:07:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 80 @ 7919 updates, score 1.326) (writing took 3.108168357051909 seconds)
2022-01-09 21:07:56 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-01-09 21:07:56 | INFO | train | epoch 080 | loss 1 | ppl 2 | wps 8288.5 | ups 8.31 | wpb 997.9 | bsz 31.9 | num_updates 7919 | lr 1.85862e-05 | gnorm 2.007 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 1076
2022-01-09 21:07:56 | INFO | fairseq.trainer | begin training epoch 81
2022-01-09 21:07:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:07:56 | INFO | train_inner | epoch 081:      1 / 99 loss=0.821, ppl=1.77, wps=1697.6, ups=2.04, wpb=830.4, bsz=32, num_updates=7920, lr=1.85846e-05, gnorm=2.092, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1076
2022-01-09 21:07:56 | INFO | train_inner | epoch 081:     11 / 99 loss=0.771, ppl=1.71, wps=10940.4, ups=13.37, wpb=818, bsz=32, num_updates=7930, lr=1.85692e-05, gnorm=1.951, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1077
2022-01-09 21:07:57 | INFO | train_inner | epoch 081:     21 / 99 loss=0.927, ppl=1.9, wps=12974.9, ups=11.87, wpb=1093.4, bsz=32, num_updates=7940, lr=1.85538e-05, gnorm=1.914, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1078
2022-01-09 21:07:58 | INFO | train_inner | epoch 081:     31 / 99 loss=0.871, ppl=1.83, wps=11826.3, ups=12.93, wpb=914.8, bsz=32, num_updates=7950, lr=1.85385e-05, gnorm=2.005, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1079
2022-01-09 21:07:59 | INFO | train_inner | epoch 081:     41 / 99 loss=1.121, ppl=2.17, wps=14312.4, ups=11.78, wpb=1215.3, bsz=32, num_updates=7960, lr=1.85231e-05, gnorm=1.885, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1079
2022-01-09 21:08:00 | INFO | train_inner | epoch 081:     51 / 99 loss=0.958, ppl=1.94, wps=11945.2, ups=12.55, wpb=952.1, bsz=32, num_updates=7970, lr=1.85077e-05, gnorm=1.939, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1080
2022-01-09 21:08:00 | INFO | train_inner | epoch 081:     61 / 99 loss=0.799, ppl=1.74, wps=12186.1, ups=13.5, wpb=902.6, bsz=32, num_updates=7980, lr=1.84923e-05, gnorm=1.885, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=1081
2022-01-09 21:08:01 | INFO | train_inner | epoch 081:     71 / 99 loss=0.744, ppl=1.67, wps=9793, ups=13.41, wpb=730.4, bsz=32, num_updates=7990, lr=1.84769e-05, gnorm=1.904, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1082
2022-01-09 21:08:02 | INFO | train_inner | epoch 081:     81 / 99 loss=1.466, ppl=2.76, wps=13399.6, ups=11.56, wpb=1158.7, bsz=31.5, num_updates=8000, lr=1.84615e-05, gnorm=1.943, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1083
2022-01-09 21:08:03 | INFO | train_inner | epoch 081:     91 / 99 loss=0.99, ppl=1.99, wps=14624, ups=13.44, wpb=1088.5, bsz=32, num_updates=8010, lr=1.84462e-05, gnorm=1.986, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1083
2022-01-09 21:08:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:08:04 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 1.284 | ppl 2.43 | wps 32431 | wpb 930.4 | bsz 31.3 | num_updates 8018 | best_loss 1.284
2022-01-09 21:08:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 8018 updates
2022-01-09 21:08:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:08:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:08:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 81 @ 8018 updates, score 1.284) (writing took 4.09983593900688 seconds)
2022-01-09 21:08:09 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-01-09 21:08:09 | INFO | train | epoch 081 | loss 0.988 | ppl 1.98 | wps 7577.2 | ups 7.59 | wpb 997.9 | bsz 31.9 | num_updates 8018 | lr 1.84338e-05 | gnorm 1.935 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.4 | wall 1089
2022-01-09 21:08:09 | INFO | fairseq.trainer | begin training epoch 82
2022-01-09 21:08:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:08:09 | INFO | train_inner | epoch 082:      2 / 99 loss=1.013, ppl=2.02, wps=1956.7, ups=1.67, wpb=1169.6, bsz=32, num_updates=8020, lr=1.84308e-05, gnorm=1.944, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1089
2022-01-09 21:08:10 | INFO | train_inner | epoch 082:     12 / 99 loss=1.021, ppl=2.03, wps=13220.7, ups=12.48, wpb=1059.1, bsz=32, num_updates=8030, lr=1.84154e-05, gnorm=2.001, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1090
2022-01-09 21:08:10 | INFO | train_inner | epoch 082:     22 / 99 loss=1.047, ppl=2.07, wps=13724.9, ups=13.14, wpb=1044.8, bsz=32, num_updates=8040, lr=1.84e-05, gnorm=2.027, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1091
2022-01-09 21:08:11 | INFO | train_inner | epoch 082:     32 / 99 loss=0.779, ppl=1.72, wps=8891.8, ups=10.89, wpb=816.8, bsz=32, num_updates=8050, lr=1.83846e-05, gnorm=2.007, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1092
2022-01-09 21:08:12 | INFO | train_inner | epoch 082:     42 / 99 loss=0.785, ppl=1.72, wps=13131.9, ups=13.9, wpb=944.7, bsz=32, num_updates=8060, lr=1.83692e-05, gnorm=1.928, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1093
2022-01-09 21:08:13 | INFO | train_inner | epoch 082:     52 / 99 loss=0.933, ppl=1.91, wps=13350.1, ups=12.51, wpb=1067.2, bsz=32, num_updates=8070, lr=1.83538e-05, gnorm=1.864, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1093
2022-01-09 21:08:14 | INFO | train_inner | epoch 082:     62 / 99 loss=0.83, ppl=1.78, wps=10939, ups=12.31, wpb=888.8, bsz=32, num_updates=8080, lr=1.83385e-05, gnorm=1.929, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1094
2022-01-09 21:08:14 | INFO | train_inner | epoch 082:     72 / 99 loss=0.778, ppl=1.71, wps=11016.3, ups=13.22, wpb=833.2, bsz=32, num_updates=8090, lr=1.83231e-05, gnorm=1.947, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1095
2022-01-09 21:08:15 | INFO | train_inner | epoch 082:     82 / 99 loss=1, ppl=2, wps=14684.4, ups=12.78, wpb=1149.3, bsz=32, num_updates=8100, lr=1.83077e-05, gnorm=1.949, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=1096
2022-01-09 21:08:16 | INFO | train_inner | epoch 082:     92 / 99 loss=1.457, ppl=2.75, wps=14369.7, ups=11.32, wpb=1269.2, bsz=31.5, num_updates=8110, lr=1.82923e-05, gnorm=2.004, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1097
2022-01-09 21:08:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:08:18 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 1.343 | ppl 2.54 | wps 29633.5 | wpb 930.4 | bsz 31.3 | num_updates 8117 | best_loss 1.284
2022-01-09 21:08:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 8117 updates
2022-01-09 21:08:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:08:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:08:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 82 @ 8117 updates, score 1.343) (writing took 3.362756773014553 seconds)
2022-01-09 21:08:21 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-01-09 21:08:21 | INFO | train | epoch 082 | loss 0.976 | ppl 1.97 | wps 7923.4 | ups 7.94 | wpb 997.9 | bsz 31.9 | num_updates 8117 | lr 1.82815e-05 | gnorm 1.961 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.1 | wall 1102
2022-01-09 21:08:21 | INFO | fairseq.trainer | begin training epoch 83
2022-01-09 21:08:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:08:21 | INFO | train_inner | epoch 083:      3 / 99 loss=0.808, ppl=1.75, wps=1573.5, ups=1.9, wpb=829, bsz=32, num_updates=8120, lr=1.82769e-05, gnorm=1.943, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1102
2022-01-09 21:08:22 | INFO | train_inner | epoch 083:     13 / 99 loss=0.936, ppl=1.91, wps=11956.1, ups=11.69, wpb=1022.4, bsz=32, num_updates=8130, lr=1.82615e-05, gnorm=1.92, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1103
2022-01-09 21:08:23 | INFO | train_inner | epoch 083:     23 / 99 loss=1.508, ppl=2.85, wps=15415.2, ups=11.96, wpb=1288.4, bsz=31.5, num_updates=8140, lr=1.82462e-05, gnorm=2.012, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1104
2022-01-09 21:08:24 | INFO | train_inner | epoch 083:     33 / 99 loss=0.967, ppl=1.95, wps=14228.8, ups=12.96, wpb=1097.5, bsz=32, num_updates=8150, lr=1.82308e-05, gnorm=1.949, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1104
2022-01-09 21:08:25 | INFO | train_inner | epoch 083:     43 / 99 loss=1.023, ppl=2.03, wps=14556.3, ups=13.13, wpb=1108.9, bsz=32, num_updates=8160, lr=1.82154e-05, gnorm=2.06, clip=100, loss_scale=64, train_wall=1, gb_free=19.5, wall=1105
2022-01-09 21:08:25 | INFO | train_inner | epoch 083:     53 / 99 loss=1.037, ppl=2.05, wps=13676.3, ups=13.97, wpb=979, bsz=32, num_updates=8170, lr=1.82e-05, gnorm=1.907, clip=100, loss_scale=64, train_wall=1, gb_free=18.8, wall=1106
2022-01-09 21:08:26 | INFO | train_inner | epoch 083:     63 / 99 loss=0.747, ppl=1.68, wps=11776.3, ups=13.87, wpb=849, bsz=32, num_updates=8180, lr=1.81846e-05, gnorm=1.842, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1107
2022-01-09 21:08:27 | INFO | train_inner | epoch 083:     73 / 99 loss=0.716, ppl=1.64, wps=10647.3, ups=13.9, wpb=766.1, bsz=32, num_updates=8190, lr=1.81692e-05, gnorm=1.922, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1107
2022-01-09 21:08:27 | INFO | train_inner | epoch 083:     83 / 99 loss=0.837, ppl=1.79, wps=12349.5, ups=13.97, wpb=884.1, bsz=32, num_updates=8200, lr=1.81538e-05, gnorm=1.976, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1108
2022-01-09 21:08:28 | INFO | train_inner | epoch 083:     93 / 99 loss=0.816, ppl=1.76, wps=11820.4, ups=12.87, wpb=918.7, bsz=32, num_updates=8210, lr=1.81385e-05, gnorm=1.978, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1109
2022-01-09 21:08:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:08:30 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 1.302 | ppl 2.47 | wps 30555.4 | wpb 930.4 | bsz 31.3 | num_updates 8216 | best_loss 1.284
2022-01-09 21:08:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 8216 updates
2022-01-09 21:08:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:08:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:08:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 83 @ 8216 updates, score 1.302) (writing took 2.872435895027593 seconds)
2022-01-09 21:08:33 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-01-09 21:08:33 | INFO | train | epoch 083 | loss 0.977 | ppl 1.97 | wps 8458.1 | ups 8.48 | wpb 997.9 | bsz 31.9 | num_updates 8216 | lr 1.81292e-05 | gnorm 1.946 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.5 | wall 1113
2022-01-09 21:08:33 | INFO | fairseq.trainer | begin training epoch 84
2022-01-09 21:08:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:08:33 | INFO | train_inner | epoch 084:      4 / 99 loss=0.925, ppl=1.9, wps=2173.9, ups=2.07, wpb=1050.9, bsz=32, num_updates=8220, lr=1.81231e-05, gnorm=1.925, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1114
2022-01-09 21:08:34 | INFO | train_inner | epoch 084:     14 / 99 loss=1.286, ppl=2.44, wps=13927.8, ups=13.37, wpb=1042.1, bsz=31.5, num_updates=8230, lr=1.81077e-05, gnorm=1.828, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1114
2022-01-09 21:08:35 | INFO | train_inner | epoch 084:     24 / 99 loss=1.137, ppl=2.2, wps=15537.5, ups=12.45, wpb=1248.2, bsz=32, num_updates=8240, lr=1.80923e-05, gnorm=1.94, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=1115
2022-01-09 21:08:35 | INFO | train_inner | epoch 084:     34 / 99 loss=0.769, ppl=1.7, wps=12438.7, ups=14.41, wpb=862.9, bsz=32, num_updates=8250, lr=1.80769e-05, gnorm=1.952, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=1116
2022-01-09 21:08:36 | INFO | train_inner | epoch 084:     44 / 99 loss=0.923, ppl=1.9, wps=13701.6, ups=13.23, wpb=1035.4, bsz=32, num_updates=8260, lr=1.80615e-05, gnorm=1.989, clip=100, loss_scale=64, train_wall=1, gb_free=19.5, wall=1117
2022-01-09 21:08:37 | INFO | train_inner | epoch 084:     54 / 99 loss=0.773, ppl=1.71, wps=10129.4, ups=12.66, wpb=800.4, bsz=32, num_updates=8270, lr=1.80462e-05, gnorm=2.012, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1117
2022-01-09 21:08:38 | INFO | train_inner | epoch 084:     64 / 99 loss=0.82, ppl=1.77, wps=12024.8, ups=12.38, wpb=971.4, bsz=32, num_updates=8280, lr=1.80308e-05, gnorm=1.885, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1118
2022-01-09 21:08:39 | INFO | train_inner | epoch 084:     74 / 99 loss=0.806, ppl=1.75, wps=9660.2, ups=10.64, wpb=907.7, bsz=32, num_updates=8290, lr=1.80154e-05, gnorm=1.934, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=1119
2022-01-09 21:08:39 | INFO | train_inner | epoch 084:     84 / 99 loss=1.128, ppl=2.19, wps=15335.4, ups=12.55, wpb=1221.5, bsz=32, num_updates=8300, lr=1.8e-05, gnorm=2.003, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1120
2022-01-09 21:08:41 | INFO | train_inner | epoch 084:     94 / 99 loss=0.878, ppl=1.84, wps=9724.4, ups=9.47, wpb=1027.1, bsz=32, num_updates=8310, lr=1.79846e-05, gnorm=1.923, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1121
2022-01-09 21:08:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:08:42 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 1.281 | ppl 2.43 | wps 35318.6 | wpb 930.4 | bsz 31.3 | num_updates 8315 | best_loss 1.281
2022-01-09 21:08:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 8315 updates
2022-01-09 21:08:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:08:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:08:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 84 @ 8315 updates, score 1.281) (writing took 4.748555327998474 seconds)
2022-01-09 21:08:47 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-01-09 21:08:47 | INFO | train | epoch 084 | loss 0.958 | ppl 1.94 | wps 7112.6 | ups 7.13 | wpb 997.9 | bsz 31.9 | num_updates 8315 | lr 1.79769e-05 | gnorm 1.945 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.3 | wall 1127
2022-01-09 21:08:47 | INFO | fairseq.trainer | begin training epoch 85
2022-01-09 21:08:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:08:47 | INFO | train_inner | epoch 085:      5 / 99 loss=0.804, ppl=1.75, wps=1301.2, ups=1.51, wpb=864, bsz=32, num_updates=8320, lr=1.79692e-05, gnorm=1.885, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1128
2022-01-09 21:08:48 | INFO | train_inner | epoch 085:     15 / 99 loss=1.015, ppl=2.02, wps=9967.4, ups=10.89, wpb=915.3, bsz=32, num_updates=8330, lr=1.79538e-05, gnorm=1.86, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=1129
2022-01-09 21:08:49 | INFO | train_inner | epoch 085:     25 / 99 loss=0.818, ppl=1.76, wps=11905.9, ups=12.59, wpb=945.3, bsz=32, num_updates=8340, lr=1.79385e-05, gnorm=1.831, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1129
2022-01-09 21:08:50 | INFO | train_inner | epoch 085:     35 / 99 loss=0.984, ppl=1.98, wps=12307.7, ups=10.1, wpb=1218.8, bsz=32, num_updates=8350, lr=1.79231e-05, gnorm=1.923, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1130
2022-01-09 21:08:51 | INFO | train_inner | epoch 085:     45 / 99 loss=0.933, ppl=1.91, wps=13790.2, ups=12.94, wpb=1065.3, bsz=32, num_updates=8360, lr=1.79077e-05, gnorm=2.091, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1131
2022-01-09 21:08:51 | INFO | train_inner | epoch 085:     55 / 99 loss=1.107, ppl=2.15, wps=15460.5, ups=13.34, wpb=1158.9, bsz=32, num_updates=8370, lr=1.78923e-05, gnorm=1.938, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1132
2022-01-09 21:08:52 | INFO | train_inner | epoch 085:     65 / 99 loss=0.764, ppl=1.7, wps=12386.7, ups=14.54, wpb=851.9, bsz=32, num_updates=8380, lr=1.78769e-05, gnorm=1.922, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=1133
2022-01-09 21:08:53 | INFO | train_inner | epoch 085:     75 / 99 loss=1.279, ppl=2.43, wps=12013.1, ups=11.43, wpb=1051.2, bsz=31.5, num_updates=8390, lr=1.78615e-05, gnorm=1.935, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1133
2022-01-09 21:08:54 | INFO | train_inner | epoch 085:     85 / 99 loss=0.854, ppl=1.81, wps=15074.5, ups=14.6, wpb=1032.6, bsz=32, num_updates=8400, lr=1.78462e-05, gnorm=1.914, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1134
2022-01-09 21:08:54 | INFO | train_inner | epoch 085:     95 / 99 loss=0.603, ppl=1.52, wps=9227.7, ups=12.54, wpb=735.7, bsz=32, num_updates=8410, lr=1.78308e-05, gnorm=1.808, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1135
2022-01-09 21:08:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:08:56 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 1.286 | ppl 2.44 | wps 31361.8 | wpb 930.4 | bsz 31.3 | num_updates 8414 | best_loss 1.281
2022-01-09 21:08:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 8414 updates
2022-01-09 21:08:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:08:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 85 @ 8414 updates, score 1.286) (writing took 3.1820340040139854 seconds)
2022-01-09 21:08:59 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-01-09 21:08:59 | INFO | train | epoch 085 | loss 0.938 | ppl 1.92 | wps 8032 | ups 8.05 | wpb 997.9 | bsz 31.9 | num_updates 8414 | lr 1.78246e-05 | gnorm 1.909 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.3 | wall 1139
2022-01-09 21:08:59 | INFO | fairseq.trainer | begin training epoch 86
2022-01-09 21:08:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:09:00 | INFO | train_inner | epoch 086:      6 / 99 loss=0.858, ppl=1.81, wps=2049.1, ups=1.94, wpb=1056.8, bsz=32, num_updates=8420, lr=1.78154e-05, gnorm=1.871, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1140
2022-01-09 21:09:00 | INFO | train_inner | epoch 086:     16 / 99 loss=1.002, ppl=2, wps=10686.8, ups=12.06, wpb=886.5, bsz=32, num_updates=8430, lr=1.78e-05, gnorm=1.865, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1141
2022-01-09 21:09:01 | INFO | train_inner | epoch 086:     26 / 99 loss=0.958, ppl=1.94, wps=12178.2, ups=11.55, wpb=1054, bsz=32, num_updates=8440, lr=1.77846e-05, gnorm=2.043, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1142
2022-01-09 21:09:02 | INFO | train_inner | epoch 086:     36 / 99 loss=1.052, ppl=2.07, wps=14879.1, ups=12.24, wpb=1216, bsz=32, num_updates=8450, lr=1.77692e-05, gnorm=1.892, clip=100, loss_scale=64, train_wall=1, gb_free=18.8, wall=1143
2022-01-09 21:09:03 | INFO | train_inner | epoch 086:     46 / 99 loss=0.856, ppl=1.81, wps=13476.9, ups=13.41, wpb=1004.7, bsz=32, num_updates=8460, lr=1.77538e-05, gnorm=1.893, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1143
2022-01-09 21:09:04 | INFO | train_inner | epoch 086:     56 / 99 loss=1.03, ppl=2.04, wps=12052.9, ups=11.05, wpb=1091.1, bsz=32, num_updates=8470, lr=1.77385e-05, gnorm=1.932, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1144
2022-01-09 21:09:05 | INFO | train_inner | epoch 086:     66 / 99 loss=0.637, ppl=1.56, wps=9980.5, ups=12.06, wpb=827.5, bsz=32, num_updates=8480, lr=1.77231e-05, gnorm=1.769, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1145
2022-01-09 21:09:06 | INFO | train_inner | epoch 086:     76 / 99 loss=0.818, ppl=1.76, wps=10802.6, ups=11, wpb=982.5, bsz=32, num_updates=8490, lr=1.77077e-05, gnorm=1.891, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1146
2022-01-09 21:09:06 | INFO | train_inner | epoch 086:     86 / 99 loss=0.761, ppl=1.7, wps=9197.5, ups=11.58, wpb=794, bsz=32, num_updates=8500, lr=1.76923e-05, gnorm=1.997, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1147
2022-01-09 21:09:07 | INFO | train_inner | epoch 086:     96 / 99 loss=0.791, ppl=1.73, wps=8766.2, ups=9.94, wpb=882.2, bsz=32, num_updates=8510, lr=1.76769e-05, gnorm=1.975, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1148
2022-01-09 21:09:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:09:09 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 1.274 | ppl 2.42 | wps 31235.2 | wpb 930.4 | bsz 31.3 | num_updates 8513 | best_loss 1.274
2022-01-09 21:09:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 8513 updates
2022-01-09 21:09:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:09:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:09:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 86 @ 8513 updates, score 1.274) (writing took 4.240185323986225 seconds)
2022-01-09 21:09:13 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-01-09 21:09:13 | INFO | train | epoch 086 | loss 0.938 | ppl 1.92 | wps 7044.9 | ups 7.06 | wpb 997.9 | bsz 31.9 | num_updates 8513 | lr 1.76723e-05 | gnorm 1.916 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20 | wall 1153
2022-01-09 21:09:13 | INFO | fairseq.trainer | begin training epoch 87
2022-01-09 21:09:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:09:14 | INFO | train_inner | epoch 087:      7 / 99 loss=1.433, ppl=2.7, wps=2300.3, ups=1.61, wpb=1424.9, bsz=31.5, num_updates=8520, lr=1.76615e-05, gnorm=2.049, clip=100, loss_scale=64, train_wall=1, gb_free=18.4, wall=1154
2022-01-09 21:09:14 | INFO | train_inner | epoch 087:     17 / 99 loss=0.95, ppl=1.93, wps=12085.2, ups=12.27, wpb=985.3, bsz=32, num_updates=8530, lr=1.76462e-05, gnorm=1.861, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1155
2022-01-09 21:09:15 | INFO | train_inner | epoch 087:     27 / 99 loss=0.753, ppl=1.69, wps=12642.3, ups=14.14, wpb=894.2, bsz=32, num_updates=8540, lr=1.76308e-05, gnorm=1.932, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1156
2022-01-09 21:09:16 | INFO | train_inner | epoch 087:     37 / 99 loss=0.882, ppl=1.84, wps=13813.9, ups=14.54, wpb=950.3, bsz=32, num_updates=8550, lr=1.76154e-05, gnorm=1.933, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1156
2022-01-09 21:09:17 | INFO | train_inner | epoch 087:     47 / 99 loss=0.737, ppl=1.67, wps=7583.2, ups=9.48, wpb=799.6, bsz=32, num_updates=8560, lr=1.76e-05, gnorm=1.904, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1157
2022-01-09 21:09:18 | INFO | train_inner | epoch 087:     57 / 99 loss=0.67, ppl=1.59, wps=12085.7, ups=14.52, wpb=832.4, bsz=32, num_updates=8570, lr=1.75846e-05, gnorm=1.846, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1158
2022-01-09 21:09:18 | INFO | train_inner | epoch 087:     67 / 99 loss=0.876, ppl=1.84, wps=14703.5, ups=13.32, wpb=1104.1, bsz=32, num_updates=8580, lr=1.75692e-05, gnorm=1.92, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=1159
2022-01-09 21:09:19 | INFO | train_inner | epoch 087:     77 / 99 loss=1.279, ppl=2.43, wps=14298.4, ups=12.25, wpb=1166.9, bsz=31.5, num_updates=8590, lr=1.75538e-05, gnorm=2.039, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1160
2022-01-09 21:09:20 | INFO | train_inner | epoch 087:     87 / 99 loss=0.72, ppl=1.65, wps=12117.1, ups=13.31, wpb=910.1, bsz=32, num_updates=8600, lr=1.75385e-05, gnorm=1.922, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1160
2022-01-09 21:09:21 | INFO | train_inner | epoch 087:     97 / 99 loss=0.939, ppl=1.92, wps=10289.7, ups=9.48, wpb=1085.4, bsz=32, num_updates=8610, lr=1.75231e-05, gnorm=2.087, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1161
2022-01-09 21:09:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:09:22 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 1.263 | ppl 2.4 | wps 31102.4 | wpb 930.4 | bsz 31.3 | num_updates 8612 | best_loss 1.263
2022-01-09 21:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 8612 updates
2022-01-09 21:09:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:09:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:09:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 87 @ 8612 updates, score 1.263) (writing took 4.873948503984138 seconds)
2022-01-09 21:09:27 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-01-09 21:09:27 | INFO | train | epoch 087 | loss 0.923 | ppl 1.9 | wps 7028.4 | ups 7.04 | wpb 997.9 | bsz 31.9 | num_updates 8612 | lr 1.752e-05 | gnorm 1.948 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.1 | wall 1168
2022-01-09 21:09:27 | INFO | fairseq.trainer | begin training epoch 88
2022-01-09 21:09:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:09:28 | INFO | train_inner | epoch 088:      8 / 99 loss=1.098, ppl=2.14, wps=1646.6, ups=1.48, wpb=1111.6, bsz=32, num_updates=8620, lr=1.75077e-05, gnorm=1.969, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=1168
2022-01-09 21:09:29 | INFO | train_inner | epoch 088:     18 / 99 loss=0.868, ppl=1.83, wps=13619.8, ups=11.95, wpb=1139.4, bsz=32, num_updates=8630, lr=1.74923e-05, gnorm=1.878, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1169
2022-01-09 21:09:29 | INFO | train_inner | epoch 088:     28 / 99 loss=1.217, ppl=2.32, wps=14897.2, ups=12.15, wpb=1225.9, bsz=31.5, num_updates=8640, lr=1.74769e-05, gnorm=1.856, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1170
2022-01-09 21:09:30 | INFO | train_inner | epoch 088:     38 / 99 loss=0.936, ppl=1.91, wps=13156.1, ups=12.07, wpb=1090.2, bsz=32, num_updates=8650, lr=1.74615e-05, gnorm=1.926, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=1171
2022-01-09 21:09:31 | INFO | train_inner | epoch 088:     48 / 99 loss=0.905, ppl=1.87, wps=12340, ups=11.86, wpb=1040.3, bsz=32, num_updates=8660, lr=1.74462e-05, gnorm=1.948, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1172
2022-01-09 21:09:32 | INFO | train_inner | epoch 088:     58 / 99 loss=0.722, ppl=1.65, wps=10713.3, ups=12.11, wpb=884.8, bsz=32, num_updates=8670, lr=1.74308e-05, gnorm=1.816, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1172
2022-01-09 21:09:33 | INFO | train_inner | epoch 088:     68 / 99 loss=0.684, ppl=1.61, wps=9462.5, ups=12.36, wpb=765.7, bsz=32, num_updates=8680, lr=1.74154e-05, gnorm=1.924, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=1173
2022-01-09 21:09:34 | INFO | train_inner | epoch 088:     78 / 99 loss=1.062, ppl=2.09, wps=11692.5, ups=10.73, wpb=1089.3, bsz=32, num_updates=8690, lr=1.74e-05, gnorm=1.975, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1174
2022-01-09 21:09:34 | INFO | train_inner | epoch 088:     88 / 99 loss=0.683, ppl=1.61, wps=9339.8, ups=12.54, wpb=744.8, bsz=32, num_updates=8700, lr=1.73846e-05, gnorm=1.936, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1175
2022-01-09 21:09:35 | INFO | train_inner | epoch 088:     98 / 99 loss=0.951, ppl=1.93, wps=10934.4, ups=10.75, wpb=1016.9, bsz=32, num_updates=8710, lr=1.73692e-05, gnorm=1.961, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1176
2022-01-09 21:09:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:09:36 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 1.269 | ppl 2.41 | wps 31704.2 | wpb 930.4 | bsz 31.3 | num_updates 8711 | best_loss 1.263
2022-01-09 21:09:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 8711 updates
2022-01-09 21:09:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:09:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 88 @ 8711 updates, score 1.269) (writing took 2.6174084990052506 seconds)
2022-01-09 21:09:39 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-01-09 21:09:39 | INFO | train | epoch 088 | loss 0.921 | ppl 1.89 | wps 8235.7 | ups 8.25 | wpb 997.9 | bsz 31.9 | num_updates 8711 | lr 1.73677e-05 | gnorm 1.912 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 1180
2022-01-09 21:09:39 | INFO | fairseq.trainer | begin training epoch 89
2022-01-09 21:09:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:09:40 | INFO | train_inner | epoch 089:      9 / 99 loss=0.759, ppl=1.69, wps=1986.7, ups=2.26, wpb=878.8, bsz=32, num_updates=8720, lr=1.73538e-05, gnorm=1.893, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=1180
2022-01-09 21:09:41 | INFO | train_inner | epoch 089:     19 / 99 loss=1.289, ppl=2.44, wps=13082, ups=11.52, wpb=1135.8, bsz=31.5, num_updates=8730, lr=1.73385e-05, gnorm=1.939, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1181
2022-01-09 21:09:42 | INFO | train_inner | epoch 089:     29 / 99 loss=0.812, ppl=1.76, wps=12341.7, ups=11.15, wpb=1107.2, bsz=32, num_updates=8740, lr=1.73231e-05, gnorm=1.842, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1182
2022-01-09 21:09:43 | INFO | train_inner | epoch 089:     39 / 99 loss=1.09, ppl=2.13, wps=9777.4, ups=8.81, wpb=1110.3, bsz=32, num_updates=8750, lr=1.73077e-05, gnorm=2.085, clip=100, loss_scale=64, train_wall=1, gb_free=19.1, wall=1183
2022-01-09 21:09:44 | INFO | train_inner | epoch 089:     49 / 99 loss=1.032, ppl=2.05, wps=11680.4, ups=11.55, wpb=1011.3, bsz=32, num_updates=8760, lr=1.72923e-05, gnorm=1.949, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1184
2022-01-09 21:09:44 | INFO | train_inner | epoch 089:     59 / 99 loss=0.6, ppl=1.52, wps=8913.6, ups=12.75, wpb=699, bsz=32, num_updates=8770, lr=1.72769e-05, gnorm=1.86, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1185
2022-01-09 21:09:45 | INFO | train_inner | epoch 089:     69 / 99 loss=0.802, ppl=1.74, wps=14172.1, ups=13.12, wpb=1080.1, bsz=32, num_updates=8780, lr=1.72615e-05, gnorm=1.964, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1186
2022-01-09 21:09:46 | INFO | train_inner | epoch 089:     79 / 99 loss=0.84, ppl=1.79, wps=13491.3, ups=12.2, wpb=1105.4, bsz=32, num_updates=8790, lr=1.72462e-05, gnorm=1.827, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1186
2022-01-09 21:09:47 | INFO | train_inner | epoch 089:     89 / 99 loss=0.746, ppl=1.68, wps=9340.1, ups=11.35, wpb=822.9, bsz=32, num_updates=8800, lr=1.72308e-05, gnorm=1.946, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1187
2022-01-09 21:09:48 | INFO | train_inner | epoch 089:     99 / 99 loss=0.8, ppl=1.74, wps=11773.5, ups=11.87, wpb=992, bsz=32, num_updates=8810, lr=1.72154e-05, gnorm=1.839, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=1188
2022-01-09 21:09:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:09:49 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 1.23 | ppl 2.35 | wps 30270.5 | wpb 930.4 | bsz 31.3 | num_updates 8810 | best_loss 1.23
2022-01-09 21:09:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 8810 updates
2022-01-09 21:09:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:09:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:09:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 89 @ 8810 updates, score 1.23) (writing took 4.707888331962749 seconds)
2022-01-09 21:09:53 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-01-09 21:09:53 | INFO | train | epoch 089 | loss 0.899 | ppl 1.86 | wps 6877.2 | ups 6.89 | wpb 997.9 | bsz 31.9 | num_updates 8810 | lr 1.72154e-05 | gnorm 1.918 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.3 | wall 1194
2022-01-09 21:09:53 | INFO | fairseq.trainer | begin training epoch 90
2022-01-09 21:09:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:09:54 | INFO | train_inner | epoch 090:     10 / 99 loss=0.946, ppl=1.93, wps=1568.8, ups=1.53, wpb=1026.5, bsz=32, num_updates=8820, lr=1.72e-05, gnorm=1.908, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=1195
2022-01-09 21:09:55 | INFO | train_inner | epoch 090:     20 / 99 loss=0.754, ppl=1.69, wps=12680, ups=13.27, wpb=955.8, bsz=32, num_updates=8830, lr=1.71846e-05, gnorm=1.878, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1196
2022-01-09 21:09:56 | INFO | train_inner | epoch 090:     30 / 99 loss=0.925, ppl=1.9, wps=11134.1, ups=13.37, wpb=832.8, bsz=32, num_updates=8840, lr=1.71692e-05, gnorm=1.926, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1196
2022-01-09 21:09:57 | INFO | train_inner | epoch 090:     40 / 99 loss=0.559, ppl=1.47, wps=9905.7, ups=12.28, wpb=806.4, bsz=32, num_updates=8850, lr=1.71538e-05, gnorm=1.656, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1197
2022-01-09 21:09:57 | INFO | train_inner | epoch 090:     50 / 99 loss=0.803, ppl=1.75, wps=11691.5, ups=11.88, wpb=984.4, bsz=32, num_updates=8860, lr=1.71385e-05, gnorm=1.931, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1198
2022-01-09 21:09:58 | INFO | train_inner | epoch 090:     60 / 99 loss=0.768, ppl=1.7, wps=9831, ups=10.68, wpb=920.1, bsz=32, num_updates=8870, lr=1.71231e-05, gnorm=1.88, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1199
2022-01-09 21:09:59 | INFO | train_inner | epoch 090:     70 / 99 loss=1.242, ppl=2.37, wps=11593.2, ups=10.32, wpb=1123.8, bsz=31.5, num_updates=8880, lr=1.71077e-05, gnorm=2.043, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1200
2022-01-09 21:10:00 | INFO | train_inner | epoch 090:     80 / 99 loss=0.877, ppl=1.84, wps=11009.1, ups=10.94, wpb=1006, bsz=32, num_updates=8890, lr=1.70923e-05, gnorm=1.8, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1201
2022-01-09 21:10:01 | INFO | train_inner | epoch 090:     90 / 99 loss=0.858, ppl=1.81, wps=12489.2, ups=10.9, wpb=1145.7, bsz=32, num_updates=8900, lr=1.70769e-05, gnorm=1.858, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1202
2022-01-09 21:10:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:10:03 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 1.256 | ppl 2.39 | wps 31189.8 | wpb 930.4 | bsz 31.3 | num_updates 8909 | best_loss 1.23
2022-01-09 21:10:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 8909 updates
2022-01-09 21:10:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:10:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:10:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 90 @ 8909 updates, score 1.256) (writing took 3.7350791100179777 seconds)
2022-01-09 21:10:07 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-01-09 21:10:07 | INFO | train | epoch 090 | loss 0.888 | ppl 1.85 | wps 7420.6 | ups 7.44 | wpb 997.9 | bsz 31.9 | num_updates 8909 | lr 1.70631e-05 | gnorm 1.882 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 18.4 | wall 1207
2022-01-09 21:10:07 | INFO | fairseq.trainer | begin training epoch 91
2022-01-09 21:10:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:10:07 | INFO | train_inner | epoch 091:      1 / 99 loss=1.096, ppl=2.14, wps=2250, ups=1.74, wpb=1291.4, bsz=32, num_updates=8910, lr=1.70615e-05, gnorm=1.979, clip=100, loss_scale=64, train_wall=1, gb_free=19.1, wall=1207
2022-01-09 21:10:08 | INFO | train_inner | epoch 091:     11 / 99 loss=0.727, ppl=1.65, wps=10522.2, ups=12.01, wpb=876.1, bsz=32, num_updates=8920, lr=1.70462e-05, gnorm=1.913, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=1208
2022-01-09 21:10:09 | INFO | train_inner | epoch 091:     21 / 99 loss=0.871, ppl=1.83, wps=13074.2, ups=11.67, wpb=1120.6, bsz=32, num_updates=8930, lr=1.70308e-05, gnorm=1.87, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1209
2022-01-09 21:10:10 | INFO | train_inner | epoch 091:     31 / 99 loss=1.227, ppl=2.34, wps=11708.4, ups=11.07, wpb=1057.5, bsz=31.5, num_updates=8940, lr=1.70154e-05, gnorm=1.801, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1210
2022-01-09 21:10:10 | INFO | train_inner | epoch 091:     41 / 99 loss=0.705, ppl=1.63, wps=8255, ups=10.74, wpb=768.6, bsz=32, num_updates=8950, lr=1.7e-05, gnorm=1.889, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1211
2022-01-09 21:10:11 | INFO | train_inner | epoch 091:     51 / 99 loss=1.039, ppl=2.05, wps=14358.8, ups=11.61, wpb=1237.1, bsz=32, num_updates=8960, lr=1.69846e-05, gnorm=1.924, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1212
2022-01-09 21:10:12 | INFO | train_inner | epoch 091:     61 / 99 loss=0.701, ppl=1.63, wps=10720.4, ups=12.9, wpb=831.1, bsz=32, num_updates=8970, lr=1.69692e-05, gnorm=2.019, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1213
2022-01-09 21:10:13 | INFO | train_inner | epoch 091:     71 / 99 loss=0.695, ppl=1.62, wps=10440.1, ups=11.95, wpb=873.6, bsz=32, num_updates=8980, lr=1.69538e-05, gnorm=1.86, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1213
2022-01-09 21:10:14 | INFO | train_inner | epoch 091:     81 / 99 loss=1.016, ppl=2.02, wps=15022.3, ups=12.42, wpb=1209.1, bsz=32, num_updates=8990, lr=1.69385e-05, gnorm=1.918, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1214
2022-01-09 21:10:15 | INFO | train_inner | epoch 091:     91 / 99 loss=0.826, ppl=1.77, wps=11828.5, ups=11.49, wpb=1029.7, bsz=32, num_updates=9000, lr=1.69231e-05, gnorm=1.947, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1215
2022-01-09 21:10:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:10:16 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 1.242 | ppl 2.37 | wps 31948.9 | wpb 930.4 | bsz 31.3 | num_updates 9008 | best_loss 1.23
2022-01-09 21:10:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 9008 updates
2022-01-09 21:10:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:10:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:10:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 91 @ 9008 updates, score 1.242) (writing took 3.409552116994746 seconds)
2022-01-09 21:10:20 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-01-09 21:10:20 | INFO | train | epoch 091 | loss 0.887 | ppl 1.85 | wps 7638.6 | ups 7.65 | wpb 997.9 | bsz 31.9 | num_updates 9008 | lr 1.69108e-05 | gnorm 1.904 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 1220
2022-01-09 21:10:20 | INFO | fairseq.trainer | begin training epoch 92
2022-01-09 21:10:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:10:20 | INFO | train_inner | epoch 092:      2 / 99 loss=0.641, ppl=1.56, wps=1557, ups=1.88, wpb=826.6, bsz=32, num_updates=9010, lr=1.69077e-05, gnorm=1.8, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1220
2022-01-09 21:10:21 | INFO | train_inner | epoch 092:     12 / 99 loss=1.225, ppl=2.34, wps=15069.4, ups=12.57, wpb=1198.4, bsz=31.5, num_updates=9020, lr=1.68923e-05, gnorm=2.065, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1221
2022-01-09 21:10:21 | INFO | train_inner | epoch 092:     22 / 99 loss=0.787, ppl=1.73, wps=14832.1, ups=13.79, wpb=1075.7, bsz=32, num_updates=9030, lr=1.68769e-05, gnorm=1.853, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1222
2022-01-09 21:10:22 | INFO | train_inner | epoch 092:     32 / 99 loss=0.689, ppl=1.61, wps=12587.9, ups=13.97, wpb=901, bsz=32, num_updates=9040, lr=1.68615e-05, gnorm=1.864, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1223
2022-01-09 21:10:23 | INFO | train_inner | epoch 092:     42 / 99 loss=0.874, ppl=1.83, wps=13970.8, ups=13.22, wpb=1056.6, bsz=32, num_updates=9050, lr=1.68462e-05, gnorm=1.892, clip=100, loss_scale=64, train_wall=1, gb_free=19.1, wall=1223
2022-01-09 21:10:24 | INFO | train_inner | epoch 092:     52 / 99 loss=0.952, ppl=1.93, wps=13152.7, ups=14.01, wpb=938.6, bsz=32, num_updates=9060, lr=1.68308e-05, gnorm=1.874, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1224
2022-01-09 21:10:24 | INFO | train_inner | epoch 092:     62 / 99 loss=0.752, ppl=1.68, wps=12649.2, ups=13.5, wpb=936.7, bsz=32, num_updates=9070, lr=1.68154e-05, gnorm=1.851, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1225
2022-01-09 21:10:25 | INFO | train_inner | epoch 092:     72 / 99 loss=0.766, ppl=1.7, wps=12644.9, ups=13.4, wpb=944, bsz=32, num_updates=9080, lr=1.68e-05, gnorm=1.901, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1226
2022-01-09 21:10:26 | INFO | train_inner | epoch 092:     82 / 99 loss=0.792, ppl=1.73, wps=8693.4, ups=9.59, wpb=906.7, bsz=32, num_updates=9090, lr=1.67846e-05, gnorm=1.811, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1227
2022-01-09 21:10:27 | INFO | train_inner | epoch 092:     92 / 99 loss=0.754, ppl=1.69, wps=9681.8, ups=11.19, wpb=865.5, bsz=32, num_updates=9100, lr=1.67692e-05, gnorm=1.867, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1228
2022-01-09 21:10:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:10:29 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 1.197 | ppl 2.29 | wps 29766.8 | wpb 930.4 | bsz 31.3 | num_updates 9107 | best_loss 1.197
2022-01-09 21:10:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 9107 updates
2022-01-09 21:10:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:10:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-09 21:10:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 92 @ 9107 updates, score 1.197) (writing took 5.345333251985721 seconds)
2022-01-09 21:10:34 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-01-09 21:10:34 | INFO | train | epoch 092 | loss 0.873 | ppl 1.83 | wps 6821.2 | ups 6.84 | wpb 997.9 | bsz 31.9 | num_updates 9107 | lr 1.67585e-05 | gnorm 1.885 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 1235
2022-01-09 21:10:34 | INFO | fairseq.trainer | begin training epoch 93
2022-01-09 21:10:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:10:34 | INFO | train_inner | epoch 093:      3 / 99 loss=1.001, ppl=2, wps=1584.2, ups=1.36, wpb=1169.1, bsz=32, num_updates=9110, lr=1.67538e-05, gnorm=1.852, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=1235
2022-01-09 21:10:35 | INFO | train_inner | epoch 093:     13 / 99 loss=0.998, ppl=2, wps=12327.2, ups=12.8, wpb=962.9, bsz=32, num_updates=9120, lr=1.67385e-05, gnorm=1.874, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1236
2022-01-09 21:10:36 | INFO | train_inner | epoch 093:     23 / 99 loss=0.554, ppl=1.47, wps=10081.2, ups=12.9, wpb=781.7, bsz=32, num_updates=9130, lr=1.67231e-05, gnorm=1.693, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1237
2022-01-09 21:10:37 | INFO | train_inner | epoch 093:     33 / 99 loss=0.723, ppl=1.65, wps=12686.4, ups=13.18, wpb=962.7, bsz=32, num_updates=9140, lr=1.67077e-05, gnorm=1.84, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=1237
2022-01-09 21:10:38 | INFO | train_inner | epoch 093:     43 / 99 loss=0.684, ppl=1.61, wps=10960.4, ups=12.45, wpb=880, bsz=32, num_updates=9150, lr=1.66923e-05, gnorm=1.787, clip=100, loss_scale=64, train_wall=1, gb_free=19.7, wall=1238
2022-01-09 21:10:39 | INFO | train_inner | epoch 093:     53 / 99 loss=1.39, ppl=2.62, wps=15640.2, ups=10.75, wpb=1454.8, bsz=31.5, num_updates=9160, lr=1.66769e-05, gnorm=2.06, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1239
2022-01-09 21:10:39 | INFO | train_inner | epoch 093:     63 / 99 loss=0.832, ppl=1.78, wps=15245.7, ups=13, wpb=1172.9, bsz=32, num_updates=9170, lr=1.66615e-05, gnorm=1.844, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=1240
2022-01-09 21:10:40 | INFO | train_inner | epoch 093:     73 / 99 loss=0.735, ppl=1.66, wps=9902.5, ups=12.91, wpb=767, bsz=32, num_updates=9180, lr=1.66462e-05, gnorm=2.005, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1241
2022-01-09 21:10:41 | INFO | train_inner | epoch 093:     83 / 99 loss=0.973, ppl=1.96, wps=14454.6, ups=12.97, wpb=1114.1, bsz=32, num_updates=9190, lr=1.66308e-05, gnorm=1.918, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1241
2022-01-09 21:10:42 | INFO | train_inner | epoch 093:     93 / 99 loss=0.629, ppl=1.55, wps=8660.7, ups=10.65, wpb=813, bsz=32, num_updates=9200, lr=1.66154e-05, gnorm=1.749, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1242
2022-01-09 21:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:10:43 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 1.212 | ppl 2.32 | wps 29817.2 | wpb 930.4 | bsz 31.3 | num_updates 9206 | best_loss 1.197
2022-01-09 21:10:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 9206 updates
2022-01-09 21:10:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:10:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:10:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 93 @ 9206 updates, score 1.212) (writing took 2.7569096200168133 seconds)
2022-01-09 21:10:46 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-01-09 21:10:46 | INFO | train | epoch 093 | loss 0.871 | ppl 1.83 | wps 8195.4 | ups 8.21 | wpb 997.9 | bsz 31.9 | num_updates 9206 | lr 1.66062e-05 | gnorm 1.853 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 1247
2022-01-09 21:10:46 | INFO | fairseq.trainer | begin training epoch 94
2022-01-09 21:10:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:10:47 | INFO | train_inner | epoch 094:      4 / 99 loss=0.764, ppl=1.7, wps=2265.7, ups=2.1, wpb=1077.9, bsz=32, num_updates=9210, lr=1.66e-05, gnorm=1.753, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1247
2022-01-09 21:10:47 | INFO | train_inner | epoch 094:     14 / 99 loss=1.018, ppl=2.03, wps=15986, ups=13.1, wpb=1220.5, bsz=32, num_updates=9220, lr=1.65846e-05, gnorm=1.895, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1248
2022-01-09 21:10:48 | INFO | train_inner | epoch 094:     24 / 99 loss=1.131, ppl=2.19, wps=13353.9, ups=12.29, wpb=1086.6, bsz=31.5, num_updates=9230, lr=1.65692e-05, gnorm=1.8, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1249
2022-01-09 21:10:49 | INFO | train_inner | epoch 094:     34 / 99 loss=0.697, ppl=1.62, wps=12965.5, ups=13.15, wpb=986, bsz=32, num_updates=9240, lr=1.65538e-05, gnorm=1.749, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1249
2022-01-09 21:10:50 | INFO | train_inner | epoch 094:     44 / 99 loss=0.729, ppl=1.66, wps=12775.2, ups=13.67, wpb=934.3, bsz=32, num_updates=9250, lr=1.65385e-05, gnorm=1.769, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1250
2022-01-09 21:10:50 | INFO | train_inner | epoch 094:     54 / 99 loss=0.712, ppl=1.64, wps=12453.1, ups=13.44, wpb=926.6, bsz=32, num_updates=9260, lr=1.65231e-05, gnorm=1.876, clip=100, loss_scale=64, train_wall=1, gb_free=20, wall=1251
2022-01-09 21:10:51 | INFO | train_inner | epoch 094:     64 / 99 loss=0.952, ppl=1.93, wps=14220.7, ups=12.17, wpb=1168.2, bsz=32, num_updates=9270, lr=1.65077e-05, gnorm=1.851, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1252
2022-01-09 21:10:52 | INFO | train_inner | epoch 094:     74 / 99 loss=0.856, ppl=1.81, wps=12951.3, ups=12.88, wpb=1005.8, bsz=32, num_updates=9280, lr=1.64923e-05, gnorm=1.975, clip=100, loss_scale=64, train_wall=1, gb_free=19.6, wall=1253
2022-01-09 21:10:53 | INFO | train_inner | epoch 094:     84 / 99 loss=0.574, ppl=1.49, wps=8041.1, ups=10.63, wpb=756.4, bsz=32, num_updates=9290, lr=1.64769e-05, gnorm=1.777, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1253
2022-01-09 21:10:54 | INFO | train_inner | epoch 094:     94 / 99 loss=0.668, ppl=1.59, wps=10536.4, ups=12.69, wpb=830.5, bsz=32, num_updates=9300, lr=1.64615e-05, gnorm=1.817, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1254
2022-01-09 21:10:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:10:55 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 1.215 | ppl 2.32 | wps 30138.9 | wpb 930.4 | bsz 31.3 | num_updates 9305 | best_loss 1.197
2022-01-09 21:10:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 9305 updates
2022-01-09 21:10:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:10:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:10:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 94 @ 9305 updates, score 1.215) (writing took 3.008797938004136 seconds)
2022-01-09 21:10:58 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-01-09 21:10:58 | INFO | train | epoch 094 | loss 0.837 | ppl 1.79 | wps 8271.3 | ups 8.29 | wpb 997.9 | bsz 31.9 | num_updates 9305 | lr 1.64538e-05 | gnorm 1.825 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.1 | wall 1259
2022-01-09 21:10:58 | INFO | fairseq.trainer | begin training epoch 95
2022-01-09 21:10:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:10:59 | INFO | train_inner | epoch 095:      5 / 99 loss=0.945, ppl=1.92, wps=2444.6, ups=2.05, wpb=1190.8, bsz=32, num_updates=9310, lr=1.64462e-05, gnorm=1.88, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1259
2022-01-09 21:10:59 | INFO | train_inner | epoch 095:     15 / 99 loss=1.221, ppl=2.33, wps=13365.3, ups=11.6, wpb=1151.8, bsz=31.5, num_updates=9320, lr=1.64308e-05, gnorm=1.854, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1260
2022-01-09 21:11:00 | INFO | train_inner | epoch 095:     25 / 99 loss=0.665, ppl=1.59, wps=10627.6, ups=13.21, wpb=804.3, bsz=32, num_updates=9330, lr=1.64154e-05, gnorm=1.754, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1261
2022-01-09 21:11:01 | INFO | train_inner | epoch 095:     35 / 99 loss=0.683, ppl=1.61, wps=10205.5, ups=12.34, wpb=826.8, bsz=32, num_updates=9340, lr=1.64e-05, gnorm=1.753, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1262
2022-01-09 21:11:02 | INFO | train_inner | epoch 095:     45 / 99 loss=0.672, ppl=1.59, wps=12387.2, ups=13.86, wpb=893.7, bsz=32, num_updates=9350, lr=1.63846e-05, gnorm=1.856, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1262
2022-01-09 21:11:03 | INFO | train_inner | epoch 095:     55 / 99 loss=0.737, ppl=1.67, wps=13138.7, ups=13.31, wpb=987.1, bsz=32, num_updates=9360, lr=1.63692e-05, gnorm=1.75, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=1263
2022-01-09 21:11:03 | INFO | train_inner | epoch 095:     65 / 99 loss=1.027, ppl=2.04, wps=13766.4, ups=13.21, wpb=1042.4, bsz=32, num_updates=9370, lr=1.63538e-05, gnorm=2.012, clip=100, loss_scale=64, train_wall=1, gb_free=18.8, wall=1264
2022-01-09 21:11:04 | INFO | train_inner | epoch 095:     75 / 99 loss=0.898, ppl=1.86, wps=11434.1, ups=10.75, wpb=1063.4, bsz=32, num_updates=9380, lr=1.63385e-05, gnorm=1.814, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=1265
2022-01-09 21:11:05 | INFO | train_inner | epoch 095:     85 / 99 loss=0.78, ppl=1.72, wps=13089.5, ups=11.38, wpb=1150.7, bsz=32, num_updates=9390, lr=1.63231e-05, gnorm=1.925, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1266
2022-01-09 21:11:06 | INFO | train_inner | epoch 095:     95 / 99 loss=0.61, ppl=1.53, wps=10565, ups=12.02, wpb=879.2, bsz=32, num_updates=9400, lr=1.63077e-05, gnorm=1.766, clip=100, loss_scale=64, train_wall=1, gb_free=20.2, wall=1266
2022-01-09 21:11:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:11:07 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 1.202 | ppl 2.3 | wps 31900.1 | wpb 930.4 | bsz 31.3 | num_updates 9404 | best_loss 1.197
2022-01-09 21:11:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 9404 updates
2022-01-09 21:11:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:11:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:11:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 95 @ 9404 updates, score 1.202) (writing took 2.8081874300260097 seconds)
2022-01-09 21:11:10 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-01-09 21:11:10 | INFO | train | epoch 095 | loss 0.839 | ppl 1.79 | wps 8330.5 | ups 8.35 | wpb 997.9 | bsz 31.9 | num_updates 9404 | lr 1.63015e-05 | gnorm 1.841 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 19.3 | wall 1271
2022-01-09 21:11:10 | INFO | fairseq.trainer | begin training epoch 96
2022-01-09 21:11:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:11:11 | INFO | train_inner | epoch 096:      6 / 99 loss=0.859, ppl=1.81, wps=2276.9, ups=2.13, wpb=1067, bsz=32, num_updates=9410, lr=1.62923e-05, gnorm=1.852, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1271
2022-01-09 21:11:11 | INFO | train_inner | epoch 096:     16 / 99 loss=0.898, ppl=1.86, wps=13914.5, ups=12.67, wpb=1098.5, bsz=32, num_updates=9420, lr=1.62769e-05, gnorm=1.811, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1272
2022-01-09 21:11:12 | INFO | train_inner | epoch 096:     26 / 99 loss=0.673, ppl=1.59, wps=12481.3, ups=12.19, wpb=1023.8, bsz=32, num_updates=9430, lr=1.62615e-05, gnorm=1.751, clip=100, loss_scale=64, train_wall=1, gb_free=20.6, wall=1273
2022-01-09 21:11:13 | INFO | train_inner | epoch 096:     36 / 99 loss=1.09, ppl=2.13, wps=8193.8, ups=8.87, wpb=923.9, bsz=31.5, num_updates=9440, lr=1.62462e-05, gnorm=1.831, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1274
2022-01-09 21:11:14 | INFO | train_inner | epoch 096:     46 / 99 loss=0.643, ppl=1.56, wps=9630, ups=12.1, wpb=795.8, bsz=32, num_updates=9450, lr=1.62308e-05, gnorm=1.816, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1275
2022-01-09 21:11:15 | INFO | train_inner | epoch 096:     56 / 99 loss=0.789, ppl=1.73, wps=12230.5, ups=13.01, wpb=939.8, bsz=32, num_updates=9460, lr=1.62154e-05, gnorm=1.991, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1275
2022-01-09 21:11:16 | INFO | train_inner | epoch 096:     66 / 99 loss=0.615, ppl=1.53, wps=11802.9, ups=13.91, wpb=848.4, bsz=32, num_updates=9470, lr=1.62e-05, gnorm=1.72, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1276
2022-01-09 21:11:16 | INFO | train_inner | epoch 096:     76 / 99 loss=0.945, ppl=1.93, wps=14653.1, ups=12.53, wpb=1169.1, bsz=32, num_updates=9480, lr=1.61846e-05, gnorm=2.041, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1277
2022-01-09 21:11:17 | INFO | train_inner | epoch 096:     86 / 99 loss=0.993, ppl=1.99, wps=13297, ups=11.83, wpb=1123.6, bsz=32, num_updates=9490, lr=1.61692e-05, gnorm=1.929, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1278
2022-01-09 21:11:18 | INFO | train_inner | epoch 096:     96 / 99 loss=0.827, ppl=1.77, wps=14334.4, ups=13.03, wpb=1100.5, bsz=32, num_updates=9500, lr=1.61538e-05, gnorm=1.749, clip=100, loss_scale=64, train_wall=1, gb_free=20.3, wall=1279
2022-01-09 21:11:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:11:19 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 1.223 | ppl 2.33 | wps 30983.6 | wpb 930.4 | bsz 31.3 | num_updates 9503 | best_loss 1.197
2022-01-09 21:11:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 9503 updates
2022-01-09 21:11:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:11:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 96 @ 9503 updates, score 1.223) (writing took 3.5611014959868044 seconds)
2022-01-09 21:11:23 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-01-09 21:11:23 | INFO | train | epoch 096 | loss 0.833 | ppl 1.78 | wps 7668.3 | ups 7.68 | wpb 997.9 | bsz 31.9 | num_updates 9503 | lr 1.61492e-05 | gnorm 1.846 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.4 | wall 1283
2022-01-09 21:11:23 | INFO | fairseq.trainer | begin training epoch 97
2022-01-09 21:11:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:11:24 | INFO | train_inner | epoch 097:      7 / 99 loss=0.544, ppl=1.46, wps=1464.2, ups=1.85, wpb=792.3, bsz=32, num_updates=9510, lr=1.61385e-05, gnorm=1.717, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1284
2022-01-09 21:11:24 | INFO | train_inner | epoch 097:     17 / 99 loss=0.804, ppl=1.75, wps=14779.3, ups=12.66, wpb=1167.7, bsz=32, num_updates=9520, lr=1.61231e-05, gnorm=1.743, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=1285
2022-01-09 21:11:25 | INFO | train_inner | epoch 097:     27 / 99 loss=1.05, ppl=2.07, wps=14168.2, ups=11.64, wpb=1217.1, bsz=32, num_updates=9530, lr=1.61077e-05, gnorm=1.973, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1286
2022-01-09 21:11:26 | INFO | train_inner | epoch 097:     37 / 99 loss=0.666, ppl=1.59, wps=11880.8, ups=13.03, wpb=911.6, bsz=32, num_updates=9540, lr=1.60923e-05, gnorm=1.806, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1286
2022-01-09 21:11:27 | INFO | train_inner | epoch 097:     47 / 99 loss=0.661, ppl=1.58, wps=9991.1, ups=10.53, wpb=949, bsz=32, num_updates=9550, lr=1.60769e-05, gnorm=1.715, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1287
2022-01-09 21:11:28 | INFO | train_inner | epoch 097:     57 / 99 loss=0.599, ppl=1.52, wps=8382, ups=12.23, wpb=685.2, bsz=32, num_updates=9560, lr=1.60615e-05, gnorm=1.849, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1288
2022-01-09 21:11:29 | INFO | train_inner | epoch 097:     67 / 99 loss=0.818, ppl=1.76, wps=11319.2, ups=11.66, wpb=970.8, bsz=32, num_updates=9570, lr=1.60462e-05, gnorm=1.802, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1289
2022-01-09 21:11:29 | INFO | train_inner | epoch 097:     77 / 99 loss=0.985, ppl=1.98, wps=14594.6, ups=11.49, wpb=1270.6, bsz=32, num_updates=9580, lr=1.60308e-05, gnorm=1.866, clip=100, loss_scale=64, train_wall=1, gb_free=19.8, wall=1290
2022-01-09 21:11:30 | INFO | train_inner | epoch 097:     87 / 99 loss=0.658, ppl=1.58, wps=11625.4, ups=12.26, wpb=948, bsz=32, num_updates=9590, lr=1.60154e-05, gnorm=1.802, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1291
2022-01-09 21:11:31 | INFO | train_inner | epoch 097:     97 / 99 loss=1.113, ppl=2.16, wps=10299, ups=9.88, wpb=1042.5, bsz=31.5, num_updates=9600, lr=1.6e-05, gnorm=1.877, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1292
2022-01-09 21:11:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:11:32 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 1.198 | ppl 2.29 | wps 30385.4 | wpb 930.4 | bsz 31.3 | num_updates 9602 | best_loss 1.197
2022-01-09 21:11:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 9602 updates
2022-01-09 21:11:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:11:36 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:11:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 97 @ 9602 updates, score 1.198) (writing took 3.2527092449599877 seconds)
2022-01-09 21:11:36 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-01-09 21:11:36 | INFO | train | epoch 097 | loss 0.82 | ppl 1.77 | wps 7706.5 | ups 7.72 | wpb 997.9 | bsz 31.9 | num_updates 9602 | lr 1.59969e-05 | gnorm 1.817 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 1296
2022-01-09 21:11:36 | INFO | fairseq.trainer | begin training epoch 98
2022-01-09 21:11:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:11:37 | INFO | train_inner | epoch 098:      8 / 99 loss=0.89, ppl=1.85, wps=1880.9, ups=1.92, wpb=979.5, bsz=32, num_updates=9610, lr=1.59846e-05, gnorm=1.879, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1297
2022-01-09 21:11:37 | INFO | train_inner | epoch 098:     18 / 99 loss=0.701, ppl=1.63, wps=12320.2, ups=12.72, wpb=968.4, bsz=32, num_updates=9620, lr=1.59692e-05, gnorm=1.839, clip=100, loss_scale=64, train_wall=1, gb_free=20.5, wall=1298
2022-01-09 21:11:38 | INFO | train_inner | epoch 098:     28 / 99 loss=0.841, ppl=1.79, wps=11201.9, ups=11.51, wpb=973.1, bsz=32, num_updates=9630, lr=1.59538e-05, gnorm=1.711, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1299
2022-01-09 21:11:39 | INFO | train_inner | epoch 098:     38 / 99 loss=0.7, ppl=1.62, wps=11948.4, ups=13.87, wpb=861.7, bsz=32, num_updates=9640, lr=1.59385e-05, gnorm=1.743, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1299
2022-01-09 21:11:40 | INFO | train_inner | epoch 098:     48 / 99 loss=0.724, ppl=1.65, wps=13155.7, ups=12.86, wpb=1023.1, bsz=32, num_updates=9650, lr=1.59231e-05, gnorm=1.721, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1300
2022-01-09 21:11:41 | INFO | train_inner | epoch 098:     58 / 99 loss=1.159, ppl=2.23, wps=11368, ups=9.08, wpb=1251.4, bsz=31.5, num_updates=9660, lr=1.59077e-05, gnorm=1.895, clip=100, loss_scale=64, train_wall=1, gb_free=19.3, wall=1301
2022-01-09 21:11:42 | INFO | train_inner | epoch 098:     68 / 99 loss=0.774, ppl=1.71, wps=11867.7, ups=10.54, wpb=1125.7, bsz=32, num_updates=9670, lr=1.58923e-05, gnorm=1.814, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1302
2022-01-09 21:11:43 | INFO | train_inner | epoch 098:     78 / 99 loss=0.781, ppl=1.72, wps=12970.2, ups=11.36, wpb=1141.3, bsz=32, num_updates=9680, lr=1.58769e-05, gnorm=1.854, clip=100, loss_scale=64, train_wall=1, gb_free=20.4, wall=1303
2022-01-09 21:11:43 | INFO | train_inner | epoch 098:     88 / 99 loss=0.769, ppl=1.7, wps=9939.8, ups=12.51, wpb=794.4, bsz=32, num_updates=9690, lr=1.58615e-05, gnorm=1.917, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1304
2022-01-09 21:11:44 | INFO | train_inner | epoch 098:     98 / 99 loss=0.579, ppl=1.49, wps=8662.4, ups=10.06, wpb=861.3, bsz=32, num_updates=9700, lr=1.58462e-05, gnorm=1.76, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1305
2022-01-09 21:11:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:11:46 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 1.199 | ppl 2.3 | wps 27675.7 | wpb 930.4 | bsz 31.3 | num_updates 9701 | best_loss 1.197
2022-01-09 21:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 9701 updates
2022-01-09 21:11:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:11:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:11:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 98 @ 9701 updates, score 1.199) (writing took 3.008589210920036 seconds)
2022-01-09 21:11:49 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-01-09 21:11:49 | INFO | train | epoch 098 | loss 0.805 | ppl 1.75 | wps 7627.4 | ups 7.64 | wpb 997.9 | bsz 31.9 | num_updates 9701 | lr 1.58446e-05 | gnorm 1.812 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.8 | wall 1309
2022-01-09 21:11:49 | INFO | fairseq.trainer | begin training epoch 99
2022-01-09 21:11:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:11:49 | INFO | train_inner | epoch 099:      9 / 99 loss=0.795, ppl=1.74, wps=1847.3, ups=1.97, wpb=936.7, bsz=32, num_updates=9710, lr=1.58308e-05, gnorm=1.871, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1310
2022-01-09 21:11:50 | INFO | train_inner | epoch 099:     19 / 99 loss=0.652, ppl=1.57, wps=11026.2, ups=12.52, wpb=880.5, bsz=32, num_updates=9720, lr=1.58154e-05, gnorm=1.806, clip=100, loss_scale=64, train_wall=1, gb_free=20.1, wall=1311
2022-01-09 21:11:51 | INFO | train_inner | epoch 099:     29 / 99 loss=0.925, ppl=1.9, wps=14802.4, ups=12.38, wpb=1196.1, bsz=32, num_updates=9730, lr=1.58e-05, gnorm=1.925, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1312
2022-01-09 21:11:52 | INFO | train_inner | epoch 099:     39 / 99 loss=0.733, ppl=1.66, wps=12900.8, ups=12.88, wpb=1001.5, bsz=32, num_updates=9740, lr=1.57846e-05, gnorm=1.691, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1312
2022-01-09 21:11:53 | INFO | train_inner | epoch 099:     49 / 99 loss=0.822, ppl=1.77, wps=14001.4, ups=12.69, wpb=1103.1, bsz=32, num_updates=9750, lr=1.57692e-05, gnorm=2.014, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1313
2022-01-09 21:11:53 | INFO | train_inner | epoch 099:     59 / 99 loss=0.572, ppl=1.49, wps=10952.7, ups=13.6, wpb=805.4, bsz=32, num_updates=9760, lr=1.57538e-05, gnorm=1.661, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1314
2022-01-09 21:11:54 | INFO | train_inner | epoch 099:     69 / 99 loss=0.943, ppl=1.92, wps=15498.7, ups=12.82, wpb=1209, bsz=32, num_updates=9770, lr=1.57385e-05, gnorm=1.898, clip=100, loss_scale=64, train_wall=1, gb_free=19.9, wall=1315
2022-01-09 21:11:55 | INFO | train_inner | epoch 099:     79 / 99 loss=0.688, ppl=1.61, wps=11182.8, ups=12.51, wpb=894.1, bsz=32, num_updates=9780, lr=1.57231e-05, gnorm=1.803, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1316
2022-01-09 21:11:56 | INFO | train_inner | epoch 099:     89 / 99 loss=1.039, ppl=2.05, wps=11224.9, ups=11.37, wpb=986.9, bsz=31.5, num_updates=9790, lr=1.57077e-05, gnorm=1.762, clip=100, loss_scale=64, train_wall=1, gb_free=20.8, wall=1316
2022-01-09 21:11:57 | INFO | train_inner | epoch 099:     99 / 99 loss=0.638, ppl=1.56, wps=10520.7, ups=11.16, wpb=942.3, bsz=32, num_updates=9800, lr=1.56923e-05, gnorm=1.798, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1317
2022-01-09 21:11:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-09 21:11:58 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 1.201 | ppl 2.3 | wps 27898.6 | wpb 930.4 | bsz 31.3 | num_updates 9800 | best_loss 1.197
2022-01-09 21:11:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 9800 updates
2022-01-09 21:11:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:12:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-09 21:12:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 99 @ 9800 updates, score 1.201) (writing took 3.3243547059828416 seconds)
2022-01-09 21:12:01 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-01-09 21:12:01 | INFO | train | epoch 099 | loss 0.797 | ppl 1.74 | wps 7883.6 | ups 7.9 | wpb 997.9 | bsz 31.9 | num_updates 9800 | lr 1.56923e-05 | gnorm 1.823 | clip 100 | loss_scale 64 | train_wall 8 | gb_free 20.7 | wall 1322
2022-01-09 21:12:01 | INFO | fairseq.trainer | begin training epoch 100
2022-01-09 21:12:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-09 21:12:02 | INFO | train_inner | epoch 100:     10 / 99 loss=0.659, ppl=1.58, wps=1694.4, ups=1.88, wpb=899.2, bsz=32, num_updates=9810, lr=1.56769e-05, gnorm=1.684, clip=100, loss_scale=64, train_wall=1, gb_free=20.7, wall=1323
