2022-01-10 00:20:35 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 10, 'log_format': None, 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 8192, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 5000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 12288, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.abst', 'restore_file': 'bartabst/checkpoints/bart.mlm/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 10, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.1, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=3.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='aspect_base_denoising', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.1, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=3.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 20000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-10 00:20:35 | INFO | bartabst.tasks.aspect_base_denoising | dictionary: 51200 types
2022-01-10 00:20:37 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-01-10 00:20:37 | INFO | fairseq_cli.train | task: AspectBaseDenoisingTask
2022-01-10 00:20:37 | INFO | fairseq_cli.train | model: BARTMLModel
2022-01-10 00:20:37 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-01-10 00:20:37 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-01-10 00:20:37 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-10 00:20:38 | INFO | bartabst.data.data_utils | loaded 907 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-01-10 00:20:38 | INFO | bartabst.tasks.aspect_base_denoising | Split: valid, Loaded 907 samples of denoising_dataset
2022-01-10 00:20:41 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-01-10 00:20:41 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-10 00:20:41 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-01-10 00:20:41 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-10 00:20:41 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-01-10 00:20:41 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-10 00:20:41 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-10 00:20:41 | INFO | fairseq_cli.train | max tokens per device = 8192 and max sentences per device = 32
2022-01-10 00:20:41 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.mlm/checkpoint_best.pt
2022-01-10 00:20:43 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.mlm/checkpoint_best.pt (epoch 153 @ 0 updates)
2022-01-10 00:20:43 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-10 00:20:44 | INFO | bartabst.data.data_utils | loaded 3,163 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-01-10 00:20:44 | INFO | bartabst.tasks.aspect_base_denoising | Split: train, Loaded 3163 samples of denoising_dataset
2022-01-10 00:20:44 | INFO | fairseq.trainer | begin training epoch 1
2022-01-10 00:20:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:20:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-01-10 00:20:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-01-10 00:20:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-01-10 00:20:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-01-10 00:20:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-01-10 00:20:46 | INFO | train_inner | epoch 001:     15 / 99 loss=6.363, ppl=82.32, wps=5036.2, ups=9.55, wpb=495, bsz=32, num_updates=10, lr=6e-07, gnorm=24.894, clip=100, loss_scale=4, train_wall=1, gb_free=20.4, wall=5
2022-01-10 00:20:47 | INFO | train_inner | epoch 001:     25 / 99 loss=6.247, ppl=75.94, wps=8311.6, ups=12.62, wpb=658.7, bsz=32, num_updates=20, lr=1.2e-06, gnorm=25.473, clip=100, loss_scale=4, train_wall=1, gb_free=20.8, wall=5
2022-01-10 00:20:47 | INFO | train_inner | epoch 001:     35 / 99 loss=5.72, ppl=52.72, wps=8817.5, ups=13.34, wpb=660.8, bsz=32, num_updates=30, lr=1.8e-06, gnorm=25.294, clip=100, loss_scale=4, train_wall=1, gb_free=20.8, wall=6
2022-01-10 00:20:48 | INFO | train_inner | epoch 001:     45 / 99 loss=4.956, ppl=31.04, wps=8554.3, ups=14.07, wpb=607.8, bsz=32, num_updates=40, lr=2.4e-06, gnorm=24.14, clip=100, loss_scale=4, train_wall=1, gb_free=20.7, wall=7
2022-01-10 00:20:49 | INFO | train_inner | epoch 001:     55 / 99 loss=4.019, ppl=16.21, wps=8793.9, ups=12.48, wpb=704.5, bsz=32, num_updates=50, lr=3e-06, gnorm=18.082, clip=100, loss_scale=4, train_wall=1, gb_free=20.8, wall=8
2022-01-10 00:20:50 | INFO | train_inner | epoch 001:     65 / 99 loss=3.644, ppl=12.51, wps=6404.8, ups=11.05, wpb=579.4, bsz=32, num_updates=60, lr=3.6e-06, gnorm=15.56, clip=100, loss_scale=4, train_wall=1, gb_free=20.8, wall=9
2022-01-10 00:20:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-01-10 00:20:51 | INFO | train_inner | epoch 001:     76 / 99 loss=3.279, ppl=9.71, wps=8191.2, ups=11.25, wpb=727.8, bsz=31.5, num_updates=70, lr=4.2e-06, gnorm=12.927, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=10
2022-01-10 00:20:51 | INFO | train_inner | epoch 001:     86 / 99 loss=2.906, ppl=7.5, wps=8233.2, ups=12.23, wpb=673.1, bsz=32, num_updates=80, lr=4.8e-06, gnorm=16.365, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=10
2022-01-10 00:20:52 | INFO | train_inner | epoch 001:     96 / 99 loss=2.849, ppl=7.21, wps=7427.8, ups=10.68, wpb=695.2, bsz=32, num_updates=90, lr=5.4e-06, gnorm=11.636, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=11
2022-01-10 00:20:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:20:54 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 2.095 | ppl 4.27 | wps 21571.3 | wpb 606.6 | bsz 31.3 | num_updates 93
2022-01-10 00:20:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 93 updates
2022-01-10 00:20:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:20:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:20:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 1 @ 93 updates, score 2.095) (writing took 4.551820585038513 seconds)
2022-01-10 00:20:58 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-10 00:20:58 | INFO | train | epoch 001 | loss 4.302 | ppl 19.73 | wps 4490 | ups 6.9 | wpb 645.6 | bsz 31.9 | num_updates 93 | lr 5.58e-06 | gnorm 19.089 | clip 100 | loss_scale 2 | train_wall 8 | gb_free 20.8 | wall 17
2022-01-10 00:20:58 | INFO | fairseq.trainer | begin training epoch 2
2022-01-10 00:20:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:20:59 | INFO | train_inner | epoch 002:      7 / 99 loss=2.501, ppl=5.66, wps=1000.6, ups=1.59, wpb=630.5, bsz=32, num_updates=100, lr=6e-06, gnorm=23.639, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=18
2022-01-10 00:20:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2022-01-10 00:20:59 | INFO | train_inner | epoch 002:     18 / 99 loss=2.491, ppl=5.62, wps=9363.3, ups=12.34, wpb=758.5, bsz=31.5, num_updates=110, lr=6.6e-06, gnorm=10.875, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=18
2022-01-10 00:21:00 | INFO | train_inner | epoch 002:     28 / 99 loss=2.211, ppl=4.63, wps=11023.3, ups=14.13, wpb=780.3, bsz=32, num_updates=120, lr=7.2e-06, gnorm=8.147, clip=100, loss_scale=1, train_wall=1, gb_free=20.3, wall=19
2022-01-10 00:21:01 | INFO | train_inner | epoch 002:     38 / 99 loss=2.139, ppl=4.4, wps=10772.2, ups=13.77, wpb=782.5, bsz=32, num_updates=130, lr=7.8e-06, gnorm=10.243, clip=100, loss_scale=1, train_wall=1, gb_free=19.6, wall=20
2022-01-10 00:21:02 | INFO | train_inner | epoch 002:     48 / 99 loss=2.554, ppl=5.87, wps=6482.5, ups=13.42, wpb=483.1, bsz=32, num_updates=140, lr=8.4e-06, gnorm=29.556, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=21
2022-01-10 00:21:02 | INFO | train_inner | epoch 002:     58 / 99 loss=2.273, ppl=4.83, wps=6420.3, ups=12.91, wpb=497.2, bsz=32, num_updates=150, lr=9e-06, gnorm=14.705, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=21
2022-01-10 00:21:03 | INFO | train_inner | epoch 002:     68 / 99 loss=1.939, ppl=3.84, wps=8637.7, ups=13.15, wpb=656.7, bsz=32, num_updates=160, lr=9.6e-06, gnorm=13.541, clip=100, loss_scale=1, train_wall=1, gb_free=20.2, wall=22
2022-01-10 00:21:04 | INFO | train_inner | epoch 002:     78 / 99 loss=1.99, ppl=3.97, wps=9138.9, ups=13.06, wpb=699.7, bsz=32, num_updates=170, lr=1.02e-05, gnorm=8.22, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=23
2022-01-10 00:21:05 | INFO | train_inner | epoch 002:     88 / 99 loss=2.043, ppl=4.12, wps=7173.6, ups=11.81, wpb=607.2, bsz=32, num_updates=180, lr=1.08e-05, gnorm=8.75, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=24
2022-01-10 00:21:06 | INFO | train_inner | epoch 002:     98 / 99 loss=1.882, ppl=3.69, wps=7429.4, ups=11.55, wpb=643.2, bsz=32, num_updates=190, lr=1.14e-05, gnorm=7.856, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=25
2022-01-10 00:21:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:21:07 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.641 | ppl 3.12 | wps 21473 | wpb 606.6 | bsz 31.3 | num_updates 191 | best_loss 1.641
2022-01-10 00:21:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 191 updates
2022-01-10 00:21:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:21:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:21:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 2 @ 191 updates, score 1.641) (writing took 4.202097202069126 seconds)
2022-01-10 00:21:11 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-10 00:21:11 | INFO | train | epoch 002 | loss 2.186 | ppl 4.55 | wps 4990.5 | ups 7.67 | wpb 650.5 | bsz 31.9 | num_updates 191 | lr 1.146e-05 | gnorm 13.957 | clip 100 | loss_scale 1 | train_wall 7 | gb_free 20.8 | wall 30
2022-01-10 00:21:11 | INFO | fairseq.trainer | begin training epoch 3
2022-01-10 00:21:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:21:12 | INFO | train_inner | epoch 003:      9 / 99 loss=1.998, ppl=4, wps=980.7, ups=1.57, wpb=623.9, bsz=31.5, num_updates=200, lr=1.2e-05, gnorm=13.587, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=31
2022-01-10 00:21:13 | INFO | train_inner | epoch 003:     19 / 99 loss=1.845, ppl=3.59, wps=8973.9, ups=15.1, wpb=594.4, bsz=32, num_updates=210, lr=1.26e-05, gnorm=8.062, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=32
2022-01-10 00:21:13 | INFO | train_inner | epoch 003:     29 / 99 loss=2.05, ppl=4.14, wps=8127.8, ups=15.83, wpb=513.6, bsz=32, num_updates=220, lr=1.32e-05, gnorm=8.147, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=32
2022-01-10 00:21:14 | INFO | train_inner | epoch 003:     39 / 99 loss=1.805, ppl=3.49, wps=10986.1, ups=15.41, wpb=712.7, bsz=32, num_updates=230, lr=1.38e-05, gnorm=7.358, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=33
2022-01-10 00:21:15 | INFO | train_inner | epoch 003:     49 / 99 loss=1.665, ppl=3.17, wps=10611.7, ups=15.53, wpb=683.4, bsz=32, num_updates=240, lr=1.44e-05, gnorm=6.294, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=34
2022-01-10 00:21:15 | INFO | train_inner | epoch 003:     59 / 99 loss=1.659, ppl=3.16, wps=10536.9, ups=15.62, wpb=674.5, bsz=32, num_updates=250, lr=1.5e-05, gnorm=6.85, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=34
2022-01-10 00:21:16 | INFO | train_inner | epoch 003:     69 / 99 loss=1.713, ppl=3.28, wps=9926.9, ups=15.6, wpb=636.3, bsz=32, num_updates=260, lr=1.56e-05, gnorm=14.022, clip=100, loss_scale=1, train_wall=1, gb_free=20.1, wall=35
2022-01-10 00:21:17 | INFO | train_inner | epoch 003:     79 / 99 loss=1.401, ppl=2.64, wps=13449.7, ups=14.78, wpb=910, bsz=32, num_updates=270, lr=1.62e-05, gnorm=5.855, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=36
2022-01-10 00:21:17 | INFO | train_inner | epoch 003:     89 / 99 loss=1.835, ppl=3.57, wps=7903.3, ups=14.38, wpb=549.5, bsz=32, num_updates=280, lr=1.68e-05, gnorm=7.854, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=36
2022-01-10 00:21:18 | INFO | train_inner | epoch 003:     99 / 99 loss=1.713, ppl=3.28, wps=7425.6, ups=12.54, wpb=592, bsz=32, num_updates=290, lr=1.74e-05, gnorm=6.955, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=37
2022-01-10 00:21:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:21:19 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.419 | ppl 2.67 | wps 20737.8 | wpb 606.6 | bsz 31.3 | num_updates 290 | best_loss 1.419
2022-01-10 00:21:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 290 updates
2022-01-10 00:21:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:21:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:21:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 3 @ 290 updates, score 1.419) (writing took 4.043970728991553 seconds)
2022-01-10 00:21:23 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-10 00:21:23 | INFO | train | epoch 003 | loss 1.74 | ppl 3.34 | wps 5471.1 | ups 8.4 | wpb 651.7 | bsz 31.9 | num_updates 290 | lr 1.74e-05 | gnorm 8.14 | clip 100 | loss_scale 1 | train_wall 7 | gb_free 20.7 | wall 42
2022-01-10 00:21:23 | INFO | fairseq.trainer | begin training epoch 4
2022-01-10 00:21:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:21:24 | INFO | train_inner | epoch 004:     10 / 99 loss=1.75, ppl=3.36, wps=995.7, ups=1.74, wpb=571, bsz=32, num_updates=300, lr=1.8e-05, gnorm=7.306, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=43
2022-01-10 00:21:25 | INFO | train_inner | epoch 004:     20 / 99 loss=1.671, ppl=3.18, wps=10552.1, ups=15.04, wpb=701.6, bsz=32, num_updates=310, lr=1.86e-05, gnorm=9.382, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=43
2022-01-10 00:21:25 | INFO | train_inner | epoch 004:     30 / 99 loss=1.526, ppl=2.88, wps=9535.1, ups=15.5, wpb=615.1, bsz=32, num_updates=320, lr=1.92e-05, gnorm=6.837, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=44
2022-01-10 00:21:26 | INFO | train_inner | epoch 004:     40 / 99 loss=1.677, ppl=3.2, wps=8960.8, ups=15.66, wpb=572.3, bsz=32, num_updates=330, lr=1.98e-05, gnorm=6.527, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=45
2022-01-10 00:21:26 | INFO | train_inner | epoch 004:     50 / 99 loss=1.567, ppl=2.96, wps=9550.3, ups=15.52, wpb=615.4, bsz=32, num_updates=340, lr=2.04e-05, gnorm=10.503, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=45
2022-01-10 00:21:27 | INFO | train_inner | epoch 004:     60 / 99 loss=1.611, ppl=3.05, wps=8373.9, ups=15.79, wpb=530.3, bsz=32, num_updates=350, lr=2.1e-05, gnorm=11.25, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=46
2022-01-10 00:21:28 | INFO | train_inner | epoch 004:     70 / 99 loss=1.333, ppl=2.52, wps=13149, ups=14.64, wpb=898.2, bsz=31.5, num_updates=360, lr=2.16e-05, gnorm=5.065, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=47
2022-01-10 00:21:28 | INFO | train_inner | epoch 004:     80 / 99 loss=1.344, ppl=2.54, wps=12609.9, ups=15.13, wpb=833.5, bsz=32, num_updates=370, lr=2.22e-05, gnorm=6.418, clip=100, loss_scale=1, train_wall=1, gb_free=20.4, wall=47
2022-01-10 00:21:29 | INFO | train_inner | epoch 004:     90 / 99 loss=1.48, ppl=2.79, wps=8169.5, ups=13.88, wpb=588.7, bsz=32, num_updates=380, lr=2.28e-05, gnorm=5.829, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=48
2022-01-10 00:21:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:21:31 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.306 | ppl 2.47 | wps 21167.2 | wpb 606.6 | bsz 31.3 | num_updates 389 | best_loss 1.306
2022-01-10 00:21:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 389 updates
2022-01-10 00:21:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:21:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 4 @ 389 updates, score 1.306) (writing took 4.222870173980482 seconds)
2022-01-10 00:21:35 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-10 00:21:36 | INFO | train | epoch 004 | loss 1.54 | ppl 2.91 | wps 5332.3 | ups 8.18 | wpb 651.7 | bsz 31.9 | num_updates 389 | lr 2.334e-05 | gnorm 7.509 | clip 100 | loss_scale 1 | train_wall 7 | gb_free 20.8 | wall 54
2022-01-10 00:21:36 | INFO | fairseq.trainer | begin training epoch 5
2022-01-10 00:21:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:21:36 | INFO | train_inner | epoch 005:      1 / 99 loss=1.616, ppl=3.07, wps=929.3, ups=1.53, wpb=607.5, bsz=32, num_updates=390, lr=2.34e-05, gnorm=5.649, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=55
2022-01-10 00:21:36 | INFO | train_inner | epoch 005:     11 / 99 loss=1.33, ppl=2.51, wps=10960.6, ups=14.96, wpb=732.6, bsz=32, num_updates=400, lr=2.4e-05, gnorm=14.21, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=55
2022-01-10 00:21:37 | INFO | train_inner | epoch 005:     21 / 99 loss=1.345, ppl=2.54, wps=9336.8, ups=15.03, wpb=621.2, bsz=32, num_updates=410, lr=2.46e-05, gnorm=9.285, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=56
2022-01-10 00:21:38 | INFO | train_inner | epoch 005:     31 / 99 loss=1.386, ppl=2.61, wps=11750.6, ups=14.78, wpb=794.9, bsz=32, num_updates=420, lr=2.52e-05, gnorm=11.29, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=57
2022-01-10 00:21:38 | INFO | train_inner | epoch 005:     41 / 99 loss=1.61, ppl=3.05, wps=8779.1, ups=15.23, wpb=576.4, bsz=32, num_updates=430, lr=2.58e-05, gnorm=6.868, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=57
2022-01-10 00:21:39 | INFO | train_inner | epoch 005:     51 / 99 loss=1.601, ppl=3.03, wps=7603.8, ups=15.77, wpb=482.2, bsz=32, num_updates=440, lr=2.64e-05, gnorm=6.106, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=58
2022-01-10 00:21:40 | INFO | train_inner | epoch 005:     61 / 99 loss=1.321, ppl=2.5, wps=9849.3, ups=15.25, wpb=645.9, bsz=32, num_updates=450, lr=2.7e-05, gnorm=5.729, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=59
2022-01-10 00:21:40 | INFO | train_inner | epoch 005:     71 / 99 loss=1.617, ppl=3.07, wps=7765.7, ups=15.4, wpb=504.1, bsz=32, num_updates=460, lr=2.76e-05, gnorm=14.268, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=59
2022-01-10 00:21:41 | INFO | train_inner | epoch 005:     81 / 99 loss=1.39, ppl=2.62, wps=8682.8, ups=14.17, wpb=612.6, bsz=32, num_updates=470, lr=2.82e-05, gnorm=8.916, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=60
2022-01-10 00:21:42 | INFO | train_inner | epoch 005:     91 / 99 loss=1.167, ppl=2.25, wps=10851.4, ups=11.17, wpb=971.8, bsz=31.5, num_updates=480, lr=2.88e-05, gnorm=14.988, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=61
2022-01-10 00:21:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:21:44 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.246 | ppl 2.37 | wps 20960.8 | wpb 606.6 | bsz 31.3 | num_updates 488 | best_loss 1.246
2022-01-10 00:21:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 488 updates
2022-01-10 00:21:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:21:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:21:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 5 @ 488 updates, score 1.246) (writing took 4.084399180021137 seconds)
2022-01-10 00:21:48 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-10 00:21:48 | INFO | train | epoch 005 | loss 1.397 | ppl 2.63 | wps 5291.3 | ups 8.12 | wpb 651.7 | bsz 31.9 | num_updates 488 | lr 2.928e-05 | gnorm 10.212 | clip 100 | loss_scale 1 | train_wall 7 | gb_free 20.8 | wall 67
2022-01-10 00:21:48 | INFO | fairseq.trainer | begin training epoch 6
2022-01-10 00:21:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:21:48 | INFO | train_inner | epoch 006:      2 / 99 loss=1.401, ppl=2.64, wps=950.3, ups=1.65, wpb=576.3, bsz=32, num_updates=490, lr=2.94e-05, gnorm=10.087, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=67
2022-01-10 00:21:49 | INFO | train_inner | epoch 006:     12 / 99 loss=1.306, ppl=2.47, wps=9376.4, ups=15.12, wpb=620.3, bsz=32, num_updates=500, lr=3e-05, gnorm=8.887, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=68
2022-01-10 00:21:49 | INFO | train_inner | epoch 006:     22 / 99 loss=1.323, ppl=2.5, wps=10098.7, ups=14.99, wpb=673.9, bsz=32, num_updates=510, lr=2.99846e-05, gnorm=6.232, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=68
2022-01-10 00:21:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2022-01-10 00:21:50 | INFO | train_inner | epoch 006:     33 / 99 loss=1.214, ppl=2.32, wps=10434.1, ups=13.66, wpb=763.9, bsz=32, num_updates=520, lr=2.99692e-05, gnorm=5.144, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=69
2022-01-10 00:21:51 | INFO | train_inner | epoch 006:     43 / 99 loss=1.287, ppl=2.44, wps=9910, ups=15.32, wpb=646.9, bsz=32, num_updates=530, lr=2.99538e-05, gnorm=6.006, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=70
2022-01-10 00:21:51 | INFO | train_inner | epoch 006:     53 / 99 loss=1.33, ppl=2.51, wps=9679.1, ups=14.82, wpb=652.9, bsz=31.5, num_updates=540, lr=2.99385e-05, gnorm=7.595, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=70
2022-01-10 00:21:52 | INFO | train_inner | epoch 006:     63 / 99 loss=1.35, ppl=2.55, wps=8378.4, ups=15.78, wpb=531.1, bsz=32, num_updates=550, lr=2.99231e-05, gnorm=5.727, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=71
2022-01-10 00:21:53 | INFO | train_inner | epoch 006:     73 / 99 loss=1.355, ppl=2.56, wps=10955.5, ups=15.43, wpb=710.1, bsz=32, num_updates=560, lr=2.99077e-05, gnorm=6.226, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=72
2022-01-10 00:21:53 | INFO | train_inner | epoch 006:     83 / 99 loss=1.439, ppl=2.71, wps=8259.7, ups=15.7, wpb=526.1, bsz=32, num_updates=570, lr=2.98923e-05, gnorm=8.733, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=72
2022-01-10 00:21:54 | INFO | train_inner | epoch 006:     93 / 99 loss=1.191, ppl=2.28, wps=9692, ups=13.42, wpb=722, bsz=32, num_updates=580, lr=2.98769e-05, gnorm=5.032, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=73
2022-01-10 00:21:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:21:55 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 1.163 | ppl 2.24 | wps 21003.3 | wpb 606.6 | bsz 31.3 | num_updates 586 | best_loss 1.163
2022-01-10 00:21:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 586 updates
2022-01-10 00:21:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:21:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:21:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 6 @ 586 updates, score 1.163) (writing took 3.9896199190989137 seconds)
2022-01-10 00:21:59 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-10 00:21:59 | INFO | train | epoch 006 | loss 1.294 | ppl 2.45 | wps 5480.1 | ups 8.37 | wpb 654.4 | bsz 31.9 | num_updates 586 | lr 2.98677e-05 | gnorm 6.561 | clip 100 | loss_scale 0.5 | train_wall 6 | gb_free 20.8 | wall 78
2022-01-10 00:21:59 | INFO | fairseq.trainer | begin training epoch 7
2022-01-10 00:21:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:22:00 | INFO | train_inner | epoch 007:      4 / 99 loss=1.261, ppl=2.4, wps=1179.6, ups=1.74, wpb=677.6, bsz=32, num_updates=590, lr=2.98615e-05, gnorm=5.943, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=79
2022-01-10 00:22:01 | INFO | train_inner | epoch 007:     14 / 99 loss=1.417, ppl=2.67, wps=7428.3, ups=14.72, wpb=504.5, bsz=32, num_updates=600, lr=2.98462e-05, gnorm=8.442, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=79
2022-01-10 00:22:01 | INFO | train_inner | epoch 007:     24 / 99 loss=1.074, ppl=2.11, wps=11717.5, ups=14.05, wpb=833.7, bsz=31.5, num_updates=610, lr=2.98308e-05, gnorm=17.995, clip=100, loss_scale=0.5, train_wall=1, gb_free=18.6, wall=80
2022-01-10 00:22:02 | INFO | train_inner | epoch 007:     34 / 99 loss=1.2, ppl=2.3, wps=9143.7, ups=15.19, wpb=601.9, bsz=32, num_updates=620, lr=2.98154e-05, gnorm=5.2, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=81
2022-01-10 00:22:03 | INFO | train_inner | epoch 007:     44 / 99 loss=1.137, ppl=2.2, wps=11408.1, ups=14.84, wpb=768.8, bsz=32, num_updates=630, lr=2.98e-05, gnorm=4.808, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=82
2022-01-10 00:22:03 | INFO | train_inner | epoch 007:     54 / 99 loss=1.21, ppl=2.31, wps=9591.4, ups=14.48, wpb=662.4, bsz=32, num_updates=640, lr=2.97846e-05, gnorm=5.981, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=82
2022-01-10 00:22:04 | INFO | train_inner | epoch 007:     64 / 99 loss=1.412, ppl=2.66, wps=6839.5, ups=15.85, wpb=431.4, bsz=32, num_updates=650, lr=2.97692e-05, gnorm=6.129, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=83
2022-01-10 00:22:05 | INFO | train_inner | epoch 007:     74 / 99 loss=1.123, ppl=2.18, wps=11033.2, ups=15.09, wpb=731.2, bsz=32, num_updates=660, lr=2.97538e-05, gnorm=6.977, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=84
2022-01-10 00:22:05 | INFO | train_inner | epoch 007:     84 / 99 loss=1.104, ppl=2.15, wps=12155, ups=15.28, wpb=795.6, bsz=32, num_updates=670, lr=2.97385e-05, gnorm=4.892, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=84
2022-01-10 00:22:06 | INFO | train_inner | epoch 007:     94 / 99 loss=1.268, ppl=2.41, wps=6928.1, ups=11.4, wpb=607.6, bsz=32, num_updates=680, lr=2.97231e-05, gnorm=9.377, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=85
2022-01-10 00:22:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:22:07 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 1.11 | ppl 2.16 | wps 25975.6 | wpb 606.6 | bsz 31.3 | num_updates 685 | best_loss 1.11
2022-01-10 00:22:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 685 updates
2022-01-10 00:22:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:22:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:22:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 7 @ 685 updates, score 1.11) (writing took 4.542301584966481 seconds)
2022-01-10 00:22:12 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-10 00:22:12 | INFO | train | epoch 007 | loss 1.201 | ppl 2.3 | wps 5200 | ups 7.98 | wpb 651.7 | bsz 31.9 | num_updates 685 | lr 2.97154e-05 | gnorm 7.534 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 91
2022-01-10 00:22:12 | INFO | fairseq.trainer | begin training epoch 8
2022-01-10 00:22:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:22:12 | INFO | train_inner | epoch 008:      5 / 99 loss=1.096, ppl=2.14, wps=1037.9, ups=1.6, wpb=647.5, bsz=32, num_updates=690, lr=2.97077e-05, gnorm=4.693, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=91
2022-01-10 00:22:13 | INFO | train_inner | epoch 008:     15 / 99 loss=1.057, ppl=2.08, wps=9225.3, ups=13.96, wpb=660.6, bsz=31.5, num_updates=700, lr=2.96923e-05, gnorm=5.408, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=92
2022-01-10 00:22:14 | INFO | train_inner | epoch 008:     25 / 99 loss=1.077, ppl=2.11, wps=10101.4, ups=15.55, wpb=649.5, bsz=32, num_updates=710, lr=2.96769e-05, gnorm=5.439, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=93
2022-01-10 00:22:14 | INFO | train_inner | epoch 008:     35 / 99 loss=1.119, ppl=2.17, wps=10146.9, ups=13.82, wpb=734, bsz=32, num_updates=720, lr=2.96615e-05, gnorm=5.909, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=93
2022-01-10 00:22:15 | INFO | train_inner | epoch 008:     45 / 99 loss=1.292, ppl=2.45, wps=8467.1, ups=15.81, wpb=535.4, bsz=32, num_updates=730, lr=2.96462e-05, gnorm=5.777, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=94
2022-01-10 00:22:16 | INFO | train_inner | epoch 008:     55 / 99 loss=1.302, ppl=2.47, wps=8599.4, ups=15.92, wpb=540.2, bsz=32, num_updates=740, lr=2.96308e-05, gnorm=5.685, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=95
2022-01-10 00:22:16 | INFO | train_inner | epoch 008:     65 / 99 loss=1.054, ppl=2.08, wps=10667.9, ups=14.64, wpb=728.5, bsz=32, num_updates=750, lr=2.96154e-05, gnorm=5.159, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=95
2022-01-10 00:22:17 | INFO | train_inner | epoch 008:     75 / 99 loss=0.997, ppl=2, wps=10074.8, ups=14.22, wpb=708.6, bsz=32, num_updates=760, lr=2.96e-05, gnorm=4.342, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=96
2022-01-10 00:22:18 | INFO | train_inner | epoch 008:     85 / 99 loss=1.205, ppl=2.31, wps=8151.7, ups=15.38, wpb=530, bsz=32, num_updates=770, lr=2.95846e-05, gnorm=5.731, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=97
2022-01-10 00:22:19 | INFO | train_inner | epoch 008:     95 / 99 loss=1.15, ppl=2.22, wps=7897.1, ups=11.53, wpb=684.9, bsz=32, num_updates=780, lr=2.95692e-05, gnorm=5.969, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.1, wall=98
2022-01-10 00:22:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:22:20 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 1.07 | ppl 2.1 | wps 23803.2 | wpb 606.6 | bsz 31.3 | num_updates 784 | best_loss 1.07
2022-01-10 00:22:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 784 updates
2022-01-10 00:22:20 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:22:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:22:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 8 @ 784 updates, score 1.07) (writing took 4.148659017984755 seconds)
2022-01-10 00:22:24 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-10 00:22:24 | INFO | train | epoch 008 | loss 1.123 | ppl 2.18 | wps 5356 | ups 8.22 | wpb 651.7 | bsz 31.9 | num_updates 784 | lr 2.95631e-05 | gnorm 5.406 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 103
2022-01-10 00:22:24 | INFO | fairseq.trainer | begin training epoch 9
2022-01-10 00:22:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:22:24 | INFO | train_inner | epoch 009:      6 / 99 loss=1.096, ppl=2.14, wps=1143.3, ups=1.72, wpb=664.6, bsz=32, num_updates=790, lr=2.95538e-05, gnorm=4.789, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=103
2022-01-10 00:22:25 | INFO | train_inner | epoch 009:     16 / 99 loss=1.053, ppl=2.08, wps=9252.1, ups=13.81, wpb=670, bsz=32, num_updates=800, lr=2.95385e-05, gnorm=5.049, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=104
2022-01-10 00:22:26 | INFO | train_inner | epoch 009:     26 / 99 loss=1.09, ppl=2.13, wps=8554.4, ups=15.21, wpb=562.4, bsz=32, num_updates=810, lr=2.95231e-05, gnorm=5.277, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=105
2022-01-10 00:22:27 | INFO | train_inner | epoch 009:     36 / 99 loss=0.95, ppl=1.93, wps=12368.9, ups=13.57, wpb=911.5, bsz=32, num_updates=820, lr=2.95077e-05, gnorm=4.795, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.5, wall=106
2022-01-10 00:22:27 | INFO | train_inner | epoch 009:     46 / 99 loss=1, ppl=2, wps=8616.5, ups=12.38, wpb=695.8, bsz=32, num_updates=830, lr=2.94923e-05, gnorm=5.046, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=106
2022-01-10 00:22:28 | INFO | train_inner | epoch 009:     56 / 99 loss=1.144, ppl=2.21, wps=6749, ups=13.44, wpb=502.2, bsz=32, num_updates=840, lr=2.94769e-05, gnorm=5.771, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=107
2022-01-10 00:22:29 | INFO | train_inner | epoch 009:     66 / 99 loss=1.155, ppl=2.23, wps=6836.1, ups=13.67, wpb=500.2, bsz=32, num_updates=850, lr=2.94615e-05, gnorm=6.885, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=108
2022-01-10 00:22:30 | INFO | train_inner | epoch 009:     76 / 99 loss=0.986, ppl=1.98, wps=9697.2, ups=12.68, wpb=764.8, bsz=31.5, num_updates=860, lr=2.94462e-05, gnorm=5.282, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=109
2022-01-10 00:22:30 | INFO | train_inner | epoch 009:     86 / 99 loss=1.145, ppl=2.21, wps=7933.9, ups=14.82, wpb=535.5, bsz=32, num_updates=870, lr=2.94308e-05, gnorm=5.31, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=109
2022-01-10 00:22:31 | INFO | train_inner | epoch 009:     96 / 99 loss=1.078, ppl=2.11, wps=6734.3, ups=10.56, wpb=637.6, bsz=32, num_updates=880, lr=2.94154e-05, gnorm=5.011, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=110
2022-01-10 00:22:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:22:32 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 1.062 | ppl 2.09 | wps 22038.7 | wpb 606.6 | bsz 31.3 | num_updates 883 | best_loss 1.062
2022-01-10 00:22:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 883 updates
2022-01-10 00:22:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:22:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:22:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 9 @ 883 updates, score 1.062) (writing took 5.008825400960632 seconds)
2022-01-10 00:22:37 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-10 00:22:37 | INFO | train | epoch 009 | loss 1.05 | ppl 2.07 | wps 4789.2 | ups 7.35 | wpb 651.7 | bsz 31.9 | num_updates 883 | lr 2.94108e-05 | gnorm 5.296 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 116
2022-01-10 00:22:37 | INFO | fairseq.trainer | begin training epoch 10
2022-01-10 00:22:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:22:38 | INFO | train_inner | epoch 010:      7 / 99 loss=0.996, ppl=2, wps=931.4, ups=1.49, wpb=624.3, bsz=32, num_updates=890, lr=2.94e-05, gnorm=4.739, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=117
2022-01-10 00:22:39 | INFO | train_inner | epoch 010:     17 / 99 loss=1.053, ppl=2.07, wps=7537.5, ups=12.96, wpb=581.4, bsz=32, num_updates=900, lr=2.93846e-05, gnorm=5.189, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=118
2022-01-10 00:22:39 | INFO | train_inner | epoch 010:     27 / 99 loss=0.874, ppl=1.83, wps=13301.7, ups=15.27, wpb=870.9, bsz=32, num_updates=910, lr=2.93692e-05, gnorm=3.756, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=118
2022-01-10 00:22:40 | INFO | train_inner | epoch 010:     37 / 99 loss=0.905, ppl=1.87, wps=11735.8, ups=13.56, wpb=865.5, bsz=32, num_updates=920, lr=2.93538e-05, gnorm=3.944, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=119
2022-01-10 00:22:41 | INFO | train_inner | epoch 010:     47 / 99 loss=1.001, ppl=2, wps=7960, ups=12.58, wpb=633, bsz=32, num_updates=930, lr=2.93385e-05, gnorm=4.932, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=120
2022-01-10 00:22:42 | INFO | train_inner | epoch 010:     57 / 99 loss=1.182, ppl=2.27, wps=7601.2, ups=15.11, wpb=503, bsz=32, num_updates=940, lr=2.93231e-05, gnorm=8.444, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=121
2022-01-10 00:22:42 | INFO | train_inner | epoch 010:     67 / 99 loss=1.118, ppl=2.17, wps=7157.6, ups=13.53, wpb=529.2, bsz=32, num_updates=950, lr=2.93077e-05, gnorm=5.76, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=121
2022-01-10 00:22:43 | INFO | train_inner | epoch 010:     77 / 99 loss=1.07, ppl=2.1, wps=8843.9, ups=15.4, wpb=574.1, bsz=32, num_updates=960, lr=2.92923e-05, gnorm=4.527, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=122
2022-01-10 00:22:44 | INFO | train_inner | epoch 010:     87 / 99 loss=0.929, ppl=1.9, wps=10342.6, ups=14.39, wpb=718.5, bsz=32, num_updates=970, lr=2.92769e-05, gnorm=4.609, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=123
2022-01-10 00:22:45 | INFO | train_inner | epoch 010:     97 / 99 loss=1.009, ppl=2.01, wps=8610.6, ups=11.16, wpb=771.8, bsz=31.5, num_updates=980, lr=2.92615e-05, gnorm=5.069, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=124
2022-01-10 00:22:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:22:46 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 1.01 | ppl 2.01 | wps 21940.3 | wpb 606.6 | bsz 31.3 | num_updates 982 | best_loss 1.01
2022-01-10 00:22:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 982 updates
2022-01-10 00:22:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:22:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:22:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 10 @ 982 updates, score 1.01) (writing took 4.229814217076637 seconds)
2022-01-10 00:22:50 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-10 00:22:50 | INFO | train | epoch 010 | loss 1.004 | ppl 2.01 | wps 5167.4 | ups 7.93 | wpb 651.7 | bsz 31.9 | num_updates 982 | lr 2.92585e-05 | gnorm 5.158 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 129
2022-01-10 00:22:50 | INFO | fairseq.trainer | begin training epoch 11
2022-01-10 00:22:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:22:51 | INFO | train_inner | epoch 011:      8 / 99 loss=0.967, ppl=1.96, wps=887.7, ups=1.62, wpb=546.9, bsz=32, num_updates=990, lr=2.92462e-05, gnorm=4.853, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=130
2022-01-10 00:22:51 | INFO | train_inner | epoch 011:     18 / 99 loss=0.9, ppl=1.87, wps=10837.9, ups=14.27, wpb=759.7, bsz=32, num_updates=1000, lr=2.92308e-05, gnorm=4.044, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=130
2022-01-10 00:22:52 | INFO | train_inner | epoch 011:     28 / 99 loss=0.935, ppl=1.91, wps=8212.6, ups=13.19, wpb=622.5, bsz=32, num_updates=1010, lr=2.92154e-05, gnorm=5.294, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=131
2022-01-10 00:22:53 | INFO | train_inner | epoch 011:     38 / 99 loss=1.05, ppl=2.07, wps=8576.9, ups=14.49, wpb=592, bsz=31.5, num_updates=1020, lr=2.92e-05, gnorm=5.576, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=132
2022-01-10 00:22:54 | INFO | train_inner | epoch 011:     48 / 99 loss=0.897, ppl=1.86, wps=9786.9, ups=14.58, wpb=671.3, bsz=32, num_updates=1030, lr=2.91846e-05, gnorm=4.54, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=133
2022-01-10 00:22:54 | INFO | train_inner | epoch 011:     58 / 99 loss=0.957, ppl=1.94, wps=8530.4, ups=12.96, wpb=658.2, bsz=32, num_updates=1040, lr=2.91692e-05, gnorm=4.919, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=133
2022-01-10 00:22:55 | INFO | train_inner | epoch 011:     68 / 99 loss=0.948, ppl=1.93, wps=8596.9, ups=11.6, wpb=741.1, bsz=32, num_updates=1050, lr=2.91538e-05, gnorm=5.18, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=134
2022-01-10 00:22:56 | INFO | train_inner | epoch 011:     78 / 99 loss=0.921, ppl=1.89, wps=8511.7, ups=12.78, wpb=665.9, bsz=32, num_updates=1060, lr=2.91385e-05, gnorm=4.902, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=135
2022-01-10 00:22:57 | INFO | train_inner | epoch 011:     88 / 99 loss=0.998, ppl=2, wps=9236.6, ups=13.91, wpb=664, bsz=32, num_updates=1070, lr=2.91231e-05, gnorm=4.458, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=136
2022-01-10 00:22:58 | INFO | train_inner | epoch 011:     98 / 99 loss=0.97, ppl=1.96, wps=7192.9, ups=12.74, wpb=564.6, bsz=32, num_updates=1080, lr=2.91077e-05, gnorm=4.844, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=136
2022-01-10 00:22:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:22:59 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 1.002 | ppl 2 | wps 21070.7 | wpb 606.6 | bsz 31.3 | num_updates 1081 | best_loss 1.002
2022-01-10 00:22:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1081 updates
2022-01-10 00:22:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:23:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:23:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 11 @ 1081 updates, score 1.002) (writing took 4.766520404955372 seconds)
2022-01-10 00:23:03 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-10 00:23:03 | INFO | train | epoch 011 | loss 0.951 | ppl 1.93 | wps 4805.3 | ups 7.37 | wpb 651.7 | bsz 31.9 | num_updates 1081 | lr 2.91062e-05 | gnorm 4.864 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 142
2022-01-10 00:23:03 | INFO | fairseq.trainer | begin training epoch 12
2022-01-10 00:23:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:23:04 | INFO | train_inner | epoch 012:      9 / 99 loss=0.953, ppl=1.94, wps=994.6, ups=1.54, wpb=645, bsz=32, num_updates=1090, lr=2.90923e-05, gnorm=5.941, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=143
2022-01-10 00:23:05 | INFO | train_inner | epoch 012:     19 / 99 loss=0.84, ppl=1.79, wps=10419.8, ups=14.25, wpb=731, bsz=32, num_updates=1100, lr=2.90769e-05, gnorm=4.77, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=144
2022-01-10 00:23:05 | INFO | train_inner | epoch 012:     29 / 99 loss=0.86, ppl=1.81, wps=11357.2, ups=14.12, wpb=804.2, bsz=32, num_updates=1110, lr=2.90615e-05, gnorm=4.001, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=144
2022-01-10 00:23:06 | INFO | train_inner | epoch 012:     39 / 99 loss=1.003, ppl=2, wps=8899.1, ups=14.5, wpb=613.9, bsz=32, num_updates=1120, lr=2.90462e-05, gnorm=4.817, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=145
2022-01-10 00:23:07 | INFO | train_inner | epoch 012:     49 / 99 loss=0.853, ppl=1.81, wps=11652.2, ups=13.65, wpb=853.5, bsz=31.5, num_updates=1130, lr=2.90308e-05, gnorm=3.947, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=146
2022-01-10 00:23:08 | INFO | train_inner | epoch 012:     59 / 99 loss=0.872, ppl=1.83, wps=9860.1, ups=13.37, wpb=737.4, bsz=32, num_updates=1140, lr=2.90154e-05, gnorm=7.484, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=147
2022-01-10 00:23:08 | INFO | train_inner | epoch 012:     69 / 99 loss=0.979, ppl=1.97, wps=6014.5, ups=12.41, wpb=484.8, bsz=32, num_updates=1150, lr=2.9e-05, gnorm=12.498, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=147
2022-01-10 00:23:09 | INFO | train_inner | epoch 012:     79 / 99 loss=1.029, ppl=2.04, wps=6558.8, ups=11.29, wpb=580.8, bsz=32, num_updates=1160, lr=2.89846e-05, gnorm=5.501, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=148
2022-01-10 00:23:10 | INFO | train_inner | epoch 012:     89 / 99 loss=0.887, ppl=1.85, wps=7200.4, ups=11.61, wpb=620.1, bsz=32, num_updates=1170, lr=2.89692e-05, gnorm=5.165, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=149
2022-01-10 00:23:11 | INFO | train_inner | epoch 012:     99 / 99 loss=1.008, ppl=2.01, wps=4437.7, ups=10.5, wpb=422.8, bsz=32, num_updates=1180, lr=2.89538e-05, gnorm=6.834, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=150
2022-01-10 00:23:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:23:12 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.99 | ppl 1.99 | wps 22196.4 | wpb 606.6 | bsz 31.3 | num_updates 1180 | best_loss 0.99
2022-01-10 00:23:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1180 updates
2022-01-10 00:23:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:23:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:23:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 12 @ 1180 updates, score 0.99) (writing took 4.399478371022269 seconds)
2022-01-10 00:23:16 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-10 00:23:16 | INFO | train | epoch 012 | loss 0.914 | ppl 1.88 | wps 4925.2 | ups 7.56 | wpb 651.7 | bsz 31.9 | num_updates 1180 | lr 2.89538e-05 | gnorm 6.081 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 155
2022-01-10 00:23:16 | INFO | fairseq.trainer | begin training epoch 13
2022-01-10 00:23:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:23:17 | INFO | train_inner | epoch 013:     10 / 99 loss=0.856, ppl=1.81, wps=1427.2, ups=1.64, wpb=871.2, bsz=32, num_updates=1190, lr=2.89385e-05, gnorm=4.083, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=156
2022-01-10 00:23:18 | INFO | train_inner | epoch 013:     20 / 99 loss=0.867, ppl=1.82, wps=7188.3, ups=13.2, wpb=544.5, bsz=32, num_updates=1200, lr=2.89231e-05, gnorm=4.844, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=157
2022-01-10 00:23:19 | INFO | train_inner | epoch 013:     30 / 99 loss=0.867, ppl=1.82, wps=8876.2, ups=12.75, wpb=696.2, bsz=32, num_updates=1210, lr=2.89077e-05, gnorm=4.343, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=158
2022-01-10 00:23:20 | INFO | train_inner | epoch 013:     40 / 99 loss=0.887, ppl=1.85, wps=8588.5, ups=14.36, wpb=598.2, bsz=32, num_updates=1220, lr=2.88923e-05, gnorm=6.497, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=158
2022-01-10 00:23:20 | INFO | train_inner | epoch 013:     50 / 99 loss=0.915, ppl=1.89, wps=7018.4, ups=14.38, wpb=488.2, bsz=32, num_updates=1230, lr=2.88769e-05, gnorm=5.309, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=159
2022-01-10 00:23:21 | INFO | train_inner | epoch 013:     60 / 99 loss=0.87, ppl=1.83, wps=8145.7, ups=13.57, wpb=600.2, bsz=32, num_updates=1240, lr=2.88615e-05, gnorm=5.037, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=160
2022-01-10 00:23:22 | INFO | train_inner | epoch 013:     70 / 99 loss=0.888, ppl=1.85, wps=9862, ups=14.36, wpb=686.6, bsz=32, num_updates=1250, lr=2.88462e-05, gnorm=5.168, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=161
2022-01-10 00:23:22 | INFO | train_inner | epoch 013:     80 / 99 loss=0.911, ppl=1.88, wps=7561.6, ups=12.29, wpb=615.4, bsz=32, num_updates=1260, lr=2.88308e-05, gnorm=4.711, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=161
2022-01-10 00:23:23 | INFO | train_inner | epoch 013:     90 / 99 loss=0.86, ppl=1.81, wps=10989, ups=13.44, wpb=817.5, bsz=31.5, num_updates=1270, lr=2.88154e-05, gnorm=4.505, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=162
2022-01-10 00:23:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:23:25 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 0.959 | ppl 1.94 | wps 19948.1 | wpb 606.6 | bsz 31.3 | num_updates 1279 | best_loss 0.959
2022-01-10 00:23:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1279 updates
2022-01-10 00:23:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:23:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:23:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 13 @ 1279 updates, score 0.959) (writing took 4.396975388051942 seconds)
2022-01-10 00:23:29 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-10 00:23:29 | INFO | train | epoch 013 | loss 0.885 | ppl 1.85 | wps 4973.9 | ups 7.63 | wpb 651.7 | bsz 31.9 | num_updates 1279 | lr 2.88015e-05 | gnorm 4.919 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.7 | wall 168
2022-01-10 00:23:29 | INFO | fairseq.trainer | begin training epoch 14
2022-01-10 00:23:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:23:30 | INFO | train_inner | epoch 014:      1 / 99 loss=0.969, ppl=1.96, wps=902.8, ups=1.57, wpb=573.4, bsz=32, num_updates=1280, lr=2.88e-05, gnorm=4.678, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=169
2022-01-10 00:23:30 | INFO | train_inner | epoch 014:     11 / 99 loss=0.763, ppl=1.7, wps=9753.8, ups=15.39, wpb=633.7, bsz=32, num_updates=1290, lr=2.87846e-05, gnorm=14.848, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=169
2022-01-10 00:23:31 | INFO | train_inner | epoch 014:     21 / 99 loss=0.828, ppl=1.78, wps=6777.6, ups=12.49, wpb=542.8, bsz=32, num_updates=1300, lr=2.87692e-05, gnorm=4.987, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=170
2022-01-10 00:23:32 | INFO | train_inner | epoch 014:     31 / 99 loss=0.912, ppl=1.88, wps=8781.2, ups=13.6, wpb=645.6, bsz=32, num_updates=1310, lr=2.87538e-05, gnorm=5.362, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=171
2022-01-10 00:23:32 | INFO | train_inner | epoch 014:     41 / 99 loss=0.789, ppl=1.73, wps=11458.5, ups=13.94, wpb=821.8, bsz=32, num_updates=1320, lr=2.87385e-05, gnorm=3.599, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=171
2022-01-10 00:23:33 | INFO | train_inner | epoch 014:     51 / 99 loss=0.871, ppl=1.83, wps=8852, ups=12.9, wpb=686.4, bsz=31.5, num_updates=1330, lr=2.87231e-05, gnorm=5.164, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=172
2022-01-10 00:23:34 | INFO | train_inner | epoch 014:     61 / 99 loss=0.831, ppl=1.78, wps=10846.4, ups=14.55, wpb=745.4, bsz=32, num_updates=1340, lr=2.87077e-05, gnorm=4.482, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=173
2022-01-10 00:23:35 | INFO | train_inner | epoch 014:     71 / 99 loss=0.928, ppl=1.9, wps=6950.7, ups=15.23, wpb=456.3, bsz=32, num_updates=1350, lr=2.86923e-05, gnorm=4.9, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=174
2022-01-10 00:23:35 | INFO | train_inner | epoch 014:     81 / 99 loss=0.841, ppl=1.79, wps=10522.5, ups=12.99, wpb=809.8, bsz=32, num_updates=1360, lr=2.86769e-05, gnorm=4.082, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=174
2022-01-10 00:23:36 | INFO | train_inner | epoch 014:     91 / 99 loss=0.871, ppl=1.83, wps=7702.9, ups=13.35, wpb=577, bsz=32, num_updates=1370, lr=2.86615e-05, gnorm=4.799, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=175
2022-01-10 00:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:23:38 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 0.944 | ppl 1.92 | wps 20954.4 | wpb 606.6 | bsz 31.3 | num_updates 1378 | best_loss 0.944
2022-01-10 00:23:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1378 updates
2022-01-10 00:23:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:23:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:23:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 14 @ 1378 updates, score 0.944) (writing took 6.3567888219840825 seconds)
2022-01-10 00:23:44 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-10 00:23:45 | INFO | train | epoch 014 | loss 0.846 | ppl 1.8 | wps 4372.2 | ups 6.71 | wpb 651.7 | bsz 31.9 | num_updates 1378 | lr 2.86492e-05 | gnorm 5.741 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.5 | wall 183
2022-01-10 00:23:45 | INFO | fairseq.trainer | begin training epoch 15
2022-01-10 00:23:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:23:45 | INFO | train_inner | epoch 015:      2 / 99 loss=0.861, ppl=1.82, wps=660.8, ups=1.12, wpb=589.6, bsz=32, num_updates=1380, lr=2.86462e-05, gnorm=5.045, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=184
2022-01-10 00:23:46 | INFO | train_inner | epoch 015:     12 / 99 loss=0.858, ppl=1.81, wps=8294.8, ups=14.29, wpb=580.3, bsz=32, num_updates=1390, lr=2.86308e-05, gnorm=4.787, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=185
2022-01-10 00:23:46 | INFO | train_inner | epoch 015:     22 / 99 loss=0.799, ppl=1.74, wps=11991.2, ups=15.25, wpb=786.1, bsz=32, num_updates=1400, lr=2.86154e-05, gnorm=4.061, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=185
2022-01-10 00:23:47 | INFO | train_inner | epoch 015:     32 / 99 loss=0.948, ppl=1.93, wps=8843.3, ups=15.72, wpb=562.7, bsz=32, num_updates=1410, lr=2.86e-05, gnorm=5.032, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=186
2022-01-10 00:23:48 | INFO | train_inner | epoch 015:     42 / 99 loss=0.819, ppl=1.76, wps=9269.9, ups=15.01, wpb=617.6, bsz=32, num_updates=1420, lr=2.85846e-05, gnorm=4.919, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=187
2022-01-10 00:23:49 | INFO | train_inner | epoch 015:     52 / 99 loss=0.834, ppl=1.78, wps=7323.6, ups=12.36, wpb=592.6, bsz=32, num_updates=1430, lr=2.85692e-05, gnorm=16.773, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=187
2022-01-10 00:23:49 | INFO | train_inner | epoch 015:     62 / 99 loss=0.797, ppl=1.74, wps=9116.3, ups=12.72, wpb=716.6, bsz=32, num_updates=1440, lr=2.85538e-05, gnorm=7.037, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.2, wall=188
2022-01-10 00:23:50 | INFO | train_inner | epoch 015:     72 / 99 loss=0.811, ppl=1.75, wps=8516.9, ups=13.6, wpb=626.3, bsz=32, num_updates=1450, lr=2.85385e-05, gnorm=9.16, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=189
2022-01-10 00:23:51 | INFO | train_inner | epoch 015:     82 / 99 loss=0.773, ppl=1.71, wps=8517.5, ups=13.59, wpb=626.9, bsz=32, num_updates=1460, lr=2.85231e-05, gnorm=4.417, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=190
2022-01-10 00:23:52 | INFO | train_inner | epoch 015:     92 / 99 loss=0.786, ppl=1.72, wps=6976.4, ups=10.09, wpb=691.2, bsz=32, num_updates=1470, lr=2.85077e-05, gnorm=4.718, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=191
2022-01-10 00:23:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:23:53 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 0.95 | ppl 1.93 | wps 21667.6 | wpb 606.6 | bsz 31.3 | num_updates 1477 | best_loss 0.944
2022-01-10 00:23:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1477 updates
2022-01-10 00:23:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:23:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:23:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 15 @ 1477 updates, score 0.95) (writing took 2.674427737016231 seconds)
2022-01-10 00:23:56 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-10 00:23:56 | INFO | train | epoch 015 | loss 0.815 | ppl 1.76 | wps 5788.4 | ups 8.88 | wpb 651.7 | bsz 31.9 | num_updates 1477 | lr 2.84969e-05 | gnorm 6.534 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.7 | wall 195
2022-01-10 00:23:56 | INFO | fairseq.trainer | begin training epoch 16
2022-01-10 00:23:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:23:56 | INFO | train_inner | epoch 016:      3 / 99 loss=0.731, ppl=1.66, wps=1548.4, ups=2.2, wpb=703.8, bsz=31.5, num_updates=1480, lr=2.84923e-05, gnorm=4.13, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=195
2022-01-10 00:23:57 | INFO | train_inner | epoch 016:     13 / 99 loss=0.773, ppl=1.71, wps=8971.5, ups=13.6, wpb=659.9, bsz=32, num_updates=1490, lr=2.84769e-05, gnorm=4.217, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=196
2022-01-10 00:23:58 | INFO | train_inner | epoch 016:     23 / 99 loss=0.762, ppl=1.7, wps=8643.2, ups=13.95, wpb=619.7, bsz=32, num_updates=1500, lr=2.84615e-05, gnorm=4.309, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=197
2022-01-10 00:23:59 | INFO | train_inner | epoch 016:     33 / 99 loss=0.752, ppl=1.68, wps=9470.1, ups=13.76, wpb=688.1, bsz=32, num_updates=1510, lr=2.84462e-05, gnorm=4.913, clip=100, loss_scale=0.5, train_wall=1, gb_free=19.6, wall=197
2022-01-10 00:23:59 | INFO | train_inner | epoch 016:     43 / 99 loss=0.829, ppl=1.78, wps=7389.4, ups=14.43, wpb=512, bsz=32, num_updates=1520, lr=2.84308e-05, gnorm=5.346, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=198
2022-01-10 00:24:00 | INFO | train_inner | epoch 016:     53 / 99 loss=0.803, ppl=1.74, wps=9046.5, ups=11.17, wpb=809.6, bsz=32, num_updates=1530, lr=2.84154e-05, gnorm=3.92, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=199
2022-01-10 00:24:01 | INFO | train_inner | epoch 016:     63 / 99 loss=0.72, ppl=1.65, wps=10350.6, ups=13.96, wpb=741.6, bsz=32, num_updates=1540, lr=2.84e-05, gnorm=3.97, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=200
2022-01-10 00:24:02 | INFO | train_inner | epoch 016:     73 / 99 loss=0.772, ppl=1.71, wps=8672.5, ups=13.33, wpb=650.5, bsz=32, num_updates=1550, lr=2.83846e-05, gnorm=4.194, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=201
2022-01-10 00:24:02 | INFO | train_inner | epoch 016:     83 / 99 loss=0.799, ppl=1.74, wps=8734.9, ups=15.51, wpb=563.2, bsz=32, num_updates=1560, lr=2.83692e-05, gnorm=4.12, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=201
2022-01-10 00:24:03 | INFO | train_inner | epoch 016:     93 / 99 loss=0.706, ppl=1.63, wps=10007.1, ups=12.31, wpb=813, bsz=31.5, num_updates=1570, lr=2.83538e-05, gnorm=4.859, clip=100, loss_scale=0.5, train_wall=1, gb_free=18.6, wall=202
2022-01-10 00:24:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:24:05 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 0.937 | ppl 1.91 | wps 21339.6 | wpb 606.6 | bsz 31.3 | num_updates 1576 | best_loss 0.937
2022-01-10 00:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1576 updates
2022-01-10 00:24:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:24:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 16 @ 1576 updates, score 0.937) (writing took 4.3851184719242156 seconds)
2022-01-10 00:24:09 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-10 00:24:09 | INFO | train | epoch 016 | loss 0.772 | ppl 1.71 | wps 5002.8 | ups 7.68 | wpb 651.7 | bsz 31.9 | num_updates 1576 | lr 2.83446e-05 | gnorm 4.483 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 208
2022-01-10 00:24:09 | INFO | fairseq.trainer | begin training epoch 17
2022-01-10 00:24:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:24:09 | INFO | train_inner | epoch 017:      4 / 99 loss=0.823, ppl=1.77, wps=931.2, ups=1.59, wpb=587.4, bsz=32, num_updates=1580, lr=2.83385e-05, gnorm=4.738, clip=100, loss_scale=0.5, train_wall=1, gb_free=20, wall=208
2022-01-10 00:24:10 | INFO | train_inner | epoch 017:     14 / 99 loss=0.776, ppl=1.71, wps=7994.9, ups=13.5, wpb=592.4, bsz=32, num_updates=1590, lr=2.83231e-05, gnorm=4.623, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=209
2022-01-10 00:24:11 | INFO | train_inner | epoch 017:     24 / 99 loss=0.748, ppl=1.68, wps=10275.9, ups=14.15, wpb=726.4, bsz=31.5, num_updates=1600, lr=2.83077e-05, gnorm=4.378, clip=100, loss_scale=0.5, train_wall=1, gb_free=18.6, wall=210
2022-01-10 00:24:12 | INFO | train_inner | epoch 017:     34 / 99 loss=0.783, ppl=1.72, wps=9252, ups=14.82, wpb=624.5, bsz=32, num_updates=1610, lr=2.82923e-05, gnorm=5.899, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=210
2022-01-10 00:24:12 | INFO | train_inner | epoch 017:     44 / 99 loss=0.789, ppl=1.73, wps=9632.8, ups=14.02, wpb=687.3, bsz=32, num_updates=1620, lr=2.82769e-05, gnorm=8.36, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=211
2022-01-10 00:24:13 | INFO | train_inner | epoch 017:     54 / 99 loss=0.721, ppl=1.65, wps=8541.3, ups=13.21, wpb=646.6, bsz=32, num_updates=1630, lr=2.82615e-05, gnorm=4.249, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.2, wall=212
2022-01-10 00:24:14 | INFO | train_inner | epoch 017:     64 / 99 loss=0.68, ppl=1.6, wps=9390, ups=14.72, wpb=637.8, bsz=32, num_updates=1640, lr=2.82462e-05, gnorm=4.768, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=213
2022-01-10 00:24:14 | INFO | train_inner | epoch 017:     74 / 99 loss=0.771, ppl=1.71, wps=7921.4, ups=12.35, wpb=641.4, bsz=32, num_updates=1650, lr=2.82308e-05, gnorm=5.018, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=213
2022-01-10 00:24:15 | INFO | train_inner | epoch 017:     84 / 99 loss=0.775, ppl=1.71, wps=8175, ups=13.34, wpb=612.8, bsz=32, num_updates=1660, lr=2.82154e-05, gnorm=4.282, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=214
2022-01-10 00:24:16 | INFO | train_inner | epoch 017:     94 / 99 loss=0.749, ppl=1.68, wps=7214.3, ups=11.75, wpb=613.9, bsz=32, num_updates=1670, lr=2.82e-05, gnorm=4.237, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=215
2022-01-10 00:24:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:24:17 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 0.912 | ppl 1.88 | wps 21035.2 | wpb 606.6 | bsz 31.3 | num_updates 1675 | best_loss 0.912
2022-01-10 00:24:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1675 updates
2022-01-10 00:24:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:24:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:24:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 17 @ 1675 updates, score 0.912) (writing took 3.8725311749149114 seconds)
2022-01-10 00:24:21 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-10 00:24:21 | INFO | train | epoch 017 | loss 0.753 | ppl 1.69 | wps 5229.4 | ups 8.02 | wpb 651.7 | bsz 31.9 | num_updates 1675 | lr 2.81923e-05 | gnorm 4.985 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 220
2022-01-10 00:24:21 | INFO | fairseq.trainer | begin training epoch 18
2022-01-10 00:24:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:24:22 | INFO | train_inner | epoch 018:      5 / 99 loss=0.715, ppl=1.64, wps=1304.6, ups=1.75, wpb=744.7, bsz=32, num_updates=1680, lr=2.81846e-05, gnorm=4.422, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=221
2022-01-10 00:24:22 | INFO | train_inner | epoch 018:     15 / 99 loss=0.756, ppl=1.69, wps=8881.5, ups=14.51, wpb=611.9, bsz=32, num_updates=1690, lr=2.81692e-05, gnorm=4.499, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=221
2022-01-10 00:24:23 | INFO | train_inner | epoch 018:     25 / 99 loss=0.713, ppl=1.64, wps=10630.6, ups=15.48, wpb=686.8, bsz=32, num_updates=1700, lr=2.81538e-05, gnorm=3.828, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=222
2022-01-10 00:24:24 | INFO | train_inner | epoch 018:     35 / 99 loss=0.686, ppl=1.61, wps=12035.5, ups=14.33, wpb=839.6, bsz=32, num_updates=1710, lr=2.81385e-05, gnorm=3.58, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=223
2022-01-10 00:24:25 | INFO | train_inner | epoch 018:     45 / 99 loss=0.732, ppl=1.66, wps=6812.8, ups=12.34, wpb=552.3, bsz=32, num_updates=1720, lr=2.81231e-05, gnorm=4.568, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=224
2022-01-10 00:24:25 | INFO | train_inner | epoch 018:     55 / 99 loss=0.729, ppl=1.66, wps=7863.6, ups=15.3, wpb=513.8, bsz=32, num_updates=1730, lr=2.81077e-05, gnorm=5.561, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=224
2022-01-10 00:24:26 | INFO | train_inner | epoch 018:     65 / 99 loss=0.726, ppl=1.65, wps=6958.9, ups=12.3, wpb=565.9, bsz=32, num_updates=1740, lr=2.80923e-05, gnorm=4.912, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=225
2022-01-10 00:24:27 | INFO | train_inner | epoch 018:     75 / 99 loss=0.787, ppl=1.73, wps=8124.7, ups=12.68, wpb=640.7, bsz=31.5, num_updates=1750, lr=2.80769e-05, gnorm=4.769, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=226
2022-01-10 00:24:28 | INFO | train_inner | epoch 018:     85 / 99 loss=0.726, ppl=1.65, wps=10846.9, ups=14.78, wpb=733.7, bsz=32, num_updates=1760, lr=2.80615e-05, gnorm=3.996, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=227
2022-01-10 00:24:28 | INFO | train_inner | epoch 018:     95 / 99 loss=0.717, ppl=1.64, wps=7161.3, ups=11.41, wpb=627.6, bsz=32, num_updates=1770, lr=2.80462e-05, gnorm=4.194, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=227
2022-01-10 00:24:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:24:30 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 0.898 | ppl 1.86 | wps 22495.1 | wpb 606.6 | bsz 31.3 | num_updates 1774 | best_loss 0.898
2022-01-10 00:24:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1774 updates
2022-01-10 00:24:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:24:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:24:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 18 @ 1774 updates, score 0.898) (writing took 4.579414269072004 seconds)
2022-01-10 00:24:34 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-10 00:24:34 | INFO | train | epoch 018 | loss 0.725 | ppl 1.65 | wps 4984.5 | ups 7.65 | wpb 651.7 | bsz 31.9 | num_updates 1774 | lr 2.804e-05 | gnorm 4.432 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.7 | wall 233
2022-01-10 00:24:34 | INFO | fairseq.trainer | begin training epoch 19
2022-01-10 00:24:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:24:35 | INFO | train_inner | epoch 019:      6 / 99 loss=0.715, ppl=1.64, wps=888, ups=1.59, wpb=560.2, bsz=32, num_updates=1780, lr=2.80308e-05, gnorm=4.977, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=234
2022-01-10 00:24:35 | INFO | train_inner | epoch 019:     16 / 99 loss=0.656, ppl=1.58, wps=10720.7, ups=14.7, wpb=729.5, bsz=32, num_updates=1790, lr=2.80154e-05, gnorm=4.245, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=234
2022-01-10 00:24:36 | INFO | train_inner | epoch 019:     26 / 99 loss=0.706, ppl=1.63, wps=7170.5, ups=13.72, wpb=522.8, bsz=32, num_updates=1800, lr=2.8e-05, gnorm=4.931, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=235
2022-01-10 00:24:37 | INFO | train_inner | epoch 019:     36 / 99 loss=0.729, ppl=1.66, wps=8867.9, ups=14.12, wpb=628, bsz=32, num_updates=1810, lr=2.79846e-05, gnorm=5.214, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.2, wall=236
2022-01-10 00:24:38 | INFO | train_inner | epoch 019:     46 / 99 loss=0.745, ppl=1.68, wps=7089.5, ups=14, wpb=506.5, bsz=32, num_updates=1820, lr=2.79692e-05, gnorm=5.248, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=237
2022-01-10 00:24:38 | INFO | train_inner | epoch 019:     56 / 99 loss=0.701, ppl=1.63, wps=7892.1, ups=12.85, wpb=614.3, bsz=32, num_updates=1830, lr=2.79538e-05, gnorm=4.404, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=237
2022-01-10 00:24:39 | INFO | train_inner | epoch 019:     66 / 99 loss=0.688, ppl=1.61, wps=11378.2, ups=13.43, wpb=847.4, bsz=31.5, num_updates=1840, lr=2.79385e-05, gnorm=3.563, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=238
2022-01-10 00:24:40 | INFO | train_inner | epoch 019:     76 / 99 loss=0.679, ppl=1.6, wps=9802.5, ups=13.73, wpb=713.8, bsz=32, num_updates=1850, lr=2.79231e-05, gnorm=4.556, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=239
2022-01-10 00:24:41 | INFO | train_inner | epoch 019:     86 / 99 loss=0.688, ppl=1.61, wps=9687.4, ups=13.81, wpb=701.4, bsz=32, num_updates=1860, lr=2.79077e-05, gnorm=3.9, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=240
2022-01-10 00:24:41 | INFO | train_inner | epoch 019:     96 / 99 loss=0.707, ppl=1.63, wps=9707.5, ups=12.57, wpb=772, bsz=32, num_updates=1870, lr=2.78923e-05, gnorm=3.559, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=240
2022-01-10 00:24:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:24:43 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 0.874 | ppl 1.83 | wps 21304.1 | wpb 606.6 | bsz 31.3 | num_updates 1873 | best_loss 0.874
2022-01-10 00:24:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1873 updates
2022-01-10 00:24:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:24:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:24:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 19 @ 1873 updates, score 0.874) (writing took 4.014650315977633 seconds)
2022-01-10 00:24:47 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-10 00:24:47 | INFO | train | epoch 019 | loss 0.699 | ppl 1.62 | wps 5248.7 | ups 8.05 | wpb 651.7 | bsz 31.9 | num_updates 1873 | lr 2.78877e-05 | gnorm 4.484 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 246
2022-01-10 00:24:47 | INFO | fairseq.trainer | begin training epoch 20
2022-01-10 00:24:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:24:47 | INFO | train_inner | epoch 020:      7 / 99 loss=0.661, ppl=1.58, wps=1237.7, ups=1.72, wpb=718.2, bsz=31.5, num_updates=1880, lr=2.78769e-05, gnorm=3.777, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=246
2022-01-10 00:24:48 | INFO | train_inner | epoch 020:     17 / 99 loss=0.659, ppl=1.58, wps=9532.1, ups=14.82, wpb=643.3, bsz=32, num_updates=1890, lr=2.78615e-05, gnorm=4.425, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=247
2022-01-10 00:24:49 | INFO | train_inner | epoch 020:     27 / 99 loss=0.686, ppl=1.61, wps=11020.4, ups=15.27, wpb=721.8, bsz=32, num_updates=1900, lr=2.78462e-05, gnorm=4.373, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=248
2022-01-10 00:24:49 | INFO | train_inner | epoch 020:     37 / 99 loss=0.684, ppl=1.61, wps=9409.2, ups=12.16, wpb=773.7, bsz=32, num_updates=1910, lr=2.78308e-05, gnorm=4.507, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=248
2022-01-10 00:24:50 | INFO | train_inner | epoch 020:     47 / 99 loss=0.72, ppl=1.65, wps=6753.6, ups=13.81, wpb=488.9, bsz=32, num_updates=1920, lr=2.78154e-05, gnorm=5.22, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=249
2022-01-10 00:24:51 | INFO | train_inner | epoch 020:     57 / 99 loss=0.664, ppl=1.58, wps=10507.3, ups=14.82, wpb=709.2, bsz=32, num_updates=1930, lr=2.78e-05, gnorm=4.063, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=250
2022-01-10 00:24:51 | INFO | train_inner | epoch 020:     67 / 99 loss=0.67, ppl=1.59, wps=10799.4, ups=14.63, wpb=738, bsz=32, num_updates=1940, lr=2.77846e-05, gnorm=3.887, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=250
2022-01-10 00:24:52 | INFO | train_inner | epoch 020:     77 / 99 loss=0.751, ppl=1.68, wps=7631.9, ups=14.08, wpb=542.1, bsz=32, num_updates=1950, lr=2.77692e-05, gnorm=4.905, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=251
2022-01-10 00:24:53 | INFO | train_inner | epoch 020:     87 / 99 loss=0.642, ppl=1.56, wps=7659.3, ups=12.52, wpb=611.9, bsz=32, num_updates=1960, lr=2.77538e-05, gnorm=4.575, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=252
2022-01-10 00:24:54 | INFO | train_inner | epoch 020:     97 / 99 loss=0.711, ppl=1.64, wps=6688.1, ups=12.24, wpb=546.4, bsz=32, num_updates=1970, lr=2.77385e-05, gnorm=4.534, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=253
2022-01-10 00:24:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:24:55 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 0.872 | ppl 1.83 | wps 21266.7 | wpb 606.6 | bsz 31.3 | num_updates 1972 | best_loss 0.872
2022-01-10 00:24:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1972 updates
2022-01-10 00:24:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:24:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:24:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 20 @ 1972 updates, score 0.872) (writing took 4.154120160965249 seconds)
2022-01-10 00:24:59 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-10 00:24:59 | INFO | train | epoch 020 | loss 0.683 | ppl 1.61 | wps 5187.9 | ups 7.96 | wpb 651.7 | bsz 31.9 | num_updates 1972 | lr 2.77354e-05 | gnorm 4.439 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 258
2022-01-10 00:24:59 | INFO | fairseq.trainer | begin training epoch 21
2022-01-10 00:24:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:25:00 | INFO | train_inner | epoch 021:      8 / 99 loss=0.612, ppl=1.53, wps=900.1, ups=1.7, wpb=529.3, bsz=32, num_updates=1980, lr=2.77231e-05, gnorm=4.728, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=259
2022-01-10 00:25:00 | INFO | train_inner | epoch 021:     18 / 99 loss=0.633, ppl=1.55, wps=9727.6, ups=13.03, wpb=746.8, bsz=32, num_updates=1990, lr=2.77077e-05, gnorm=3.706, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.2, wall=259
2022-01-10 00:25:01 | INFO | train_inner | epoch 021:     28 / 99 loss=0.652, ppl=1.57, wps=11686.8, ups=13.76, wpb=849.3, bsz=32, num_updates=2000, lr=2.76923e-05, gnorm=3.761, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=260
2022-01-10 00:25:02 | INFO | train_inner | epoch 021:     38 / 99 loss=0.676, ppl=1.6, wps=7847, ups=14.52, wpb=540.6, bsz=32, num_updates=2010, lr=2.76769e-05, gnorm=4.827, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=261
2022-01-10 00:25:03 | INFO | train_inner | epoch 021:     48 / 99 loss=0.73, ppl=1.66, wps=8080.8, ups=12.96, wpb=623.3, bsz=32, num_updates=2020, lr=2.76615e-05, gnorm=4.41, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.1, wall=262
2022-01-10 00:25:03 | INFO | train_inner | epoch 021:     58 / 99 loss=0.644, ppl=1.56, wps=9866.3, ups=13.2, wpb=747.3, bsz=32, num_updates=2030, lr=2.76462e-05, gnorm=3.967, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=262
2022-01-10 00:25:04 | INFO | train_inner | epoch 021:     68 / 99 loss=0.638, ppl=1.56, wps=7856.4, ups=11.25, wpb=698.5, bsz=32, num_updates=2040, lr=2.76308e-05, gnorm=3.788, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=263
2022-01-10 00:25:05 | INFO | train_inner | epoch 021:     78 / 99 loss=0.678, ppl=1.6, wps=6736, ups=12.52, wpb=537.9, bsz=32, num_updates=2050, lr=2.76154e-05, gnorm=4.501, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.5, wall=264
2022-01-10 00:25:06 | INFO | train_inner | epoch 021:     88 / 99 loss=0.717, ppl=1.64, wps=5274.3, ups=11.6, wpb=454.5, bsz=32, num_updates=2060, lr=2.76e-05, gnorm=4.741, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=265
2022-01-10 00:25:07 | INFO | train_inner | epoch 021:     98 / 99 loss=0.717, ppl=1.64, wps=9272.4, ups=12.42, wpb=746.8, bsz=31.5, num_updates=2070, lr=2.75846e-05, gnorm=4.601, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=266
2022-01-10 00:25:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:25:08 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 0.883 | ppl 1.84 | wps 20626.9 | wpb 606.6 | bsz 31.3 | num_updates 2071 | best_loss 0.872
2022-01-10 00:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2071 updates
2022-01-10 00:25:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:25:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:25:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 21 @ 2071 updates, score 0.883) (writing took 2.6847211900167167 seconds)
2022-01-10 00:25:10 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-10 00:25:11 | INFO | train | epoch 021 | loss 0.666 | ppl 1.59 | wps 5633.4 | ups 8.64 | wpb 651.7 | bsz 31.9 | num_updates 2071 | lr 2.75831e-05 | gnorm 4.278 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 269
2022-01-10 00:25:11 | INFO | fairseq.trainer | begin training epoch 22
2022-01-10 00:25:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:25:11 | INFO | train_inner | epoch 022:      9 / 99 loss=0.604, ppl=1.52, wps=1457.8, ups=2.24, wpb=650.4, bsz=32, num_updates=2080, lr=2.75692e-05, gnorm=3.983, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=270
2022-01-10 00:25:12 | INFO | train_inner | epoch 022:     19 / 99 loss=0.613, ppl=1.53, wps=8335.8, ups=12.7, wpb=656.1, bsz=32, num_updates=2090, lr=2.75538e-05, gnorm=5.028, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=271
2022-01-10 00:25:13 | INFO | train_inner | epoch 022:     29 / 99 loss=0.658, ppl=1.58, wps=8403.7, ups=11.88, wpb=707.6, bsz=31.5, num_updates=2100, lr=2.75385e-05, gnorm=4.135, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=272
2022-01-10 00:25:14 | INFO | train_inner | epoch 022:     39 / 99 loss=0.628, ppl=1.55, wps=8868.9, ups=12.86, wpb=689.4, bsz=32, num_updates=2110, lr=2.75231e-05, gnorm=3.576, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=273
2022-01-10 00:25:15 | INFO | train_inner | epoch 022:     49 / 99 loss=0.615, ppl=1.53, wps=8394.4, ups=11.89, wpb=706, bsz=32, num_updates=2120, lr=2.75077e-05, gnorm=4.134, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.5, wall=273
2022-01-10 00:25:15 | INFO | train_inner | epoch 022:     59 / 99 loss=0.596, ppl=1.51, wps=8825.7, ups=12.07, wpb=731.2, bsz=32, num_updates=2130, lr=2.74923e-05, gnorm=5.023, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=274
2022-01-10 00:25:16 | INFO | train_inner | epoch 022:     69 / 99 loss=0.671, ppl=1.59, wps=8065.9, ups=13.81, wpb=584, bsz=32, num_updates=2140, lr=2.74769e-05, gnorm=4.869, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=275
2022-01-10 00:25:17 | INFO | train_inner | epoch 022:     79 / 99 loss=0.65, ppl=1.57, wps=8233.3, ups=12.95, wpb=635.9, bsz=32, num_updates=2150, lr=2.74615e-05, gnorm=4.971, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=276
2022-01-10 00:25:18 | INFO | train_inner | epoch 022:     89 / 99 loss=0.682, ppl=1.6, wps=7881.1, ups=13.36, wpb=590, bsz=32, num_updates=2160, lr=2.74462e-05, gnorm=10.556, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=277
2022-01-10 00:25:18 | INFO | train_inner | epoch 022:     99 / 99 loss=0.584, ppl=1.5, wps=7400.8, ups=13.32, wpb=555.7, bsz=32, num_updates=2170, lr=2.74308e-05, gnorm=4.078, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=277
2022-01-10 00:25:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:25:19 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 0.887 | ppl 1.85 | wps 21428.6 | wpb 606.6 | bsz 31.3 | num_updates 2170 | best_loss 0.872
2022-01-10 00:25:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2170 updates
2022-01-10 00:25:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:25:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:25:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 22 @ 2170 updates, score 0.887) (writing took 2.4142874389654025 seconds)
2022-01-10 00:25:22 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-10 00:25:22 | INFO | train | epoch 022 | loss 0.628 | ppl 1.55 | wps 5763.2 | ups 8.84 | wpb 651.7 | bsz 31.9 | num_updates 2170 | lr 2.74308e-05 | gnorm 5.044 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 281
2022-01-10 00:25:22 | INFO | fairseq.trainer | begin training epoch 23
2022-01-10 00:25:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:25:22 | INFO | train_inner | epoch 023:     10 / 99 loss=0.585, ppl=1.5, wps=1782.5, ups=2.45, wpb=726.4, bsz=32, num_updates=2180, lr=2.74154e-05, gnorm=3.529, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=281
2022-01-10 00:25:23 | INFO | train_inner | epoch 023:     20 / 99 loss=0.613, ppl=1.53, wps=8944.8, ups=13.62, wpb=656.7, bsz=32, num_updates=2190, lr=2.74e-05, gnorm=3.813, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=282
2022-01-10 00:25:24 | INFO | train_inner | epoch 023:     30 / 99 loss=0.573, ppl=1.49, wps=10245.7, ups=14.92, wpb=686.9, bsz=32, num_updates=2200, lr=2.73846e-05, gnorm=3.857, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=283
2022-01-10 00:25:25 | INFO | train_inner | epoch 023:     40 / 99 loss=0.614, ppl=1.53, wps=8713.8, ups=13.96, wpb=624.3, bsz=32, num_updates=2210, lr=2.73692e-05, gnorm=4.775, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=284
2022-01-10 00:25:25 | INFO | train_inner | epoch 023:     50 / 99 loss=0.641, ppl=1.56, wps=9168.7, ups=13.74, wpb=667.4, bsz=32, num_updates=2220, lr=2.73538e-05, gnorm=4.513, clip=100, loss_scale=0.5, train_wall=1, gb_free=19.6, wall=284
2022-01-10 00:25:26 | INFO | train_inner | epoch 023:     60 / 99 loss=0.581, ppl=1.5, wps=9547.5, ups=13.13, wpb=727.1, bsz=31.5, num_updates=2230, lr=2.73385e-05, gnorm=3.885, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=285
2022-01-10 00:25:27 | INFO | train_inner | epoch 023:     70 / 99 loss=0.621, ppl=1.54, wps=7950.9, ups=13.16, wpb=604, bsz=32, num_updates=2240, lr=2.73231e-05, gnorm=4.357, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=286
2022-01-10 00:25:28 | INFO | train_inner | epoch 023:     80 / 99 loss=0.647, ppl=1.57, wps=6831, ups=12.88, wpb=530.3, bsz=32, num_updates=2250, lr=2.73077e-05, gnorm=11.275, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=287
2022-01-10 00:25:28 | INFO | train_inner | epoch 023:     90 / 99 loss=0.625, ppl=1.54, wps=8231, ups=12.24, wpb=672.5, bsz=32, num_updates=2260, lr=2.72923e-05, gnorm=4.046, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=287
2022-01-10 00:25:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:25:30 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 0.866 | ppl 1.82 | wps 20267 | wpb 606.6 | bsz 31.3 | num_updates 2269 | best_loss 0.866
2022-01-10 00:25:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2269 updates
2022-01-10 00:25:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:25:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:25:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 23 @ 2269 updates, score 0.866) (writing took 4.1919484710088 seconds)
2022-01-10 00:25:34 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-10 00:25:34 | INFO | train | epoch 023 | loss 0.611 | ppl 1.53 | wps 5126.4 | ups 7.87 | wpb 651.7 | bsz 31.9 | num_updates 2269 | lr 2.72785e-05 | gnorm 4.843 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 293
2022-01-10 00:25:34 | INFO | fairseq.trainer | begin training epoch 24
2022-01-10 00:25:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:25:34 | INFO | train_inner | epoch 024:      1 / 99 loss=0.596, ppl=1.51, wps=1119.3, ups=1.66, wpb=672.8, bsz=32, num_updates=2270, lr=2.72769e-05, gnorm=4.166, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=293
2022-01-10 00:25:35 | INFO | train_inner | epoch 024:     11 / 99 loss=0.677, ppl=1.6, wps=8113.2, ups=14.95, wpb=542.6, bsz=32, num_updates=2280, lr=2.72615e-05, gnorm=9.666, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=294
2022-01-10 00:25:36 | INFO | train_inner | epoch 024:     21 / 99 loss=0.596, ppl=1.51, wps=9748.1, ups=13.94, wpb=699.4, bsz=32, num_updates=2290, lr=2.72462e-05, gnorm=3.628, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=295
2022-01-10 00:25:37 | INFO | train_inner | epoch 024:     31 / 99 loss=0.578, ppl=1.49, wps=10090.7, ups=13.5, wpb=747.7, bsz=31.5, num_updates=2300, lr=2.72308e-05, gnorm=3.902, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=296
2022-01-10 00:25:37 | INFO | train_inner | epoch 024:     41 / 99 loss=0.585, ppl=1.5, wps=9402.4, ups=14.21, wpb=661.8, bsz=32, num_updates=2310, lr=2.72154e-05, gnorm=3.9, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.5, wall=296
2022-01-10 00:25:38 | INFO | train_inner | epoch 024:     51 / 99 loss=0.602, ppl=1.52, wps=7061, ups=13.77, wpb=512.7, bsz=32, num_updates=2320, lr=2.72e-05, gnorm=4.476, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=297
2022-01-10 00:25:39 | INFO | train_inner | epoch 024:     61 / 99 loss=0.563, ppl=1.48, wps=8815.1, ups=13.62, wpb=647.3, bsz=32, num_updates=2330, lr=2.71846e-05, gnorm=3.727, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=298
2022-01-10 00:25:39 | INFO | train_inner | epoch 024:     71 / 99 loss=0.591, ppl=1.51, wps=10321.4, ups=14.8, wpb=697.6, bsz=32, num_updates=2340, lr=2.71692e-05, gnorm=3.812, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=298
2022-01-10 00:25:40 | INFO | train_inner | epoch 024:     81 / 99 loss=0.62, ppl=1.54, wps=6635.6, ups=13.74, wpb=483.1, bsz=32, num_updates=2350, lr=2.71538e-05, gnorm=5.444, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=299
2022-01-10 00:25:41 | INFO | train_inner | epoch 024:     91 / 99 loss=0.642, ppl=1.56, wps=9028.3, ups=12.93, wpb=698.2, bsz=32, num_updates=2360, lr=2.71385e-05, gnorm=4.353, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=300
2022-01-10 00:25:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:25:43 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 0.884 | ppl 1.85 | wps 20264.5 | wpb 606.6 | bsz 31.3 | num_updates 2368 | best_loss 0.866
2022-01-10 00:25:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2368 updates
2022-01-10 00:25:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:25:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:25:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 24 @ 2368 updates, score 0.884) (writing took 2.9859846479957923 seconds)
2022-01-10 00:25:46 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-10 00:25:46 | INFO | train | epoch 024 | loss 0.597 | ppl 1.51 | wps 5669.9 | ups 8.7 | wpb 651.7 | bsz 31.9 | num_updates 2368 | lr 2.71262e-05 | gnorm 4.65 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 305
2022-01-10 00:25:46 | INFO | fairseq.trainer | begin training epoch 25
2022-01-10 00:25:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:25:46 | INFO | train_inner | epoch 025:      2 / 99 loss=0.573, ppl=1.49, wps=1462, ups=2.03, wpb=721.8, bsz=32, num_updates=2370, lr=2.71231e-05, gnorm=3.946, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=305
2022-01-10 00:25:47 | INFO | train_inner | epoch 025:     12 / 99 loss=0.598, ppl=1.51, wps=10660.1, ups=14.11, wpb=755.6, bsz=32, num_updates=2380, lr=2.71077e-05, gnorm=3.738, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=306
2022-01-10 00:25:47 | INFO | train_inner | epoch 025:     22 / 99 loss=0.621, ppl=1.54, wps=8738.7, ups=13.77, wpb=634.4, bsz=31.5, num_updates=2390, lr=2.70923e-05, gnorm=5.207, clip=100, loss_scale=0.5, train_wall=1, gb_free=18.6, wall=306
2022-01-10 00:25:48 | INFO | train_inner | epoch 025:     32 / 99 loss=0.67, ppl=1.59, wps=5727.4, ups=13.72, wpb=417.6, bsz=32, num_updates=2400, lr=2.70769e-05, gnorm=6.333, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=307
2022-01-10 00:25:49 | INFO | train_inner | epoch 025:     42 / 99 loss=0.575, ppl=1.49, wps=8142.2, ups=14.07, wpb=578.8, bsz=32, num_updates=2410, lr=2.70615e-05, gnorm=4.593, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=308
2022-01-10 00:25:49 | INFO | train_inner | epoch 025:     52 / 99 loss=0.546, ppl=1.46, wps=9075.9, ups=14.01, wpb=647.7, bsz=32, num_updates=2420, lr=2.70462e-05, gnorm=3.688, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.5, wall=308
2022-01-10 00:25:50 | INFO | train_inner | epoch 025:     62 / 99 loss=0.576, ppl=1.49, wps=9728.8, ups=14.32, wpb=679.3, bsz=32, num_updates=2430, lr=2.70308e-05, gnorm=4.156, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=309
2022-01-10 00:25:51 | INFO | train_inner | epoch 025:     72 / 99 loss=0.593, ppl=1.51, wps=10746.3, ups=13.68, wpb=785.8, bsz=32, num_updates=2440, lr=2.70154e-05, gnorm=3.783, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=310
2022-01-10 00:25:52 | INFO | train_inner | epoch 025:     82 / 99 loss=0.531, ppl=1.44, wps=8512.1, ups=13.57, wpb=627.1, bsz=32, num_updates=2450, lr=2.7e-05, gnorm=3.703, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=311
2022-01-10 00:25:53 | INFO | train_inner | epoch 025:     92 / 99 loss=0.543, ppl=1.46, wps=8263.2, ups=10.43, wpb=791.9, bsz=32, num_updates=2460, lr=2.69846e-05, gnorm=3.293, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=312
2022-01-10 00:25:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:25:54 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 0.883 | ppl 1.84 | wps 21193.9 | wpb 606.6 | bsz 31.3 | num_updates 2467 | best_loss 0.866
2022-01-10 00:25:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2467 updates
2022-01-10 00:25:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:25:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:25:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 25 @ 2467 updates, score 0.883) (writing took 2.99016018898692 seconds)
2022-01-10 00:25:57 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-10 00:25:57 | INFO | train | epoch 025 | loss 0.584 | ppl 1.5 | wps 5578.8 | ups 8.56 | wpb 651.7 | bsz 31.9 | num_updates 2467 | lr 2.69738e-05 | gnorm 4.274 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 316
2022-01-10 00:25:57 | INFO | fairseq.trainer | begin training epoch 26
2022-01-10 00:25:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:25:58 | INFO | train_inner | epoch 026:      3 / 99 loss=0.57, ppl=1.48, wps=1553.4, ups=2.04, wpb=762.7, bsz=32, num_updates=2470, lr=2.69692e-05, gnorm=3.536, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=316
2022-01-10 00:25:58 | INFO | train_inner | epoch 026:     13 / 99 loss=0.616, ppl=1.53, wps=8846.2, ups=15.03, wpb=588.6, bsz=32, num_updates=2480, lr=2.69538e-05, gnorm=4.188, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=317
2022-01-10 00:25:59 | INFO | train_inner | epoch 026:     23 / 99 loss=0.589, ppl=1.5, wps=13597.9, ups=14.04, wpb=968.6, bsz=31.5, num_updates=2490, lr=2.69385e-05, gnorm=3.507, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=318
2022-01-10 00:26:00 | INFO | train_inner | epoch 026:     33 / 99 loss=0.547, ppl=1.46, wps=9827.9, ups=14.21, wpb=691.7, bsz=32, num_updates=2500, lr=2.69231e-05, gnorm=3.874, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=319
2022-01-10 00:26:00 | INFO | train_inner | epoch 026:     43 / 99 loss=0.634, ppl=1.55, wps=5646.6, ups=13.45, wpb=419.7, bsz=32, num_updates=2510, lr=2.69077e-05, gnorm=4.878, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=319
2022-01-10 00:26:01 | INFO | train_inner | epoch 026:     53 / 99 loss=0.553, ppl=1.47, wps=7966, ups=13.46, wpb=592, bsz=32, num_updates=2520, lr=2.68923e-05, gnorm=3.949, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=320
2022-01-10 00:26:02 | INFO | train_inner | epoch 026:     63 / 99 loss=0.544, ppl=1.46, wps=9189.1, ups=14.22, wpb=646.2, bsz=32, num_updates=2530, lr=2.68769e-05, gnorm=3.89, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=321
2022-01-10 00:26:03 | INFO | train_inner | epoch 026:     73 / 99 loss=0.547, ppl=1.46, wps=10358.6, ups=14.16, wpb=731.6, bsz=32, num_updates=2540, lr=2.68615e-05, gnorm=3.708, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=321
2022-01-10 00:26:03 | INFO | train_inner | epoch 026:     83 / 99 loss=0.504, ppl=1.42, wps=7654.3, ups=14.36, wpb=533, bsz=32, num_updates=2550, lr=2.68462e-05, gnorm=12.399, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=322
2022-01-10 00:26:04 | INFO | train_inner | epoch 026:     93 / 99 loss=0.522, ppl=1.44, wps=7727.6, ups=12.13, wpb=637, bsz=32, num_updates=2560, lr=2.68308e-05, gnorm=3.641, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=323
2022-01-10 00:26:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:26:05 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 0.856 | ppl 1.81 | wps 21107.1 | wpb 606.6 | bsz 31.3 | num_updates 2566 | best_loss 0.856
2022-01-10 00:26:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2566 updates
2022-01-10 00:26:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:26:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 26 @ 2566 updates, score 0.856) (writing took 3.860459554940462 seconds)
2022-01-10 00:26:09 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-10 00:26:09 | INFO | train | epoch 026 | loss 0.563 | ppl 1.48 | wps 5334.3 | ups 8.19 | wpb 651.7 | bsz 31.9 | num_updates 2566 | lr 2.68215e-05 | gnorm 4.784 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 328
2022-01-10 00:26:09 | INFO | fairseq.trainer | begin training epoch 27
2022-01-10 00:26:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:26:10 | INFO | train_inner | epoch 027:      4 / 99 loss=0.609, ppl=1.52, wps=1048.1, ups=1.77, wpb=590.7, bsz=32, num_updates=2570, lr=2.68154e-05, gnorm=4.056, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=329
2022-01-10 00:26:10 | INFO | train_inner | epoch 027:     14 / 99 loss=0.491, ppl=1.41, wps=11320.2, ups=15.3, wpb=740, bsz=32, num_updates=2580, lr=2.68e-05, gnorm=3.812, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=329
2022-01-10 00:26:11 | INFO | train_inner | epoch 027:     24 / 99 loss=0.524, ppl=1.44, wps=10983.2, ups=14.98, wpb=733.2, bsz=32, num_updates=2590, lr=2.67846e-05, gnorm=3.8, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=330
2022-01-10 00:26:12 | INFO | train_inner | epoch 027:     34 / 99 loss=0.568, ppl=1.48, wps=8591.7, ups=14.71, wpb=583.9, bsz=32, num_updates=2600, lr=2.67692e-05, gnorm=4.327, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=331
2022-01-10 00:26:12 | INFO | train_inner | epoch 027:     44 / 99 loss=0.545, ppl=1.46, wps=8678, ups=14.26, wpb=608.4, bsz=32, num_updates=2610, lr=2.67538e-05, gnorm=4.012, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=331
2022-01-10 00:26:13 | INFO | train_inner | epoch 027:     54 / 99 loss=0.508, ppl=1.42, wps=11367.6, ups=14.66, wpb=775.5, bsz=31.5, num_updates=2620, lr=2.67385e-05, gnorm=3.619, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=332
2022-01-10 00:26:14 | INFO | train_inner | epoch 027:     64 / 99 loss=0.485, ppl=1.4, wps=8114.9, ups=15.4, wpb=527, bsz=32, num_updates=2630, lr=2.67231e-05, gnorm=6.645, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=333
2022-01-10 00:26:14 | INFO | train_inner | epoch 027:     74 / 99 loss=0.585, ppl=1.5, wps=8769.8, ups=14.38, wpb=609.7, bsz=32, num_updates=2640, lr=2.67077e-05, gnorm=3.943, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=333
2022-01-10 00:26:15 | INFO | train_inner | epoch 027:     84 / 99 loss=0.554, ppl=1.47, wps=9850.1, ups=15.65, wpb=629.3, bsz=32, num_updates=2650, lr=2.66923e-05, gnorm=4.097, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=334
2022-01-10 00:26:16 | INFO | train_inner | epoch 027:     94 / 99 loss=0.545, ppl=1.46, wps=6717.5, ups=11.38, wpb=590.4, bsz=32, num_updates=2660, lr=2.66769e-05, gnorm=3.939, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=335
2022-01-10 00:26:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:26:17 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 0.881 | ppl 1.84 | wps 20591.4 | wpb 606.6 | bsz 31.3 | num_updates 2665 | best_loss 0.856
2022-01-10 00:26:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2665 updates
2022-01-10 00:26:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:26:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 27 @ 2665 updates, score 0.881) (writing took 2.457457166048698 seconds)
2022-01-10 00:26:20 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-10 00:26:20 | INFO | train | epoch 027 | loss 0.531 | ppl 1.44 | wps 6189.5 | ups 9.5 | wpb 651.7 | bsz 31.9 | num_updates 2665 | lr 2.66692e-05 | gnorm 4.158 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 339
2022-01-10 00:26:20 | INFO | fairseq.trainer | begin training epoch 28
2022-01-10 00:26:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:26:20 | INFO | train_inner | epoch 028:      5 / 99 loss=0.515, ppl=1.43, wps=1695.3, ups=2.35, wpb=721.5, bsz=32, num_updates=2670, lr=2.66615e-05, gnorm=3.26, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=339
2022-01-10 00:26:21 | INFO | train_inner | epoch 028:     15 / 99 loss=0.517, ppl=1.43, wps=8333.1, ups=13.62, wpb=611.6, bsz=32, num_updates=2680, lr=2.66462e-05, gnorm=3.805, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=340
2022-01-10 00:26:22 | INFO | train_inner | epoch 028:     25 / 99 loss=0.536, ppl=1.45, wps=9846.9, ups=13.43, wpb=733.4, bsz=32, num_updates=2690, lr=2.66308e-05, gnorm=3.457, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=341
2022-01-10 00:26:22 | INFO | train_inner | epoch 028:     35 / 99 loss=0.548, ppl=1.46, wps=8041.9, ups=13.3, wpb=604.7, bsz=32, num_updates=2700, lr=2.66154e-05, gnorm=20.333, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=341
2022-01-10 00:26:23 | INFO | train_inner | epoch 028:     45 / 99 loss=0.508, ppl=1.42, wps=9422.1, ups=13.79, wpb=683.1, bsz=32, num_updates=2710, lr=2.66e-05, gnorm=3.696, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=342
2022-01-10 00:26:24 | INFO | train_inner | epoch 028:     55 / 99 loss=0.531, ppl=1.44, wps=5450, ups=9.64, wpb=565.6, bsz=32, num_updates=2720, lr=2.65846e-05, gnorm=3.792, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=343
2022-01-10 00:26:25 | INFO | train_inner | epoch 028:     65 / 99 loss=0.545, ppl=1.46, wps=8455.1, ups=10.82, wpb=781.1, bsz=31.5, num_updates=2730, lr=2.65692e-05, gnorm=3.326, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=344
2022-01-10 00:26:26 | INFO | train_inner | epoch 028:     75 / 99 loss=0.557, ppl=1.47, wps=7337.3, ups=14.54, wpb=504.5, bsz=32, num_updates=2740, lr=2.65538e-05, gnorm=4.437, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=345
2022-01-10 00:26:27 | INFO | train_inner | epoch 028:     85 / 99 loss=0.533, ppl=1.45, wps=9157.3, ups=14.54, wpb=629.8, bsz=32, num_updates=2750, lr=2.65385e-05, gnorm=4.108, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=346
2022-01-10 00:26:28 | INFO | train_inner | epoch 028:     95 / 99 loss=0.524, ppl=1.44, wps=7252.3, ups=10.1, wpb=717.8, bsz=32, num_updates=2760, lr=2.65231e-05, gnorm=3.877, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=347
2022-01-10 00:26:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:26:29 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 0.883 | ppl 1.84 | wps 20297.7 | wpb 606.6 | bsz 31.3 | num_updates 2764 | best_loss 0.856
2022-01-10 00:26:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2764 updates
2022-01-10 00:26:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:26:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:26:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 28 @ 2764 updates, score 0.883) (writing took 3.3050362340873107 seconds)
2022-01-10 00:26:32 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-10 00:26:32 | INFO | train | epoch 028 | loss 0.529 | ppl 1.44 | wps 5203.3 | ups 7.98 | wpb 651.7 | bsz 31.9 | num_updates 2764 | lr 2.65169e-05 | gnorm 5.446 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.7 | wall 351
2022-01-10 00:26:32 | INFO | fairseq.trainer | begin training epoch 29
2022-01-10 00:26:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:26:33 | INFO | train_inner | epoch 029:      6 / 99 loss=0.488, ppl=1.4, wps=1537.4, ups=1.93, wpb=798, bsz=32, num_updates=2770, lr=2.65077e-05, gnorm=3.196, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=352
2022-01-10 00:26:33 | INFO | train_inner | epoch 029:     16 / 99 loss=0.502, ppl=1.42, wps=9031.5, ups=14.42, wpb=626.4, bsz=32, num_updates=2780, lr=2.64923e-05, gnorm=16.842, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=352
2022-01-10 00:26:34 | INFO | train_inner | epoch 029:     26 / 99 loss=0.454, ppl=1.37, wps=9301.9, ups=13.53, wpb=687.6, bsz=32, num_updates=2790, lr=2.64769e-05, gnorm=3.458, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=353
2022-01-10 00:26:35 | INFO | train_inner | epoch 029:     36 / 99 loss=0.511, ppl=1.43, wps=10182.1, ups=13.04, wpb=781, bsz=32, num_updates=2800, lr=2.64615e-05, gnorm=3.524, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=354
2022-01-10 00:26:36 | INFO | train_inner | epoch 029:     46 / 99 loss=0.589, ppl=1.5, wps=8029.5, ups=14.26, wpb=563.2, bsz=32, num_updates=2810, lr=2.64462e-05, gnorm=4.012, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=355
2022-01-10 00:26:36 | INFO | train_inner | epoch 029:     56 / 99 loss=0.486, ppl=1.4, wps=7850.5, ups=14, wpb=560.7, bsz=32, num_updates=2820, lr=2.64308e-05, gnorm=3.696, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=355
2022-01-10 00:26:37 | INFO | train_inner | epoch 029:     66 / 99 loss=0.493, ppl=1.41, wps=9183.1, ups=12.18, wpb=754, bsz=31.5, num_updates=2830, lr=2.64154e-05, gnorm=4.123, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=356
2022-01-10 00:26:38 | INFO | train_inner | epoch 029:     76 / 99 loss=0.477, ppl=1.39, wps=5967.2, ups=11.45, wpb=521, bsz=32, num_updates=2840, lr=2.64e-05, gnorm=3.815, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=357
2022-01-10 00:26:39 | INFO | train_inner | epoch 029:     86 / 99 loss=0.527, ppl=1.44, wps=10135.3, ups=13.7, wpb=739.9, bsz=32, num_updates=2850, lr=2.63846e-05, gnorm=3.587, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=358
2022-01-10 00:26:40 | INFO | train_inner | epoch 029:     96 / 99 loss=0.575, ppl=1.49, wps=6078.3, ups=10.68, wpb=569, bsz=32, num_updates=2860, lr=2.63692e-05, gnorm=4.945, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=359
2022-01-10 00:26:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:26:41 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 0.846 | ppl 1.8 | wps 20899.3 | wpb 606.6 | bsz 31.3 | num_updates 2863 | best_loss 0.846
2022-01-10 00:26:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2863 updates
2022-01-10 00:26:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:26:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:26:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 29 @ 2863 updates, score 0.846) (writing took 3.8781370180658996 seconds)
2022-01-10 00:26:45 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-10 00:26:45 | INFO | train | epoch 029 | loss 0.508 | ppl 1.42 | wps 5072.4 | ups 7.78 | wpb 651.7 | bsz 31.9 | num_updates 2863 | lr 2.63646e-05 | gnorm 5.143 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 364
2022-01-10 00:26:45 | INFO | fairseq.trainer | begin training epoch 30
2022-01-10 00:26:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:26:46 | INFO | train_inner | epoch 030:      7 / 99 loss=0.443, ppl=1.36, wps=1000.2, ups=1.74, wpb=574.4, bsz=32, num_updates=2870, lr=2.63538e-05, gnorm=3.566, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=364
2022-01-10 00:26:46 | INFO | train_inner | epoch 030:     17 / 99 loss=0.475, ppl=1.39, wps=7699.8, ups=12.34, wpb=624, bsz=32, num_updates=2880, lr=2.63385e-05, gnorm=3.971, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=365
2022-01-10 00:26:47 | INFO | train_inner | epoch 030:     27 / 99 loss=0.555, ppl=1.47, wps=9392.7, ups=14.17, wpb=663, bsz=32, num_updates=2890, lr=2.63231e-05, gnorm=3.999, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=366
2022-01-10 00:26:48 | INFO | train_inner | epoch 030:     37 / 99 loss=0.473, ppl=1.39, wps=6557, ups=12.49, wpb=525, bsz=32, num_updates=2900, lr=2.63077e-05, gnorm=9.214, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=367
2022-01-10 00:26:49 | INFO | train_inner | epoch 030:     47 / 99 loss=0.499, ppl=1.41, wps=10807.9, ups=13.43, wpb=804.5, bsz=32, num_updates=2910, lr=2.62923e-05, gnorm=5.58, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=368
2022-01-10 00:26:49 | INFO | train_inner | epoch 030:     57 / 99 loss=0.478, ppl=1.39, wps=7849.4, ups=11.6, wpb=676.8, bsz=32, num_updates=2920, lr=2.62769e-05, gnorm=3.938, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=368
2022-01-10 00:26:50 | INFO | train_inner | epoch 030:     67 / 99 loss=0.529, ppl=1.44, wps=7804.3, ups=10.61, wpb=735.9, bsz=31.5, num_updates=2930, lr=2.62615e-05, gnorm=3.609, clip=100, loss_scale=0.5, train_wall=1, gb_free=18.6, wall=369
2022-01-10 00:26:51 | INFO | train_inner | epoch 030:     77 / 99 loss=0.496, ppl=1.41, wps=6604.2, ups=12.81, wpb=515.5, bsz=32, num_updates=2940, lr=2.62462e-05, gnorm=4.083, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=370
2022-01-10 00:26:52 | INFO | train_inner | epoch 030:     87 / 99 loss=0.492, ppl=1.41, wps=8785.6, ups=12.7, wpb=692, bsz=32, num_updates=2950, lr=2.62308e-05, gnorm=3.429, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=371
2022-01-10 00:26:53 | INFO | train_inner | epoch 030:     97 / 99 loss=0.523, ppl=1.44, wps=8710.1, ups=13.46, wpb=647, bsz=32, num_updates=2960, lr=2.62154e-05, gnorm=3.603, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=372
2022-01-10 00:26:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:26:54 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 0.854 | ppl 1.81 | wps 20693.5 | wpb 606.6 | bsz 31.3 | num_updates 2962 | best_loss 0.846
2022-01-10 00:26:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2962 updates
2022-01-10 00:26:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:26:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:26:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 30 @ 2962 updates, score 0.854) (writing took 2.658165524015203 seconds)
2022-01-10 00:26:56 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-10 00:26:56 | INFO | train | epoch 030 | loss 0.499 | ppl 1.41 | wps 5595 | ups 8.59 | wpb 651.7 | bsz 31.9 | num_updates 2962 | lr 2.62123e-05 | gnorm 4.521 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.4 | wall 375
2022-01-10 00:26:56 | INFO | fairseq.trainer | begin training epoch 31
2022-01-10 00:26:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:26:57 | INFO | train_inner | epoch 031:      8 / 99 loss=0.482, ppl=1.4, wps=1545.5, ups=2.29, wpb=674.6, bsz=32, num_updates=2970, lr=2.62e-05, gnorm=3.565, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=376
2022-01-10 00:26:58 | INFO | train_inner | epoch 031:     18 / 99 loss=0.481, ppl=1.4, wps=8623, ups=14.87, wpb=579.9, bsz=32, num_updates=2980, lr=2.61846e-05, gnorm=4.014, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=377
2022-01-10 00:26:58 | INFO | train_inner | epoch 031:     28 / 99 loss=0.489, ppl=1.4, wps=8155.9, ups=14.9, wpb=547.2, bsz=32, num_updates=2990, lr=2.61692e-05, gnorm=4.075, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=377
2022-01-10 00:26:59 | INFO | train_inner | epoch 031:     38 / 99 loss=0.498, ppl=1.41, wps=6752.8, ups=13.29, wpb=508, bsz=32, num_updates=3000, lr=2.61538e-05, gnorm=7.263, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=378
2022-01-10 00:27:00 | INFO | train_inner | epoch 031:     48 / 99 loss=0.48, ppl=1.39, wps=8898.3, ups=12.24, wpb=727.2, bsz=32, num_updates=3010, lr=2.61385e-05, gnorm=3.639, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=379
2022-01-10 00:27:01 | INFO | train_inner | epoch 031:     58 / 99 loss=0.495, ppl=1.41, wps=10031, ups=13.79, wpb=727.6, bsz=32, num_updates=3020, lr=2.61231e-05, gnorm=3.322, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=380
2022-01-10 00:27:02 | INFO | train_inner | epoch 031:     68 / 99 loss=0.502, ppl=1.42, wps=11549.4, ups=12.1, wpb=954.5, bsz=31.5, num_updates=3030, lr=2.61077e-05, gnorm=3.229, clip=100, loss_scale=0.5, train_wall=1, gb_free=20, wall=381
2022-01-10 00:27:02 | INFO | train_inner | epoch 031:     78 / 99 loss=0.455, ppl=1.37, wps=7146.1, ups=12.91, wpb=553.5, bsz=32, num_updates=3040, lr=2.60923e-05, gnorm=3.4, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=381
2022-01-10 00:27:03 | INFO | train_inner | epoch 031:     88 / 99 loss=0.485, ppl=1.4, wps=8141, ups=10.61, wpb=767.4, bsz=32, num_updates=3050, lr=2.60769e-05, gnorm=3.666, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=382
2022-01-10 00:27:04 | INFO | train_inner | epoch 031:     98 / 99 loss=0.483, ppl=1.4, wps=5552.7, ups=10.7, wpb=518.8, bsz=32, num_updates=3060, lr=2.60615e-05, gnorm=3.81, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=383
2022-01-10 00:27:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:27:05 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 0.841 | ppl 1.79 | wps 20804.7 | wpb 606.6 | bsz 31.3 | num_updates 3061 | best_loss 0.841
2022-01-10 00:27:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 3061 updates
2022-01-10 00:27:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:27:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 31 @ 3061 updates, score 0.841) (writing took 4.102511539007537 seconds)
2022-01-10 00:27:09 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-10 00:27:09 | INFO | train | epoch 031 | loss 0.487 | ppl 1.4 | wps 5008.5 | ups 7.69 | wpb 651.7 | bsz 31.9 | num_updates 3061 | lr 2.606e-05 | gnorm 3.999 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 388
2022-01-10 00:27:09 | INFO | fairseq.trainer | begin training epoch 32
2022-01-10 00:27:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:27:10 | INFO | train_inner | epoch 032:      9 / 99 loss=0.43, ppl=1.35, wps=916.9, ups=1.7, wpb=540.8, bsz=32, num_updates=3070, lr=2.60462e-05, gnorm=4.106, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=389
2022-01-10 00:27:11 | INFO | train_inner | epoch 032:     19 / 99 loss=0.468, ppl=1.38, wps=9372.3, ups=14.72, wpb=636.5, bsz=32, num_updates=3080, lr=2.60308e-05, gnorm=3.544, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=390
2022-01-10 00:27:12 | INFO | train_inner | epoch 032:     29 / 99 loss=0.484, ppl=1.4, wps=8329.6, ups=14.08, wpb=591.7, bsz=32, num_updates=3090, lr=2.60154e-05, gnorm=3.729, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=390
2022-01-10 00:27:12 | INFO | train_inner | epoch 032:     39 / 99 loss=0.507, ppl=1.42, wps=8885.7, ups=12.06, wpb=736.9, bsz=31.5, num_updates=3100, lr=2.6e-05, gnorm=3.639, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=391
2022-01-10 00:27:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2022-01-10 00:27:13 | INFO | train_inner | epoch 032:     50 / 99 loss=0.503, ppl=1.42, wps=7375.7, ups=12.38, wpb=595.8, bsz=32, num_updates=3110, lr=2.59846e-05, gnorm=4.372, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=392
2022-01-10 00:27:14 | INFO | train_inner | epoch 032:     60 / 99 loss=0.418, ppl=1.34, wps=8734.3, ups=15.66, wpb=557.7, bsz=32, num_updates=3120, lr=2.59692e-05, gnorm=3.932, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=393
2022-01-10 00:27:15 | INFO | train_inner | epoch 032:     70 / 99 loss=0.484, ppl=1.4, wps=10158.1, ups=12.1, wpb=839.8, bsz=32, num_updates=3130, lr=2.59538e-05, gnorm=5.528, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=394
2022-01-10 00:27:15 | INFO | train_inner | epoch 032:     80 / 99 loss=0.457, ppl=1.37, wps=7995.8, ups=13.3, wpb=601, bsz=32, num_updates=3140, lr=2.59385e-05, gnorm=3.543, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=394
2022-01-10 00:27:16 | INFO | train_inner | epoch 032:     90 / 99 loss=0.451, ppl=1.37, wps=8529.6, ups=11.91, wpb=716.1, bsz=32, num_updates=3150, lr=2.59231e-05, gnorm=3.155, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=395
2022-01-10 00:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:27:18 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 0.841 | ppl 1.79 | wps 21333.3 | wpb 606.6 | bsz 31.3 | num_updates 3159 | best_loss 0.841
2022-01-10 00:27:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3159 updates
2022-01-10 00:27:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:27:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_best.pt
2022-01-10 00:27:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_best.pt (epoch 32 @ 3159 updates, score 0.841) (writing took 3.93363829201553 seconds)
2022-01-10 00:27:22 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-10 00:27:22 | INFO | train | epoch 032 | loss 0.473 | ppl 1.39 | wps 5069.8 | ups 7.78 | wpb 651.8 | bsz 31.9 | num_updates 3159 | lr 2.59092e-05 | gnorm 4.112 | clip 100 | loss_scale 0.25 | train_wall 7 | gb_free 20.8 | wall 401
2022-01-10 00:27:22 | INFO | fairseq.trainer | begin training epoch 33
2022-01-10 00:27:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:27:22 | INFO | train_inner | epoch 033:      1 / 99 loss=0.509, ppl=1.42, wps=1121.8, ups=1.7, wpb=658, bsz=32, num_updates=3160, lr=2.59077e-05, gnorm=5.478, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=401
2022-01-10 00:27:23 | INFO | train_inner | epoch 033:     11 / 99 loss=0.428, ppl=1.35, wps=5671.6, ups=11.82, wpb=479.7, bsz=32, num_updates=3170, lr=2.58923e-05, gnorm=3.942, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=402
2022-01-10 00:27:24 | INFO | train_inner | epoch 033:     21 / 99 loss=0.433, ppl=1.35, wps=11184.3, ups=13.4, wpb=834.4, bsz=31.5, num_updates=3180, lr=2.58769e-05, gnorm=3.35, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=403
2022-01-10 00:27:24 | INFO | train_inner | epoch 033:     31 / 99 loss=0.421, ppl=1.34, wps=8160.5, ups=14.54, wpb=561.2, bsz=32, num_updates=3190, lr=2.58615e-05, gnorm=3.777, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=403
2022-01-10 00:27:25 | INFO | train_inner | epoch 033:     41 / 99 loss=0.474, ppl=1.39, wps=9802.6, ups=12.84, wpb=763.3, bsz=32, num_updates=3200, lr=2.58462e-05, gnorm=3.545, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=404
2022-01-10 00:27:26 | INFO | train_inner | epoch 033:     51 / 99 loss=0.481, ppl=1.4, wps=7477.5, ups=12.03, wpb=621.7, bsz=32, num_updates=3210, lr=2.58308e-05, gnorm=3.753, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=405
2022-01-10 00:27:27 | INFO | train_inner | epoch 033:     61 / 99 loss=0.501, ppl=1.42, wps=5920.6, ups=10.85, wpb=545.8, bsz=32, num_updates=3220, lr=2.58154e-05, gnorm=4.939, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=406
2022-01-10 00:27:28 | INFO | train_inner | epoch 033:     71 / 99 loss=0.408, ppl=1.33, wps=11427.5, ups=15.32, wpb=745.7, bsz=32, num_updates=3230, lr=2.58e-05, gnorm=2.991, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.5, wall=407
2022-01-10 00:27:28 | INFO | train_inner | epoch 033:     81 / 99 loss=0.474, ppl=1.39, wps=9156.8, ups=14.49, wpb=632, bsz=32, num_updates=3240, lr=2.57846e-05, gnorm=3.852, clip=100, loss_scale=0.25, train_wall=1, gb_free=20, wall=407
2022-01-10 00:27:29 | INFO | train_inner | epoch 033:     91 / 99 loss=0.456, ppl=1.37, wps=9702.2, ups=14.75, wpb=657.7, bsz=32, num_updates=3250, lr=2.57692e-05, gnorm=3.324, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=408
2022-01-10 00:27:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:27:31 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 0.845 | ppl 1.8 | wps 21576.2 | wpb 606.6 | bsz 31.3 | num_updates 3258 | best_loss 0.841
2022-01-10 00:27:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3258 updates
2022-01-10 00:27:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:27:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:27:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 33 @ 3258 updates, score 0.845) (writing took 3.1451731639681384 seconds)
2022-01-10 00:27:34 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-10 00:27:34 | INFO | train | epoch 033 | loss 0.456 | ppl 1.37 | wps 5512.1 | ups 8.46 | wpb 651.7 | bsz 31.9 | num_updates 3258 | lr 2.57569e-05 | gnorm 3.701 | clip 100 | loss_scale 0.25 | train_wall 7 | gb_free 20.8 | wall 413
2022-01-10 00:27:34 | INFO | fairseq.trainer | begin training epoch 34
2022-01-10 00:27:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-10 00:27:34 | INFO | train_inner | epoch 034:      2 / 99 loss=0.488, ppl=1.4, wps=1352.3, ups=2.04, wpb=664.3, bsz=32, num_updates=3260, lr=2.57538e-05, gnorm=3.533, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=413
2022-01-10 00:27:35 | INFO | train_inner | epoch 034:     12 / 99 loss=0.41, ppl=1.33, wps=8614, ups=14.03, wpb=613.9, bsz=32, num_updates=3270, lr=2.57385e-05, gnorm=3.798, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=414
2022-01-10 00:27:35 | INFO | train_inner | epoch 034:     22 / 99 loss=0.429, ppl=1.35, wps=9960.7, ups=13.75, wpb=724.6, bsz=31.5, num_updates=3280, lr=2.57231e-05, gnorm=2.989, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=414
2022-01-10 00:27:36 | INFO | train_inner | epoch 034:     32 / 99 loss=0.507, ppl=1.42, wps=11446.7, ups=14.04, wpb=815.2, bsz=32, num_updates=3290, lr=2.57077e-05, gnorm=3.474, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.7, wall=415
2022-01-10 00:27:37 | INFO | train_inner | epoch 034:     42 / 99 loss=0.469, ppl=1.38, wps=9056.7, ups=14.42, wpb=628.1, bsz=32, num_updates=3300, lr=2.56923e-05, gnorm=3.334, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=416
2022-01-10 00:27:37 | INFO | train_inner | epoch 034:     52 / 99 loss=0.445, ppl=1.36, wps=8074.8, ups=15.01, wpb=537.8, bsz=32, num_updates=3310, lr=2.56769e-05, gnorm=3.647, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=416
2022-01-10 00:27:38 | INFO | train_inner | epoch 034:     62 / 99 loss=0.457, ppl=1.37, wps=9690.2, ups=14.37, wpb=674.2, bsz=32, num_updates=3320, lr=2.56615e-05, gnorm=3.893, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=417
2022-01-10 00:27:39 | INFO | train_inner | epoch 034:     72 / 99 loss=0.42, ppl=1.34, wps=9820.4, ups=14.95, wpb=656.8, bsz=32, num_updates=3330, lr=2.56462e-05, gnorm=3.214, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=418
2022-01-10 00:27:39 | INFO | train_inner | epoch 034:     82 / 99 loss=0.455, ppl=1.37, wps=10104.9, ups=15.26, wpb=662.4, bsz=32, num_updates=3340, lr=2.56308e-05, gnorm=3.415, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=418
2022-01-10 00:27:40 | INFO | train_inner | epoch 034:     92 / 99 loss=0.442, ppl=1.36, wps=9669, ups=13.03, wpb=742, bsz=32, num_updates=3350, lr=2.56154e-05, gnorm=3.147, clip=100, loss_scale=0.25, train_wall=1, gb_free=20.8, wall=419
2022-01-10 00:27:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-10 00:27:42 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 0.853 | ppl 1.81 | wps 20443.2 | wpb 606.6 | bsz 31.3 | num_updates 3357 | best_loss 0.841
2022-01-10 00:27:42 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-01-10 00:27:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3357 updates
2022-01-10 00:27:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:27:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/checkpoint_last.pt
2022-01-10 00:27:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/checkpoint_last.pt (epoch 34 @ 3357 updates, score 0.853) (writing took 2.643715179990977 seconds)
2022-01-10 00:27:44 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-10 00:27:44 | INFO | train | epoch 034 | loss 0.455 | ppl 1.37 | wps 6034.9 | ups 9.26 | wpb 651.7 | bsz 31.9 | num_updates 3357 | lr 2.56046e-05 | gnorm 3.638 | clip 100 | loss_scale 0.25 | train_wall 7 | gb_free 20.8 | wall 423
2022-01-10 00:27:44 | INFO | fairseq_cli.train | done training in 419.9 seconds
