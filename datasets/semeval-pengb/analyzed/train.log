2022-01-13 00:03:31 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 10, 'log_format': None, 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 42, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 8192, 'batch_size': 32, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 5000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 12288, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.abst/dev', 'restore_file': 'bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 10, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.1, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=3.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst/dev', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_epoch=15, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='aspect_base_denoising', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, insert=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=10, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', mask=0.1, mask_length='subword', mask_random=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_tokens_valid='12288', max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, permute=0.0, permute_sentences=0.0, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', poisson_lambda=3.0, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, replace_length=1, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt', rotate=0.0, sample_break_mode='eos', save_dir='bartabst/checkpoints/bart.abst/dev', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=42, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='aspect_base_denoising', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='20000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[1], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=5000, wandb_project=None, warmup_epoch=15, warmup_updates=500, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 20000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-13 00:03:31 | INFO | bartabst.tasks.aspect_base_denoising | dictionary: 51200 types
2022-01-13 00:03:34 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-01-13 00:03:34 | INFO | fairseq_cli.train | task: AspectBaseDenoisingTask
2022-01-13 00:03:34 | INFO | fairseq_cli.train | model: BARTMLModel
2022-01-13 00:03:34 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-01-13 00:03:34 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-01-13 00:03:34 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-13 00:03:35 | INFO | bartabst.data.data_utils | loaded 908 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-01-13 00:03:35 | INFO | bartabst.tasks.aspect_base_denoising | Split: valid, Loaded 908 samples of denoising_dataset
2022-01-13 00:03:39 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-01-13 00:03:39 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-13 00:03:39 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-01-13 00:03:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-13 00:03:39 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-01-13 00:03:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-13 00:03:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-13 00:03:39 | INFO | fairseq_cli.train | max tokens per device = 8192 and max sentences per device = 32
2022-01-13 00:03:39 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:03:40 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 70 @ 0 updates)
2022-01-13 00:03:40 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-13 00:03:43 | INFO | bartabst.data.data_utils | loaded 3,174 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-01-13 00:03:43 | INFO | bartabst.tasks.aspect_base_denoising | Split: train, Loaded 3174 samples of denoising_dataset
2022-01-13 00:03:43 | INFO | fairseq.trainer | begin training epoch 1
2022-01-13 00:03:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:03:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-01-13 00:03:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-01-13 00:03:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-01-13 00:03:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-01-13 00:03:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-01-13 00:03:45 | INFO | train_inner | epoch 001:     15 / 100 loss=6.569, ppl=94.91, wps=5626.9, ups=10.08, wpb=523.8, bsz=32, num_updates=10, lr=6e-07, gnorm=27.792, clip=100, loss_scale=4, train_wall=1, gb_free=20.4, wall=6
2022-01-13 00:03:45 | INFO | train_inner | epoch 001:     25 / 100 loss=6.557, ppl=94.17, wps=8947.8, ups=13.37, wpb=669.3, bsz=32, num_updates=20, lr=1.2e-06, gnorm=25.661, clip=100, loss_scale=4, train_wall=1, gb_free=20.8, wall=7
2022-01-13 00:03:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-01-13 00:03:46 | INFO | train_inner | epoch 001:     36 / 100 loss=5.688, ppl=51.57, wps=6790.7, ups=10.62, wpb=639.3, bsz=32, num_updates=30, lr=1.8e-06, gnorm=25.996, clip=100, loss_scale=2, train_wall=1, gb_free=20.2, wall=8
2022-01-13 00:03:47 | INFO | train_inner | epoch 001:     46 / 100 loss=4.755, ppl=26.99, wps=8251.8, ups=14.03, wpb=588, bsz=32, num_updates=40, lr=2.4e-06, gnorm=29.239, clip=100, loss_scale=2, train_wall=1, gb_free=20.5, wall=9
2022-01-13 00:03:48 | INFO | train_inner | epoch 001:     56 / 100 loss=4.105, ppl=17.2, wps=10067.7, ups=14.87, wpb=676.9, bsz=32, num_updates=50, lr=3e-06, gnorm=17.455, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=9
2022-01-13 00:03:48 | INFO | train_inner | epoch 001:     66 / 100 loss=3.621, ppl=12.3, wps=8473, ups=13.79, wpb=614.3, bsz=32, num_updates=60, lr=3.6e-06, gnorm=15.434, clip=100, loss_scale=2, train_wall=1, gb_free=20.7, wall=10
2022-01-13 00:03:49 | INFO | train_inner | epoch 001:     76 / 100 loss=3.311, ppl=9.93, wps=8366.5, ups=14.03, wpb=596.4, bsz=29.4, num_updates=70, lr=4.2e-06, gnorm=14.926, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=11
2022-01-13 00:03:50 | INFO | train_inner | epoch 001:     86 / 100 loss=2.92, ppl=7.57, wps=9132, ups=13.3, wpb=686.6, bsz=32, num_updates=80, lr=4.8e-06, gnorm=14.21, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=11
2022-01-13 00:03:51 | INFO | train_inner | epoch 001:     96 / 100 loss=2.869, ppl=7.31, wps=7174.1, ups=9.84, wpb=728.8, bsz=32, num_updates=90, lr=5.4e-06, gnorm=20.591, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=12
2022-01-13 00:03:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:52 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.996 | ppl 3.99 | wps 19452.6 | wpb 607.1 | bsz 31.3 | num_updates 94
2022-01-13 00:03:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 94 updates
2022-01-13 00:03:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:03:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:03:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 1 @ 94 updates, score 1.996) (writing took 6.919160688994452 seconds)
2022-01-13 00:03:59 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-13 00:03:59 | INFO | train | epoch 001 | loss 4.354 | ppl 20.44 | wps 3821.8 | ups 5.98 | wpb 634.4 | bsz 31.7 | num_updates 94 | lr 5.64e-06 | gnorm 20.881 | clip 100 | loss_scale 2 | train_wall 8 | gb_free 20.8 | wall 21
2022-01-13 00:03:59 | INFO | fairseq.trainer | begin training epoch 2
2022-01-13 00:03:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:04:00 | INFO | train_inner | epoch 002:      6 / 100 loss=2.457, ppl=5.49, wps=731.4, ups=1.13, wpb=646.5, bsz=32, num_updates=100, lr=6e-06, gnorm=10.605, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=21
2022-01-13 00:04:01 | INFO | train_inner | epoch 002:     16 / 100 loss=2.765, ppl=6.8, wps=7597, ups=14.13, wpb=537.7, bsz=29.4, num_updates=110, lr=6.6e-06, gnorm=26.549, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=22
2022-01-13 00:04:01 | INFO | train_inner | epoch 002:     26 / 100 loss=2.333, ppl=5.04, wps=11355.3, ups=15.02, wpb=755.9, bsz=32, num_updates=120, lr=7.2e-06, gnorm=10.074, clip=100, loss_scale=2, train_wall=1, gb_free=20.4, wall=23
2022-01-13 00:04:02 | INFO | train_inner | epoch 002:     36 / 100 loss=2.071, ppl=4.2, wps=10298.9, ups=12.88, wpb=799.9, bsz=32, num_updates=130, lr=7.8e-06, gnorm=12.079, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=23
2022-01-13 00:04:03 | INFO | train_inner | epoch 002:     46 / 100 loss=2.411, ppl=5.32, wps=8991.5, ups=14.24, wpb=631.3, bsz=32, num_updates=140, lr=8.4e-06, gnorm=12.931, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=24
2022-01-13 00:04:03 | INFO | train_inner | epoch 002:     56 / 100 loss=2.383, ppl=5.22, wps=7821, ups=15.62, wpb=500.7, bsz=32, num_updates=150, lr=9e-06, gnorm=13.176, clip=100, loss_scale=2, train_wall=1, gb_free=20.6, wall=25
2022-01-13 00:04:04 | INFO | train_inner | epoch 002:     66 / 100 loss=2.255, ppl=4.77, wps=7655.7, ups=13.92, wpb=549.8, bsz=32, num_updates=160, lr=9.6e-06, gnorm=18.999, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=25
2022-01-13 00:04:05 | INFO | train_inner | epoch 002:     76 / 100 loss=1.929, ppl=3.81, wps=9378.5, ups=12.43, wpb=754.5, bsz=32, num_updates=170, lr=1.02e-05, gnorm=10.277, clip=100, loss_scale=2, train_wall=1, gb_free=20.7, wall=26
2022-01-13 00:04:06 | INFO | train_inner | epoch 002:     86 / 100 loss=2.283, ppl=4.87, wps=7330.2, ups=12.94, wpb=566.3, bsz=32, num_updates=180, lr=1.08e-05, gnorm=10.303, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=27
2022-01-13 00:04:07 | INFO | train_inner | epoch 002:     96 / 100 loss=1.889, ppl=3.7, wps=8833.3, ups=11.47, wpb=769.9, bsz=32, num_updates=190, lr=1.14e-05, gnorm=16.501, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=28
2022-01-13 00:04:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:04:08 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 1.588 | ppl 3.01 | wps 19798.9 | wpb 607.1 | bsz 31.3 | num_updates 194 | best_loss 1.588
2022-01-13 00:04:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 194 updates
2022-01-13 00:04:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:04:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:04:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 2 @ 194 updates, score 1.588) (writing took 4.022444445174187 seconds)
2022-01-13 00:04:12 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-13 00:04:12 | INFO | train | epoch 002 | loss 2.235 | ppl 4.71 | wps 5170.5 | ups 7.98 | wpb 648.2 | bsz 31.7 | num_updates 194 | lr 1.164e-05 | gnorm 13.989 | clip 100 | loss_scale 2 | train_wall 7 | gb_free 20.8 | wall 33
2022-01-13 00:04:12 | INFO | fairseq.trainer | begin training epoch 3
2022-01-13 00:04:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:04:12 | INFO | train_inner | epoch 003:      6 / 100 loss=2.2, ppl=4.59, wps=849.6, ups=1.7, wpb=498.7, bsz=29.4, num_updates=200, lr=1.2e-05, gnorm=10.85, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=34
2022-01-13 00:04:13 | INFO | train_inner | epoch 003:     16 / 100 loss=2.049, ppl=4.14, wps=7634.4, ups=13.19, wpb=578.8, bsz=32, num_updates=210, lr=1.26e-05, gnorm=9.359, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=35
2022-01-13 00:04:14 | INFO | train_inner | epoch 003:     26 / 100 loss=2.071, ppl=4.2, wps=5973.3, ups=11.25, wpb=530.8, bsz=32, num_updates=220, lr=1.32e-05, gnorm=11.14, clip=100, loss_scale=2, train_wall=1, gb_free=20.8, wall=35
2022-01-13 00:04:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2022-01-13 00:04:15 | INFO | train_inner | epoch 003:     37 / 100 loss=1.903, ppl=3.74, wps=7664.6, ups=11.32, wpb=677, bsz=32, num_updates=230, lr=1.38e-05, gnorm=8.609, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=36
2022-01-13 00:04:16 | INFO | train_inner | epoch 003:     47 / 100 loss=1.749, ppl=3.36, wps=7125.8, ups=10.42, wpb=684, bsz=32, num_updates=240, lr=1.44e-05, gnorm=11.893, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=37
2022-01-13 00:04:17 | INFO | train_inner | epoch 003:     57 / 100 loss=1.873, ppl=3.66, wps=5538.9, ups=8.91, wpb=621.9, bsz=32, num_updates=250, lr=1.5e-05, gnorm=7.415, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=38
2022-01-13 00:04:18 | INFO | train_inner | epoch 003:     67 / 100 loss=1.813, ppl=3.51, wps=7003.3, ups=11.78, wpb=594.3, bsz=32, num_updates=260, lr=1.56e-05, gnorm=8.684, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=39
2022-01-13 00:04:19 | INFO | train_inner | epoch 003:     77 / 100 loss=1.439, ppl=2.71, wps=12987.7, ups=12.71, wpb=1021.9, bsz=32, num_updates=270, lr=1.62e-05, gnorm=6.411, clip=100, loss_scale=1, train_wall=1, gb_free=20.3, wall=40
2022-01-13 00:04:19 | INFO | train_inner | epoch 003:     87 / 100 loss=1.961, ppl=3.89, wps=7435.9, ups=12.72, wpb=584.5, bsz=32, num_updates=280, lr=1.68e-05, gnorm=13.552, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=41
2022-01-13 00:04:20 | INFO | train_inner | epoch 003:     97 / 100 loss=1.681, ppl=3.21, wps=8040.9, ups=11.96, wpb=672.1, bsz=32, num_updates=290, lr=1.74e-05, gnorm=6.58, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=42
2022-01-13 00:04:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:04:21 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 1.469 | ppl 2.77 | wps 20291 | wpb 607.1 | bsz 31.3 | num_updates 293 | best_loss 1.469
2022-01-13 00:04:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 293 updates
2022-01-13 00:04:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:04:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:04:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 3 @ 293 updates, score 1.469) (writing took 4.255831663031131 seconds)
2022-01-13 00:04:26 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-13 00:04:26 | INFO | train | epoch 003 | loss 1.824 | ppl 3.54 | wps 4611.6 | ups 7.13 | wpb 647 | bsz 31.7 | num_updates 293 | lr 1.758e-05 | gnorm 9.449 | clip 100 | loss_scale 1 | train_wall 8 | gb_free 20.7 | wall 47
2022-01-13 00:04:26 | INFO | fairseq.trainer | begin training epoch 4
2022-01-13 00:04:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:04:26 | INFO | train_inner | epoch 004:      7 / 100 loss=2.055, ppl=4.16, wps=816.2, ups=1.64, wpb=499.2, bsz=32, num_updates=300, lr=1.8e-05, gnorm=7.725, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=48
2022-01-13 00:04:27 | INFO | train_inner | epoch 004:     17 / 100 loss=1.543, ppl=2.91, wps=10838.6, ups=14.02, wpb=773, bsz=32, num_updates=310, lr=1.86e-05, gnorm=5.863, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=49
2022-01-13 00:04:28 | INFO | train_inner | epoch 004:     27 / 100 loss=1.78, ppl=3.43, wps=8407.2, ups=14.32, wpb=587.1, bsz=32, num_updates=320, lr=1.92e-05, gnorm=9.244, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=49
2022-01-13 00:04:29 | INFO | train_inner | epoch 004:     37 / 100 loss=1.519, ppl=2.87, wps=10375.2, ups=13.83, wpb=750, bsz=32, num_updates=330, lr=1.98e-05, gnorm=6.635, clip=100, loss_scale=1, train_wall=1, gb_free=19.9, wall=50
2022-01-13 00:04:29 | INFO | train_inner | epoch 004:     47 / 100 loss=1.861, ppl=3.63, wps=7053.1, ups=14.98, wpb=470.9, bsz=32, num_updates=340, lr=2.04e-05, gnorm=8.099, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=51
2022-01-13 00:04:30 | INFO | train_inner | epoch 004:     57 / 100 loss=1.812, ppl=3.51, wps=7938.1, ups=13.9, wpb=571.1, bsz=32, num_updates=350, lr=2.1e-05, gnorm=8.689, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=51
2022-01-13 00:04:31 | INFO | train_inner | epoch 004:     67 / 100 loss=1.341, ppl=2.53, wps=11183.6, ups=14.77, wpb=757.4, bsz=29.4, num_updates=360, lr=2.16e-05, gnorm=5.21, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=52
2022-01-13 00:04:31 | INFO | train_inner | epoch 004:     77 / 100 loss=1.489, ppl=2.81, wps=9395.5, ups=12.74, wpb=737.5, bsz=32, num_updates=370, lr=2.22e-05, gnorm=6.531, clip=100, loss_scale=1, train_wall=1, gb_free=20.4, wall=53
2022-01-13 00:04:32 | INFO | train_inner | epoch 004:     87 / 100 loss=1.498, ppl=2.82, wps=9436.1, ups=13.07, wpb=721.9, bsz=32, num_updates=380, lr=2.28e-05, gnorm=6.51, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=54
2022-01-13 00:04:33 | INFO | train_inner | epoch 004:     97 / 100 loss=1.596, ppl=3.02, wps=6551.5, ups=11.52, wpb=568.9, bsz=32, num_updates=390, lr=2.34e-05, gnorm=7.099, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=54
2022-01-13 00:04:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:04:34 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.36 | ppl 2.57 | wps 19913 | wpb 607.1 | bsz 31.3 | num_updates 393 | best_loss 1.36
2022-01-13 00:04:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 393 updates
2022-01-13 00:04:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:04:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:04:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 4 @ 393 updates, score 1.36) (writing took 4.816271574003622 seconds)
2022-01-13 00:04:39 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-13 00:04:39 | INFO | train | epoch 004 | loss 1.612 | ppl 3.06 | wps 4854.8 | ups 7.49 | wpb 648.2 | bsz 31.7 | num_updates 393 | lr 2.358e-05 | gnorm 7.082 | clip 100 | loss_scale 1 | train_wall 7 | gb_free 20.8 | wall 61
2022-01-13 00:04:39 | INFO | fairseq.trainer | begin training epoch 5
2022-01-13 00:04:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:04:40 | INFO | train_inner | epoch 005:      7 / 100 loss=1.5, ppl=2.83, wps=1073.3, ups=1.48, wpb=724.6, bsz=32, num_updates=400, lr=2.4e-05, gnorm=6.391, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=61
2022-01-13 00:04:40 | INFO | train_inner | epoch 005:     17 / 100 loss=1.42, ppl=2.68, wps=11048.3, ups=14.84, wpb=744.6, bsz=32, num_updates=410, lr=2.46e-05, gnorm=6.884, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=62
2022-01-13 00:04:41 | INFO | train_inner | epoch 005:     27 / 100 loss=1.511, ppl=2.85, wps=8181.3, ups=12.98, wpb=630.3, bsz=32, num_updates=420, lr=2.52e-05, gnorm=9.063, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=63
2022-01-13 00:04:42 | INFO | train_inner | epoch 005:     37 / 100 loss=1.597, ppl=3.03, wps=10256.3, ups=14.95, wpb=685.9, bsz=32, num_updates=430, lr=2.58e-05, gnorm=7.057, clip=100, loss_scale=1, train_wall=1, gb_free=20.4, wall=63
2022-01-13 00:04:43 | INFO | train_inner | epoch 005:     47 / 100 loss=1.589, ppl=3.01, wps=9292.6, ups=14.91, wpb=623.1, bsz=32, num_updates=440, lr=2.64e-05, gnorm=24.157, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=64
2022-01-13 00:04:43 | INFO | train_inner | epoch 005:     57 / 100 loss=1.732, ppl=3.32, wps=7010.3, ups=13.24, wpb=529.3, bsz=32, num_updates=450, lr=2.7e-05, gnorm=7.161, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=65
2022-01-13 00:04:44 | INFO | train_inner | epoch 005:     67 / 100 loss=1.533, ppl=2.89, wps=7416.2, ups=13.21, wpb=561.2, bsz=32, num_updates=460, lr=2.76e-05, gnorm=6.214, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=66
2022-01-13 00:04:45 | INFO | train_inner | epoch 005:     77 / 100 loss=1.637, ppl=3.11, wps=7347.4, ups=13.53, wpb=543, bsz=32, num_updates=470, lr=2.82e-05, gnorm=7.628, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=66
2022-01-13 00:04:46 | INFO | train_inner | epoch 005:     87 / 100 loss=1.33, ppl=2.51, wps=10318.7, ups=13.85, wpb=745.2, bsz=32, num_updates=480, lr=2.88e-05, gnorm=5.151, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=67
2022-01-13 00:04:47 | INFO | train_inner | epoch 005:     97 / 100 loss=1.29, ppl=2.45, wps=7964.2, ups=10.47, wpb=761, bsz=29.4, num_updates=490, lr=2.94e-05, gnorm=9.007, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=68
2022-01-13 00:04:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:04:48 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.262 | ppl 2.4 | wps 21474.3 | wpb 607.1 | bsz 31.3 | num_updates 493 | best_loss 1.262
2022-01-13 00:04:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 493 updates
2022-01-13 00:04:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:04:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 5 @ 493 updates, score 1.262) (writing took 5.840844612102956 seconds)
2022-01-13 00:04:54 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-13 00:04:54 | INFO | train | epoch 005 | loss 1.5 | ppl 2.83 | wps 4482.9 | ups 6.92 | wpb 648.2 | bsz 31.7 | num_updates 493 | lr 2.958e-05 | gnorm 8.919 | clip 100 | loss_scale 1 | train_wall 7 | gb_free 20.8 | wall 75
2022-01-13 00:04:54 | INFO | fairseq.trainer | begin training epoch 6
2022-01-13 00:04:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:04:54 | INFO | train_inner | epoch 006:      7 / 100 loss=1.534, ppl=2.9, wps=763.3, ups=1.31, wpb=584.9, bsz=32, num_updates=500, lr=3e-05, gnorm=6.277, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=76
2022-01-13 00:04:55 | INFO | train_inner | epoch 006:     17 / 100 loss=1.322, ppl=2.5, wps=10692.6, ups=14.7, wpb=727.6, bsz=32, num_updates=510, lr=2.99846e-05, gnorm=5.78, clip=100, loss_scale=1, train_wall=1, gb_free=19.9, wall=76
2022-01-13 00:04:56 | INFO | train_inner | epoch 006:     27 / 100 loss=1.33, ppl=2.51, wps=11103.6, ups=14.19, wpb=782.7, bsz=32, num_updates=520, lr=2.99692e-05, gnorm=5.421, clip=100, loss_scale=1, train_wall=1, gb_free=18.9, wall=77
2022-01-13 00:04:56 | INFO | train_inner | epoch 006:     37 / 100 loss=1.646, ppl=3.13, wps=6863.6, ups=13.65, wpb=502.8, bsz=32, num_updates=530, lr=2.99538e-05, gnorm=6.696, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=78
2022-01-13 00:04:57 | INFO | train_inner | epoch 006:     47 / 100 loss=1.378, ppl=2.6, wps=8914, ups=14.5, wpb=614.7, bsz=29.4, num_updates=540, lr=2.99385e-05, gnorm=6.734, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=78
2022-01-13 00:04:58 | INFO | train_inner | epoch 006:     57 / 100 loss=1.375, ppl=2.59, wps=9644.7, ups=14.67, wpb=657.6, bsz=32, num_updates=550, lr=2.99231e-05, gnorm=6.842, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=79
2022-01-13 00:04:58 | INFO | train_inner | epoch 006:     67 / 100 loss=1.474, ppl=2.78, wps=8418.4, ups=14.7, wpb=572.5, bsz=32, num_updates=560, lr=2.99077e-05, gnorm=7.406, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=80
2022-01-13 00:04:59 | INFO | train_inner | epoch 006:     77 / 100 loss=1.442, ppl=2.72, wps=9319.2, ups=14.58, wpb=639.1, bsz=32, num_updates=570, lr=2.98923e-05, gnorm=23.341, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=81
2022-01-13 00:05:00 | INFO | train_inner | epoch 006:     87 / 100 loss=1.404, ppl=2.65, wps=8519.4, ups=13.41, wpb=635.3, bsz=32, num_updates=580, lr=2.98769e-05, gnorm=13.524, clip=100, loss_scale=1, train_wall=1, gb_free=20.5, wall=81
2022-01-13 00:05:01 | INFO | train_inner | epoch 006:     97 / 100 loss=1.24, ppl=2.36, wps=8497.7, ups=11.75, wpb=723.2, bsz=32, num_updates=590, lr=2.98615e-05, gnorm=7.117, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=82
2022-01-13 00:05:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:05:02 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 1.206 | ppl 2.31 | wps 20631.7 | wpb 607.1 | bsz 31.3 | num_updates 593 | best_loss 1.206
2022-01-13 00:05:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 593 updates
2022-01-13 00:05:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:05:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:05:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 6 @ 593 updates, score 1.206) (writing took 4.441692718071863 seconds)
2022-01-13 00:05:06 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-13 00:05:06 | INFO | train | epoch 006 | loss 1.394 | ppl 2.63 | wps 5076.1 | ups 7.83 | wpb 648.2 | bsz 31.7 | num_updates 593 | lr 2.98569e-05 | gnorm 8.915 | clip 100 | loss_scale 1 | train_wall 7 | gb_free 20.8 | wall 88
2022-01-13 00:05:06 | INFO | fairseq.trainer | begin training epoch 7
2022-01-13 00:05:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:05:07 | INFO | train_inner | epoch 007:      7 / 100 loss=1.429, ppl=2.69, wps=999.9, ups=1.59, wpb=627.9, bsz=32, num_updates=600, lr=2.98462e-05, gnorm=6.153, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=88
2022-01-13 00:05:08 | INFO | train_inner | epoch 007:     17 / 100 loss=1.333, ppl=2.52, wps=8233.1, ups=14.37, wpb=573.1, bsz=32, num_updates=610, lr=2.98308e-05, gnorm=6.402, clip=100, loss_scale=1, train_wall=1, gb_free=20.3, wall=89
2022-01-13 00:05:08 | INFO | train_inner | epoch 007:     27 / 100 loss=1.277, ppl=2.42, wps=7740.4, ups=14.28, wpb=542.1, bsz=29.4, num_updates=620, lr=2.98154e-05, gnorm=5.777, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=90
2022-01-13 00:05:09 | INFO | train_inner | epoch 007:     37 / 100 loss=1.206, ppl=2.31, wps=9943.8, ups=13.65, wpb=728.4, bsz=32, num_updates=630, lr=2.98e-05, gnorm=5.384, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=91
2022-01-13 00:05:10 | INFO | train_inner | epoch 007:     47 / 100 loss=1.356, ppl=2.56, wps=9206.6, ups=13.15, wpb=700.2, bsz=32, num_updates=640, lr=2.97846e-05, gnorm=5.805, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=91
2022-01-13 00:05:11 | INFO | train_inner | epoch 007:     57 / 100 loss=1.384, ppl=2.61, wps=8065.6, ups=14.13, wpb=570.8, bsz=32, num_updates=650, lr=2.97692e-05, gnorm=6.578, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=92
2022-01-13 00:05:11 | INFO | train_inner | epoch 007:     67 / 100 loss=1.32, ppl=2.5, wps=8206.2, ups=12.08, wpb=679.3, bsz=32, num_updates=660, lr=2.97538e-05, gnorm=6.556, clip=100, loss_scale=1, train_wall=1, gb_free=20.5, wall=93
2022-01-13 00:05:12 | INFO | train_inner | epoch 007:     77 / 100 loss=1.206, ppl=2.31, wps=10616.6, ups=14.39, wpb=737.7, bsz=32, num_updates=670, lr=2.97385e-05, gnorm=5.168, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=94
2022-01-13 00:05:13 | INFO | train_inner | epoch 007:     87 / 100 loss=1.311, ppl=2.48, wps=9861.2, ups=13.71, wpb=719.2, bsz=32, num_updates=680, lr=2.97231e-05, gnorm=5.284, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=94
2022-01-13 00:05:14 | INFO | train_inner | epoch 007:     97 / 100 loss=1.343, ppl=2.54, wps=6327.7, ups=10.34, wpb=611.9, bsz=32, num_updates=690, lr=2.97077e-05, gnorm=8.399, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=95
2022-01-13 00:05:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:05:15 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 1.172 | ppl 2.25 | wps 20982.1 | wpb 607.1 | bsz 31.3 | num_updates 693 | best_loss 1.172
2022-01-13 00:05:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 693 updates
2022-01-13 00:05:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:05:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:05:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 7 @ 693 updates, score 1.172) (writing took 6.191601395141333 seconds)
2022-01-13 00:05:21 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-13 00:05:21 | INFO | train | epoch 007 | loss 1.315 | ppl 2.49 | wps 4379 | ups 6.76 | wpb 648.2 | bsz 31.7 | num_updates 693 | lr 2.97031e-05 | gnorm 6.083 | clip 100 | loss_scale 1 | train_wall 7 | gb_free 20.8 | wall 103
2022-01-13 00:05:21 | INFO | fairseq.trainer | begin training epoch 8
2022-01-13 00:05:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:05:22 | INFO | train_inner | epoch 008:      7 / 100 loss=1.321, ppl=2.5, wps=731.9, ups=1.26, wpb=580.7, bsz=32, num_updates=700, lr=2.96923e-05, gnorm=5.685, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=103
2022-01-13 00:05:22 | INFO | train_inner | epoch 008:     17 / 100 loss=1.372, ppl=2.59, wps=8228.6, ups=15.15, wpb=543.3, bsz=32, num_updates=710, lr=2.96769e-05, gnorm=5.61, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=104
2022-01-13 00:05:23 | INFO | train_inner | epoch 008:     27 / 100 loss=1.233, ppl=2.35, wps=9873.9, ups=13.76, wpb=717.8, bsz=32, num_updates=720, lr=2.96615e-05, gnorm=5.351, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=105
2022-01-13 00:05:24 | INFO | train_inner | epoch 008:     37 / 100 loss=1.108, ppl=2.16, wps=10064.5, ups=12.71, wpb=791.9, bsz=32, num_updates=730, lr=2.96462e-05, gnorm=5.16, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=105
2022-01-13 00:05:25 | INFO | train_inner | epoch 008:     47 / 100 loss=1.227, ppl=2.34, wps=8675.6, ups=13.95, wpb=622.1, bsz=32, num_updates=740, lr=2.96308e-05, gnorm=5.961, clip=100, loss_scale=1, train_wall=1, gb_free=20.3, wall=106
2022-01-13 00:05:26 | INFO | train_inner | epoch 008:     57 / 100 loss=1.394, ppl=2.63, wps=4676.8, ups=9.8, wpb=477.4, bsz=29.4, num_updates=750, lr=2.96154e-05, gnorm=5.787, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=107
2022-01-13 00:05:26 | INFO | train_inner | epoch 008:     67 / 100 loss=1.134, ppl=2.19, wps=9455.9, ups=13.04, wpb=725.2, bsz=32, num_updates=760, lr=2.96e-05, gnorm=5.285, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=108
2022-01-13 00:05:27 | INFO | train_inner | epoch 008:     77 / 100 loss=1.183, ppl=2.27, wps=11160.7, ups=14.31, wpb=779.7, bsz=32, num_updates=770, lr=2.95846e-05, gnorm=4.525, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=109
2022-01-13 00:05:28 | INFO | train_inner | epoch 008:     87 / 100 loss=1.38, ppl=2.6, wps=7991.6, ups=15.3, wpb=522.3, bsz=32, num_updates=780, lr=2.95692e-05, gnorm=30.959, clip=100, loss_scale=1, train_wall=1, gb_free=20.7, wall=109
2022-01-13 00:05:29 | INFO | train_inner | epoch 008:     97 / 100 loss=1.278, ppl=2.42, wps=7822.8, ups=12.11, wpb=645.8, bsz=32, num_updates=790, lr=2.95538e-05, gnorm=6.472, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=110
2022-01-13 00:05:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:05:30 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 1.138 | ppl 2.2 | wps 21029.4 | wpb 607.1 | bsz 31.3 | num_updates 793 | best_loss 1.138
2022-01-13 00:05:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 793 updates
2022-01-13 00:05:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:05:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:05:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 8 @ 793 updates, score 1.138) (writing took 5.316452286904678 seconds)
2022-01-13 00:05:35 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-13 00:05:35 | INFO | train | epoch 008 | loss 1.243 | ppl 2.37 | wps 4647 | ups 7.17 | wpb 648.2 | bsz 31.7 | num_updates 793 | lr 2.95492e-05 | gnorm 8.075 | clip 100 | loss_scale 1 | train_wall 7 | gb_free 20.8 | wall 117
2022-01-13 00:05:35 | INFO | fairseq.trainer | begin training epoch 9
2022-01-13 00:05:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:05:36 | INFO | train_inner | epoch 009:      7 / 100 loss=1.2, ppl=2.3, wps=1004.3, ups=1.41, wpb=710.8, bsz=32, num_updates=800, lr=2.95385e-05, gnorm=4.808, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=117
2022-01-13 00:05:36 | INFO | train_inner | epoch 009:     17 / 100 loss=1.19, ppl=2.28, wps=9377.1, ups=14.54, wpb=644.9, bsz=32, num_updates=810, lr=2.95231e-05, gnorm=5.277, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=118
2022-01-13 00:05:37 | INFO | train_inner | epoch 009:     27 / 100 loss=1.133, ppl=2.19, wps=9106.5, ups=13.37, wpb=681, bsz=32, num_updates=820, lr=2.95077e-05, gnorm=5.372, clip=100, loss_scale=1, train_wall=1, gb_free=20.6, wall=119
2022-01-13 00:05:38 | INFO | train_inner | epoch 009:     37 / 100 loss=1.168, ppl=2.25, wps=11335.1, ups=14.44, wpb=785.1, bsz=32, num_updates=830, lr=2.94923e-05, gnorm=10.513, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=119
2022-01-13 00:05:39 | INFO | train_inner | epoch 009:     47 / 100 loss=1.092, ppl=2.13, wps=10890.6, ups=14.26, wpb=763.7, bsz=32, num_updates=840, lr=2.94769e-05, gnorm=6.553, clip=100, loss_scale=1, train_wall=1, gb_free=20.5, wall=120
2022-01-13 00:05:39 | INFO | train_inner | epoch 009:     57 / 100 loss=1.391, ppl=2.62, wps=5757.1, ups=13.58, wpb=423.8, bsz=32, num_updates=850, lr=2.94615e-05, gnorm=6.15, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=121
2022-01-13 00:05:40 | INFO | train_inner | epoch 009:     67 / 100 loss=1.317, ppl=2.49, wps=6874.4, ups=13.82, wpb=497.5, bsz=32, num_updates=860, lr=2.94462e-05, gnorm=6.271, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=121
2022-01-13 00:05:41 | INFO | train_inner | epoch 009:     77 / 100 loss=1.156, ppl=2.23, wps=10655.1, ups=14.33, wpb=743.5, bsz=29.4, num_updates=870, lr=2.94308e-05, gnorm=5.248, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=122
2022-01-13 00:05:41 | INFO | train_inner | epoch 009:     87 / 100 loss=1.433, ppl=2.7, wps=7306.5, ups=13.65, wpb=535.3, bsz=32, num_updates=880, lr=2.94154e-05, gnorm=7.98, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=123
2022-01-13 00:05:42 | INFO | train_inner | epoch 009:     97 / 100 loss=1.247, ppl=2.37, wps=7780.9, ups=12.14, wpb=640.9, bsz=32, num_updates=890, lr=2.94e-05, gnorm=9.783, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=124
2022-01-13 00:05:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:05:44 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 1.12 | ppl 2.17 | wps 20540.3 | wpb 607.1 | bsz 31.3 | num_updates 893 | best_loss 1.12
2022-01-13 00:05:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 893 updates
2022-01-13 00:05:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:05:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:05:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 9 @ 893 updates, score 1.12) (writing took 4.006476478883997 seconds)
2022-01-13 00:05:48 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-13 00:05:48 | INFO | train | epoch 009 | loss 1.207 | ppl 2.31 | wps 5246.1 | ups 8.09 | wpb 648.2 | bsz 31.7 | num_updates 893 | lr 2.93954e-05 | gnorm 6.77 | clip 100 | loss_scale 1 | train_wall 7 | gb_free 20.8 | wall 129
2022-01-13 00:05:48 | INFO | fairseq.trainer | begin training epoch 10
2022-01-13 00:05:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:05:48 | INFO | train_inner | epoch 010:      7 / 100 loss=1.148, ppl=2.22, wps=1119.8, ups=1.7, wpb=658.4, bsz=32, num_updates=900, lr=2.93846e-05, gnorm=112.201, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=130
2022-01-13 00:05:49 | INFO | train_inner | epoch 010:     17 / 100 loss=1.09, ppl=2.13, wps=8802.7, ups=11.42, wpb=770.6, bsz=32, num_updates=910, lr=2.93692e-05, gnorm=11.581, clip=100, loss_scale=1, train_wall=1, gb_free=20.4, wall=130
2022-01-13 00:05:50 | INFO | train_inner | epoch 010:     27 / 100 loss=1.091, ppl=2.13, wps=8227.8, ups=11.71, wpb=702.6, bsz=32, num_updates=920, lr=2.93538e-05, gnorm=4.841, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=131
2022-01-13 00:05:51 | INFO | train_inner | epoch 010:     37 / 100 loss=1.043, ppl=2.06, wps=8475.4, ups=12.15, wpb=697.4, bsz=32, num_updates=930, lr=2.93385e-05, gnorm=5.212, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=132
2022-01-13 00:05:52 | INFO | train_inner | epoch 010:     47 / 100 loss=1.12, ppl=2.17, wps=9865.5, ups=12.68, wpb=777.9, bsz=32, num_updates=940, lr=2.93231e-05, gnorm=5.567, clip=100, loss_scale=1, train_wall=1, gb_free=20.8, wall=133
2022-01-13 00:05:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2022-01-13 00:05:52 | INFO | train_inner | epoch 010:     58 / 100 loss=1.244, ppl=2.37, wps=5796.8, ups=10.6, wpb=547.1, bsz=32, num_updates=950, lr=2.93077e-05, gnorm=40.24, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=134
2022-01-13 00:05:53 | INFO | train_inner | epoch 010:     68 / 100 loss=1.316, ppl=2.49, wps=6425.7, ups=12.16, wpb=528.6, bsz=32, num_updates=960, lr=2.92923e-05, gnorm=5.252, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=135
2022-01-13 00:05:54 | INFO | train_inner | epoch 010:     78 / 100 loss=1.251, ppl=2.38, wps=5938.5, ups=10.27, wpb=578.4, bsz=32, num_updates=970, lr=2.92769e-05, gnorm=4.886, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=136
2022-01-13 00:05:55 | INFO | train_inner | epoch 010:     88 / 100 loss=1.119, ppl=2.17, wps=8849, ups=12.28, wpb=720.6, bsz=32, num_updates=980, lr=2.92615e-05, gnorm=4.384, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=137
2022-01-13 00:05:56 | INFO | train_inner | epoch 010:     98 / 100 loss=1.189, ppl=2.28, wps=8315.8, ups=12.54, wpb=663.1, bsz=29.4, num_updates=990, lr=2.92462e-05, gnorm=5.986, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=137
2022-01-13 00:05:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:05:57 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 1.072 | ppl 2.1 | wps 20496.3 | wpb 607.1 | bsz 31.3 | num_updates 992 | best_loss 1.072
2022-01-13 00:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 992 updates
2022-01-13 00:05:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:06:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:06:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 10 @ 992 updates, score 1.072) (writing took 4.844264738960192 seconds)
2022-01-13 00:06:02 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-13 00:06:02 | INFO | train | epoch 010 | loss 1.163 | ppl 2.24 | wps 4468.2 | ups 6.88 | wpb 649.3 | bsz 31.7 | num_updates 992 | lr 2.92431e-05 | gnorm 20.599 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 143
2022-01-13 00:06:02 | INFO | fairseq.trainer | begin training epoch 11
2022-01-13 00:06:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:06:03 | INFO | train_inner | epoch 011:      8 / 100 loss=1.191, ppl=2.28, wps=822.8, ups=1.51, wpb=545.2, bsz=32, num_updates=1000, lr=2.92308e-05, gnorm=31.824, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=144
2022-01-13 00:06:03 | INFO | train_inner | epoch 011:     18 / 100 loss=0.999, ppl=2, wps=10048.9, ups=13.05, wpb=770.2, bsz=32, num_updates=1010, lr=2.92154e-05, gnorm=4.907, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=145
2022-01-13 00:06:04 | INFO | train_inner | epoch 011:     28 / 100 loss=1.097, ppl=2.14, wps=9452.1, ups=15.11, wpb=625.5, bsz=32, num_updates=1020, lr=2.92e-05, gnorm=5.671, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=145
2022-01-13 00:06:05 | INFO | train_inner | epoch 011:     38 / 100 loss=1.186, ppl=2.28, wps=7050.8, ups=14.87, wpb=474.1, bsz=29.4, num_updates=1030, lr=2.91846e-05, gnorm=5.768, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=146
2022-01-13 00:06:05 | INFO | train_inner | epoch 011:     48 / 100 loss=1.001, ppl=2, wps=8942.2, ups=13.25, wpb=674.8, bsz=32, num_updates=1040, lr=2.91692e-05, gnorm=4.246, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=147
2022-01-13 00:06:06 | INFO | train_inner | epoch 011:     58 / 100 loss=1.115, ppl=2.17, wps=9147, ups=13.83, wpb=661.5, bsz=32, num_updates=1050, lr=2.91538e-05, gnorm=18.678, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=148
2022-01-13 00:06:07 | INFO | train_inner | epoch 011:     68 / 100 loss=1.089, ppl=2.13, wps=9814.3, ups=12.71, wpb=772, bsz=32, num_updates=1060, lr=2.91385e-05, gnorm=5.458, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=148
2022-01-13 00:06:08 | INFO | train_inner | epoch 011:     78 / 100 loss=1.043, ppl=2.06, wps=9012, ups=13.4, wpb=672.6, bsz=32, num_updates=1070, lr=2.91231e-05, gnorm=4.639, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=149
2022-01-13 00:06:08 | INFO | train_inner | epoch 011:     88 / 100 loss=1.136, ppl=2.2, wps=8049.9, ups=12.54, wpb=641.8, bsz=32, num_updates=1080, lr=2.91077e-05, gnorm=5.253, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=150
2022-01-13 00:06:09 | INFO | train_inner | epoch 011:     98 / 100 loss=1.083, ppl=2.12, wps=7728.1, ups=12.2, wpb=633.6, bsz=32, num_updates=1090, lr=2.90923e-05, gnorm=4.985, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=151
2022-01-13 00:06:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:06:10 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 1.055 | ppl 2.08 | wps 21319 | wpb 607.1 | bsz 31.3 | num_updates 1092 | best_loss 1.055
2022-01-13 00:06:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1092 updates
2022-01-13 00:06:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:06:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:06:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 11 @ 1092 updates, score 1.055) (writing took 4.596290481975302 seconds)
2022-01-13 00:06:15 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-13 00:06:15 | INFO | train | epoch 011 | loss 1.087 | ppl 2.13 | wps 4968.8 | ups 7.67 | wpb 648.2 | bsz 31.7 | num_updates 1092 | lr 2.90892e-05 | gnorm 9.186 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 156
2022-01-13 00:06:15 | INFO | fairseq.trainer | begin training epoch 12
2022-01-13 00:06:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:06:16 | INFO | train_inner | epoch 012:      8 / 100 loss=1.131, ppl=2.19, wps=1021.7, ups=1.58, wpb=647.8, bsz=32, num_updates=1100, lr=2.90769e-05, gnorm=9.093, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=157
2022-01-13 00:06:16 | INFO | train_inner | epoch 012:     18 / 100 loss=1.002, ppl=2, wps=10178.9, ups=14.01, wpb=726.8, bsz=32, num_updates=1110, lr=2.90615e-05, gnorm=5.07, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=158
2022-01-13 00:06:17 | INFO | train_inner | epoch 012:     28 / 100 loss=1, ppl=2, wps=10418.9, ups=12.76, wpb=816.7, bsz=32, num_updates=1120, lr=2.90462e-05, gnorm=4.414, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=159
2022-01-13 00:06:18 | INFO | train_inner | epoch 012:     38 / 100 loss=1.122, ppl=2.18, wps=8962.2, ups=13.98, wpb=641.2, bsz=32, num_updates=1130, lr=2.90308e-05, gnorm=5.344, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=159
2022-01-13 00:06:19 | INFO | train_inner | epoch 012:     48 / 100 loss=1.03, ppl=2.04, wps=9120.9, ups=13.35, wpb=683, bsz=29.4, num_updates=1140, lr=2.90154e-05, gnorm=4.496, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=160
2022-01-13 00:06:19 | INFO | train_inner | epoch 012:     58 / 100 loss=0.994, ppl=1.99, wps=9893.7, ups=12.96, wpb=763.3, bsz=32, num_updates=1150, lr=2.9e-05, gnorm=4.126, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=161
2022-01-13 00:06:20 | INFO | train_inner | epoch 012:     68 / 100 loss=1.106, ppl=2.15, wps=6439.4, ups=11.96, wpb=538.2, bsz=32, num_updates=1160, lr=2.89846e-05, gnorm=5.029, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=162
2022-01-13 00:06:21 | INFO | train_inner | epoch 012:     78 / 100 loss=1.135, ppl=2.2, wps=7559.1, ups=12.14, wpb=622.6, bsz=32, num_updates=1170, lr=2.89692e-05, gnorm=5.727, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=162
2022-01-13 00:06:22 | INFO | train_inner | epoch 012:     88 / 100 loss=1.034, ppl=2.05, wps=6879, ups=12.04, wpb=571.5, bsz=32, num_updates=1180, lr=2.89538e-05, gnorm=5.544, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=163
2022-01-13 00:06:23 | INFO | train_inner | epoch 012:     98 / 100 loss=1.067, ppl=2.1, wps=5445.4, ups=11.89, wpb=458, bsz=32, num_updates=1190, lr=2.89385e-05, gnorm=5.395, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=164
2022-01-13 00:06:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:06:24 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 1.055 | ppl 2.08 | wps 20320 | wpb 607.1 | bsz 31.3 | num_updates 1192 | best_loss 1.055
2022-01-13 00:06:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1192 updates
2022-01-13 00:06:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:06:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:06:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 12 @ 1192 updates, score 1.055) (writing took 4.268704150104895 seconds)
2022-01-13 00:06:28 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-13 00:06:28 | INFO | train | epoch 012 | loss 1.053 | ppl 2.08 | wps 4930.6 | ups 7.61 | wpb 648.2 | bsz 31.7 | num_updates 1192 | lr 2.89354e-05 | gnorm 5.008 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 170
2022-01-13 00:06:28 | INFO | fairseq.trainer | begin training epoch 13
2022-01-13 00:06:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:06:29 | INFO | train_inner | epoch 013:      8 / 100 loss=0.968, ppl=1.96, wps=1457.5, ups=1.66, wpb=879.4, bsz=32, num_updates=1200, lr=2.89231e-05, gnorm=4.381, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=170
2022-01-13 00:06:29 | INFO | train_inner | epoch 013:     18 / 100 loss=1.061, ppl=2.09, wps=8661.3, ups=15.56, wpb=556.8, bsz=32, num_updates=1210, lr=2.89077e-05, gnorm=5.662, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=171
2022-01-13 00:06:30 | INFO | train_inner | epoch 013:     28 / 100 loss=0.971, ppl=1.96, wps=8972.3, ups=13.49, wpb=665.3, bsz=32, num_updates=1220, lr=2.88923e-05, gnorm=4.292, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=172
2022-01-13 00:06:31 | INFO | train_inner | epoch 013:     38 / 100 loss=1.02, ppl=2.03, wps=9882.1, ups=15.57, wpb=634.5, bsz=32, num_updates=1230, lr=2.88769e-05, gnorm=4.698, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=172
2022-01-13 00:06:31 | INFO | train_inner | epoch 013:     48 / 100 loss=1.131, ppl=2.19, wps=6900.6, ups=14.53, wpb=474.9, bsz=32, num_updates=1240, lr=2.88615e-05, gnorm=6.174, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=173
2022-01-13 00:06:32 | INFO | train_inner | epoch 013:     58 / 100 loss=1.013, ppl=2.02, wps=7893.8, ups=13.83, wpb=570.9, bsz=32, num_updates=1250, lr=2.88462e-05, gnorm=10.359, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=174
2022-01-13 00:06:33 | INFO | train_inner | epoch 013:     68 / 100 loss=0.921, ppl=1.89, wps=10254.6, ups=14.81, wpb=692.6, bsz=32, num_updates=1260, lr=2.88308e-05, gnorm=4.13, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=174
2022-01-13 00:06:34 | INFO | train_inner | epoch 013:     78 / 100 loss=1.07, ppl=2.1, wps=8015.2, ups=12.91, wpb=620.8, bsz=32, num_updates=1270, lr=2.88154e-05, gnorm=4.901, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=175
2022-01-13 00:06:35 | INFO | train_inner | epoch 013:     88 / 100 loss=1, ppl=2, wps=8745.5, ups=12.02, wpb=727.4, bsz=29.4, num_updates=1280, lr=2.88e-05, gnorm=4.936, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=176
2022-01-13 00:06:35 | INFO | train_inner | epoch 013:     98 / 100 loss=1.034, ppl=2.05, wps=8291.2, ups=13.15, wpb=630.3, bsz=32, num_updates=1290, lr=2.87846e-05, gnorm=4.974, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=177
2022-01-13 00:06:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:06:36 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 1.013 | ppl 2.02 | wps 20540.2 | wpb 607.1 | bsz 31.3 | num_updates 1292 | best_loss 1.013
2022-01-13 00:06:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1292 updates
2022-01-13 00:06:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:06:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:06:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 13 @ 1292 updates, score 1.013) (writing took 4.508921053959057 seconds)
2022-01-13 00:06:41 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-13 00:06:41 | INFO | train | epoch 013 | loss 1.01 | ppl 2.01 | wps 5063.3 | ups 7.81 | wpb 648.2 | bsz 31.7 | num_updates 1292 | lr 2.87815e-05 | gnorm 5.412 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.7 | wall 182
2022-01-13 00:06:41 | INFO | fairseq.trainer | begin training epoch 14
2022-01-13 00:06:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:06:42 | INFO | train_inner | epoch 014:      8 / 100 loss=1.027, ppl=2.04, wps=1013.1, ups=1.51, wpb=672, bsz=32, num_updates=1300, lr=2.87692e-05, gnorm=4.658, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=183
2022-01-13 00:06:43 | INFO | train_inner | epoch 014:     18 / 100 loss=0.967, ppl=1.95, wps=9437.8, ups=14.31, wpb=659.3, bsz=32, num_updates=1310, lr=2.87538e-05, gnorm=4.979, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=184
2022-01-13 00:06:43 | INFO | train_inner | epoch 014:     28 / 100 loss=0.986, ppl=1.98, wps=8281.3, ups=15.65, wpb=529.3, bsz=32, num_updates=1320, lr=2.87385e-05, gnorm=4.822, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=185
2022-01-13 00:06:44 | INFO | train_inner | epoch 014:     38 / 100 loss=0.964, ppl=1.95, wps=11362.9, ups=15.5, wpb=733.3, bsz=32, num_updates=1330, lr=2.87231e-05, gnorm=4.331, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=185
2022-01-13 00:06:45 | INFO | train_inner | epoch 014:     48 / 100 loss=1.002, ppl=2, wps=8993.2, ups=15.74, wpb=571.3, bsz=32, num_updates=1340, lr=2.87077e-05, gnorm=5.113, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=186
2022-01-13 00:06:45 | INFO | train_inner | epoch 014:     58 / 100 loss=0.935, ppl=1.91, wps=9546.4, ups=13.89, wpb=687.5, bsz=29.4, num_updates=1350, lr=2.86923e-05, gnorm=4.438, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=187
2022-01-13 00:06:46 | INFO | train_inner | epoch 014:     68 / 100 loss=1.034, ppl=2.05, wps=8705.1, ups=13.75, wpb=633.1, bsz=32, num_updates=1360, lr=2.86769e-05, gnorm=4.806, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=187
2022-01-13 00:06:47 | INFO | train_inner | epoch 014:     78 / 100 loss=0.974, ppl=1.96, wps=11296.8, ups=14.72, wpb=767.7, bsz=32, num_updates=1370, lr=2.86615e-05, gnorm=5.626, clip=100, loss_scale=0.5, train_wall=1, gb_free=18.9, wall=188
2022-01-13 00:06:47 | INFO | train_inner | epoch 014:     88 / 100 loss=1.045, ppl=2.06, wps=7782.3, ups=12.67, wpb=614, bsz=32, num_updates=1380, lr=2.86462e-05, gnorm=4.774, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=189
2022-01-13 00:06:48 | INFO | train_inner | epoch 014:     98 / 100 loss=0.969, ppl=1.96, wps=6887.1, ups=11.33, wpb=607.8, bsz=32, num_updates=1390, lr=2.86308e-05, gnorm=4.466, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=190
2022-01-13 00:06:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:06:49 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 1.005 | ppl 2.01 | wps 20560.3 | wpb 607.1 | bsz 31.3 | num_updates 1392 | best_loss 1.005
2022-01-13 00:06:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1392 updates
2022-01-13 00:06:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:06:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:06:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 14 @ 1392 updates, score 1.005) (writing took 4.587983769131824 seconds)
2022-01-13 00:06:54 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-13 00:06:54 | INFO | train | epoch 014 | loss 0.989 | ppl 1.99 | wps 5061.5 | ups 7.81 | wpb 648.2 | bsz 31.7 | num_updates 1392 | lr 2.86277e-05 | gnorm 4.811 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.5 | wall 196
2022-01-13 00:06:54 | INFO | fairseq.trainer | begin training epoch 15
2022-01-13 00:06:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:06:55 | INFO | train_inner | epoch 015:      8 / 100 loss=1.214, ppl=2.32, wps=995.8, ups=1.57, wpb=633.6, bsz=32, num_updates=1400, lr=2.86154e-05, gnorm=5.434, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=196
2022-01-13 00:06:55 | INFO | train_inner | epoch 015:     18 / 100 loss=1.173, ppl=2.25, wps=9948.5, ups=13.3, wpb=748, bsz=32, num_updates=1410, lr=2.86e-05, gnorm=4.082, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=197
2022-01-13 00:06:56 | INFO | train_inner | epoch 015:     28 / 100 loss=1.241, ppl=2.36, wps=8499, ups=14.46, wpb=587.8, bsz=32, num_updates=1420, lr=2.85846e-05, gnorm=5.091, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=198
2022-01-13 00:06:57 | INFO | train_inner | epoch 015:     38 / 100 loss=1.247, ppl=2.37, wps=8251.9, ups=13.91, wpb=593.1, bsz=32, num_updates=1430, lr=2.85692e-05, gnorm=4.871, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=198
2022-01-13 00:06:58 | INFO | train_inner | epoch 015:     48 / 100 loss=1.22, ppl=2.33, wps=8489.8, ups=13.93, wpb=609.5, bsz=32, num_updates=1440, lr=2.85538e-05, gnorm=4.843, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=199
2022-01-13 00:06:58 | INFO | train_inner | epoch 015:     58 / 100 loss=1.179, ppl=2.26, wps=8937.7, ups=13.65, wpb=654.7, bsz=32, num_updates=1450, lr=2.85385e-05, gnorm=4.791, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=200
2022-01-13 00:06:59 | INFO | train_inner | epoch 015:     68 / 100 loss=1.129, ppl=2.19, wps=9852.2, ups=13.44, wpb=732.8, bsz=32, num_updates=1460, lr=2.85231e-05, gnorm=4.219, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=201
2022-01-13 00:07:00 | INFO | train_inner | epoch 015:     78 / 100 loss=1.228, ppl=2.34, wps=6446.5, ups=13.56, wpb=475.4, bsz=32, num_updates=1470, lr=2.85077e-05, gnorm=5.546, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=201
2022-01-13 00:07:01 | INFO | train_inner | epoch 015:     88 / 100 loss=1.145, ppl=2.21, wps=8039.4, ups=10.09, wpb=797, bsz=32, num_updates=1480, lr=2.84923e-05, gnorm=4.628, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=202
2022-01-13 00:07:02 | INFO | train_inner | epoch 015:     98 / 100 loss=1.145, ppl=2.21, wps=6709.1, ups=11.33, wpb=592.4, bsz=29.4, num_updates=1490, lr=2.84769e-05, gnorm=4.781, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=203
2022-01-13 00:07:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:07:03 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 0.983 | ppl 1.98 | wps 20262.4 | wpb 607.1 | bsz 31.3 | num_updates 1492 | best_loss 0.983
2022-01-13 00:07:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1492 updates
2022-01-13 00:07:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:07:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:07:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 15 @ 1492 updates, score 0.983) (writing took 4.478898502187803 seconds)
2022-01-13 00:07:07 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-13 00:07:07 | INFO | train | epoch 015 | loss 1.191 | ppl 2.28 | wps 4897.4 | ups 7.56 | wpb 648.2 | bsz 31.7 | num_updates 1492 | lr 2.84738e-05 | gnorm 4.808 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.7 | wall 209
2022-01-13 00:07:07 | INFO | fairseq.trainer | begin training epoch 16
2022-01-13 00:07:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:07:08 | INFO | train_inner | epoch 016:      8 / 100 loss=1.195, ppl=2.29, wps=1176, ups=1.6, wpb=733, bsz=32, num_updates=1500, lr=2.84615e-05, gnorm=4.166, clip=100, loss_scale=0.5, train_wall=1, gb_free=20, wall=209
2022-01-13 00:07:09 | INFO | train_inner | epoch 016:     18 / 100 loss=1.139, ppl=2.2, wps=9380.7, ups=15.87, wpb=591.1, bsz=32, num_updates=1510, lr=2.84462e-05, gnorm=9.458, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=210
2022-01-13 00:07:09 | INFO | train_inner | epoch 016:     28 / 100 loss=1.141, ppl=2.21, wps=7760.1, ups=13.53, wpb=573.4, bsz=32, num_updates=1520, lr=2.84308e-05, gnorm=4.756, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=211
2022-01-13 00:07:10 | INFO | train_inner | epoch 016:     38 / 100 loss=1.163, ppl=2.24, wps=8646.8, ups=12.67, wpb=682.3, bsz=32, num_updates=1530, lr=2.84154e-05, gnorm=4.939, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=212
2022-01-13 00:07:11 | INFO | train_inner | epoch 016:     48 / 100 loss=1.202, ppl=2.3, wps=7549.4, ups=12.8, wpb=589.7, bsz=32, num_updates=1540, lr=2.84e-05, gnorm=4.79, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=212
2022-01-13 00:07:12 | INFO | train_inner | epoch 016:     58 / 100 loss=1.052, ppl=2.07, wps=9087.1, ups=10.8, wpb=841.6, bsz=32, num_updates=1550, lr=2.83846e-05, gnorm=4.166, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=213
2022-01-13 00:07:13 | INFO | train_inner | epoch 016:     68 / 100 loss=1.1, ppl=2.14, wps=7598.4, ups=11.05, wpb=687.5, bsz=32, num_updates=1560, lr=2.83692e-05, gnorm=5.144, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=214
2022-01-13 00:07:14 | INFO | train_inner | epoch 016:     78 / 100 loss=1.239, ppl=2.36, wps=7960, ups=12.34, wpb=645.1, bsz=32, num_updates=1570, lr=2.83538e-05, gnorm=4.372, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=215
2022-01-13 00:07:14 | INFO | train_inner | epoch 016:     88 / 100 loss=1.067, ppl=2.09, wps=7645.1, ups=11.62, wpb=658.2, bsz=32, num_updates=1580, lr=2.83385e-05, gnorm=4.298, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=216
2022-01-13 00:07:15 | INFO | train_inner | epoch 016:     98 / 100 loss=1.147, ppl=2.21, wps=5678.2, ups=9.83, wpb=577.5, bsz=29.4, num_updates=1590, lr=2.83231e-05, gnorm=5.207, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=217
2022-01-13 00:07:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:07:17 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 0.982 | ppl 1.98 | wps 20337.6 | wpb 607.1 | bsz 31.3 | num_updates 1592 | best_loss 0.982
2022-01-13 00:07:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1592 updates
2022-01-13 00:07:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:07:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:07:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 16 @ 1592 updates, score 0.982) (writing took 4.099811422871426 seconds)
2022-01-13 00:07:21 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-13 00:07:21 | INFO | train | epoch 016 | loss 1.142 | ppl 2.21 | wps 4850.8 | ups 7.48 | wpb 648.2 | bsz 31.7 | num_updates 1592 | lr 2.832e-05 | gnorm 5.151 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 222
2022-01-13 00:07:21 | INFO | fairseq.trainer | begin training epoch 17
2022-01-13 00:07:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:07:21 | INFO | train_inner | epoch 017:      8 / 100 loss=1.11, ppl=2.16, wps=1153.5, ups=1.7, wpb=678.4, bsz=32, num_updates=1600, lr=2.83077e-05, gnorm=4.414, clip=100, loss_scale=0.5, train_wall=1, gb_free=19.9, wall=223
2022-01-13 00:07:22 | INFO | train_inner | epoch 017:     18 / 100 loss=1.052, ppl=2.07, wps=8422.6, ups=15.83, wpb=531.9, bsz=29.4, num_updates=1610, lr=2.82923e-05, gnorm=4.466, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=223
2022-01-13 00:07:23 | INFO | train_inner | epoch 017:     28 / 100 loss=1.119, ppl=2.17, wps=8535, ups=13.4, wpb=637.1, bsz=32, num_updates=1620, lr=2.82769e-05, gnorm=5.076, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=224
2022-01-13 00:07:23 | INFO | train_inner | epoch 017:     38 / 100 loss=1.114, ppl=2.16, wps=9974.8, ups=14.41, wpb=692.4, bsz=32, num_updates=1630, lr=2.82615e-05, gnorm=5.305, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=225
2022-01-13 00:07:24 | INFO | train_inner | epoch 017:     48 / 100 loss=1.126, ppl=2.18, wps=7782, ups=12.77, wpb=609.6, bsz=32, num_updates=1640, lr=2.82462e-05, gnorm=4.91, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=226
2022-01-13 00:07:25 | INFO | train_inner | epoch 017:     58 / 100 loss=1.048, ppl=2.07, wps=8862.1, ups=12.35, wpb=717.3, bsz=32, num_updates=1650, lr=2.82308e-05, gnorm=4.601, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=226
2022-01-13 00:07:26 | INFO | train_inner | epoch 017:     68 / 100 loss=1.122, ppl=2.18, wps=7925.7, ups=12.84, wpb=617.5, bsz=32, num_updates=1660, lr=2.82154e-05, gnorm=5.029, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=227
2022-01-13 00:07:27 | INFO | train_inner | epoch 017:     78 / 100 loss=1.186, ppl=2.28, wps=9696.6, ups=13.34, wpb=726.9, bsz=32, num_updates=1670, lr=2.82e-05, gnorm=4.585, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=228
2022-01-13 00:07:27 | INFO | train_inner | epoch 017:     88 / 100 loss=1.037, ppl=2.05, wps=8941.4, ups=14.89, wpb=600.6, bsz=32, num_updates=1680, lr=2.81846e-05, gnorm=4.533, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=229
2022-01-13 00:07:28 | INFO | train_inner | epoch 017:     98 / 100 loss=0.999, ppl=2, wps=7568, ups=11.6, wpb=652.5, bsz=32, num_updates=1690, lr=2.81692e-05, gnorm=4.558, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=230
2022-01-13 00:07:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:07:29 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 0.954 | ppl 1.94 | wps 18505.5 | wpb 607.1 | bsz 31.3 | num_updates 1692 | best_loss 0.954
2022-01-13 00:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1692 updates
2022-01-13 00:07:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:07:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:07:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 17 @ 1692 updates, score 0.954) (writing took 4.656262712087482 seconds)
2022-01-13 00:07:34 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-13 00:07:34 | INFO | train | epoch 017 | loss 1.094 | ppl 2.13 | wps 4882.1 | ups 7.53 | wpb 648.2 | bsz 31.7 | num_updates 1692 | lr 2.81662e-05 | gnorm 4.751 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 235
2022-01-13 00:07:34 | INFO | fairseq.trainer | begin training epoch 18
2022-01-13 00:07:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:07:35 | INFO | train_inner | epoch 018:      8 / 100 loss=1.085, ppl=2.12, wps=817.1, ups=1.53, wpb=533.4, bsz=32, num_updates=1700, lr=2.81538e-05, gnorm=5.315, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=236
2022-01-13 00:07:35 | INFO | train_inner | epoch 018:     18 / 100 loss=1.166, ppl=2.24, wps=9435.6, ups=15.14, wpb=623.4, bsz=32, num_updates=1710, lr=2.81385e-05, gnorm=15.083, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=237
2022-01-13 00:07:36 | INFO | train_inner | epoch 018:     28 / 100 loss=0.93, ppl=1.91, wps=11116.1, ups=14.28, wpb=778.2, bsz=32, num_updates=1720, lr=2.81231e-05, gnorm=3.763, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=237
2022-01-13 00:07:37 | INFO | train_inner | epoch 018:     38 / 100 loss=1.035, ppl=2.05, wps=9809.3, ups=14.25, wpb=688.3, bsz=29.4, num_updates=1730, lr=2.81077e-05, gnorm=4.91, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=238
2022-01-13 00:07:37 | INFO | train_inner | epoch 018:     48 / 100 loss=1.109, ppl=2.16, wps=10034.8, ups=15.68, wpb=640.1, bsz=32, num_updates=1740, lr=2.80923e-05, gnorm=5.513, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=239
2022-01-13 00:07:38 | INFO | train_inner | epoch 018:     58 / 100 loss=1.074, ppl=2.11, wps=7196, ups=15.15, wpb=474.9, bsz=32, num_updates=1750, lr=2.80769e-05, gnorm=5.787, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=239
2022-01-13 00:07:39 | INFO | train_inner | epoch 018:     68 / 100 loss=1.106, ppl=2.15, wps=11954, ups=14.26, wpb=838.4, bsz=32, num_updates=1760, lr=2.80615e-05, gnorm=4.203, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=240
2022-01-13 00:07:39 | INFO | train_inner | epoch 018:     78 / 100 loss=1.091, ppl=2.13, wps=9119.1, ups=15.67, wpb=582, bsz=32, num_updates=1770, lr=2.80462e-05, gnorm=7.881, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=241
2022-01-13 00:07:40 | INFO | train_inner | epoch 018:     88 / 100 loss=1.091, ppl=2.13, wps=10647, ups=14.87, wpb=715.9, bsz=32, num_updates=1780, lr=2.80308e-05, gnorm=4.689, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=241
2022-01-13 00:07:41 | INFO | train_inner | epoch 018:     98 / 100 loss=1.141, ppl=2.21, wps=6717.5, ups=11.94, wpb=562.4, bsz=32, num_updates=1790, lr=2.80154e-05, gnorm=5.034, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=242
2022-01-13 00:07:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:07:42 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 0.961 | ppl 1.95 | wps 19377.6 | wpb 607.1 | bsz 31.3 | num_updates 1792 | best_loss 0.954
2022-01-13 00:07:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1792 updates
2022-01-13 00:07:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:07:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:07:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 18 @ 1792 updates, score 0.961) (writing took 2.6539784881751984 seconds)
2022-01-13 00:07:45 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-13 00:07:45 | INFO | train | epoch 018 | loss 1.074 | ppl 2.11 | wps 6070.8 | ups 9.37 | wpb 648.2 | bsz 31.7 | num_updates 1792 | lr 2.80123e-05 | gnorm 6.192 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.7 | wall 246
2022-01-13 00:07:45 | INFO | fairseq.trainer | begin training epoch 19
2022-01-13 00:07:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:07:45 | INFO | train_inner | epoch 019:      8 / 100 loss=1.004, ppl=2.01, wps=1461.9, ups=2.26, wpb=647.5, bsz=32, num_updates=1800, lr=2.8e-05, gnorm=4.578, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=247
2022-01-13 00:07:46 | INFO | train_inner | epoch 019:     18 / 100 loss=1.003, ppl=2, wps=9722.7, ups=14.41, wpb=674.5, bsz=32, num_updates=1810, lr=2.79846e-05, gnorm=4.172, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=247
2022-01-13 00:07:47 | INFO | train_inner | epoch 019:     28 / 100 loss=1.073, ppl=2.1, wps=6674.6, ups=11.93, wpb=559.3, bsz=32, num_updates=1820, lr=2.79692e-05, gnorm=4.934, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=248
2022-01-13 00:07:48 | INFO | train_inner | epoch 019:     38 / 100 loss=0.981, ppl=1.97, wps=8047.5, ups=13.86, wpb=580.5, bsz=32, num_updates=1830, lr=2.79538e-05, gnorm=5.74, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=249
2022-01-13 00:07:48 | INFO | train_inner | epoch 019:     48 / 100 loss=1.102, ppl=2.15, wps=5747.7, ups=12.34, wpb=465.8, bsz=32, num_updates=1840, lr=2.79385e-05, gnorm=5.096, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=250
2022-01-13 00:07:49 | INFO | train_inner | epoch 019:     58 / 100 loss=1.073, ppl=2.1, wps=7531.2, ups=12.16, wpb=619.4, bsz=32, num_updates=1850, lr=2.79231e-05, gnorm=5.298, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=251
2022-01-13 00:07:50 | INFO | train_inner | epoch 019:     68 / 100 loss=1.057, ppl=2.08, wps=9648.3, ups=10.65, wpb=905.9, bsz=32, num_updates=1860, lr=2.79077e-05, gnorm=3.756, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=252
2022-01-13 00:07:51 | INFO | train_inner | epoch 019:     78 / 100 loss=0.995, ppl=1.99, wps=8766, ups=13.76, wpb=636.9, bsz=32, num_updates=1870, lr=2.78923e-05, gnorm=5.217, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=252
2022-01-13 00:07:52 | INFO | train_inner | epoch 019:     88 / 100 loss=1.025, ppl=2.04, wps=9135.3, ups=12.41, wpb=736.1, bsz=32, num_updates=1880, lr=2.78769e-05, gnorm=3.927, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=253
2022-01-13 00:07:53 | INFO | train_inner | epoch 019:     98 / 100 loss=0.987, ppl=1.98, wps=7676.6, ups=10.57, wpb=726.6, bsz=32, num_updates=1890, lr=2.78615e-05, gnorm=4.015, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=254
2022-01-13 00:07:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:07:54 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 0.931 | ppl 1.91 | wps 19768.2 | wpb 607.1 | bsz 31.3 | num_updates 1892 | best_loss 0.931
2022-01-13 00:07:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1892 updates
2022-01-13 00:07:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:07:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:08:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 19 @ 1892 updates, score 0.931) (writing took 6.08690868713893 seconds)
2022-01-13 00:08:00 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-13 00:08:00 | INFO | train | epoch 019 | loss 1.025 | ppl 2.04 | wps 4257.4 | ups 6.57 | wpb 648.2 | bsz 31.7 | num_updates 1892 | lr 2.78585e-05 | gnorm 4.725 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 261
2022-01-13 00:08:00 | INFO | fairseq.trainer | begin training epoch 20
2022-01-13 00:08:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:08:01 | INFO | train_inner | epoch 020:      8 / 100 loss=0.86, ppl=1.82, wps=829.9, ups=1.26, wpb=657.9, bsz=26.8, num_updates=1900, lr=2.78462e-05, gnorm=4.396, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=262
2022-01-13 00:08:01 | INFO | train_inner | epoch 020:     18 / 100 loss=0.999, ppl=2, wps=8320.9, ups=12.84, wpb=647.9, bsz=32, num_updates=1910, lr=2.78308e-05, gnorm=4.75, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=263
2022-01-13 00:08:02 | INFO | train_inner | epoch 020:     28 / 100 loss=0.982, ppl=1.98, wps=9951.9, ups=14.82, wpb=671.4, bsz=32, num_updates=1920, lr=2.78154e-05, gnorm=12.868, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=263
2022-01-13 00:08:03 | INFO | train_inner | epoch 020:     38 / 100 loss=1.184, ppl=2.27, wps=10809, ups=12.25, wpb=882.5, bsz=32, num_updates=1930, lr=2.78e-05, gnorm=4.232, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.2, wall=264
2022-01-13 00:08:04 | INFO | train_inner | epoch 020:     48 / 100 loss=1.04, ppl=2.06, wps=4392.7, ups=9.61, wpb=456.9, bsz=32, num_updates=1940, lr=2.77846e-05, gnorm=5.071, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=265
2022-01-13 00:08:05 | INFO | train_inner | epoch 020:     58 / 100 loss=0.983, ppl=1.98, wps=9751.2, ups=12.9, wpb=755.7, bsz=32, num_updates=1950, lr=2.77692e-05, gnorm=4.098, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=266
2022-01-13 00:08:06 | INFO | train_inner | epoch 020:     68 / 100 loss=0.945, ppl=1.93, wps=7955.1, ups=11.61, wpb=685, bsz=32, num_updates=1960, lr=2.77538e-05, gnorm=3.917, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=267
2022-01-13 00:08:06 | INFO | train_inner | epoch 020:     78 / 100 loss=1.102, ppl=2.15, wps=5958.9, ups=10.4, wpb=572.8, bsz=32, num_updates=1970, lr=2.77385e-05, gnorm=5.249, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=268
2022-01-13 00:08:07 | INFO | train_inner | epoch 020:     88 / 100 loss=1.036, ppl=2.05, wps=6122.4, ups=9.94, wpb=615.9, bsz=32, num_updates=1980, lr=2.77231e-05, gnorm=5.002, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=269
2022-01-13 00:08:08 | INFO | train_inner | epoch 020:     98 / 100 loss=0.912, ppl=1.88, wps=6178.1, ups=11.33, wpb=545.1, bsz=32, num_updates=1990, lr=2.77077e-05, gnorm=4.531, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=270
2022-01-13 00:08:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:08:09 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 0.955 | ppl 1.94 | wps 20173.1 | wpb 607.1 | bsz 31.3 | num_updates 1992 | best_loss 0.931
2022-01-13 00:08:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1992 updates
2022-01-13 00:08:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:08:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:08:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 20 @ 1992 updates, score 0.955) (writing took 3.6181780041661114 seconds)
2022-01-13 00:08:13 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-13 00:08:13 | INFO | train | epoch 020 | loss 1.014 | ppl 2.02 | wps 4899.9 | ups 7.56 | wpb 648.2 | bsz 31.7 | num_updates 1992 | lr 2.77046e-05 | gnorm 5.399 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 275
2022-01-13 00:08:13 | INFO | fairseq.trainer | begin training epoch 21
2022-01-13 00:08:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:08:14 | INFO | train_inner | epoch 021:      8 / 100 loss=0.909, ppl=1.88, wps=983.2, ups=1.86, wpb=528.6, bsz=32, num_updates=2000, lr=2.76923e-05, gnorm=4.574, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=275
2022-01-13 00:08:14 | INFO | train_inner | epoch 021:     18 / 100 loss=0.96, ppl=1.95, wps=10971.9, ups=14.63, wpb=749.7, bsz=32, num_updates=2010, lr=2.76769e-05, gnorm=4.085, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.1, wall=276
2022-01-13 00:08:15 | INFO | train_inner | epoch 021:     28 / 100 loss=0.987, ppl=1.98, wps=12393.8, ups=14.14, wpb=876.7, bsz=32, num_updates=2020, lr=2.76615e-05, gnorm=3.874, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=277
2022-01-13 00:08:16 | INFO | train_inner | epoch 021:     38 / 100 loss=1.009, ppl=2.01, wps=6606, ups=12.24, wpb=539.9, bsz=32, num_updates=2030, lr=2.76462e-05, gnorm=5.021, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=277
2022-01-13 00:08:17 | INFO | train_inner | epoch 021:     48 / 100 loss=1.013, ppl=2.02, wps=7694.2, ups=12.27, wpb=627, bsz=32, num_updates=2040, lr=2.76308e-05, gnorm=4.559, clip=100, loss_scale=0.5, train_wall=1, gb_free=20, wall=278
2022-01-13 00:08:18 | INFO | train_inner | epoch 021:     58 / 100 loss=0.992, ppl=1.99, wps=8058.4, ups=10.66, wpb=756.1, bsz=32, num_updates=2050, lr=2.76154e-05, gnorm=6.409, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=279
2022-01-13 00:08:19 | INFO | train_inner | epoch 021:     68 / 100 loss=0.939, ppl=1.92, wps=8893.5, ups=12.56, wpb=707.8, bsz=32, num_updates=2060, lr=2.76e-05, gnorm=4.761, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=280
2022-01-13 00:08:19 | INFO | train_inner | epoch 021:     78 / 100 loss=0.942, ppl=1.92, wps=6770.4, ups=12.61, wpb=536.8, bsz=32, num_updates=2070, lr=2.75846e-05, gnorm=4.888, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.5, wall=281
2022-01-13 00:08:20 | INFO | train_inner | epoch 021:     88 / 100 loss=0.989, ppl=1.98, wps=5235.9, ups=11.54, wpb=453.9, bsz=32, num_updates=2080, lr=2.75692e-05, gnorm=5.327, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=282
2022-01-13 00:08:21 | INFO | train_inner | epoch 021:     98 / 100 loss=0.914, ppl=1.88, wps=9136.6, ups=13.12, wpb=696.6, bsz=29.4, num_updates=2090, lr=2.75538e-05, gnorm=6.14, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.5, wall=282
2022-01-13 00:08:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:08:22 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 0.919 | ppl 1.89 | wps 19517.3 | wpb 607.1 | bsz 31.3 | num_updates 2092 | best_loss 0.919
2022-01-13 00:08:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2092 updates
2022-01-13 00:08:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:08:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:08:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 21 @ 2092 updates, score 0.919) (writing took 3.965578320901841 seconds)
2022-01-13 00:08:26 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-13 00:08:26 | INFO | train | epoch 021 | loss 0.965 | ppl 1.95 | wps 4992.1 | ups 7.7 | wpb 648.2 | bsz 31.7 | num_updates 2092 | lr 2.75508e-05 | gnorm 4.959 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 288
2022-01-13 00:08:26 | INFO | fairseq.trainer | begin training epoch 22
2022-01-13 00:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:08:27 | INFO | train_inner | epoch 022:      8 / 100 loss=0.983, ppl=1.98, wps=1096.2, ups=1.73, wpb=632.8, bsz=32, num_updates=2100, lr=2.75385e-05, gnorm=4.579, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.1, wall=288
2022-01-13 00:08:27 | INFO | train_inner | epoch 022:     18 / 100 loss=0.959, ppl=1.94, wps=9207.8, ups=14.84, wpb=620.5, bsz=32, num_updates=2110, lr=2.75231e-05, gnorm=4.87, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=289
2022-01-13 00:08:28 | INFO | train_inner | epoch 022:     28 / 100 loss=0.839, ppl=1.79, wps=9754.4, ups=14.5, wpb=672.8, bsz=32, num_updates=2120, lr=2.75077e-05, gnorm=4.436, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=290
2022-01-13 00:08:29 | INFO | train_inner | epoch 022:     38 / 100 loss=0.959, ppl=1.94, wps=9938.2, ups=15.41, wpb=644.8, bsz=32, num_updates=2130, lr=2.74923e-05, gnorm=5.585, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=290
2022-01-13 00:08:30 | INFO | train_inner | epoch 022:     48 / 100 loss=0.952, ppl=1.93, wps=7650, ups=13.18, wpb=580.3, bsz=29.4, num_updates=2140, lr=2.74769e-05, gnorm=4.613, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=291
2022-01-13 00:08:30 | INFO | train_inner | epoch 022:     58 / 100 loss=0.922, ppl=1.89, wps=11852.7, ups=13.43, wpb=882.6, bsz=32, num_updates=2150, lr=2.74615e-05, gnorm=3.815, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.2, wall=292
2022-01-13 00:08:31 | INFO | train_inner | epoch 022:     68 / 100 loss=0.906, ppl=1.87, wps=9881.9, ups=14.9, wpb=663.1, bsz=32, num_updates=2160, lr=2.74462e-05, gnorm=4.342, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=292
2022-01-13 00:08:32 | INFO | train_inner | epoch 022:     78 / 100 loss=1.038, ppl=2.05, wps=8699.7, ups=13.68, wpb=636.1, bsz=32, num_updates=2170, lr=2.74308e-05, gnorm=6.849, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=293
2022-01-13 00:08:32 | INFO | train_inner | epoch 022:     88 / 100 loss=1.013, ppl=2.02, wps=8072.4, ups=13.52, wpb=597.1, bsz=32, num_updates=2180, lr=2.74154e-05, gnorm=4.493, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=294
2022-01-13 00:08:33 | INFO | train_inner | epoch 022:     98 / 100 loss=0.897, ppl=1.86, wps=6156.8, ups=12.02, wpb=512.3, bsz=32, num_updates=2190, lr=2.74e-05, gnorm=5.416, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=295
2022-01-13 00:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:08:34 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 0.895 | ppl 1.86 | wps 24571 | wpb 607.1 | bsz 31.3 | num_updates 2192 | best_loss 0.895
2022-01-13 00:08:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2192 updates
2022-01-13 00:08:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:08:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:08:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 22 @ 2192 updates, score 0.895) (writing took 4.085954895010218 seconds)
2022-01-13 00:08:38 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-13 00:08:38 | INFO | train | epoch 022 | loss 0.943 | ppl 1.92 | wps 5302 | ups 8.18 | wpb 648.2 | bsz 31.7 | num_updates 2192 | lr 2.73969e-05 | gnorm 4.872 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 300
2022-01-13 00:08:38 | INFO | fairseq.trainer | begin training epoch 23
2022-01-13 00:08:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:08:39 | INFO | train_inner | epoch 023:      8 / 100 loss=0.894, ppl=1.86, wps=1270.9, ups=1.75, wpb=727.6, bsz=32, num_updates=2200, lr=2.73846e-05, gnorm=3.708, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.5, wall=300
2022-01-13 00:08:40 | INFO | train_inner | epoch 023:     18 / 100 loss=0.914, ppl=1.88, wps=9787.2, ups=14.78, wpb=662.4, bsz=32, num_updates=2210, lr=2.73692e-05, gnorm=4.451, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.5, wall=301
2022-01-13 00:08:40 | INFO | train_inner | epoch 023:     28 / 100 loss=0.828, ppl=1.78, wps=10000.9, ups=14.97, wpb=668, bsz=32, num_updates=2220, lr=2.73538e-05, gnorm=3.979, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=302
2022-01-13 00:08:41 | INFO | train_inner | epoch 023:     38 / 100 loss=0.871, ppl=1.83, wps=9336.7, ups=13.21, wpb=706.9, bsz=32, num_updates=2230, lr=2.73385e-05, gnorm=4.306, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=303
2022-01-13 00:08:42 | INFO | train_inner | epoch 023:     48 / 100 loss=0.874, ppl=1.83, wps=6994.5, ups=13.34, wpb=524.4, bsz=32, num_updates=2240, lr=2.73231e-05, gnorm=4.788, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=303
2022-01-13 00:08:43 | INFO | train_inner | epoch 023:     58 / 100 loss=0.945, ppl=1.93, wps=9330.8, ups=13.24, wpb=704.7, bsz=29.4, num_updates=2250, lr=2.73077e-05, gnorm=4.489, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=304
2022-01-13 00:08:43 | INFO | train_inner | epoch 023:     68 / 100 loss=0.999, ppl=2, wps=8500.8, ups=13.52, wpb=628.7, bsz=32, num_updates=2260, lr=2.72923e-05, gnorm=4.706, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=305
2022-01-13 00:08:44 | INFO | train_inner | epoch 023:     78 / 100 loss=0.946, ppl=1.93, wps=7079.5, ups=13.51, wpb=524.2, bsz=32, num_updates=2270, lr=2.72769e-05, gnorm=8.638, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=306
2022-01-13 00:08:45 | INFO | train_inner | epoch 023:     88 / 100 loss=0.887, ppl=1.85, wps=10045.9, ups=14.2, wpb=707.7, bsz=32, num_updates=2280, lr=2.72615e-05, gnorm=4.192, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=306
2022-01-13 00:08:46 | INFO | train_inner | epoch 023:     98 / 100 loss=1.02, ppl=2.03, wps=7250, ups=11.62, wpb=624.1, bsz=32, num_updates=2290, lr=2.72462e-05, gnorm=5.064, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=307
2022-01-13 00:08:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:08:47 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 0.926 | ppl 1.9 | wps 21926.4 | wpb 607.1 | bsz 31.3 | num_updates 2292 | best_loss 0.895
2022-01-13 00:08:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2292 updates
2022-01-13 00:08:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:08:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:08:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 23 @ 2292 updates, score 0.926) (writing took 2.5516633121296763 seconds)
2022-01-13 00:08:49 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-13 00:08:49 | INFO | train | epoch 023 | loss 0.915 | ppl 1.89 | wps 5912.8 | ups 9.12 | wpb 648.2 | bsz 31.7 | num_updates 2292 | lr 2.72431e-05 | gnorm 4.838 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 311
2022-01-13 00:08:49 | INFO | fairseq.trainer | begin training epoch 24
2022-01-13 00:08:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:08:50 | INFO | train_inner | epoch 024:      8 / 100 loss=0.871, ppl=1.83, wps=1259.1, ups=2.32, wpb=543.2, bsz=32, num_updates=2300, lr=2.72308e-05, gnorm=4.954, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=311
2022-01-13 00:08:51 | INFO | train_inner | epoch 024:     18 / 100 loss=0.776, ppl=1.71, wps=7596.9, ups=12.17, wpb=624.4, bsz=32, num_updates=2310, lr=2.72154e-05, gnorm=4.142, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=312
2022-01-13 00:08:52 | INFO | train_inner | epoch 024:     28 / 100 loss=0.911, ppl=1.88, wps=8111.3, ups=12.5, wpb=649.1, bsz=32, num_updates=2320, lr=2.72e-05, gnorm=4.392, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=313
2022-01-13 00:08:52 | INFO | train_inner | epoch 024:     38 / 100 loss=0.877, ppl=1.84, wps=9087.4, ups=13.17, wpb=690, bsz=29.4, num_updates=2330, lr=2.71846e-05, gnorm=5.334, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=314
2022-01-13 00:08:53 | INFO | train_inner | epoch 024:     48 / 100 loss=0.87, ppl=1.83, wps=6461.6, ups=11.26, wpb=573.8, bsz=32, num_updates=2340, lr=2.71692e-05, gnorm=5.331, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=315
2022-01-13 00:08:54 | INFO | train_inner | epoch 024:     58 / 100 loss=0.871, ppl=1.83, wps=6342.9, ups=11.44, wpb=554.4, bsz=32, num_updates=2350, lr=2.71538e-05, gnorm=4.592, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=316
2022-01-13 00:08:55 | INFO | train_inner | epoch 024:     68 / 100 loss=0.942, ppl=1.92, wps=8324.6, ups=9.25, wpb=899.6, bsz=32, num_updates=2360, lr=2.71385e-05, gnorm=3.684, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=317
2022-01-13 00:08:56 | INFO | train_inner | epoch 024:     78 / 100 loss=1.019, ppl=2.03, wps=5806.6, ups=10.72, wpb=541.8, bsz=32, num_updates=2370, lr=2.71231e-05, gnorm=4.947, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=318
2022-01-13 00:08:57 | INFO | train_inner | epoch 024:     88 / 100 loss=0.912, ppl=1.88, wps=7044.8, ups=12.52, wpb=562.6, bsz=32, num_updates=2380, lr=2.71077e-05, gnorm=5.74, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=318
2022-01-13 00:08:58 | INFO | train_inner | epoch 024:     98 / 100 loss=0.89, ppl=1.85, wps=10603, ups=12.25, wpb=865.3, bsz=32, num_updates=2390, lr=2.70923e-05, gnorm=3.739, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=319
2022-01-13 00:08:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:08:59 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 0.889 | ppl 1.85 | wps 20109.9 | wpb 607.1 | bsz 31.3 | num_updates 2392 | best_loss 0.889
2022-01-13 00:08:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2392 updates
2022-01-13 00:08:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:09:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:09:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 24 @ 2392 updates, score 0.889) (writing took 4.415355904959142 seconds)
2022-01-13 00:09:03 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-13 00:09:03 | INFO | train | epoch 024 | loss 0.895 | ppl 1.86 | wps 4614.1 | ups 7.12 | wpb 648.2 | bsz 31.7 | num_updates 2392 | lr 2.70892e-05 | gnorm 4.694 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 325
2022-01-13 00:09:03 | INFO | fairseq.trainer | begin training epoch 25
2022-01-13 00:09:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:09:04 | INFO | train_inner | epoch 025:      8 / 100 loss=0.874, ppl=1.83, wps=1087.6, ups=1.59, wpb=685.6, bsz=32, num_updates=2400, lr=2.70769e-05, gnorm=4.828, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=326
2022-01-13 00:09:05 | INFO | train_inner | epoch 025:     18 / 100 loss=0.852, ppl=1.8, wps=7164.3, ups=12.48, wpb=573.9, bsz=32, num_updates=2410, lr=2.70615e-05, gnorm=4.601, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=326
2022-01-13 00:09:06 | INFO | train_inner | epoch 025:     28 / 100 loss=0.895, ppl=1.86, wps=5952.3, ups=12.9, wpb=461.6, bsz=29.4, num_updates=2420, lr=2.70462e-05, gnorm=8.227, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=327
2022-01-13 00:09:06 | INFO | train_inner | epoch 025:     38 / 100 loss=0.827, ppl=1.77, wps=5451, ups=12.33, wpb=442.2, bsz=32, num_updates=2430, lr=2.70308e-05, gnorm=5.564, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=328
2022-01-13 00:09:07 | INFO | train_inner | epoch 025:     48 / 100 loss=0.809, ppl=1.75, wps=8474.1, ups=12.61, wpb=672.1, bsz=32, num_updates=2440, lr=2.70154e-05, gnorm=4.7, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=329
2022-01-13 00:09:08 | INFO | train_inner | epoch 025:     58 / 100 loss=0.886, ppl=1.85, wps=7643.4, ups=11.81, wpb=647.2, bsz=32, num_updates=2450, lr=2.7e-05, gnorm=4.544, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=330
2022-01-13 00:09:09 | INFO | train_inner | epoch 025:     68 / 100 loss=0.836, ppl=1.78, wps=8912.3, ups=11.67, wpb=763.4, bsz=32, num_updates=2460, lr=2.69846e-05, gnorm=3.727, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=330
2022-01-13 00:09:10 | INFO | train_inner | epoch 025:     78 / 100 loss=0.836, ppl=1.79, wps=7772.9, ups=11.85, wpb=656.1, bsz=32, num_updates=2470, lr=2.69692e-05, gnorm=5.921, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=331
2022-01-13 00:09:11 | INFO | train_inner | epoch 025:     88 / 100 loss=0.919, ppl=1.89, wps=8968.3, ups=10.68, wpb=839.5, bsz=32, num_updates=2480, lr=2.69538e-05, gnorm=4.196, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=332
2022-01-13 00:09:12 | INFO | train_inner | epoch 025:     98 / 100 loss=0.892, ppl=1.86, wps=7325.9, ups=10.3, wpb=711.4, bsz=32, num_updates=2490, lr=2.69385e-05, gnorm=4.007, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=333
2022-01-13 00:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:09:13 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 0.883 | ppl 1.84 | wps 21370.7 | wpb 607.1 | bsz 31.3 | num_updates 2492 | best_loss 0.883
2022-01-13 00:09:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2492 updates
2022-01-13 00:09:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:09:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:09:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 25 @ 2492 updates, score 0.883) (writing took 4.118399356957525 seconds)
2022-01-13 00:09:17 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-13 00:09:17 | INFO | train | epoch 025 | loss 0.864 | ppl 1.82 | wps 4759.5 | ups 7.34 | wpb 648.2 | bsz 31.7 | num_updates 2492 | lr 2.69354e-05 | gnorm 5.015 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 338
2022-01-13 00:09:17 | INFO | fairseq.trainer | begin training epoch 26
2022-01-13 00:09:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:09:18 | INFO | train_inner | epoch 026:      8 / 100 loss=0.857, ppl=1.81, wps=1080.9, ups=1.71, wpb=630.4, bsz=32, num_updates=2500, lr=2.69231e-05, gnorm=4.302, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.1, wall=339
2022-01-13 00:09:18 | INFO | train_inner | epoch 026:     18 / 100 loss=0.898, ppl=1.86, wps=11709.6, ups=13.94, wpb=839.9, bsz=29.4, num_updates=2510, lr=2.69077e-05, gnorm=4.315, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=340
2022-01-13 00:09:19 | INFO | train_inner | epoch 026:     28 / 100 loss=0.872, ppl=1.83, wps=10098.5, ups=14.8, wpb=682.5, bsz=32, num_updates=2520, lr=2.68923e-05, gnorm=4.184, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=340
2022-01-13 00:09:20 | INFO | train_inner | epoch 026:     38 / 100 loss=0.861, ppl=1.82, wps=10363, ups=14.66, wpb=706.9, bsz=32, num_updates=2530, lr=2.68769e-05, gnorm=4.152, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=341
2022-01-13 00:09:20 | INFO | train_inner | epoch 026:     48 / 100 loss=0.853, ppl=1.81, wps=6179, ups=13.6, wpb=454.3, bsz=32, num_updates=2540, lr=2.68615e-05, gnorm=5.001, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=342
2022-01-13 00:09:21 | INFO | train_inner | epoch 026:     58 / 100 loss=0.805, ppl=1.75, wps=10426.7, ups=13.28, wpb=785.1, bsz=32, num_updates=2550, lr=2.68462e-05, gnorm=3.825, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=343
2022-01-13 00:09:22 | INFO | train_inner | epoch 026:     68 / 100 loss=0.849, ppl=1.8, wps=7401, ups=13.49, wpb=548.8, bsz=32, num_updates=2560, lr=2.68308e-05, gnorm=4.606, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=343
2022-01-13 00:09:23 | INFO | train_inner | epoch 026:     78 / 100 loss=0.746, ppl=1.68, wps=9705.2, ups=13.74, wpb=706.4, bsz=32, num_updates=2570, lr=2.68154e-05, gnorm=4.285, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=344
2022-01-13 00:09:24 | INFO | train_inner | epoch 026:     88 / 100 loss=0.814, ppl=1.76, wps=5196.6, ups=9.05, wpb=574.3, bsz=32, num_updates=2580, lr=2.68e-05, gnorm=5.257, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=345
2022-01-13 00:09:25 | INFO | train_inner | epoch 026:     98 / 100 loss=0.864, ppl=1.82, wps=6836.2, ups=12.08, wpb=566, bsz=32, num_updates=2590, lr=2.67846e-05, gnorm=5.366, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=346
2022-01-13 00:09:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:09:26 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 0.873 | ppl 1.83 | wps 19865.2 | wpb 607.1 | bsz 31.3 | num_updates 2592 | best_loss 0.873
2022-01-13 00:09:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2592 updates
2022-01-13 00:09:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:09:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:09:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 26 @ 2592 updates, score 0.873) (writing took 4.859125379938632 seconds)
2022-01-13 00:09:31 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-13 00:09:31 | INFO | train | epoch 026 | loss 0.84 | ppl 1.79 | wps 4756.5 | ups 7.34 | wpb 648.2 | bsz 31.7 | num_updates 2592 | lr 2.67815e-05 | gnorm 4.535 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 352
2022-01-13 00:09:31 | INFO | fairseq.trainer | begin training epoch 27
2022-01-13 00:09:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:09:31 | INFO | train_inner | epoch 027:      8 / 100 loss=0.836, ppl=1.79, wps=905.5, ups=1.51, wpb=600.7, bsz=32, num_updates=2600, lr=2.67692e-05, gnorm=4.333, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=353
2022-01-13 00:09:32 | INFO | train_inner | epoch 027:     18 / 100 loss=0.814, ppl=1.76, wps=11868.7, ups=13.65, wpb=869.4, bsz=32, num_updates=2610, lr=2.67538e-05, gnorm=3.756, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=353
2022-01-13 00:09:33 | INFO | train_inner | epoch 027:     28 / 100 loss=0.8, ppl=1.74, wps=10530.9, ups=15.15, wpb=695.1, bsz=32, num_updates=2620, lr=2.67385e-05, gnorm=3.993, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=354
2022-01-13 00:09:33 | INFO | train_inner | epoch 027:     38 / 100 loss=0.813, ppl=1.76, wps=8705.8, ups=13.46, wpb=646.8, bsz=32, num_updates=2630, lr=2.67231e-05, gnorm=4.332, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=355
2022-01-13 00:09:34 | INFO | train_inner | epoch 027:     48 / 100 loss=0.808, ppl=1.75, wps=5556.4, ups=11.79, wpb=471.2, bsz=32, num_updates=2640, lr=2.67077e-05, gnorm=4.986, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=356
2022-01-13 00:09:35 | INFO | train_inner | epoch 027:     58 / 100 loss=0.702, ppl=1.63, wps=7877.3, ups=12.49, wpb=630.7, bsz=29.4, num_updates=2650, lr=2.66923e-05, gnorm=3.873, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=356
2022-01-13 00:09:36 | INFO | train_inner | epoch 027:     68 / 100 loss=0.784, ppl=1.72, wps=7451.7, ups=12.42, wpb=599.8, bsz=32, num_updates=2660, lr=2.66769e-05, gnorm=5.037, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=357
2022-01-13 00:09:37 | INFO | train_inner | epoch 027:     78 / 100 loss=0.919, ppl=1.89, wps=7111.7, ups=11.02, wpb=645.3, bsz=32, num_updates=2670, lr=2.66615e-05, gnorm=58.451, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=358
2022-01-13 00:09:38 | INFO | train_inner | epoch 027:     88 / 100 loss=0.909, ppl=1.88, wps=7992.8, ups=12.16, wpb=657.5, bsz=32, num_updates=2680, lr=2.66462e-05, gnorm=4.371, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=359
2022-01-13 00:09:38 | INFO | train_inner | epoch 027:     98 / 100 loss=0.75, ppl=1.68, wps=7684.4, ups=11.96, wpb=642.4, bsz=32, num_updates=2690, lr=2.66308e-05, gnorm=4.033, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=360
2022-01-13 00:09:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:09:40 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 0.861 | ppl 1.82 | wps 18593.4 | wpb 607.1 | bsz 31.3 | num_updates 2692 | best_loss 0.861
2022-01-13 00:09:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2692 updates
2022-01-13 00:09:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:09:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:09:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 27 @ 2692 updates, score 0.861) (writing took 4.241265110904351 seconds)
2022-01-13 00:09:44 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-13 00:09:44 | INFO | train | epoch 027 | loss 0.811 | ppl 1.75 | wps 4892.7 | ups 7.55 | wpb 648.2 | bsz 31.7 | num_updates 2692 | lr 2.66277e-05 | gnorm 9.699 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 365
2022-01-13 00:09:44 | INFO | fairseq.trainer | begin training epoch 28
2022-01-13 00:09:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:09:45 | INFO | train_inner | epoch 028:      8 / 100 loss=0.74, ppl=1.67, wps=1189.9, ups=1.63, wpb=728, bsz=32, num_updates=2700, lr=2.66154e-05, gnorm=3.697, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=366
2022-01-13 00:09:45 | INFO | train_inner | epoch 028:     18 / 100 loss=0.851, ppl=1.8, wps=8338.8, ups=14.87, wpb=560.8, bsz=32, num_updates=2710, lr=2.66e-05, gnorm=4.318, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=367
2022-01-13 00:09:46 | INFO | train_inner | epoch 028:     28 / 100 loss=0.931, ppl=1.91, wps=10306.5, ups=14.68, wpb=702, bsz=32, num_updates=2720, lr=2.65846e-05, gnorm=4.361, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=367
2022-01-13 00:09:47 | INFO | train_inner | epoch 028:     38 / 100 loss=0.779, ppl=1.72, wps=9957.5, ups=15.13, wpb=658.3, bsz=29.4, num_updates=2730, lr=2.65692e-05, gnorm=3.961, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=368
2022-01-13 00:09:47 | INFO | train_inner | epoch 028:     48 / 100 loss=0.779, ppl=1.72, wps=7945.2, ups=13.69, wpb=580.2, bsz=32, num_updates=2740, lr=2.65538e-05, gnorm=4.648, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=369
2022-01-13 00:09:48 | INFO | train_inner | epoch 028:     58 / 100 loss=0.807, ppl=1.75, wps=9514.6, ups=14.31, wpb=664.8, bsz=32, num_updates=2750, lr=2.65385e-05, gnorm=3.987, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=369
2022-01-13 00:09:49 | INFO | train_inner | epoch 028:     68 / 100 loss=0.724, ppl=1.65, wps=9268.5, ups=15.2, wpb=609.8, bsz=32, num_updates=2760, lr=2.65231e-05, gnorm=4.033, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=370
2022-01-13 00:09:49 | INFO | train_inner | epoch 028:     78 / 100 loss=0.767, ppl=1.7, wps=7600.9, ups=14.44, wpb=526.5, bsz=32, num_updates=2770, lr=2.65077e-05, gnorm=13.432, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=371
2022-01-13 00:09:50 | INFO | train_inner | epoch 028:     88 / 100 loss=0.782, ppl=1.72, wps=10258.9, ups=14.5, wpb=707.6, bsz=32, num_updates=2780, lr=2.64923e-05, gnorm=4.09, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=371
2022-01-13 00:09:51 | INFO | train_inner | epoch 028:     98 / 100 loss=0.818, ppl=1.76, wps=8143.7, ups=11.67, wpb=697.9, bsz=32, num_updates=2790, lr=2.64769e-05, gnorm=4.21, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=372
2022-01-13 00:09:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:09:52 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 0.855 | ppl 1.81 | wps 23983.8 | wpb 607.1 | bsz 31.3 | num_updates 2792 | best_loss 0.855
2022-01-13 00:09:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2792 updates
2022-01-13 00:09:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:09:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 28 @ 2792 updates, score 0.855) (writing took 4.1090851000044495 seconds)
2022-01-13 00:09:56 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-13 00:09:56 | INFO | train | epoch 028 | loss 0.801 | ppl 1.74 | wps 5317.5 | ups 8.2 | wpb 648.2 | bsz 31.7 | num_updates 2792 | lr 2.64738e-05 | gnorm 5.07 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.7 | wall 377
2022-01-13 00:09:56 | INFO | fairseq.trainer | begin training epoch 29
2022-01-13 00:09:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:09:57 | INFO | train_inner | epoch 029:      8 / 100 loss=0.832, ppl=1.78, wps=1402.5, ups=1.71, wpb=819.1, bsz=32, num_updates=2800, lr=2.64615e-05, gnorm=3.59, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=378
2022-01-13 00:09:57 | INFO | train_inner | epoch 029:     18 / 100 loss=0.751, ppl=1.68, wps=9727.9, ups=14.93, wpb=651.4, bsz=32, num_updates=2810, lr=2.64462e-05, gnorm=4.26, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=379
2022-01-13 00:09:58 | INFO | train_inner | epoch 029:     28 / 100 loss=0.747, ppl=1.68, wps=10815.4, ups=12.93, wpb=836.7, bsz=32, num_updates=2820, lr=2.64308e-05, gnorm=5.005, clip=100, loss_scale=0.5, train_wall=1, gb_free=19.7, wall=380
2022-01-13 00:09:59 | INFO | train_inner | epoch 029:     38 / 100 loss=0.779, ppl=1.72, wps=8479.3, ups=13.88, wpb=610.8, bsz=32, num_updates=2830, lr=2.64154e-05, gnorm=4.497, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=380
2022-01-13 00:10:00 | INFO | train_inner | epoch 029:     48 / 100 loss=0.838, ppl=1.79, wps=7165.8, ups=12.49, wpb=573.6, bsz=32, num_updates=2840, lr=2.64e-05, gnorm=9.912, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=381
2022-01-13 00:10:01 | INFO | train_inner | epoch 029:     58 / 100 loss=0.687, ppl=1.61, wps=6634, ups=11.54, wpb=574.8, bsz=32, num_updates=2850, lr=2.63846e-05, gnorm=3.939, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=382
2022-01-13 00:10:02 | INFO | train_inner | epoch 029:     68 / 100 loss=0.736, ppl=1.67, wps=6030.8, ups=9.97, wpb=605.1, bsz=29.4, num_updates=2860, lr=2.63692e-05, gnorm=4.231, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=383
2022-01-13 00:10:02 | INFO | train_inner | epoch 029:     78 / 100 loss=0.812, ppl=1.76, wps=8113.3, ups=11.97, wpb=677.9, bsz=32, num_updates=2870, lr=2.63538e-05, gnorm=4.326, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.1, wall=384
2022-01-13 00:10:03 | INFO | train_inner | epoch 029:     88 / 100 loss=0.823, ppl=1.77, wps=6141.6, ups=10.01, wpb=613.3, bsz=32, num_updates=2880, lr=2.63385e-05, gnorm=4.788, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=385
2022-01-13 00:10:04 | INFO | train_inner | epoch 029:     98 / 100 loss=0.757, ppl=1.69, wps=6356.2, ups=10.21, wpb=622.6, bsz=32, num_updates=2890, lr=2.63231e-05, gnorm=4.26, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=386
2022-01-13 00:10:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:10:06 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 0.854 | ppl 1.81 | wps 20647.6 | wpb 607.1 | bsz 31.3 | num_updates 2892 | best_loss 0.854
2022-01-13 00:10:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2892 updates
2022-01-13 00:10:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:10:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:10:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 29 @ 2892 updates, score 0.854) (writing took 4.085362393874675 seconds)
2022-01-13 00:10:10 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-13 00:10:10 | INFO | train | epoch 029 | loss 0.775 | ppl 1.71 | wps 4780.4 | ups 7.38 | wpb 648.2 | bsz 31.7 | num_updates 2892 | lr 2.632e-05 | gnorm 4.896 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 391
2022-01-13 00:10:10 | INFO | fairseq.trainer | begin training epoch 30
2022-01-13 00:10:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:10:10 | INFO | train_inner | epoch 030:      8 / 100 loss=0.714, ppl=1.64, wps=1361.5, ups=1.71, wpb=796.6, bsz=32, num_updates=2900, lr=2.63077e-05, gnorm=3.82, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=392
2022-01-13 00:10:11 | INFO | train_inner | epoch 030:     18 / 100 loss=0.763, ppl=1.7, wps=8886.5, ups=15.21, wpb=584.3, bsz=32, num_updates=2910, lr=2.62923e-05, gnorm=4.416, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=392
2022-01-13 00:10:12 | INFO | train_inner | epoch 030:     28 / 100 loss=0.753, ppl=1.69, wps=8749, ups=15.06, wpb=581, bsz=32, num_updates=2920, lr=2.62769e-05, gnorm=5.094, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=393
2022-01-13 00:10:12 | INFO | train_inner | epoch 030:     38 / 100 loss=0.778, ppl=1.71, wps=10493.4, ups=14.96, wpb=701.6, bsz=32, num_updates=2930, lr=2.62615e-05, gnorm=4.219, clip=100, loss_scale=0.5, train_wall=1, gb_free=19.7, wall=394
2022-01-13 00:10:13 | INFO | train_inner | epoch 030:     48 / 100 loss=0.786, ppl=1.72, wps=10616.3, ups=13.31, wpb=797.8, bsz=32, num_updates=2940, lr=2.62462e-05, gnorm=5.354, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=394
2022-01-13 00:10:14 | INFO | train_inner | epoch 030:     58 / 100 loss=0.724, ppl=1.65, wps=6351.1, ups=11.77, wpb=539.5, bsz=29.4, num_updates=2950, lr=2.62308e-05, gnorm=7.996, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=395
2022-01-13 00:10:15 | INFO | train_inner | epoch 030:     68 / 100 loss=0.799, ppl=1.74, wps=7712.1, ups=13.54, wpb=569.6, bsz=32, num_updates=2960, lr=2.62154e-05, gnorm=4.577, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.4, wall=396
2022-01-13 00:10:15 | INFO | train_inner | epoch 030:     78 / 100 loss=0.737, ppl=1.67, wps=8158.7, ups=15.84, wpb=515, bsz=32, num_updates=2970, lr=2.62e-05, gnorm=4.625, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=397
2022-01-13 00:10:16 | INFO | train_inner | epoch 030:     88 / 100 loss=0.735, ppl=1.66, wps=10234.1, ups=14.73, wpb=694.9, bsz=32, num_updates=2980, lr=2.61846e-05, gnorm=3.896, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=397
2022-01-13 00:10:17 | INFO | train_inner | epoch 030:     98 / 100 loss=0.775, ppl=1.71, wps=6535.3, ups=10.15, wpb=644, bsz=32, num_updates=2990, lr=2.61692e-05, gnorm=4.391, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=398
2022-01-13 00:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:10:18 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 0.847 | ppl 1.8 | wps 20502.7 | wpb 607.1 | bsz 31.3 | num_updates 2992 | best_loss 0.847
2022-01-13 00:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2992 updates
2022-01-13 00:10:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:10:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:10:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 30 @ 2992 updates, score 0.847) (writing took 4.29077587206848 seconds)
2022-01-13 00:10:22 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-13 00:10:22 | INFO | train | epoch 030 | loss 0.757 | ppl 1.69 | wps 5089.3 | ups 7.85 | wpb 648.2 | bsz 31.7 | num_updates 2992 | lr 2.61662e-05 | gnorm 4.833 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.4 | wall 404
2022-01-13 00:10:22 | INFO | fairseq.trainer | begin training epoch 31
2022-01-13 00:10:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:10:23 | INFO | train_inner | epoch 031:      8 / 100 loss=0.725, ppl=1.65, wps=1114.1, ups=1.65, wpb=676.7, bsz=32, num_updates=3000, lr=2.61538e-05, gnorm=3.86, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=404
2022-01-13 00:10:24 | INFO | train_inner | epoch 031:     18 / 100 loss=0.648, ppl=1.57, wps=8195.6, ups=14.04, wpb=583.9, bsz=32, num_updates=3010, lr=2.61385e-05, gnorm=51.162, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=405
2022-01-13 00:10:25 | INFO | train_inner | epoch 031:     28 / 100 loss=0.726, ppl=1.65, wps=6743.2, ups=12.29, wpb=548.8, bsz=32, num_updates=3020, lr=2.61231e-05, gnorm=4.283, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=406
2022-01-13 00:10:25 | INFO | train_inner | epoch 031:     38 / 100 loss=0.652, ppl=1.57, wps=6313.6, ups=12.41, wpb=508.9, bsz=32, num_updates=3030, lr=2.61077e-05, gnorm=4.672, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=407
2022-01-13 00:10:26 | INFO | train_inner | epoch 031:     48 / 100 loss=0.807, ppl=1.75, wps=9614.7, ups=12.7, wpb=757.1, bsz=32, num_updates=3040, lr=2.60923e-05, gnorm=3.82, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=408
2022-01-13 00:10:27 | INFO | train_inner | epoch 031:     58 / 100 loss=0.765, ppl=1.7, wps=9478.8, ups=12.84, wpb=738.4, bsz=32, num_updates=3050, lr=2.60769e-05, gnorm=3.894, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=408
2022-01-13 00:10:28 | INFO | train_inner | epoch 031:     68 / 100 loss=0.779, ppl=1.72, wps=10961.1, ups=13.09, wpb=837.6, bsz=29.4, num_updates=3060, lr=2.60615e-05, gnorm=3.796, clip=100, loss_scale=0.5, train_wall=1, gb_free=19.9, wall=409
2022-01-13 00:10:28 | INFO | train_inner | epoch 031:     78 / 100 loss=0.771, ppl=1.71, wps=7548.9, ups=13.34, wpb=565.8, bsz=32, num_updates=3070, lr=2.60462e-05, gnorm=4.693, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=410
2022-01-13 00:10:29 | INFO | train_inner | epoch 031:     88 / 100 loss=0.725, ppl=1.65, wps=9338.9, ups=11.3, wpb=826.7, bsz=32, num_updates=3080, lr=2.60308e-05, gnorm=3.678, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=411
2022-01-13 00:10:30 | INFO | train_inner | epoch 031:     98 / 100 loss=0.69, ppl=1.61, wps=6285.3, ups=12.36, wpb=508.7, bsz=32, num_updates=3090, lr=2.60154e-05, gnorm=4.286, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=412
2022-01-13 00:10:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:10:31 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 0.811 | ppl 1.75 | wps 19801.3 | wpb 607.1 | bsz 31.3 | num_updates 3092 | best_loss 0.811
2022-01-13 00:10:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 3092 updates
2022-01-13 00:10:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:10:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt
2022-01-13 00:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_best.pt (epoch 31 @ 3092 updates, score 0.811) (writing took 4.68887378112413 seconds)
2022-01-13 00:10:36 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-13 00:10:36 | INFO | train | epoch 031 | loss 0.737 | ppl 1.67 | wps 4765.9 | ups 7.35 | wpb 648.2 | bsz 31.7 | num_updates 3092 | lr 2.60123e-05 | gnorm 8.843 | clip 100 | loss_scale 0.5 | train_wall 8 | gb_free 20.8 | wall 417
2022-01-13 00:10:36 | INFO | fairseq.trainer | begin training epoch 32
2022-01-13 00:10:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:10:37 | INFO | train_inner | epoch 032:      8 / 100 loss=0.687, ppl=1.61, wps=852.8, ups=1.55, wpb=551.8, bsz=32, num_updates=3100, lr=2.6e-05, gnorm=4.134, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=418
2022-01-13 00:10:37 | INFO | train_inner | epoch 032:     18 / 100 loss=0.77, ppl=1.7, wps=8674.5, ups=15.15, wpb=572.5, bsz=32, num_updates=3110, lr=2.59846e-05, gnorm=4.348, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=419
2022-01-13 00:10:38 | INFO | train_inner | epoch 032:     28 / 100 loss=0.693, ppl=1.62, wps=8304.5, ups=13.97, wpb=594.6, bsz=32, num_updates=3120, lr=2.59692e-05, gnorm=4.052, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=419
2022-01-13 00:10:39 | INFO | train_inner | epoch 032:     38 / 100 loss=0.705, ppl=1.63, wps=10270.2, ups=14.79, wpb=694.4, bsz=29.4, num_updates=3130, lr=2.59538e-05, gnorm=4.14, clip=100, loss_scale=0.5, train_wall=1, gb_free=19.9, wall=420
2022-01-13 00:10:39 | INFO | train_inner | epoch 032:     48 / 100 loss=0.594, ppl=1.51, wps=7671.9, ups=15.65, wpb=490.1, bsz=32, num_updates=3140, lr=2.59385e-05, gnorm=4.619, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=421
2022-01-13 00:10:40 | INFO | train_inner | epoch 032:     58 / 100 loss=0.674, ppl=1.6, wps=9771.4, ups=14.68, wpb=665.7, bsz=32, num_updates=3150, lr=2.59231e-05, gnorm=3.757, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=421
2022-01-13 00:10:41 | INFO | train_inner | epoch 032:     68 / 100 loss=0.714, ppl=1.64, wps=11411.1, ups=14, wpb=815, bsz=32, num_updates=3160, lr=2.59077e-05, gnorm=3.384, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=422
2022-01-13 00:10:41 | INFO | train_inner | epoch 032:     78 / 100 loss=0.739, ppl=1.67, wps=8778.5, ups=15.93, wpb=551.2, bsz=32, num_updates=3170, lr=2.58923e-05, gnorm=6.213, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=423
2022-01-13 00:10:42 | INFO | train_inner | epoch 032:     88 / 100 loss=0.822, ppl=1.77, wps=10886.7, ups=13.63, wpb=799, bsz=32, num_updates=3180, lr=2.58769e-05, gnorm=3.914, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=424
2022-01-13 00:10:43 | INFO | train_inner | epoch 032:     98 / 100 loss=0.73, ppl=1.66, wps=8737.4, ups=13.22, wpb=660.9, bsz=32, num_updates=3190, lr=2.58615e-05, gnorm=4.3, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=424
2022-01-13 00:10:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:10:44 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 0.843 | ppl 1.79 | wps 19970.1 | wpb 607.1 | bsz 31.3 | num_updates 3192 | best_loss 0.811
2022-01-13 00:10:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3192 updates
2022-01-13 00:10:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:10:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:10:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 32 @ 3192 updates, score 0.843) (writing took 2.7014940208755434 seconds)
2022-01-13 00:10:47 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-13 00:10:47 | INFO | train | epoch 032 | loss 0.715 | ppl 1.64 | wps 6047.3 | ups 9.33 | wpb 648.2 | bsz 31.7 | num_updates 3192 | lr 2.58585e-05 | gnorm 4.265 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 428
2022-01-13 00:10:47 | INFO | fairseq.trainer | begin training epoch 33
2022-01-13 00:10:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:10:47 | INFO | train_inner | epoch 033:      8 / 100 loss=0.594, ppl=1.51, wps=1185.3, ups=2.24, wpb=529.3, bsz=32, num_updates=3200, lr=2.58462e-05, gnorm=4.141, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=429
2022-01-13 00:10:48 | INFO | train_inner | epoch 033:     18 / 100 loss=0.625, ppl=1.54, wps=11281.1, ups=14.49, wpb=778.4, bsz=29.4, num_updates=3210, lr=2.58308e-05, gnorm=3.326, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.3, wall=429
2022-01-13 00:10:49 | INFO | train_inner | epoch 033:     28 / 100 loss=0.697, ppl=1.62, wps=7444.4, ups=14.79, wpb=503.4, bsz=32, num_updates=3220, lr=2.58154e-05, gnorm=4.511, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=430
2022-01-13 00:10:49 | INFO | train_inner | epoch 033:     38 / 100 loss=0.702, ppl=1.63, wps=10426.7, ups=13.49, wpb=773, bsz=32, num_updates=3230, lr=2.58e-05, gnorm=3.567, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=431
2022-01-13 00:10:50 | INFO | train_inner | epoch 033:     48 / 100 loss=0.674, ppl=1.6, wps=7080.6, ups=13.4, wpb=528.4, bsz=32, num_updates=3240, lr=2.57846e-05, gnorm=4.698, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=432
2022-01-13 00:10:51 | INFO | train_inner | epoch 033:     58 / 100 loss=0.672, ppl=1.59, wps=9755, ups=15.52, wpb=628.4, bsz=32, num_updates=3250, lr=2.57692e-05, gnorm=4.23, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=432
2022-01-13 00:10:52 | INFO | train_inner | epoch 033:     68 / 100 loss=0.703, ppl=1.63, wps=8635.1, ups=13.73, wpb=628.9, bsz=32, num_updates=3260, lr=2.57538e-05, gnorm=4.197, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.6, wall=433
2022-01-13 00:10:52 | INFO | train_inner | epoch 033:     78 / 100 loss=0.667, ppl=1.59, wps=10601.6, ups=14.03, wpb=755.9, bsz=32, num_updates=3270, lr=2.57385e-05, gnorm=3.627, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.7, wall=434
2022-01-13 00:10:53 | INFO | train_inner | epoch 033:     88 / 100 loss=0.73, ppl=1.66, wps=8749.2, ups=14.9, wpb=587.3, bsz=32, num_updates=3280, lr=2.57231e-05, gnorm=15.117, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=434
2022-01-13 00:10:54 | INFO | train_inner | epoch 033:     98 / 100 loss=0.824, ppl=1.77, wps=10062.1, ups=12.58, wpb=799.8, bsz=32, num_updates=3290, lr=2.57077e-05, gnorm=4.129, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=435
2022-01-13 00:10:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:10:55 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 0.837 | ppl 1.79 | wps 19686.8 | wpb 607.1 | bsz 31.3 | num_updates 3292 | best_loss 0.811
2022-01-13 00:10:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3292 updates
2022-01-13 00:10:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:10:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:10:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 33 @ 3292 updates, score 0.837) (writing took 2.602637407835573 seconds)
2022-01-13 00:10:58 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-13 00:10:58 | INFO | train | epoch 033 | loss 0.691 | ppl 1.61 | wps 6003.9 | ups 9.26 | wpb 648.2 | bsz 31.7 | num_updates 3292 | lr 2.57046e-05 | gnorm 5.151 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 439
2022-01-13 00:10:58 | INFO | fairseq.trainer | begin training epoch 34
2022-01-13 00:10:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:10:58 | INFO | train_inner | epoch 034:      8 / 100 loss=0.627, ppl=1.54, wps=1344.4, ups=2.28, wpb=589.7, bsz=32, num_updates=3300, lr=2.56923e-05, gnorm=3.752, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=440
2022-01-13 00:10:59 | INFO | train_inner | epoch 034:     18 / 100 loss=0.618, ppl=1.53, wps=8573.1, ups=13.51, wpb=634.7, bsz=32, num_updates=3310, lr=2.56769e-05, gnorm=4.161, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=440
2022-01-13 00:11:00 | INFO | train_inner | epoch 034:     28 / 100 loss=0.652, ppl=1.57, wps=9411.3, ups=14.39, wpb=654.2, bsz=29.4, num_updates=3320, lr=2.56615e-05, gnorm=3.869, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=441
2022-01-13 00:11:00 | INFO | train_inner | epoch 034:     38 / 100 loss=0.759, ppl=1.69, wps=9200.9, ups=13.14, wpb=700.1, bsz=32, num_updates=3330, lr=2.56462e-05, gnorm=4.009, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=442
2022-01-13 00:11:01 | INFO | train_inner | epoch 034:     48 / 100 loss=0.595, ppl=1.51, wps=9380.2, ups=13.92, wpb=673.7, bsz=32, num_updates=3340, lr=2.56308e-05, gnorm=3.642, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=442
2022-01-13 00:11:02 | INFO | train_inner | epoch 034:     58 / 100 loss=0.678, ppl=1.6, wps=7580.5, ups=14.61, wpb=519, bsz=32, num_updates=3350, lr=2.56154e-05, gnorm=4.892, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.5, wall=443
2022-01-13 00:11:02 | INFO | train_inner | epoch 034:     68 / 100 loss=0.683, ppl=1.61, wps=9851, ups=14.09, wpb=698.9, bsz=32, num_updates=3360, lr=2.56e-05, gnorm=4.172, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=444
2022-01-13 00:11:03 | INFO | train_inner | epoch 034:     78 / 100 loss=0.613, ppl=1.53, wps=9858, ups=13.45, wpb=733.1, bsz=32, num_updates=3370, lr=2.55846e-05, gnorm=12.058, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=445
2022-01-13 00:11:04 | INFO | train_inner | epoch 034:     88 / 100 loss=0.695, ppl=1.62, wps=9268.7, ups=14.08, wpb=658.4, bsz=32, num_updates=3380, lr=2.55692e-05, gnorm=4.269, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=445
2022-01-13 00:11:05 | INFO | train_inner | epoch 034:     98 / 100 loss=0.833, ppl=1.78, wps=8042.9, ups=11.9, wpb=675.6, bsz=32, num_updates=3390, lr=2.55538e-05, gnorm=93.274, clip=100, loss_scale=0.5, train_wall=1, gb_free=20.8, wall=446
2022-01-13 00:11:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:11:06 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 0.858 | ppl 1.81 | wps 20848.8 | wpb 607.1 | bsz 31.3 | num_updates 3392 | best_loss 0.811
2022-01-13 00:11:06 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-01-13 00:11:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3392 updates
2022-01-13 00:11:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:11:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt
2022-01-13 00:11:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.abst/dev/checkpoint_last.pt (epoch 34 @ 3392 updates, score 0.858) (writing took 2.5072672839742154 seconds)
2022-01-13 00:11:08 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-13 00:11:08 | INFO | train | epoch 034 | loss 0.68 | ppl 1.6 | wps 5936.7 | ups 9.16 | wpb 648.2 | bsz 31.7 | num_updates 3392 | lr 2.55508e-05 | gnorm 13.848 | clip 100 | loss_scale 0.5 | train_wall 7 | gb_free 20.8 | wall 450
2022-01-13 00:11:08 | INFO | fairseq_cli.train | done training in 445.0 seconds
