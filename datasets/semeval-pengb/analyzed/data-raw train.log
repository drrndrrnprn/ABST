2022-01-12 23:51:38 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=50, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 50, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-12 23:51:38 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-01-12 23:51:41 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-01-12 23:51:41 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-01-12 23:51:41 | INFO | fairseq_cli.train | model: BARTMLModel
2022-01-12 23:51:41 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-01-12 23:51:41 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-01-12 23:51:41 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-12 23:51:41 | INFO | bartabst.data.data_utils | loaded 908 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-01-12 23:51:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-01-12 23:51:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-12 23:51:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-01-12 23:51:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-12 23:51:45 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-01-12 23:51:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-12 23:51:45 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-12 23:51:45 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-01-12 23:51:45 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-01-12 23:51:46 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-01-12 23:51:47 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-01-12 23:51:47 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-12 23:51:49 | INFO | bartabst.data.data_utils | loaded 3,174 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-01-12 23:51:49 | INFO | fairseq.trainer | begin training epoch 1
2022-01-12 23:51:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:51:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-01-12 23:51:51 | INFO | train_inner | epoch 001:     21 / 50 loss=17.159, ppl=146394, wps=16341.4, ups=14.05, wpb=1196, bsz=62.7, num_updates=20, lr=6e-08, gnorm=23.615, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=6
2022-01-12 23:51:53 | INFO | train_inner | epoch 001:     41 / 50 loss=17.296, ppl=160960, wps=14414.7, ups=11.41, wpb=1263.2, bsz=64, num_updates=40, lr=1.2e-07, gnorm=23.468, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=8
2022-01-12 23:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:51:54 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.132 | ppl 143673 | wps 27473.4 | wpb 556.6 | bsz 30.3 | num_updates 49
2022-01-12 23:51:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 49 updates
2022-01-12 23:51:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:51:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 49 updates, score 17.132) (writing took 5.029121475992724 seconds)
2022-01-12 23:51:59 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-12 23:51:59 | INFO | train | epoch 001 | loss 17.245 | ppl 155295 | wps 6090.9 | ups 5.05 | wpb 1219.7 | bsz 63.5 | num_updates 49 | lr 1.47e-07 | gnorm 23.417 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 14
2022-01-12 23:51:59 | INFO | fairseq.trainer | begin training epoch 2
2022-01-12 23:51:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:00 | INFO | train_inner | epoch 002:     11 / 50 loss=17.221, ppl=152767, wps=3093.8, ups=2.74, wpb=1130.5, bsz=62.7, num_updates=60, lr=1.8e-07, gnorm=23.114, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=15
2022-01-12 23:52:02 | INFO | train_inner | epoch 002:     31 / 50 loss=17.199, ppl=150443, wps=18866, ups=14.44, wpb=1306.8, bsz=64, num_updates=80, lr=2.4e-07, gnorm=22.864, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=17
2022-01-12 23:52:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 16.922 | ppl 124169 | wps 29092.5 | wpb 556.6 | bsz 30.3 | num_updates 99 | best_loss 16.922
2022-01-12 23:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 99 updates
2022-01-12 23:52:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 99 updates, score 16.922) (writing took 4.468111583031714 seconds)
2022-01-12 23:52:08 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-12 23:52:08 | INFO | train | epoch 002 | loss 17.126 | ppl 143014 | wps 7062 | ups 5.73 | wpb 1232.9 | bsz 63.5 | num_updates 99 | lr 2.97e-07 | gnorm 22.899 | clip 100 | loss_scale 64 | train_wall 3 | gb_free 20.9 | wall 23
2022-01-12 23:52:08 | INFO | fairseq.trainer | begin training epoch 3
2022-01-12 23:52:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:08 | INFO | train_inner | epoch 003:      1 / 50 loss=17.043, ppl=135068, wps=3738.2, ups=3.03, wpb=1233.1, bsz=64, num_updates=100, lr=3e-07, gnorm=22.704, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=23
2022-01-12 23:52:10 | INFO | train_inner | epoch 003:     21 / 50 loss=16.922, ppl=124168, wps=16858.9, ups=13.79, wpb=1222.8, bsz=62.7, num_updates=120, lr=3.6e-07, gnorm=22.369, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=25
2022-01-12 23:52:11 | INFO | train_inner | epoch 003:     41 / 50 loss=16.713, ppl=107421, wps=18444.4, ups=13.69, wpb=1347.1, bsz=64, num_updates=140, lr=4.2e-07, gnorm=21.783, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=26
2022-01-12 23:52:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:13 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.449 | ppl 89457.4 | wps 28067.7 | wpb 556.6 | bsz 30.3 | num_updates 149 | best_loss 16.449
2022-01-12 23:52:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 149 updates
2022-01-12 23:52:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 149 updates, score 16.449) (writing took 4.802918447880074 seconds)
2022-01-12 23:52:17 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-12 23:52:17 | INFO | train | epoch 003 | loss 16.775 | ppl 112183 | wps 6585.6 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 149 | lr 4.47e-07 | gnorm 22.104 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 32
2022-01-12 23:52:17 | INFO | fairseq.trainer | begin training epoch 4
2022-01-12 23:52:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:18 | INFO | train_inner | epoch 004:     11 / 50 loss=16.552, ppl=96075.3, wps=3100, ups=2.79, wpb=1109.7, bsz=62.7, num_updates=160, lr=4.8e-07, gnorm=22.008, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=33
2022-01-12 23:52:20 | INFO | train_inner | epoch 004:     31 / 50 loss=16.283, ppl=79750.9, wps=17829.4, ups=14.14, wpb=1261, bsz=64, num_updates=180, lr=5.4e-07, gnorm=21.031, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=35
2022-01-12 23:52:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:22 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 15.807 | ppl 57342.1 | wps 26205.5 | wpb 556.6 | bsz 30.3 | num_updates 199 | best_loss 15.807
2022-01-12 23:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 199 updates
2022-01-12 23:52:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 199 updates, score 15.807) (writing took 4.1711140119004995 seconds)
2022-01-12 23:52:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-12 23:52:26 | INFO | train | epoch 004 | loss 16.271 | ppl 79071.1 | wps 7202.4 | ups 5.84 | wpb 1232.9 | bsz 63.5 | num_updates 199 | lr 5.97e-07 | gnorm 21.383 | clip 100 | loss_scale 64 | train_wall 3 | gb_free 20.9 | wall 41
2022-01-12 23:52:26 | INFO | fairseq.trainer | begin training epoch 5
2022-01-12 23:52:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:26 | INFO | train_inner | epoch 005:      1 / 50 loss=16.097, ppl=70117.1, wps=3768.2, ups=3.11, wpb=1210.8, bsz=64, num_updates=200, lr=6e-07, gnorm=21.412, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=41
2022-01-12 23:52:28 | INFO | train_inner | epoch 005:     21 / 50 loss=15.771, ppl=55917.4, wps=16429.9, ups=13.29, wpb=1235.9, bsz=64, num_updates=220, lr=6.6e-07, gnorm=20.211, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=43
2022-01-12 23:52:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-01-12 23:52:29 | INFO | train_inner | epoch 005:     42 / 50 loss=15.559, ppl=48277.4, wps=18448.4, ups=14.02, wpb=1316.2, bsz=64, num_updates=240, lr=7.2e-07, gnorm=19.435, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=44
2022-01-12 23:52:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:30 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 15.243 | ppl 38786.9 | wps 27354.8 | wpb 556.6 | bsz 30.3 | num_updates 248 | best_loss 15.243
2022-01-12 23:52:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 248 updates
2022-01-12 23:52:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 248 updates, score 15.243) (writing took 4.165005217073485 seconds)
2022-01-12 23:52:34 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-12 23:52:35 | INFO | train | epoch 005 | loss 15.636 | ppl 50916.4 | wps 7098.7 | ups 5.71 | wpb 1243 | bsz 63.5 | num_updates 248 | lr 7.44e-07 | gnorm 20.049 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 50
2022-01-12 23:52:35 | INFO | fairseq.trainer | begin training epoch 6
2022-01-12 23:52:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:35 | INFO | train_inner | epoch 006:     12 / 50 loss=15.337, ppl=41381, wps=3760.2, ups=3.09, wpb=1217.2, bsz=62.7, num_updates=260, lr=7.8e-07, gnorm=19.805, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=51
2022-01-12 23:52:37 | INFO | train_inner | epoch 006:     32 / 50 loss=14.997, ppl=32702.2, wps=18633.8, ups=14.21, wpb=1311.7, bsz=62.7, num_updates=280, lr=8.4e-07, gnorm=19.077, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=52
2022-01-12 23:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:39 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 14.581 | ppl 24503.1 | wps 27435.2 | wpb 556.6 | bsz 30.3 | num_updates 298 | best_loss 14.581
2022-01-12 23:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 298 updates
2022-01-12 23:52:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 298 updates, score 14.581) (writing took 4.1451350569259375 seconds)
2022-01-12 23:52:43 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-12 23:52:43 | INFO | train | epoch 006 | loss 15.033 | ppl 33520 | wps 7267.8 | ups 5.9 | wpb 1232.9 | bsz 63.5 | num_updates 298 | lr 8.94e-07 | gnorm 18.987 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 58
2022-01-12 23:52:43 | INFO | fairseq.trainer | begin training epoch 7
2022-01-12 23:52:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:43 | INFO | train_inner | epoch 007:      2 / 50 loss=14.896, ppl=30486.8, wps=3582.8, ups=3.15, wpb=1136.9, bsz=64, num_updates=300, lr=9e-07, gnorm=18.768, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=58
2022-01-12 23:52:45 | INFO | train_inner | epoch 007:     22 / 50 loss=14.558, ppl=24115.6, wps=18272.6, ups=13.89, wpb=1315.3, bsz=64, num_updates=320, lr=9.6e-07, gnorm=17.902, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=60
2022-01-12 23:52:46 | INFO | train_inner | epoch 007:     42 / 50 loss=14.268, ppl=19731.4, wps=15200.6, ups=14.45, wpb=1052.3, bsz=62.7, num_updates=340, lr=1.02e-06, gnorm=19.044, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=61
2022-01-12 23:52:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:48 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 13.939 | ppl 15700.3 | wps 26097.4 | wpb 556.6 | bsz 30.3 | num_updates 348 | best_loss 13.939
2022-01-12 23:52:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 348 updates
2022-01-12 23:52:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 348 updates, score 13.939) (writing took 4.287074175197631 seconds)
2022-01-12 23:52:52 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-12 23:52:52 | INFO | train | epoch 007 | loss 14.362 | ppl 21061.2 | wps 6958.9 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 348 | lr 1.044e-06 | gnorm 18.136 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 67
2022-01-12 23:52:52 | INFO | fairseq.trainer | begin training epoch 8
2022-01-12 23:52:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:53 | INFO | train_inner | epoch 008:     12 / 50 loss=13.982, ppl=16177, wps=4133.3, ups=2.95, wpb=1399.5, bsz=64, num_updates=360, lr=1.08e-06, gnorm=18.713, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=68
2022-01-12 23:52:54 | INFO | train_inner | epoch 008:     32 / 50 loss=13.673, ppl=13065.5, wps=17796, ups=14.28, wpb=1246.2, bsz=64, num_updates=380, lr=1.14e-06, gnorm=16.927, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=69
2022-01-12 23:52:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 13.325 | ppl 10265.3 | wps 26425.9 | wpb 556.6 | bsz 30.3 | num_updates 398 | best_loss 13.325
2022-01-12 23:52:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 398 updates
2022-01-12 23:52:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 398 updates, score 13.325) (writing took 4.099818815011531 seconds)
2022-01-12 23:53:00 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-12 23:53:00 | INFO | train | epoch 008 | loss 13.671 | ppl 13041.4 | wps 7217.3 | ups 5.85 | wpb 1232.9 | bsz 63.5 | num_updates 398 | lr 1.194e-06 | gnorm 17.703 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 76
2022-01-12 23:53:00 | INFO | fairseq.trainer | begin training epoch 9
2022-01-12 23:53:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:01 | INFO | train_inner | epoch 009:      2 / 50 loss=13.445, ppl=11152.4, wps=3693.6, ups=3.12, wpb=1185.2, bsz=62.7, num_updates=400, lr=1.2e-06, gnorm=16.722, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=76
2022-01-12 23:53:02 | INFO | train_inner | epoch 009:     22 / 50 loss=13.342, ppl=10380.7, wps=16812.5, ups=13.58, wpb=1238, bsz=62.7, num_updates=420, lr=1.26e-06, gnorm=17.348, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=77
2022-01-12 23:53:04 | INFO | train_inner | epoch 009:     42 / 50 loss=13.019, ppl=8299.91, wps=16697.7, ups=13.69, wpb=1220, bsz=64, num_updates=440, lr=1.32e-06, gnorm=14.824, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=79
2022-01-12 23:53:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 12.838 | ppl 7324.38 | wps 26997 | wpb 556.6 | bsz 30.3 | num_updates 448 | best_loss 12.838
2022-01-12 23:53:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 448 updates
2022-01-12 23:53:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 448 updates, score 12.838) (writing took 4.447679174831137 seconds)
2022-01-12 23:53:09 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-12 23:53:09 | INFO | train | epoch 009 | loss 13.134 | ppl 8992.19 | wps 6858.8 | ups 5.56 | wpb 1232.9 | bsz 63.5 | num_updates 448 | lr 1.344e-06 | gnorm 15.786 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 85
2022-01-12 23:53:09 | INFO | fairseq.trainer | begin training epoch 10
2022-01-12 23:53:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:10 | INFO | train_inner | epoch 010:     12 / 50 loss=12.786, ppl=7061.32, wps=3611, ups=2.97, wpb=1217.8, bsz=62.7, num_updates=460, lr=1.38e-06, gnorm=14.703, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=85
2022-01-12 23:53:12 | INFO | train_inner | epoch 010:     32 / 50 loss=12.692, ppl=6618.46, wps=17043.9, ups=13.82, wpb=1232.8, bsz=64, num_updates=480, lr=1.44e-06, gnorm=14.378, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=87
2022-01-12 23:53:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:14 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 12.463 | ppl 5644.55 | wps 27276.5 | wpb 556.6 | bsz 30.3 | num_updates 498 | best_loss 12.463
2022-01-12 23:53:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 498 updates
2022-01-12 23:53:14 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 498 updates, score 12.463) (writing took 4.289617713075131 seconds)
2022-01-12 23:53:18 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-12 23:53:18 | INFO | train | epoch 010 | loss 12.642 | ppl 6391.42 | wps 6987.8 | ups 5.67 | wpb 1232.9 | bsz 63.5 | num_updates 498 | lr 1.494e-06 | gnorm 14.015 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 93
2022-01-12 23:53:18 | INFO | fairseq.trainer | begin training epoch 11
2022-01-12 23:53:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:19 | INFO | train_inner | epoch 011:      2 / 50 loss=12.533, ppl=5928.39, wps=3577.6, ups=2.99, wpb=1197, bsz=64, num_updates=500, lr=1.5e-06, gnorm=13.101, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=94
2022-01-12 23:53:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:19 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.415 | ppl 5460.19 | wps 26942 | wpb 556.6 | bsz 30.3 | num_updates 500 | best_loss 12.415
2022-01-12 23:53:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 500 updates
2022-01-12 23:53:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt
2022-01-12 23:53:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt
2022-01-12 23:53:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt (epoch 11 @ 500 updates, score 12.415) (writing took 7.498780622147024 seconds)
2022-01-12 23:53:28 | INFO | train_inner | epoch 011:     22 / 50 loss=12.392, ppl=5374.44, wps=2657, ups=2.03, wpb=1310, bsz=64, num_updates=520, lr=1.56e-06, gnorm=12.485, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=104
2022-01-12 23:53:30 | INFO | train_inner | epoch 011:     42 / 50 loss=12.215, ppl=4755.5, wps=15773.4, ups=13.26, wpb=1189.1, bsz=64, num_updates=540, lr=1.62e-06, gnorm=13.026, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=105
2022-01-12 23:53:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:31 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.077 | ppl 4321.61 | wps 30497.9 | wpb 556.6 | bsz 30.3 | num_updates 548 | best_loss 12.077
2022-01-12 23:53:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 548 updates
2022-01-12 23:53:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 548 updates, score 12.077) (writing took 4.4391100709326565 seconds)
2022-01-12 23:53:36 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-12 23:53:36 | INFO | train | epoch 011 | loss 12.282 | ppl 4980.5 | wps 3551.4 | ups 2.88 | wpb 1232.9 | bsz 63.5 | num_updates 548 | lr 1.644e-06 | gnorm 12.803 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 111
2022-01-12 23:53:36 | INFO | fairseq.trainer | begin training epoch 12
2022-01-12 23:53:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:37 | INFO | train_inner | epoch 012:     12 / 50 loss=12.166, ppl=4596.88, wps=3448.7, ups=2.84, wpb=1214.3, bsz=61.4, num_updates=560, lr=1.68e-06, gnorm=12.663, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=112
2022-01-12 23:53:38 | INFO | train_inner | epoch 012:     32 / 50 loss=12.011, ppl=4127.61, wps=16454.9, ups=13.47, wpb=1221.2, bsz=64, num_updates=580, lr=1.74e-06, gnorm=12.837, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=114
2022-01-12 23:53:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:40 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.796 | ppl 3555.78 | wps 28198.2 | wpb 556.6 | bsz 30.3 | num_updates 598 | best_loss 11.796
2022-01-12 23:53:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 598 updates
2022-01-12 23:53:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 598 updates, score 11.796) (writing took 4.454071433050558 seconds)
2022-01-12 23:53:45 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-12 23:53:45 | INFO | train | epoch 012 | loss 12.036 | ppl 4199.48 | wps 6988 | ups 5.67 | wpb 1232.9 | bsz 63.5 | num_updates 598 | lr 1.794e-06 | gnorm 12.537 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 120
2022-01-12 23:53:45 | INFO | fairseq.trainer | begin training epoch 13
2022-01-12 23:53:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:45 | INFO | train_inner | epoch 013:      2 / 50 loss=11.921, ppl=3878.97, wps=3726.4, ups=3.02, wpb=1232.8, bsz=64, num_updates=600, lr=1.8e-06, gnorm=12.194, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=120
2022-01-12 23:53:46 | INFO | train_inner | epoch 013:     22 / 50 loss=11.721, ppl=3376.6, wps=16894.1, ups=13.92, wpb=1213.5, bsz=62.7, num_updates=620, lr=1.86e-06, gnorm=11.924, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=122
2022-01-12 23:53:48 | INFO | train_inner | epoch 013:     42 / 50 loss=11.651, ppl=3215.5, wps=17250.3, ups=13.97, wpb=1234.5, bsz=64, num_updates=640, lr=1.92e-06, gnorm=11.082, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=123
2022-01-12 23:53:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:49 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.555 | ppl 3009.11 | wps 26811.2 | wpb 556.6 | bsz 30.3 | num_updates 648 | best_loss 11.555
2022-01-12 23:53:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 648 updates
2022-01-12 23:53:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 648 updates, score 11.555) (writing took 4.713082507951185 seconds)
2022-01-12 23:53:54 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-12 23:53:54 | INFO | train | epoch 013 | loss 11.741 | ppl 3423.89 | wps 6679 | ups 5.42 | wpb 1232.9 | bsz 63.5 | num_updates 648 | lr 1.944e-06 | gnorm 11.452 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 129
2022-01-12 23:53:54 | INFO | fairseq.trainer | begin training epoch 14
2022-01-12 23:53:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:55 | INFO | train_inner | epoch 014:     12 / 50 loss=11.813, ppl=3596.98, wps=3568.5, ups=2.83, wpb=1259.2, bsz=64, num_updates=660, lr=1.98e-06, gnorm=11.379, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=130
2022-01-12 23:53:56 | INFO | train_inner | epoch 014:     32 / 50 loss=11.503, ppl=2902.06, wps=18879.4, ups=14.21, wpb=1329, bsz=62.7, num_updates=680, lr=2.04e-06, gnorm=10.932, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=132
2022-01-12 23:53:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:58 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.387 | ppl 2678 | wps 25868 | wpb 556.6 | bsz 30.3 | num_updates 698 | best_loss 11.387
2022-01-12 23:53:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 698 updates
2022-01-12 23:53:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 698 updates, score 11.387) (writing took 4.417004777817056 seconds)
2022-01-12 23:54:03 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-12 23:54:03 | INFO | train | epoch 014 | loss 11.52 | ppl 2937.44 | wps 6941.2 | ups 5.63 | wpb 1232.9 | bsz 63.5 | num_updates 698 | lr 2.094e-06 | gnorm 11.088 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 138
2022-01-12 23:54:03 | INFO | fairseq.trainer | begin training epoch 15
2022-01-12 23:54:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:03 | INFO | train_inner | epoch 015:      2 / 50 loss=11.432, ppl=2762.96, wps=3347.2, ups=2.95, wpb=1136.4, bsz=64, num_updates=700, lr=2.1e-06, gnorm=10.866, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=138
2022-01-12 23:54:05 | INFO | train_inner | epoch 015:     22 / 50 loss=11.372, ppl=2650.52, wps=17312.9, ups=14.56, wpb=1188.8, bsz=64, num_updates=720, lr=2.16e-06, gnorm=11.018, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=140
2022-01-12 23:54:06 | INFO | train_inner | epoch 015:     42 / 50 loss=11.221, ppl=2387.45, wps=19193.5, ups=14.02, wpb=1368.7, bsz=64, num_updates=740, lr=2.22e-06, gnorm=10.342, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=141
2022-01-12 23:54:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:07 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.136 | ppl 2250.2 | wps 29516.2 | wpb 556.6 | bsz 30.3 | num_updates 748 | best_loss 11.136
2022-01-12 23:54:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 748 updates
2022-01-12 23:54:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 748 updates, score 11.136) (writing took 4.124612756073475 seconds)
2022-01-12 23:54:11 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-12 23:54:11 | INFO | train | epoch 015 | loss 11.292 | ppl 2507.03 | wps 7200.7 | ups 5.84 | wpb 1232.9 | bsz 63.5 | num_updates 748 | lr 2.244e-06 | gnorm 10.679 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 147
2022-01-12 23:54:12 | INFO | fairseq.trainer | begin training epoch 16
2022-01-12 23:54:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:12 | INFO | train_inner | epoch 016:     12 / 50 loss=11.361, ppl=2629.88, wps=3515.4, ups=3.09, wpb=1138.8, bsz=62.7, num_updates=760, lr=2.28e-06, gnorm=10.219, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=148
2022-01-12 23:54:14 | INFO | train_inner | epoch 016:     32 / 50 loss=11.075, ppl=2156.92, wps=15435.3, ups=13.67, wpb=1129.2, bsz=64, num_updates=780, lr=2.34e-06, gnorm=11.173, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=149
2022-01-12 23:54:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:16 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.99 | ppl 2033.53 | wps 26551.2 | wpb 556.6 | bsz 30.3 | num_updates 798 | best_loss 10.99
2022-01-12 23:54:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 798 updates
2022-01-12 23:54:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 798 updates, score 10.99) (writing took 4.637598752975464 seconds)
2022-01-12 23:54:21 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-12 23:54:21 | INFO | train | epoch 016 | loss 11.192 | ppl 2340.24 | wps 6766.1 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 798 | lr 2.394e-06 | gnorm 10.345 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 156
2022-01-12 23:54:21 | INFO | fairseq.trainer | begin training epoch 17
2022-01-12 23:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:21 | INFO | train_inner | epoch 017:      2 / 50 loss=11.151, ppl=2273.34, wps=3784.2, ups=2.88, wpb=1312.2, bsz=62.7, num_updates=800, lr=2.4e-06, gnorm=9.816, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=156
2022-01-12 23:54:22 | INFO | train_inner | epoch 017:     22 / 50 loss=10.972, ppl=2008.1, wps=15993, ups=13.78, wpb=1160.7, bsz=62.7, num_updates=820, lr=2.46e-06, gnorm=10.458, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=157
2022-01-12 23:54:24 | INFO | train_inner | epoch 017:     42 / 50 loss=11.179, ppl=2318.11, wps=19648.3, ups=13.37, wpb=1469.3, bsz=64, num_updates=840, lr=2.52e-06, gnorm=9.358, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=159
2022-01-12 23:54:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:25 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.807 | ppl 1791.87 | wps 25449.6 | wpb 556.6 | bsz 30.3 | num_updates 848 | best_loss 10.807
2022-01-12 23:54:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 848 updates
2022-01-12 23:54:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 848 updates, score 10.807) (writing took 4.269093121169135 seconds)
2022-01-12 23:54:29 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-12 23:54:29 | INFO | train | epoch 017 | loss 11.03 | ppl 2090.71 | wps 6977.1 | ups 5.66 | wpb 1232.9 | bsz 63.5 | num_updates 848 | lr 2.544e-06 | gnorm 9.918 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 165
2022-01-12 23:54:29 | INFO | fairseq.trainer | begin training epoch 18
2022-01-12 23:54:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:30 | INFO | train_inner | epoch 018:     12 / 50 loss=10.843, ppl=1836.91, wps=3308.3, ups=3.04, wpb=1087.5, bsz=64, num_updates=860, lr=2.58e-06, gnorm=9.365, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=166
2022-01-12 23:54:32 | INFO | train_inner | epoch 018:     32 / 50 loss=10.828, ppl=1818.15, wps=17520.8, ups=13.82, wpb=1268, bsz=62.7, num_updates=880, lr=2.64e-06, gnorm=10.213, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=167
2022-01-12 23:54:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:34 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.597 | ppl 1548.95 | wps 26539 | wpb 556.6 | bsz 30.3 | num_updates 898 | best_loss 10.597
2022-01-12 23:54:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 898 updates
2022-01-12 23:54:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 898 updates, score 10.597) (writing took 4.445276870159432 seconds)
2022-01-12 23:54:39 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-12 23:54:39 | INFO | train | epoch 018 | loss 10.846 | ppl 1840.12 | wps 6763.6 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 898 | lr 2.694e-06 | gnorm 9.592 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 174
2022-01-12 23:54:39 | INFO | fairseq.trainer | begin training epoch 19
2022-01-12 23:54:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:39 | INFO | train_inner | epoch 019:      2 / 50 loss=10.821, ppl=1808.99, wps=3363.1, ups=2.89, wpb=1164.9, bsz=64, num_updates=900, lr=2.7e-06, gnorm=9.536, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=174
2022-01-12 23:54:40 | INFO | train_inner | epoch 019:     22 / 50 loss=10.713, ppl=1678.08, wps=16347.1, ups=13.48, wpb=1212.3, bsz=62.7, num_updates=920, lr=2.76e-06, gnorm=9.34, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=175
2022-01-12 23:54:42 | INFO | train_inner | epoch 019:     42 / 50 loss=10.76, ppl=1734.71, wps=15970.7, ups=12.28, wpb=1300.9, bsz=64, num_updates=940, lr=2.82e-06, gnorm=9.489, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=177
2022-01-12 23:54:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:43 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 10.489 | ppl 1437.09 | wps 26452.6 | wpb 556.6 | bsz 30.3 | num_updates 948 | best_loss 10.489
2022-01-12 23:54:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 948 updates
2022-01-12 23:54:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 948 updates, score 10.489) (writing took 4.3387409390416 seconds)
2022-01-12 23:54:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-12 23:54:48 | INFO | train | epoch 019 | loss 10.694 | ppl 1657.03 | wps 6778.2 | ups 5.5 | wpb 1232.9 | bsz 63.5 | num_updates 948 | lr 2.844e-06 | gnorm 9.546 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 183
2022-01-12 23:54:48 | INFO | fairseq.trainer | begin training epoch 20
2022-01-12 23:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:49 | INFO | train_inner | epoch 020:     12 / 50 loss=10.489, ppl=1437.05, wps=3413.7, ups=2.98, wpb=1145.7, bsz=64, num_updates=960, lr=2.88e-06, gnorm=10.143, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=184
2022-01-12 23:54:50 | INFO | train_inner | epoch 020:     32 / 50 loss=10.624, ppl=1578.44, wps=17115, ups=13.8, wpb=1240, bsz=62.7, num_updates=980, lr=2.94e-06, gnorm=9.196, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=185
2022-01-12 23:54:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:52 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 10.395 | ppl 1346.34 | wps 27010.2 | wpb 556.6 | bsz 30.3 | num_updates 998 | best_loss 10.395
2022-01-12 23:54:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 998 updates
2022-01-12 23:54:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 998 updates, score 10.395) (writing took 4.420613712863997 seconds)
2022-01-12 23:54:57 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-12 23:54:57 | INFO | train | epoch 020 | loss 10.579 | ppl 1529.35 | wps 6888.6 | ups 5.59 | wpb 1232.9 | bsz 63.5 | num_updates 998 | lr 2.994e-06 | gnorm 9.368 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 192
2022-01-12 23:54:57 | INFO | fairseq.trainer | begin training epoch 21
2022-01-12 23:54:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:57 | INFO | train_inner | epoch 021:      2 / 50 loss=10.563, ppl=1513.22, wps=3660.2, ups=2.87, wpb=1273.9, bsz=64, num_updates=1000, lr=3e-06, gnorm=9.044, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=192
2022-01-12 23:54:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:58 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.327 | ppl 1284.77 | wps 26793.8 | wpb 556.6 | bsz 30.3 | num_updates 1000 | best_loss 10.327
2022-01-12 23:54:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1000 updates
2022-01-12 23:54:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt
2022-01-12 23:55:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt
2022-01-12 23:55:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt (epoch 21 @ 1000 updates, score 10.327) (writing took 6.069689427968115 seconds)
2022-01-12 23:55:06 | INFO | train_inner | epoch 021:     22 / 50 loss=10.557, ppl=1506.07, wps=3021.3, ups=2.32, wpb=1300.8, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=9.176, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=201
2022-01-12 23:55:07 | INFO | train_inner | epoch 021:     42 / 50 loss=10.38, ppl=1332.46, wps=16585.7, ups=13.6, wpb=1219.5, bsz=64, num_updates=1040, lr=3.12e-06, gnorm=8.935, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=202
2022-01-12 23:55:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:08 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.192 | ppl 1169.41 | wps 27626.5 | wpb 556.6 | bsz 30.3 | num_updates 1048 | best_loss 10.192
2022-01-12 23:55:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1048 updates
2022-01-12 23:55:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 1048 updates, score 10.192) (writing took 4.609479784034193 seconds)
2022-01-12 23:55:13 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-12 23:55:15 | INFO | train | epoch 021 | loss 10.483 | ppl 1431.35 | wps 3800.6 | ups 3.08 | wpb 1232.9 | bsz 63.5 | num_updates 1048 | lr 3.144e-06 | gnorm 9.147 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 208
2022-01-12 23:55:15 | INFO | fairseq.trainer | begin training epoch 22
2022-01-12 23:55:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:16 | INFO | train_inner | epoch 022:     12 / 50 loss=10.365, ppl=1318.47, wps=2851.5, ups=2.39, wpb=1195.2, bsz=62.7, num_updates=1060, lr=3.18e-06, gnorm=9.221, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=211
2022-01-12 23:55:17 | INFO | train_inner | epoch 022:     32 / 50 loss=10.429, ppl=1378.86, wps=18621.7, ups=14.71, wpb=1265.5, bsz=64, num_updates=1080, lr=3.24e-06, gnorm=8.68, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=212
2022-01-12 23:55:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:19 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 10.095 | ppl 1093.8 | wps 26662.4 | wpb 556.6 | bsz 30.3 | num_updates 1098 | best_loss 10.095
2022-01-12 23:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1098 updates
2022-01-12 23:55:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 1098 updates, score 10.095) (writing took 4.358964095823467 seconds)
2022-01-12 23:55:23 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-12 23:55:23 | INFO | train | epoch 022 | loss 10.341 | ppl 1297.09 | wps 7068.2 | ups 5.73 | wpb 1232.9 | bsz 63.5 | num_updates 1098 | lr 3.294e-06 | gnorm 9.153 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 218
2022-01-12 23:55:23 | INFO | fairseq.trainer | begin training epoch 23
2022-01-12 23:55:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:24 | INFO | train_inner | epoch 023:      2 / 50 loss=10.307, ppl=1266.61, wps=3506.3, ups=3.01, wpb=1164.5, bsz=62.7, num_updates=1100, lr=3.3e-06, gnorm=9.695, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=219
2022-01-12 23:55:25 | INFO | train_inner | epoch 023:     22 / 50 loss=10.193, ppl=1170.63, wps=16977, ups=13.89, wpb=1222.5, bsz=64, num_updates=1120, lr=3.36e-06, gnorm=8.713, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=220
2022-01-12 23:55:26 | INFO | train_inner | epoch 023:     42 / 50 loss=10.203, ppl=1178.85, wps=20223.5, ups=14.75, wpb=1370.7, bsz=64, num_updates=1140, lr=3.42e-06, gnorm=8.855, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=221
2022-01-12 23:55:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:28 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 10.011 | ppl 1031.65 | wps 26137.9 | wpb 556.6 | bsz 30.3 | num_updates 1148 | best_loss 10.011
2022-01-12 23:55:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1148 updates
2022-01-12 23:55:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 1148 updates, score 10.011) (writing took 4.447407083818689 seconds)
2022-01-12 23:55:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-12 23:55:32 | INFO | train | epoch 023 | loss 10.189 | ppl 1167.45 | wps 7030 | ups 5.7 | wpb 1232.9 | bsz 63.5 | num_updates 1148 | lr 3.444e-06 | gnorm 9.034 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 227
2022-01-12 23:55:32 | INFO | fairseq.trainer | begin training epoch 24
2022-01-12 23:55:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:33 | INFO | train_inner | epoch 024:     12 / 50 loss=10.111, ppl=1105.92, wps=2916.6, ups=2.93, wpb=994, bsz=62.7, num_updates=1160, lr=3.48e-06, gnorm=9.566, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=228
2022-01-12 23:55:35 | INFO | train_inner | epoch 024:     32 / 50 loss=10.131, ppl=1121.41, wps=17811.8, ups=12.5, wpb=1425.5, bsz=62.7, num_updates=1180, lr=3.54e-06, gnorm=8.593, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=230
2022-01-12 23:55:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:37 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.868 | ppl 934.23 | wps 26896.4 | wpb 556.6 | bsz 30.3 | num_updates 1198 | best_loss 9.868
2022-01-12 23:55:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1198 updates
2022-01-12 23:55:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 1198 updates, score 9.868) (writing took 4.69009534898214 seconds)
2022-01-12 23:55:42 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-12 23:55:42 | INFO | train | epoch 024 | loss 10.057 | ppl 1065.31 | wps 6488.8 | ups 5.26 | wpb 1232.9 | bsz 63.5 | num_updates 1198 | lr 3.594e-06 | gnorm 8.873 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 237
2022-01-12 23:55:42 | INFO | fairseq.trainer | begin training epoch 25
2022-01-12 23:55:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:42 | INFO | train_inner | epoch 025:      2 / 50 loss=9.954, ppl=991.84, wps=3271.9, ups=2.81, wpb=1164.8, bsz=64, num_updates=1200, lr=3.6e-06, gnorm=8.989, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=237
2022-01-12 23:55:43 | INFO | train_inner | epoch 025:     22 / 50 loss=9.978, ppl=1008.7, wps=18366.2, ups=13.79, wpb=1331.4, bsz=62.7, num_updates=1220, lr=3.66e-06, gnorm=9.158, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=238
2022-01-12 23:55:45 | INFO | train_inner | epoch 025:     42 / 50 loss=9.923, ppl=970.62, wps=16170.2, ups=13.64, wpb=1185.5, bsz=64, num_updates=1240, lr=3.72e-06, gnorm=8.794, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=240
2022-01-12 23:55:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:46 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.696 | ppl 829.18 | wps 26867.5 | wpb 556.6 | bsz 30.3 | num_updates 1248 | best_loss 9.696
2022-01-12 23:55:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1248 updates
2022-01-12 23:55:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 1248 updates, score 9.696) (writing took 4.413138160016388 seconds)
2022-01-12 23:55:51 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-12 23:55:51 | INFO | train | epoch 025 | loss 9.919 | ppl 968.37 | wps 6820.8 | ups 5.53 | wpb 1232.9 | bsz 63.5 | num_updates 1248 | lr 3.744e-06 | gnorm 8.967 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 246
2022-01-12 23:55:51 | INFO | fairseq.trainer | begin training epoch 26
2022-01-12 23:55:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:52 | INFO | train_inner | epoch 026:     12 / 50 loss=9.809, ppl=896.91, wps=3597, ups=2.92, wpb=1231.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=9.137, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=247
2022-01-12 23:55:53 | INFO | train_inner | epoch 026:     32 / 50 loss=9.827, ppl=908.3, wps=14846.7, ups=12.64, wpb=1174.3, bsz=64, num_updates=1280, lr=3.84e-06, gnorm=8.768, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=248
2022-01-12 23:55:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:55 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.581 | ppl 765.72 | wps 26388.5 | wpb 556.6 | bsz 30.3 | num_updates 1298 | best_loss 9.581
2022-01-12 23:55:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1298 updates
2022-01-12 23:55:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 1298 updates, score 9.581) (writing took 4.977432888932526 seconds)
2022-01-12 23:56:00 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-12 23:56:00 | INFO | train | epoch 026 | loss 9.811 | ppl 898.38 | wps 6337.7 | ups 5.14 | wpb 1232.9 | bsz 63.5 | num_updates 1298 | lr 3.894e-06 | gnorm 8.908 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 256
2022-01-12 23:56:00 | INFO | fairseq.trainer | begin training epoch 27
2022-01-12 23:56:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:01 | INFO | train_inner | epoch 027:      2 / 50 loss=9.732, ppl=850.61, wps=3375.5, ups=2.69, wpb=1252.9, bsz=64, num_updates=1300, lr=3.9e-06, gnorm=8.941, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=256
2022-01-12 23:56:02 | INFO | train_inner | epoch 027:     22 / 50 loss=9.839, ppl=916, wps=16485.7, ups=13.25, wpb=1244.4, bsz=64, num_updates=1320, lr=3.96e-06, gnorm=8.61, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=257
2022-01-12 23:56:04 | INFO | train_inner | epoch 027:     42 / 50 loss=9.762, ppl=868.56, wps=16674.8, ups=13.12, wpb=1271.2, bsz=62.7, num_updates=1340, lr=4.02e-06, gnorm=9.589, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=259
2022-01-12 23:56:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:05 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.511 | ppl 729.67 | wps 24503.1 | wpb 556.6 | bsz 30.3 | num_updates 1348 | best_loss 9.511
2022-01-12 23:56:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1348 updates
2022-01-12 23:56:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 1348 updates, score 9.511) (writing took 4.767022804124281 seconds)
2022-01-12 23:56:10 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-12 23:56:10 | INFO | train | epoch 027 | loss 9.714 | ppl 840.1 | wps 6437.5 | ups 5.22 | wpb 1232.9 | bsz 63.5 | num_updates 1348 | lr 4.044e-06 | gnorm 9.17 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 265
2022-01-12 23:56:10 | INFO | fairseq.trainer | begin training epoch 28
2022-01-12 23:56:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:11 | INFO | train_inner | epoch 028:     12 / 50 loss=9.468, ppl=707.99, wps=3060.6, ups=2.75, wpb=1113.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=9.25, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=266
2022-01-12 23:56:13 | INFO | train_inner | epoch 028:     32 / 50 loss=9.61, ppl=781.23, wps=13882.5, ups=11.36, wpb=1222.3, bsz=64, num_updates=1380, lr=4.14e-06, gnorm=8.775, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=268
2022-01-12 23:56:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:15 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.386 | ppl 668.91 | wps 25395.5 | wpb 556.6 | bsz 30.3 | num_updates 1398 | best_loss 9.386
2022-01-12 23:56:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1398 updates
2022-01-12 23:56:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 1398 updates, score 9.386) (writing took 4.761819014092907 seconds)
2022-01-12 23:56:20 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-12 23:56:20 | INFO | train | epoch 028 | loss 9.54 | ppl 744.64 | wps 6158.4 | ups 5 | wpb 1232.9 | bsz 63.5 | num_updates 1398 | lr 4.194e-06 | gnorm 9.601 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 275
2022-01-12 23:56:20 | INFO | fairseq.trainer | begin training epoch 29
2022-01-12 23:56:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:20 | INFO | train_inner | epoch 029:      2 / 50 loss=9.477, ppl=712.75, wps=3633.5, ups=2.66, wpb=1367.7, bsz=64, num_updates=1400, lr=4.2e-06, gnorm=10.426, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=275
2022-01-12 23:56:22 | INFO | train_inner | epoch 029:     22 / 50 loss=9.416, ppl=682.92, wps=14915.4, ups=12.79, wpb=1166, bsz=64, num_updates=1420, lr=4.26e-06, gnorm=8.797, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=277
2022-01-12 23:56:23 | INFO | train_inner | epoch 029:     42 / 50 loss=9.494, ppl=720.93, wps=17046.6, ups=13.03, wpb=1308.7, bsz=62.7, num_updates=1440, lr=4.32e-06, gnorm=9.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=279
2022-01-12 23:56:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:25 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.331 | ppl 643.99 | wps 25605.2 | wpb 556.6 | bsz 30.3 | num_updates 1448 | best_loss 9.331
2022-01-12 23:56:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1448 updates
2022-01-12 23:56:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 1448 updates, score 9.331) (writing took 4.545991942053661 seconds)
2022-01-12 23:56:29 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-12 23:56:29 | INFO | train | epoch 029 | loss 9.453 | ppl 700.95 | wps 6588.5 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 1448 | lr 4.344e-06 | gnorm 9.121 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 284
2022-01-12 23:56:29 | INFO | fairseq.trainer | begin training epoch 30
2022-01-12 23:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:30 | INFO | train_inner | epoch 030:     12 / 50 loss=9.476, ppl=712.32, wps=3644.8, ups=2.87, wpb=1269.3, bsz=64, num_updates=1460, lr=4.38e-06, gnorm=9.229, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=285
2022-01-12 23:56:32 | INFO | train_inner | epoch 030:     32 / 50 loss=9.288, ppl=625.25, wps=15289.5, ups=13.05, wpb=1171.5, bsz=64, num_updates=1480, lr=4.44e-06, gnorm=9.017, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=287
2022-01-12 23:56:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:34 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.253 | ppl 610.25 | wps 28299.3 | wpb 556.6 | bsz 30.3 | num_updates 1498 | best_loss 9.253
2022-01-12 23:56:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1498 updates
2022-01-12 23:56:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 30 @ 1498 updates, score 9.253) (writing took 4.596516771009192 seconds)
2022-01-12 23:56:39 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-12 23:56:39 | INFO | train | epoch 030 | loss 9.33 | ppl 643.78 | wps 6721.4 | ups 5.45 | wpb 1232.9 | bsz 63.5 | num_updates 1498 | lr 4.494e-06 | gnorm 8.929 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 294
2022-01-12 23:56:39 | INFO | fairseq.trainer | begin training epoch 31
2022-01-12 23:56:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:39 | INFO | train_inner | epoch 031:      2 / 50 loss=9.202, ppl=588.98, wps=3378.8, ups=2.84, wpb=1189.5, bsz=62.7, num_updates=1500, lr=4.5e-06, gnorm=9.095, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=294
2022-01-12 23:56:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:40 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.232 | ppl 601.18 | wps 23535 | wpb 556.6 | bsz 30.3 | num_updates 1500 | best_loss 9.232
2022-01-12 23:56:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1500 updates
2022-01-12 23:56:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt
2022-01-12 23:56:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt
2022-01-12 23:56:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt (epoch 31 @ 1500 updates, score 9.232) (writing took 9.790959219215438 seconds)
2022-01-12 23:56:51 | INFO | train_inner | epoch 031:     22 / 50 loss=9.321, ppl=639.69, wps=2117.9, ups=1.6, wpb=1321.7, bsz=62.7, num_updates=1520, lr=4.56e-06, gnorm=9.707, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=307
2022-01-12 23:56:53 | INFO | train_inner | epoch 031:     42 / 50 loss=9.174, ppl=577.5, wps=14784.7, ups=12.44, wpb=1188.6, bsz=64, num_updates=1540, lr=4.62e-06, gnorm=9.061, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=308
2022-01-12 23:56:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:54 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.117 | ppl 555.3 | wps 27764.4 | wpb 556.6 | bsz 30.3 | num_updates 1548 | best_loss 9.117
2022-01-12 23:56:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1548 updates
2022-01-12 23:56:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 1548 updates, score 9.117) (writing took 4.195408324943855 seconds)
2022-01-12 23:56:59 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-12 23:56:59 | INFO | train | epoch 031 | loss 9.198 | ppl 587.19 | wps 3091.7 | ups 2.51 | wpb 1232.9 | bsz 63.5 | num_updates 1548 | lr 4.644e-06 | gnorm 9.24 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 314
2022-01-12 23:56:59 | INFO | fairseq.trainer | begin training epoch 32
2022-01-12 23:56:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:00 | INFO | train_inner | epoch 032:     12 / 50 loss=9.169, ppl=575.58, wps=3594.8, ups=3.06, wpb=1174, bsz=64, num_updates=1560, lr=4.68e-06, gnorm=8.772, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=315
2022-01-12 23:57:01 | INFO | train_inner | epoch 032:     32 / 50 loss=9.072, ppl=538.37, wps=18087.3, ups=14.18, wpb=1275.7, bsz=64, num_updates=1580, lr=4.74e-06, gnorm=8.731, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=316
2022-01-12 23:57:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:03 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.027 | ppl 521.78 | wps 28324.7 | wpb 556.6 | bsz 30.3 | num_updates 1598 | best_loss 9.027
2022-01-12 23:57:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1598 updates
2022-01-12 23:57:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 32 @ 1598 updates, score 9.027) (writing took 4.552312697982416 seconds)
2022-01-12 23:57:07 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-12 23:57:07 | INFO | train | epoch 032 | loss 9.134 | ppl 562.01 | wps 6984.5 | ups 5.67 | wpb 1232.9 | bsz 63.5 | num_updates 1598 | lr 4.794e-06 | gnorm 8.798 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 323
2022-01-12 23:57:07 | INFO | fairseq.trainer | begin training epoch 33
2022-01-12 23:57:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:08 | INFO | train_inner | epoch 033:      2 / 50 loss=9.083, ppl=542.28, wps=3466.3, ups=2.97, wpb=1168.8, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=8.898, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=323
2022-01-12 23:57:09 | INFO | train_inner | epoch 033:     22 / 50 loss=9.16, ppl=572.2, wps=17745, ups=14.51, wpb=1223.3, bsz=62.7, num_updates=1620, lr=4.86e-06, gnorm=8.639, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=324
2022-01-12 23:57:11 | INFO | train_inner | epoch 033:     42 / 50 loss=8.97, ppl=501.37, wps=18239.8, ups=14.09, wpb=1294.6, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=8.891, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=326
2022-01-12 23:57:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:12 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.96 | ppl 498.03 | wps 27182 | wpb 556.6 | bsz 30.3 | num_updates 1648 | best_loss 8.96
2022-01-12 23:57:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1648 updates
2022-01-12 23:57:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1648 updates, score 8.96) (writing took 4.40379764395766 seconds)
2022-01-12 23:57:16 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-12 23:57:16 | INFO | train | epoch 033 | loss 9.046 | ppl 528.63 | wps 6999.9 | ups 5.68 | wpb 1232.9 | bsz 63.5 | num_updates 1648 | lr 4.944e-06 | gnorm 8.853 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 331
2022-01-12 23:57:16 | INFO | fairseq.trainer | begin training epoch 34
2022-01-12 23:57:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:17 | INFO | train_inner | epoch 034:     12 / 50 loss=8.992, ppl=509.02, wps=3243.3, ups=2.95, wpb=1098.5, bsz=64, num_updates=1660, lr=4.98e-06, gnorm=9.15, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=332
2022-01-12 23:57:19 | INFO | train_inner | epoch 034:     32 / 50 loss=8.89, ppl=474.37, wps=16938.6, ups=14.18, wpb=1194.3, bsz=64, num_updates=1680, lr=5.04e-06, gnorm=8.716, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=334
2022-01-12 23:57:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:21 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.847 | ppl 460.56 | wps 27702.4 | wpb 556.6 | bsz 30.3 | num_updates 1698 | best_loss 8.847
2022-01-12 23:57:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1698 updates
2022-01-12 23:57:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1698 updates, score 8.847) (writing took 4.3369038458913565 seconds)
2022-01-12 23:57:25 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-12 23:57:25 | INFO | train | epoch 034 | loss 8.964 | ppl 499.34 | wps 7080.6 | ups 5.74 | wpb 1232.9 | bsz 63.5 | num_updates 1698 | lr 5.094e-06 | gnorm 8.822 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 340
2022-01-12 23:57:25 | INFO | fairseq.trainer | begin training epoch 35
2022-01-12 23:57:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:25 | INFO | train_inner | epoch 035:      2 / 50 loss=9.058, ppl=532.94, wps=4522.8, ups=3.06, wpb=1478.3, bsz=62.7, num_updates=1700, lr=5.1e-06, gnorm=8.623, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=340
2022-01-12 23:57:27 | INFO | train_inner | epoch 035:     22 / 50 loss=8.871, ppl=468.19, wps=16268.2, ups=14.23, wpb=1143.2, bsz=64, num_updates=1720, lr=5.16e-06, gnorm=8.842, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=342
2022-01-12 23:57:28 | INFO | train_inner | epoch 035:     42 / 50 loss=8.779, ppl=439.19, wps=18255.3, ups=13.68, wpb=1334.7, bsz=62.7, num_updates=1740, lr=5.22e-06, gnorm=8.948, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=343
2022-01-12 23:57:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:29 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.728 | ppl 424.15 | wps 28176.1 | wpb 556.6 | bsz 30.3 | num_updates 1748 | best_loss 8.728
2022-01-12 23:57:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1748 updates
2022-01-12 23:57:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1748 updates, score 8.728) (writing took 4.49892555992119 seconds)
2022-01-12 23:57:34 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-12 23:57:34 | INFO | train | epoch 035 | loss 8.816 | ppl 450.69 | wps 6926.2 | ups 5.62 | wpb 1232.9 | bsz 63.5 | num_updates 1748 | lr 5.244e-06 | gnorm 8.869 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 349
2022-01-12 23:57:34 | INFO | fairseq.trainer | begin training epoch 36
2022-01-12 23:57:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:35 | INFO | train_inner | epoch 036:     12 / 50 loss=8.727, ppl=423.83, wps=3625.6, ups=2.92, wpb=1242.2, bsz=64, num_updates=1760, lr=5.28e-06, gnorm=8.865, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=350
2022-01-12 23:57:36 | INFO | train_inner | epoch 036:     32 / 50 loss=8.629, ppl=395.97, wps=18633.1, ups=14.4, wpb=1293.7, bsz=62.7, num_updates=1780, lr=5.34e-06, gnorm=8.793, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=352
2022-01-12 23:57:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:38 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.65 | ppl 401.69 | wps 27451.5 | wpb 556.6 | bsz 30.3 | num_updates 1798 | best_loss 8.65
2022-01-12 23:57:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1798 updates
2022-01-12 23:57:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1798 updates, score 8.65) (writing took 5.092594149056822 seconds)
2022-01-12 23:57:43 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-12 23:57:43 | INFO | train | epoch 036 | loss 8.673 | ppl 408.06 | wps 6446.3 | ups 5.23 | wpb 1232.9 | bsz 63.5 | num_updates 1798 | lr 5.394e-06 | gnorm 8.941 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 359
2022-01-12 23:57:44 | INFO | fairseq.trainer | begin training epoch 37
2022-01-12 23:57:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:44 | INFO | train_inner | epoch 037:      2 / 50 loss=8.652, ppl=402.3, wps=3091.1, ups=2.72, wpb=1136.7, bsz=64, num_updates=1800, lr=5.4e-06, gnorm=9.128, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=359
2022-01-12 23:57:45 | INFO | train_inner | epoch 037:     22 / 50 loss=8.584, ppl=383.85, wps=16061.9, ups=13.49, wpb=1191, bsz=62.7, num_updates=1820, lr=5.46e-06, gnorm=9.037, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=360
2022-01-12 23:57:47 | INFO | train_inner | epoch 037:     42 / 50 loss=8.691, ppl=413.41, wps=15540.4, ups=12.57, wpb=1236.2, bsz=64, num_updates=1840, lr=5.52e-06, gnorm=8.902, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=362
2022-01-12 23:57:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:48 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.519 | ppl 366.92 | wps 27076.3 | wpb 556.6 | bsz 30.3 | num_updates 1848 | best_loss 8.519
2022-01-12 23:57:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1848 updates
2022-01-12 23:57:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1848 updates, score 8.519) (writing took 4.238679522881284 seconds)
2022-01-12 23:57:52 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-12 23:57:53 | INFO | train | epoch 037 | loss 8.641 | ppl 399.21 | wps 6837.3 | ups 5.55 | wpb 1232.9 | bsz 63.5 | num_updates 1848 | lr 5.544e-06 | gnorm 9.062 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 368
2022-01-12 23:57:53 | INFO | fairseq.trainer | begin training epoch 38
2022-01-12 23:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:53 | INFO | train_inner | epoch 038:     12 / 50 loss=8.59, ppl=385.25, wps=3697.6, ups=3, wpb=1231.5, bsz=64, num_updates=1860, lr=5.58e-06, gnorm=9.033, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=369
2022-01-12 23:57:55 | INFO | train_inner | epoch 038:     32 / 50 loss=8.532, ppl=370.25, wps=15065.4, ups=13.6, wpb=1108.1, bsz=64, num_updates=1880, lr=5.64e-06, gnorm=10.302, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=370
2022-01-12 23:57:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:57 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.463 | ppl 352.84 | wps 27608.5 | wpb 556.6 | bsz 30.3 | num_updates 1898 | best_loss 8.463
2022-01-12 23:57:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1898 updates
2022-01-12 23:57:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 38 @ 1898 updates, score 8.463) (writing took 4.939729632111266 seconds)
2022-01-12 23:58:02 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-12 23:58:02 | INFO | train | epoch 038 | loss 8.496 | ppl 360.92 | wps 6408.3 | ups 5.2 | wpb 1232.9 | bsz 63.5 | num_updates 1898 | lr 5.694e-06 | gnorm 9.393 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 377
2022-01-12 23:58:02 | INFO | fairseq.trainer | begin training epoch 39
2022-01-12 23:58:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:02 | INFO | train_inner | epoch 039:      2 / 50 loss=8.431, ppl=345.12, wps=3629.9, ups=2.67, wpb=1357.8, bsz=62.7, num_updates=1900, lr=5.7e-06, gnorm=8.871, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=378
2022-01-12 23:58:04 | INFO | train_inner | epoch 039:     22 / 50 loss=8.31, ppl=317.3, wps=18391, ups=13.19, wpb=1394.3, bsz=62.7, num_updates=1920, lr=5.76e-06, gnorm=8.856, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=379
2022-01-12 23:58:06 | INFO | train_inner | epoch 039:     42 / 50 loss=8.6, ppl=388.07, wps=14391.7, ups=13.04, wpb=1103.3, bsz=64, num_updates=1940, lr=5.82e-06, gnorm=8.831, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=381
2022-01-12 23:58:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:07 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.338 | ppl 323.55 | wps 26376.4 | wpb 556.6 | bsz 30.3 | num_updates 1948 | best_loss 8.338
2022-01-12 23:58:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1948 updates
2022-01-12 23:58:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1948 updates, score 8.338) (writing took 4.365342371165752 seconds)
2022-01-12 23:58:11 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-12 23:58:11 | INFO | train | epoch 039 | loss 8.412 | ppl 340.57 | wps 6727.9 | ups 5.46 | wpb 1232.9 | bsz 63.5 | num_updates 1948 | lr 5.844e-06 | gnorm 8.833 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 386
2022-01-12 23:58:11 | INFO | fairseq.trainer | begin training epoch 40
2022-01-12 23:58:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:12 | INFO | train_inner | epoch 040:     12 / 50 loss=8.234, ppl=301.06, wps=3408.9, ups=2.94, wpb=1158.2, bsz=62.7, num_updates=1960, lr=5.88e-06, gnorm=8.929, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=387
2022-01-12 23:58:14 | INFO | train_inner | epoch 040:     32 / 50 loss=8.445, ppl=348.56, wps=16243, ups=13.6, wpb=1193.9, bsz=64, num_updates=1980, lr=5.94e-06, gnorm=8.674, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=389
2022-01-12 23:58:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:16 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.301 | ppl 315.3 | wps 26733.9 | wpb 556.6 | bsz 30.3 | num_updates 1998 | best_loss 8.301
2022-01-12 23:58:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1998 updates
2022-01-12 23:58:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1998 updates, score 8.301) (writing took 4.93078504386358 seconds)
2022-01-12 23:58:21 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-12 23:58:21 | INFO | train | epoch 040 | loss 8.351 | ppl 326.42 | wps 6578.5 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 1998 | lr 5.994e-06 | gnorm 8.829 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 396
2022-01-12 23:58:21 | INFO | fairseq.trainer | begin training epoch 41
2022-01-12 23:58:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:21 | INFO | train_inner | epoch 041:      2 / 50 loss=8.301, ppl=315.32, wps=3697.6, ups=2.79, wpb=1325.2, bsz=64, num_updates=2000, lr=6e-06, gnorm=8.798, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=396
2022-01-12 23:58:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:22 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.232 | ppl 300.58 | wps 25796.3 | wpb 556.6 | bsz 30.3 | num_updates 2000 | best_loss 8.232
2022-01-12 23:58:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2000 updates
2022-01-12 23:58:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt
2022-01-12 23:58:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt
2022-01-12 23:58:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt (epoch 41 @ 2000 updates, score 8.232) (writing took 9.888902221107855 seconds)
2022-01-12 23:58:33 | INFO | train_inner | epoch 041:     22 / 50 loss=8.306, ppl=316.57, wps=2124.6, ups=1.62, wpb=1308.8, bsz=64, num_updates=2020, lr=6.06e-06, gnorm=8.913, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=408
2022-01-12 23:58:35 | INFO | train_inner | epoch 041:     42 / 50 loss=8.255, ppl=305.54, wps=15575.6, ups=13.04, wpb=1194.7, bsz=62.7, num_updates=2040, lr=6.12e-06, gnorm=8.872, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=410
2022-01-12 23:58:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:36 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.254 | ppl 305.35 | wps 26718.4 | wpb 556.6 | bsz 30.3 | num_updates 2048 | best_loss 8.232
2022-01-12 23:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2048 updates
2022-01-12 23:58:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 2048 updates, score 8.254) (writing took 2.7794300818350166 seconds)
2022-01-12 23:58:39 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-12 23:58:39 | INFO | train | epoch 041 | loss 8.275 | ppl 309.75 | wps 3353.6 | ups 2.72 | wpb 1232.9 | bsz 63.5 | num_updates 2048 | lr 6.144e-06 | gnorm 8.832 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 414
2022-01-12 23:58:39 | INFO | fairseq.trainer | begin training epoch 42
2022-01-12 23:58:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:40 | INFO | train_inner | epoch 042:     12 / 50 loss=8.202, ppl=294.42, wps=4650.6, ups=3.82, wpb=1217.2, bsz=64, num_updates=2060, lr=6.18e-06, gnorm=8.577, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=415
2022-01-12 23:58:41 | INFO | train_inner | epoch 042:     32 / 50 loss=8.245, ppl=303.39, wps=17383.4, ups=13.94, wpb=1247.2, bsz=64, num_updates=2080, lr=6.24e-06, gnorm=8.729, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=417
2022-01-12 23:58:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:44 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.09 | ppl 272.57 | wps 25664.9 | wpb 556.6 | bsz 30.3 | num_updates 2098 | best_loss 8.09
2022-01-12 23:58:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2098 updates
2022-01-12 23:58:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 2098 updates, score 8.09) (writing took 4.342212746152654 seconds)
2022-01-12 23:58:48 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-12 23:58:48 | INFO | train | epoch 042 | loss 8.209 | ppl 295.91 | wps 6966.1 | ups 5.65 | wpb 1232.9 | bsz 63.5 | num_updates 2098 | lr 6.294e-06 | gnorm 8.768 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 423
2022-01-12 23:58:48 | INFO | fairseq.trainer | begin training epoch 43
2022-01-12 23:58:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:48 | INFO | train_inner | epoch 043:      2 / 50 loss=8.117, ppl=277.53, wps=3522.9, ups=2.98, wpb=1182.5, bsz=62.7, num_updates=2100, lr=6.3e-06, gnorm=8.905, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=423
2022-01-12 23:58:50 | INFO | train_inner | epoch 043:     22 / 50 loss=7.966, ppl=250.03, wps=17217.8, ups=14.07, wpb=1223.8, bsz=62.7, num_updates=2120, lr=6.36e-06, gnorm=8.955, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=425
2022-01-12 23:58:51 | INFO | train_inner | epoch 043:     42 / 50 loss=8.312, ppl=317.78, wps=16118.4, ups=12.53, wpb=1286.2, bsz=64, num_updates=2140, lr=6.42e-06, gnorm=8.445, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=426
2022-01-12 23:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:53 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.113 | ppl 276.8 | wps 25381.2 | wpb 556.6 | bsz 30.3 | num_updates 2148 | best_loss 8.09
2022-01-12 23:58:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2148 updates
2022-01-12 23:58:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 43 @ 2148 updates, score 8.113) (writing took 2.8221724259201437 seconds)
2022-01-12 23:58:55 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-12 23:58:55 | INFO | train | epoch 043 | loss 8.081 | ppl 270.87 | wps 8190.8 | ups 6.64 | wpb 1232.9 | bsz 63.5 | num_updates 2148 | lr 6.444e-06 | gnorm 8.733 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 431
2022-01-12 23:58:56 | INFO | fairseq.trainer | begin training epoch 44
2022-01-12 23:58:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:56 | INFO | train_inner | epoch 044:     12 / 50 loss=7.9, ppl=238.79, wps=4185.8, ups=3.86, wpb=1083.7, bsz=62.7, num_updates=2160, lr=6.48e-06, gnorm=8.953, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=432
2022-01-12 23:58:58 | INFO | train_inner | epoch 044:     32 / 50 loss=7.977, ppl=251.93, wps=16719.6, ups=13.21, wpb=1265.7, bsz=64, num_updates=2180, lr=6.54e-06, gnorm=8.731, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=433
2022-01-12 23:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:00 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.037 | ppl 262.58 | wps 26798.8 | wpb 556.6 | bsz 30.3 | num_updates 2198 | best_loss 8.037
2022-01-12 23:59:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2198 updates
2022-01-12 23:59:00 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 2198 updates, score 8.037) (writing took 5.0613508629612625 seconds)
2022-01-12 23:59:05 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-12 23:59:05 | INFO | train | epoch 044 | loss 7.974 | ppl 251.45 | wps 6452.4 | ups 5.23 | wpb 1232.9 | bsz 63.5 | num_updates 2198 | lr 6.594e-06 | gnorm 8.718 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 440
2022-01-12 23:59:05 | INFO | fairseq.trainer | begin training epoch 45
2022-01-12 23:59:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:05 | INFO | train_inner | epoch 045:      2 / 50 loss=7.975, ppl=251.59, wps=3463.6, ups=2.71, wpb=1277.9, bsz=64, num_updates=2200, lr=6.6e-06, gnorm=8.586, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=440
2022-01-12 23:59:07 | INFO | train_inner | epoch 045:     22 / 50 loss=7.894, ppl=237.79, wps=17351.7, ups=14.22, wpb=1220.5, bsz=62.7, num_updates=2220, lr=6.66e-06, gnorm=8.658, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=442
2022-01-12 23:59:08 | INFO | train_inner | epoch 045:     42 / 50 loss=7.856, ppl=231.67, wps=16552.2, ups=13.52, wpb=1224.2, bsz=64, num_updates=2240, lr=6.72e-06, gnorm=8.725, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=443
2022-01-12 23:59:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:10 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.94 | ppl 245.65 | wps 28042.6 | wpb 556.6 | bsz 30.3 | num_updates 2248 | best_loss 7.94
2022-01-12 23:59:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2248 updates
2022-01-12 23:59:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 2248 updates, score 7.94) (writing took 4.605860479874536 seconds)
2022-01-12 23:59:14 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-12 23:59:14 | INFO | train | epoch 045 | loss 7.891 | ppl 237.32 | wps 6729.4 | ups 5.46 | wpb 1232.9 | bsz 63.5 | num_updates 2248 | lr 6.744e-06 | gnorm 8.697 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 449
2022-01-12 23:59:14 | INFO | fairseq.trainer | begin training epoch 46
2022-01-12 23:59:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:15 | INFO | train_inner | epoch 046:     12 / 50 loss=7.994, ppl=254.98, wps=3714.2, ups=2.87, wpb=1296.3, bsz=62.7, num_updates=2260, lr=6.78e-06, gnorm=8.626, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=450
2022-01-12 23:59:17 | INFO | train_inner | epoch 046:     32 / 50 loss=7.749, ppl=215.1, wps=15671.5, ups=13.17, wpb=1189.8, bsz=64, num_updates=2280, lr=6.84e-06, gnorm=8.917, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=452
2022-01-12 23:59:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:19 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.904 | ppl 239.51 | wps 26257.2 | wpb 556.6 | bsz 30.3 | num_updates 2298 | best_loss 7.904
2022-01-12 23:59:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2298 updates
2022-01-12 23:59:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 46 @ 2298 updates, score 7.904) (writing took 4.576131146866828 seconds)
2022-01-12 23:59:23 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-12 23:59:23 | INFO | train | epoch 046 | loss 7.837 | ppl 228.63 | wps 6706.1 | ups 5.44 | wpb 1232.9 | bsz 63.5 | num_updates 2298 | lr 6.894e-06 | gnorm 8.75 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 459
2022-01-12 23:59:23 | INFO | fairseq.trainer | begin training epoch 47
2022-01-12 23:59:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:24 | INFO | train_inner | epoch 047:      2 / 50 loss=7.835, ppl=228.34, wps=3669.3, ups=2.88, wpb=1275.9, bsz=64, num_updates=2300, lr=6.9e-06, gnorm=9.032, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=459
2022-01-12 23:59:25 | INFO | train_inner | epoch 047:     22 / 50 loss=7.848, ppl=230.47, wps=17697.2, ups=14.21, wpb=1245.6, bsz=64, num_updates=2320, lr=6.96e-06, gnorm=8.976, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=460
2022-01-12 23:59:27 | INFO | train_inner | epoch 047:     42 / 50 loss=7.73, ppl=212.26, wps=17404.8, ups=13.66, wpb=1274.3, bsz=62.7, num_updates=2340, lr=7.02e-06, gnorm=8.723, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=462
2022-01-12 23:59:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:28 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.786 | ppl 220.69 | wps 24253.9 | wpb 556.6 | bsz 30.3 | num_updates 2348 | best_loss 7.786
2022-01-12 23:59:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 2348 updates
2022-01-12 23:59:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 2348 updates, score 7.786) (writing took 4.3219620999880135 seconds)
2022-01-12 23:59:32 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-12 23:59:32 | INFO | train | epoch 047 | loss 7.767 | ppl 217.86 | wps 6944.2 | ups 5.63 | wpb 1232.9 | bsz 63.5 | num_updates 2348 | lr 7.044e-06 | gnorm 8.984 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 467
2022-01-12 23:59:32 | INFO | fairseq.trainer | begin training epoch 48
2022-01-12 23:59:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:33 | INFO | train_inner | epoch 048:     12 / 50 loss=7.671, ppl=203.85, wps=3673.4, ups=2.96, wpb=1239.3, bsz=62.7, num_updates=2360, lr=7.08e-06, gnorm=8.791, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=468
2022-01-12 23:59:35 | INFO | train_inner | epoch 048:     32 / 50 loss=7.678, ppl=204.72, wps=16620.8, ups=14.11, wpb=1177.8, bsz=64, num_updates=2380, lr=7.14e-06, gnorm=8.765, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=470
2022-01-12 23:59:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:37 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.707 | ppl 208.95 | wps 25456.3 | wpb 556.6 | bsz 30.3 | num_updates 2398 | best_loss 7.707
2022-01-12 23:59:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 2398 updates
2022-01-12 23:59:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 48 @ 2398 updates, score 7.707) (writing took 4.2475571180693805 seconds)
2022-01-12 23:59:41 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-12 23:59:41 | INFO | train | epoch 048 | loss 7.747 | ppl 214.77 | wps 7075 | ups 5.74 | wpb 1232.9 | bsz 63.5 | num_updates 2398 | lr 7.194e-06 | gnorm 8.696 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 476
2022-01-12 23:59:41 | INFO | fairseq.trainer | begin training epoch 49
2022-01-12 23:59:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:41 | INFO | train_inner | epoch 049:      2 / 50 loss=7.856, ppl=231.67, wps=3639.7, ups=3.05, wpb=1192.6, bsz=64, num_updates=2400, lr=7.2e-06, gnorm=8.574, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=476
2022-01-12 23:59:43 | INFO | train_inner | epoch 049:     22 / 50 loss=7.437, ppl=173.24, wps=18505.6, ups=13.87, wpb=1334.5, bsz=64, num_updates=2420, lr=7.26e-06, gnorm=8.444, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=478
2022-01-12 23:59:44 | INFO | train_inner | epoch 049:     42 / 50 loss=7.571, ppl=190.18, wps=16310, ups=14.33, wpb=1138.3, bsz=62.7, num_updates=2440, lr=7.32e-06, gnorm=9.063, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=479
2022-01-12 23:59:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:45 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.68 | ppl 205.05 | wps 23843.5 | wpb 556.6 | bsz 30.3 | num_updates 2448 | best_loss 7.68
2022-01-12 23:59:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 2448 updates
2022-01-12 23:59:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 2448 updates, score 7.68) (writing took 4.621699879877269 seconds)
2022-01-12 23:59:50 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-12 23:59:50 | INFO | train | epoch 049 | loss 7.559 | ppl 188.55 | wps 6798.7 | ups 5.51 | wpb 1232.9 | bsz 63.5 | num_updates 2448 | lr 7.344e-06 | gnorm 8.73 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 485
2022-01-12 23:59:50 | INFO | fairseq.trainer | begin training epoch 50
2022-01-12 23:59:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:53 | INFO | train_inner | epoch 050:     12 / 50 loss=7.856, ppl=231.71, wps=3024.6, ups=2.37, wpb=1276.5, bsz=62.7, num_updates=2460, lr=7.38e-06, gnorm=8.075, clip=100, loss_scale=32, train_wall=3, gb_free=20.9, wall=488
2022-01-12 23:59:54 | INFO | train_inner | epoch 050:     32 / 50 loss=7.992, ppl=254.5, wps=17201.1, ups=13.51, wpb=1273.2, bsz=64, num_updates=2480, lr=7.44e-06, gnorm=8.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=489
2022-01-12 23:59:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:56 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.607 | ppl 194.93 | wps 26515.6 | wpb 556.6 | bsz 30.3 | num_updates 2498 | best_loss 7.607
2022-01-12 23:59:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2498 updates
2022-01-12 23:59:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 2498 updates, score 7.607) (writing took 4.406413435935974 seconds)
2022-01-13 00:00:01 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-13 00:00:01 | INFO | train | epoch 050 | loss 7.954 | ppl 247.89 | wps 5911.6 | ups 4.79 | wpb 1232.9 | bsz 63.5 | num_updates 2498 | lr 7.494e-06 | gnorm 7.901 | clip 100 | loss_scale 32 | train_wall 5 | gb_free 20.9 | wall 496
2022-01-13 00:00:01 | INFO | fairseq.trainer | begin training epoch 51
2022-01-13 00:00:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:01 | INFO | train_inner | epoch 051:      2 / 50 loss=7.965, ppl=249.94, wps=3527.3, ups=2.97, wpb=1188, bsz=64, num_updates=2500, lr=7.5e-06, gnorm=7.812, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=496
2022-01-13 00:00:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:02 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.556 | ppl 188.12 | wps 23617.3 | wpb 556.6 | bsz 30.3 | num_updates 2500 | best_loss 7.556
2022-01-13 00:00:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2500 updates
2022-01-13 00:00:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt
2022-01-13 00:00:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt
2022-01-13 00:00:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt (epoch 51 @ 2500 updates, score 7.556) (writing took 9.116204982856289 seconds)
2022-01-13 00:00:13 | INFO | train_inner | epoch 051:     22 / 50 loss=7.947, ppl=246.79, wps=2113.4, ups=1.7, wpb=1242.9, bsz=64, num_updates=2520, lr=7.56e-06, gnorm=7.705, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=508
2022-01-13 00:00:14 | INFO | train_inner | epoch 051:     42 / 50 loss=7.815, ppl=225.15, wps=16295.3, ups=13.01, wpb=1252.8, bsz=62.7, num_updates=2540, lr=7.62e-06, gnorm=7.548, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=509
2022-01-13 00:00:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:16 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.582 | ppl 191.56 | wps 24534.5 | wpb 556.6 | bsz 30.3 | num_updates 2548 | best_loss 7.556
2022-01-13 00:00:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2548 updates
2022-01-13 00:00:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 51 @ 2548 updates, score 7.582) (writing took 3.3303923478815705 seconds)
2022-01-13 00:00:19 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-13 00:00:19 | INFO | train | epoch 051 | loss 7.866 | ppl 233.29 | wps 3360.1 | ups 2.73 | wpb 1232.9 | bsz 63.5 | num_updates 2548 | lr 7.644e-06 | gnorm 7.713 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 514
2022-01-13 00:00:19 | INFO | fairseq.trainer | begin training epoch 52
2022-01-13 00:00:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:20 | INFO | train_inner | epoch 052:     12 / 50 loss=7.753, ppl=215.67, wps=4079.6, ups=3.47, wpb=1176.3, bsz=62.7, num_updates=2560, lr=7.68e-06, gnorm=8.2, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=515
2022-01-13 00:00:21 | INFO | train_inner | epoch 052:     32 / 50 loss=7.812, ppl=224.76, wps=15926.7, ups=13.37, wpb=1191, bsz=64, num_updates=2580, lr=7.74e-06, gnorm=7.874, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=517
2022-01-13 00:00:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:24 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.559 | ppl 188.59 | wps 25404.3 | wpb 556.6 | bsz 30.3 | num_updates 2598 | best_loss 7.556
2022-01-13 00:00:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 2598 updates
2022-01-13 00:00:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 52 @ 2598 updates, score 7.559) (writing took 3.254428870975971 seconds)
2022-01-13 00:00:27 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-13 00:00:27 | INFO | train | epoch 052 | loss 7.796 | ppl 222.19 | wps 7595.6 | ups 6.16 | wpb 1232.9 | bsz 63.5 | num_updates 2598 | lr 7.794e-06 | gnorm 7.966 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 522
2022-01-13 00:00:27 | INFO | fairseq.trainer | begin training epoch 53
2022-01-13 00:00:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:27 | INFO | train_inner | epoch 053:      2 / 50 loss=7.772, ppl=218.64, wps=4332.3, ups=3.41, wpb=1271.5, bsz=64, num_updates=2600, lr=7.8e-06, gnorm=7.908, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=522
2022-01-13 00:00:29 | INFO | train_inner | epoch 053:     22 / 50 loss=7.807, ppl=223.88, wps=17655, ups=13.44, wpb=1313.8, bsz=64, num_updates=2620, lr=7.86e-06, gnorm=7.316, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=524
2022-01-13 00:00:30 | INFO | train_inner | epoch 053:     42 / 50 loss=7.632, ppl=198.38, wps=13717.1, ups=11.88, wpb=1154.8, bsz=62.7, num_updates=2640, lr=7.92e-06, gnorm=8.024, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=526
2022-01-13 00:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:32 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.412 | ppl 170.27 | wps 27303.1 | wpb 556.6 | bsz 30.3 | num_updates 2648 | best_loss 7.412
2022-01-13 00:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 2648 updates
2022-01-13 00:00:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 2648 updates, score 7.412) (writing took 5.089505098061636 seconds)
2022-01-13 00:00:37 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-13 00:00:37 | INFO | train | epoch 053 | loss 7.707 | ppl 209.01 | wps 6233.1 | ups 5.06 | wpb 1232.9 | bsz 63.5 | num_updates 2648 | lr 7.944e-06 | gnorm 7.65 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 532
2022-01-13 00:00:37 | INFO | fairseq.trainer | begin training epoch 54
2022-01-13 00:00:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:38 | INFO | train_inner | epoch 054:     12 / 50 loss=7.668, ppl=203.31, wps=3287.3, ups=2.65, wpb=1242.8, bsz=64, num_updates=2660, lr=7.98e-06, gnorm=7.521, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=533
2022-01-13 00:00:39 | INFO | train_inner | epoch 054:     32 / 50 loss=7.665, ppl=202.93, wps=17643.7, ups=14.19, wpb=1243.4, bsz=64, num_updates=2680, lr=8.04e-06, gnorm=7.633, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=535
2022-01-13 00:00:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:41 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.433 | ppl 172.86 | wps 26600.4 | wpb 556.6 | bsz 30.3 | num_updates 2698 | best_loss 7.412
2022-01-13 00:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 2698 updates
2022-01-13 00:00:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 54 @ 2698 updates, score 7.433) (writing took 3.989198715193197 seconds)
2022-01-13 00:00:45 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-13 00:00:45 | INFO | train | epoch 054 | loss 7.627 | ppl 197.69 | wps 7212.2 | ups 5.85 | wpb 1232.9 | bsz 63.5 | num_updates 2698 | lr 8.094e-06 | gnorm 7.588 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 541
2022-01-13 00:00:45 | INFO | fairseq.trainer | begin training epoch 55
2022-01-13 00:00:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:46 | INFO | train_inner | epoch 055:      2 / 50 loss=7.49, ppl=179.72, wps=3767.4, ups=3.19, wpb=1182.4, bsz=62.7, num_updates=2700, lr=8.1e-06, gnorm=7.608, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=541
2022-01-13 00:00:47 | INFO | train_inner | epoch 055:     22 / 50 loss=7.332, ppl=161.16, wps=14790.9, ups=13.87, wpb=1066.3, bsz=62.7, num_updates=2720, lr=8.16e-06, gnorm=9.287, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=542
2022-01-13 00:00:49 | INFO | train_inner | epoch 055:     42 / 50 loss=7.628, ppl=197.85, wps=17417.2, ups=13.48, wpb=1291.8, bsz=64, num_updates=2740, lr=8.22e-06, gnorm=7.261, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=544
2022-01-13 00:00:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:50 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.352 | ppl 163.41 | wps 26194.6 | wpb 556.6 | bsz 30.3 | num_updates 2748 | best_loss 7.352
2022-01-13 00:00:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 2748 updates
2022-01-13 00:00:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 55 @ 2748 updates, score 7.352) (writing took 4.29561490402557 seconds)
2022-01-13 00:00:54 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-13 00:00:54 | INFO | train | epoch 055 | loss 7.528 | ppl 184.52 | wps 6895.5 | ups 5.59 | wpb 1232.9 | bsz 63.5 | num_updates 2748 | lr 8.244e-06 | gnorm 8.049 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 550
2022-01-13 00:00:54 | INFO | fairseq.trainer | begin training epoch 56
2022-01-13 00:00:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:55 | INFO | train_inner | epoch 056:     12 / 50 loss=7.617, ppl=196.36, wps=3931.7, ups=2.95, wpb=1332, bsz=64, num_updates=2760, lr=8.28e-06, gnorm=7.379, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=551
2022-01-13 00:00:57 | INFO | train_inner | epoch 056:     32 / 50 loss=7.455, ppl=175.47, wps=17931.7, ups=13.83, wpb=1297, bsz=62.7, num_updates=2780, lr=8.34e-06, gnorm=7.609, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=552
2022-01-13 00:00:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:59 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.268 | ppl 154.13 | wps 26234.9 | wpb 556.6 | bsz 30.3 | num_updates 2798 | best_loss 7.268
2022-01-13 00:00:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 2798 updates
2022-01-13 00:00:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 2798 updates, score 7.268) (writing took 4.465936854016036 seconds)
2022-01-13 00:01:03 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-13 00:01:03 | INFO | train | epoch 056 | loss 7.522 | ppl 183.82 | wps 6816.9 | ups 5.53 | wpb 1232.9 | bsz 63.5 | num_updates 2798 | lr 8.394e-06 | gnorm 7.608 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 559
2022-01-13 00:01:03 | INFO | fairseq.trainer | begin training epoch 57
2022-01-13 00:01:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:04 | INFO | train_inner | epoch 057:      2 / 50 loss=7.603, ppl=194.4, wps=3479.8, ups=2.9, wpb=1200.4, bsz=64, num_updates=2800, lr=8.4e-06, gnorm=7.566, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=559
2022-01-13 00:01:05 | INFO | train_inner | epoch 057:     22 / 50 loss=7.343, ppl=162.36, wps=18166.1, ups=13.92, wpb=1304.7, bsz=64, num_updates=2820, lr=8.46e-06, gnorm=7.346, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=560
2022-01-13 00:01:07 | INFO | train_inner | epoch 057:     42 / 50 loss=7.482, ppl=178.81, wps=14792.5, ups=12.62, wpb=1172.3, bsz=62.7, num_updates=2840, lr=8.52e-06, gnorm=7.784, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=562
2022-01-13 00:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:08 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.213 | ppl 148.41 | wps 26417.6 | wpb 556.6 | bsz 30.3 | num_updates 2848 | best_loss 7.213
2022-01-13 00:01:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 2848 updates
2022-01-13 00:01:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 57 @ 2848 updates, score 7.213) (writing took 4.1772787710651755 seconds)
2022-01-13 00:01:12 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-13 00:01:12 | INFO | train | epoch 057 | loss 7.442 | ppl 173.9 | wps 6952 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 2848 | lr 8.544e-06 | gnorm 7.599 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 567
2022-01-13 00:01:12 | INFO | fairseq.trainer | begin training epoch 58
2022-01-13 00:01:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:13 | INFO | train_inner | epoch 058:     12 / 50 loss=7.471, ppl=177.43, wps=3762.3, ups=3.07, wpb=1224.3, bsz=64, num_updates=2860, lr=8.58e-06, gnorm=7.622, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=568
2022-01-13 00:01:15 | INFO | train_inner | epoch 058:     32 / 50 loss=7.347, ppl=162.82, wps=13996.8, ups=11.97, wpb=1169.8, bsz=62.7, num_updates=2880, lr=8.64e-06, gnorm=7.758, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=570
2022-01-13 00:01:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:17 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.06 | ppl 133.46 | wps 25253.9 | wpb 556.6 | bsz 30.3 | num_updates 2898 | best_loss 7.06
2022-01-13 00:01:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 2898 updates
2022-01-13 00:01:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 58 @ 2898 updates, score 7.06) (writing took 4.362164995865896 seconds)
2022-01-13 00:01:21 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-13 00:01:22 | INFO | train | epoch 058 | loss 7.386 | ppl 167.28 | wps 6767.3 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 2898 | lr 8.694e-06 | gnorm 7.642 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 577
2022-01-13 00:01:22 | INFO | fairseq.trainer | begin training epoch 59
2022-01-13 00:01:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:22 | INFO | train_inner | epoch 059:      2 / 50 loss=7.393, ppl=168.03, wps=3606, ups=2.84, wpb=1271, bsz=62.7, num_updates=2900, lr=8.7e-06, gnorm=7.68, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=577
2022-01-13 00:01:24 | INFO | train_inner | epoch 059:     22 / 50 loss=7.228, ppl=149.9, wps=15681.9, ups=12.92, wpb=1213.8, bsz=64, num_updates=2920, lr=8.76e-06, gnorm=7.561, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=579
2022-01-13 00:01:25 | INFO | train_inner | epoch 059:     42 / 50 loss=7.287, ppl=156.23, wps=16704.3, ups=13.86, wpb=1205.3, bsz=64, num_updates=2940, lr=8.82e-06, gnorm=7.534, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=580
2022-01-13 00:01:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:26 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.04 | ppl 131.59 | wps 26928.6 | wpb 556.6 | bsz 30.3 | num_updates 2948 | best_loss 7.04
2022-01-13 00:01:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 2948 updates
2022-01-13 00:01:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 59 @ 2948 updates, score 7.04) (writing took 4.292766103055328 seconds)
2022-01-13 00:01:31 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-13 00:01:31 | INFO | train | epoch 059 | loss 7.284 | ppl 155.88 | wps 6850.3 | ups 5.56 | wpb 1232.9 | bsz 63.5 | num_updates 2948 | lr 8.844e-06 | gnorm 7.553 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 586
2022-01-13 00:01:31 | INFO | fairseq.trainer | begin training epoch 60
2022-01-13 00:01:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:32 | INFO | train_inner | epoch 060:     12 / 50 loss=7.297, ppl=157.28, wps=4063.7, ups=2.97, wpb=1369, bsz=64, num_updates=2960, lr=8.88e-06, gnorm=7.687, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=587
2022-01-13 00:01:33 | INFO | train_inner | epoch 060:     32 / 50 loss=7.322, ppl=160, wps=16409.5, ups=12.81, wpb=1281.2, bsz=62.7, num_updates=2980, lr=8.94e-06, gnorm=7.751, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=588
2022-01-13 00:01:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:35 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.034 | ppl 131.08 | wps 28028.6 | wpb 556.6 | bsz 30.3 | num_updates 2998 | best_loss 7.034
2022-01-13 00:01:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 2998 updates
2022-01-13 00:01:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 60 @ 2998 updates, score 7.034) (writing took 5.088655893923715 seconds)
2022-01-13 00:01:40 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-13 00:01:40 | INFO | train | epoch 060 | loss 7.224 | ppl 149.47 | wps 6355.5 | ups 5.15 | wpb 1232.9 | bsz 63.5 | num_updates 2998 | lr 8.994e-06 | gnorm 7.765 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 596
2022-01-13 00:01:40 | INFO | fairseq.trainer | begin training epoch 61
2022-01-13 00:01:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:41 | INFO | train_inner | epoch 061:      2 / 50 loss=7.147, ppl=141.71, wps=2947.8, ups=2.71, wpb=1087.5, bsz=64, num_updates=3000, lr=9e-06, gnorm=7.685, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=596
2022-01-13 00:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:42 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.08 | ppl 135.29 | wps 28111.9 | wpb 556.6 | bsz 30.3 | num_updates 3000 | best_loss 7.034
2022-01-13 00:01:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3000 updates
2022-01-13 00:01:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt
2022-01-13 00:01:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt
2022-01-13 00:01:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt (epoch 61 @ 3000 updates, score 7.08) (writing took 5.089682440040633 seconds)
2022-01-13 00:01:48 | INFO | train_inner | epoch 061:     22 / 50 loss=7.18, ppl=145, wps=3129.5, ups=2.65, wpb=1181.2, bsz=62.7, num_updates=3020, lr=9.06e-06, gnorm=7.701, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=603
2022-01-13 00:01:50 | INFO | train_inner | epoch 061:     42 / 50 loss=7.218, ppl=148.87, wps=16939.1, ups=13.17, wpb=1286.7, bsz=64, num_updates=3040, lr=9.12e-06, gnorm=7.684, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=605
2022-01-13 00:01:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:51 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.068 | ppl 134.22 | wps 25034.2 | wpb 556.6 | bsz 30.3 | num_updates 3048 | best_loss 7.034
2022-01-13 00:01:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3048 updates
2022-01-13 00:01:51 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:01:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:01:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 61 @ 3048 updates, score 7.068) (writing took 3.1684718979522586 seconds)
2022-01-13 00:01:54 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-13 00:01:54 | INFO | train | epoch 061 | loss 7.189 | ppl 145.9 | wps 4447.5 | ups 3.61 | wpb 1232.9 | bsz 63.5 | num_updates 3048 | lr 9.144e-06 | gnorm 7.641 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 609
2022-01-13 00:01:54 | INFO | fairseq.trainer | begin training epoch 62
2022-01-13 00:01:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:55 | INFO | train_inner | epoch 062:     12 / 50 loss=7.247, ppl=151.87, wps=4768.9, ups=3.63, wpb=1314.8, bsz=64, num_updates=3060, lr=9.18e-06, gnorm=7.253, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=610
2022-01-13 00:01:57 | INFO | train_inner | epoch 062:     32 / 50 loss=6.903, ppl=119.71, wps=14414.4, ups=13.22, wpb=1090.7, bsz=64, num_updates=3080, lr=9.24e-06, gnorm=7.925, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=612
2022-01-13 00:01:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:59 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.997 | ppl 127.75 | wps 25642.7 | wpb 556.6 | bsz 30.3 | num_updates 3098 | best_loss 6.997
2022-01-13 00:01:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 3098 updates
2022-01-13 00:01:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 62 @ 3098 updates, score 6.997) (writing took 4.368456893134862 seconds)
2022-01-13 00:02:03 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-13 00:02:03 | INFO | train | epoch 062 | loss 7.123 | ppl 139.36 | wps 6768.3 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 3098 | lr 9.294e-06 | gnorm 7.606 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 619
2022-01-13 00:02:03 | INFO | fairseq.trainer | begin training epoch 63
2022-01-13 00:02:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:04 | INFO | train_inner | epoch 063:      2 / 50 loss=7.231, ppl=150.2, wps=3953.2, ups=2.91, wpb=1357.5, bsz=62.7, num_updates=3100, lr=9.3e-06, gnorm=7.648, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=619
2022-01-13 00:02:05 | INFO | train_inner | epoch 063:     22 / 50 loss=7.031, ppl=130.81, wps=17231.3, ups=13.6, wpb=1267, bsz=64, num_updates=3120, lr=9.36e-06, gnorm=7.415, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=620
2022-01-13 00:02:07 | INFO | train_inner | epoch 063:     42 / 50 loss=7.028, ppl=130.51, wps=16932.6, ups=14.56, wpb=1162.7, bsz=62.7, num_updates=3140, lr=9.42e-06, gnorm=7.81, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=622
2022-01-13 00:02:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:08 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.847 | ppl 115.13 | wps 26909.2 | wpb 556.6 | bsz 30.3 | num_updates 3148 | best_loss 6.847
2022-01-13 00:02:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 3148 updates
2022-01-13 00:02:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 63 @ 3148 updates, score 6.847) (writing took 5.7117693750187755 seconds)
2022-01-13 00:02:14 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-13 00:02:14 | INFO | train | epoch 063 | loss 7.043 | ppl 131.85 | wps 6090.9 | ups 4.94 | wpb 1232.9 | bsz 63.5 | num_updates 3148 | lr 9.444e-06 | gnorm 7.687 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 629
2022-01-13 00:02:14 | INFO | fairseq.trainer | begin training epoch 64
2022-01-13 00:02:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:15 | INFO | train_inner | epoch 064:     12 / 50 loss=6.993, ppl=127.4, wps=2939.8, ups=2.48, wpb=1186.6, bsz=64, num_updates=3160, lr=9.48e-06, gnorm=7.72, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=630
2022-01-13 00:02:16 | INFO | train_inner | epoch 064:     32 / 50 loss=7.146, ppl=141.66, wps=16954.1, ups=13.54, wpb=1252.2, bsz=64, num_updates=3180, lr=9.54e-06, gnorm=7.821, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=631
2022-01-13 00:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:18 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.863 | ppl 116.38 | wps 26723.1 | wpb 556.6 | bsz 30.3 | num_updates 3198 | best_loss 6.847
2022-01-13 00:02:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 3198 updates
2022-01-13 00:02:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 64 @ 3198 updates, score 6.863) (writing took 2.9973171290475875 seconds)
2022-01-13 00:02:21 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-13 00:02:21 | INFO | train | epoch 064 | loss 7.015 | ppl 129.3 | wps 8209.7 | ups 6.66 | wpb 1232.9 | bsz 63.5 | num_updates 3198 | lr 9.594e-06 | gnorm 7.69 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 636
2022-01-13 00:02:21 | INFO | fairseq.trainer | begin training epoch 65
2022-01-13 00:02:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:21 | INFO | train_inner | epoch 065:      2 / 50 loss=6.916, ppl=120.75, wps=4731, ups=3.83, wpb=1234.3, bsz=62.7, num_updates=3200, lr=9.6e-06, gnorm=7.598, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=636
2022-01-13 00:02:23 | INFO | train_inner | epoch 065:     22 / 50 loss=6.815, ppl=112.58, wps=17373.8, ups=13.56, wpb=1281.6, bsz=64, num_updates=3220, lr=9.66e-06, gnorm=7.468, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=638
2022-01-13 00:02:24 | INFO | train_inner | epoch 065:     42 / 50 loss=6.897, ppl=119.19, wps=17407.9, ups=13.92, wpb=1250.8, bsz=64, num_updates=3240, lr=9.72e-06, gnorm=7.956, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=639
2022-01-13 00:02:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:26 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.754 | ppl 107.96 | wps 27435.4 | wpb 556.6 | bsz 30.3 | num_updates 3248 | best_loss 6.754
2022-01-13 00:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 3248 updates
2022-01-13 00:02:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 65 @ 3248 updates, score 6.754) (writing took 5.165929597103968 seconds)
2022-01-13 00:02:31 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-01-13 00:02:31 | INFO | train | epoch 065 | loss 6.907 | ppl 119.99 | wps 6332.7 | ups 5.14 | wpb 1232.9 | bsz 63.5 | num_updates 3248 | lr 9.744e-06 | gnorm 7.732 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 646
2022-01-13 00:02:31 | INFO | fairseq.trainer | begin training epoch 66
2022-01-13 00:02:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:32 | INFO | train_inner | epoch 066:     12 / 50 loss=6.969, ppl=125.26, wps=3262.8, ups=2.63, wpb=1240.5, bsz=62.7, num_updates=3260, lr=9.78e-06, gnorm=7.703, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=647
2022-01-13 00:02:33 | INFO | train_inner | epoch 066:     32 / 50 loss=6.83, ppl=113.75, wps=17104.4, ups=13.29, wpb=1286.8, bsz=64, num_updates=3280, lr=9.84e-06, gnorm=7.987, clip=100, loss_scale=32, train_wall=1, gb_free=20.8, wall=648
2022-01-13 00:02:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:36 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.703 | ppl 104.17 | wps 28140.3 | wpb 556.6 | bsz 30.3 | num_updates 3298 | best_loss 6.703
2022-01-13 00:02:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 3298 updates
2022-01-13 00:02:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 66 @ 3298 updates, score 6.703) (writing took 5.57637187698856 seconds)
2022-01-13 00:02:41 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-01-13 00:02:41 | INFO | train | epoch 066 | loss 6.911 | ppl 120.32 | wps 5936 | ups 4.81 | wpb 1232.9 | bsz 63.5 | num_updates 3298 | lr 9.894e-06 | gnorm 7.846 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 656
2022-01-13 00:02:41 | INFO | fairseq.trainer | begin training epoch 67
2022-01-13 00:02:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:41 | INFO | train_inner | epoch 067:      2 / 50 loss=7.013, ppl=129.17, wps=2863.2, ups=2.46, wpb=1165.1, bsz=62.7, num_updates=3300, lr=9.9e-06, gnorm=7.776, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=657
2022-01-13 00:02:43 | INFO | train_inner | epoch 067:     22 / 50 loss=6.888, ppl=118.45, wps=16588.8, ups=13.29, wpb=1248.2, bsz=64, num_updates=3320, lr=9.96e-06, gnorm=7.519, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=658
2022-01-13 00:02:45 | INFO | train_inner | epoch 067:     42 / 50 loss=6.84, ppl=114.56, wps=15103.3, ups=13.02, wpb=1160.2, bsz=62.7, num_updates=3340, lr=1.002e-05, gnorm=7.814, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=660
2022-01-13 00:02:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:46 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.727 | ppl 105.91 | wps 27156.8 | wpb 556.6 | bsz 30.3 | num_updates 3348 | best_loss 6.703
2022-01-13 00:02:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 3348 updates
2022-01-13 00:02:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 67 @ 3348 updates, score 6.727) (writing took 2.760889601893723 seconds)
2022-01-13 00:02:49 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-01-13 00:02:49 | INFO | train | epoch 067 | loss 6.829 | ppl 113.68 | wps 8100.9 | ups 6.57 | wpb 1232.9 | bsz 63.5 | num_updates 3348 | lr 1.0044e-05 | gnorm 7.7 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 664
2022-01-13 00:02:49 | INFO | fairseq.trainer | begin training epoch 68
2022-01-13 00:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:50 | INFO | train_inner | epoch 068:     12 / 50 loss=6.693, ppl=103.5, wps=5267.1, ups=3.77, wpb=1396.4, bsz=64, num_updates=3360, lr=1.008e-05, gnorm=7.661, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=665
2022-01-13 00:02:51 | INFO | train_inner | epoch 068:     32 / 50 loss=6.787, ppl=110.46, wps=16931.4, ups=13.57, wpb=1247.9, bsz=62.7, num_updates=3380, lr=1.014e-05, gnorm=7.696, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=666
2022-01-13 00:02:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:53 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.69 | ppl 103.24 | wps 25091.9 | wpb 556.6 | bsz 30.3 | num_updates 3398 | best_loss 6.69
2022-01-13 00:02:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 3398 updates
2022-01-13 00:02:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 68 @ 3398 updates, score 6.69) (writing took 3.931047295918688 seconds)
2022-01-13 00:02:57 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-01-13 00:02:57 | INFO | train | epoch 068 | loss 6.759 | ppl 108.32 | wps 7220.8 | ups 5.86 | wpb 1232.9 | bsz 63.5 | num_updates 3398 | lr 1.0194e-05 | gnorm 7.655 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 672
2022-01-13 00:02:57 | INFO | fairseq.trainer | begin training epoch 69
2022-01-13 00:02:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:58 | INFO | train_inner | epoch 069:      2 / 50 loss=6.71, ppl=104.71, wps=3431.3, ups=3.19, wpb=1076.8, bsz=64, num_updates=3400, lr=1.02e-05, gnorm=7.74, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=673
2022-01-13 00:02:59 | INFO | train_inner | epoch 069:     22 / 50 loss=6.817, ppl=112.74, wps=18982.9, ups=13.78, wpb=1377.8, bsz=64, num_updates=3420, lr=1.026e-05, gnorm=7.292, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=674
2022-01-13 00:03:01 | INFO | train_inner | epoch 069:     42 / 50 loss=6.604, ppl=97.29, wps=15050.8, ups=12.86, wpb=1170.5, bsz=62.7, num_updates=3440, lr=1.032e-05, gnorm=7.848, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=676
2022-01-13 00:03:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:02 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.554 | ppl 93.93 | wps 26835.2 | wpb 556.6 | bsz 30.3 | num_updates 3448 | best_loss 6.554
2022-01-13 00:03:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 3448 updates
2022-01-13 00:03:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:03:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 69 @ 3448 updates, score 6.554) (writing took 4.591499911854044 seconds)
2022-01-13 00:03:06 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-01-13 00:03:06 | INFO | train | epoch 069 | loss 6.691 | ppl 103.3 | wps 6762.5 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 3448 | lr 1.0344e-05 | gnorm 7.666 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 682
2022-01-13 00:03:07 | INFO | fairseq.trainer | begin training epoch 70
2022-01-13 00:03:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:03:07 | INFO | train_inner | epoch 070:     12 / 50 loss=6.726, ppl=105.89, wps=3573.3, ups=2.93, wpb=1221.6, bsz=62.7, num_updates=3460, lr=1.038e-05, gnorm=7.935, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=683
2022-01-13 00:03:09 | INFO | train_inner | epoch 070:     32 / 50 loss=6.528, ppl=92.26, wps=16406.3, ups=13.63, wpb=1203.3, bsz=64, num_updates=3480, lr=1.044e-05, gnorm=7.645, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=684
2022-01-13 00:03:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:11 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.655 | ppl 100.75 | wps 24804 | wpb 556.6 | bsz 30.3 | num_updates 3498 | best_loss 6.554
2022-01-13 00:03:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 3498 updates
2022-01-13 00:03:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 70 @ 3498 updates, score 6.655) (writing took 2.7075513841118664 seconds)
2022-01-13 00:03:14 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-01-13 00:03:14 | INFO | train | epoch 070 | loss 6.615 | ppl 98.03 | wps 8547.8 | ups 6.93 | wpb 1232.9 | bsz 63.5 | num_updates 3498 | lr 1.0494e-05 | gnorm 7.604 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 689
2022-01-13 00:03:14 | INFO | fairseq.trainer | begin training epoch 71
2022-01-13 00:03:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:03:14 | INFO | train_inner | epoch 071:      2 / 50 loss=6.691, ppl=103.3, wps=4979.5, ups=3.99, wpb=1248.8, bsz=64, num_updates=3500, lr=1.05e-05, gnorm=7.38, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=689
2022-01-13 00:03:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:15 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.669 | ppl 101.79 | wps 26710.3 | wpb 556.6 | bsz 30.3 | num_updates 3500 | best_loss 6.554
2022-01-13 00:03:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3500 updates
2022-01-13 00:03:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt
2022-01-13 00:03:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt
2022-01-13 00:03:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt (epoch 71 @ 3500 updates, score 6.669) (writing took 3.828785134013742 seconds)
2022-01-13 00:03:20 | INFO | train_inner | epoch 071:     22 / 50 loss=6.569, ppl=94.96, wps=3699.3, ups=3.16, wpb=1169.3, bsz=64, num_updates=3520, lr=1.056e-05, gnorm=7.909, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=695
2022-01-13 00:03:22 | INFO | train_inner | epoch 071:     42 / 50 loss=6.61, ppl=97.65, wps=14561.6, ups=12.03, wpb=1210.1, bsz=64, num_updates=3540, lr=1.062e-05, gnorm=7.789, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=697
2022-01-13 00:03:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:23 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.584 | ppl 95.94 | wps 24738 | wpb 556.6 | bsz 30.3 | num_updates 3548 | best_loss 6.554
2022-01-13 00:03:23 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-01-13 00:03:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3548 updates
2022-01-13 00:03:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 71 @ 3548 updates, score 6.584) (writing took 2.709606857970357 seconds)
2022-01-13 00:03:26 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-01-13 00:03:26 | INFO | train | epoch 071 | loss 6.637 | ppl 99.51 | wps 5003.6 | ups 4.06 | wpb 1232.9 | bsz 63.5 | num_updates 3548 | lr 1.0644e-05 | gnorm 7.79 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 701
2022-01-13 00:03:26 | INFO | fairseq_cli.train | done training in 696.7 seconds
