2022-01-12 23:51:38 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=50, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 50, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-12 23:51:38 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-01-12 23:51:41 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-01-12 23:51:41 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-01-12 23:51:41 | INFO | fairseq_cli.train | model: BARTMLModel
2022-01-12 23:51:41 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-01-12 23:51:41 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-01-12 23:51:41 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-12 23:51:41 | INFO | bartabst.data.data_utils | loaded 908 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-01-12 23:51:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-01-12 23:51:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-12 23:51:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-01-12 23:51:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-12 23:51:45 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-01-12 23:51:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-12 23:51:45 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-12 23:51:45 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-01-12 23:51:45 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-01-12 23:51:46 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-01-12 23:51:47 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-01-12 23:51:47 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-12 23:51:49 | INFO | bartabst.data.data_utils | loaded 3,174 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-01-12 23:51:49 | INFO | fairseq.trainer | begin training epoch 1
2022-01-12 23:51:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:51:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-01-12 23:51:51 | INFO | train_inner | epoch 001:     21 / 50 loss=17.159, ppl=146394, wps=16341.4, ups=14.05, wpb=1196, bsz=62.7, num_updates=20, lr=6e-08, gnorm=23.615, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=6
2022-01-12 23:51:53 | INFO | train_inner | epoch 001:     41 / 50 loss=17.296, ppl=160960, wps=14414.7, ups=11.41, wpb=1263.2, bsz=64, num_updates=40, lr=1.2e-07, gnorm=23.468, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=8
2022-01-12 23:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:51:54 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.132 | ppl 143673 | wps 27473.4 | wpb 556.6 | bsz 30.3 | num_updates 49
2022-01-12 23:51:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 49 updates
2022-01-12 23:51:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:51:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 49 updates, score 17.132) (writing took 5.029121475992724 seconds)
2022-01-12 23:51:59 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-12 23:51:59 | INFO | train | epoch 001 | loss 17.245 | ppl 155295 | wps 6090.9 | ups 5.05 | wpb 1219.7 | bsz 63.5 | num_updates 49 | lr 1.47e-07 | gnorm 23.417 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 14
2022-01-12 23:51:59 | INFO | fairseq.trainer | begin training epoch 2
2022-01-12 23:51:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:00 | INFO | train_inner | epoch 002:     11 / 50 loss=17.221, ppl=152767, wps=3093.8, ups=2.74, wpb=1130.5, bsz=62.7, num_updates=60, lr=1.8e-07, gnorm=23.114, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=15
2022-01-12 23:52:02 | INFO | train_inner | epoch 002:     31 / 50 loss=17.199, ppl=150443, wps=18866, ups=14.44, wpb=1306.8, bsz=64, num_updates=80, lr=2.4e-07, gnorm=22.864, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=17
2022-01-12 23:52:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 16.922 | ppl 124169 | wps 29092.5 | wpb 556.6 | bsz 30.3 | num_updates 99 | best_loss 16.922
2022-01-12 23:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 99 updates
2022-01-12 23:52:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 99 updates, score 16.922) (writing took 4.468111583031714 seconds)
2022-01-12 23:52:08 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-12 23:52:08 | INFO | train | epoch 002 | loss 17.126 | ppl 143014 | wps 7062 | ups 5.73 | wpb 1232.9 | bsz 63.5 | num_updates 99 | lr 2.97e-07 | gnorm 22.899 | clip 100 | loss_scale 64 | train_wall 3 | gb_free 20.9 | wall 23
2022-01-12 23:52:08 | INFO | fairseq.trainer | begin training epoch 3
2022-01-12 23:52:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:08 | INFO | train_inner | epoch 003:      1 / 50 loss=17.043, ppl=135068, wps=3738.2, ups=3.03, wpb=1233.1, bsz=64, num_updates=100, lr=3e-07, gnorm=22.704, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=23
2022-01-12 23:52:10 | INFO | train_inner | epoch 003:     21 / 50 loss=16.922, ppl=124168, wps=16858.9, ups=13.79, wpb=1222.8, bsz=62.7, num_updates=120, lr=3.6e-07, gnorm=22.369, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=25
2022-01-12 23:52:11 | INFO | train_inner | epoch 003:     41 / 50 loss=16.713, ppl=107421, wps=18444.4, ups=13.69, wpb=1347.1, bsz=64, num_updates=140, lr=4.2e-07, gnorm=21.783, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=26
2022-01-12 23:52:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:13 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.449 | ppl 89457.4 | wps 28067.7 | wpb 556.6 | bsz 30.3 | num_updates 149 | best_loss 16.449
2022-01-12 23:52:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 149 updates
2022-01-12 23:52:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 149 updates, score 16.449) (writing took 4.802918447880074 seconds)
2022-01-12 23:52:17 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-12 23:52:17 | INFO | train | epoch 003 | loss 16.775 | ppl 112183 | wps 6585.6 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 149 | lr 4.47e-07 | gnorm 22.104 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 32
2022-01-12 23:52:17 | INFO | fairseq.trainer | begin training epoch 4
2022-01-12 23:52:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:18 | INFO | train_inner | epoch 004:     11 / 50 loss=16.552, ppl=96075.3, wps=3100, ups=2.79, wpb=1109.7, bsz=62.7, num_updates=160, lr=4.8e-07, gnorm=22.008, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=33
2022-01-12 23:52:20 | INFO | train_inner | epoch 004:     31 / 50 loss=16.283, ppl=79750.9, wps=17829.4, ups=14.14, wpb=1261, bsz=64, num_updates=180, lr=5.4e-07, gnorm=21.031, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=35
2022-01-12 23:52:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:22 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 15.807 | ppl 57342.1 | wps 26205.5 | wpb 556.6 | bsz 30.3 | num_updates 199 | best_loss 15.807
2022-01-12 23:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 199 updates
2022-01-12 23:52:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 199 updates, score 15.807) (writing took 4.1711140119004995 seconds)
2022-01-12 23:52:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-12 23:52:26 | INFO | train | epoch 004 | loss 16.271 | ppl 79071.1 | wps 7202.4 | ups 5.84 | wpb 1232.9 | bsz 63.5 | num_updates 199 | lr 5.97e-07 | gnorm 21.383 | clip 100 | loss_scale 64 | train_wall 3 | gb_free 20.9 | wall 41
2022-01-12 23:52:26 | INFO | fairseq.trainer | begin training epoch 5
2022-01-12 23:52:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:26 | INFO | train_inner | epoch 005:      1 / 50 loss=16.097, ppl=70117.1, wps=3768.2, ups=3.11, wpb=1210.8, bsz=64, num_updates=200, lr=6e-07, gnorm=21.412, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=41
2022-01-12 23:52:28 | INFO | train_inner | epoch 005:     21 / 50 loss=15.771, ppl=55917.4, wps=16429.9, ups=13.29, wpb=1235.9, bsz=64, num_updates=220, lr=6.6e-07, gnorm=20.211, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=43
2022-01-12 23:52:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-01-12 23:52:29 | INFO | train_inner | epoch 005:     42 / 50 loss=15.559, ppl=48277.4, wps=18448.4, ups=14.02, wpb=1316.2, bsz=64, num_updates=240, lr=7.2e-07, gnorm=19.435, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=44
2022-01-12 23:52:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:30 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 15.243 | ppl 38786.9 | wps 27354.8 | wpb 556.6 | bsz 30.3 | num_updates 248 | best_loss 15.243
2022-01-12 23:52:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 248 updates
2022-01-12 23:52:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 248 updates, score 15.243) (writing took 4.165005217073485 seconds)
2022-01-12 23:52:34 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-12 23:52:35 | INFO | train | epoch 005 | loss 15.636 | ppl 50916.4 | wps 7098.7 | ups 5.71 | wpb 1243 | bsz 63.5 | num_updates 248 | lr 7.44e-07 | gnorm 20.049 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 50
2022-01-12 23:52:35 | INFO | fairseq.trainer | begin training epoch 6
2022-01-12 23:52:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:35 | INFO | train_inner | epoch 006:     12 / 50 loss=15.337, ppl=41381, wps=3760.2, ups=3.09, wpb=1217.2, bsz=62.7, num_updates=260, lr=7.8e-07, gnorm=19.805, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=51
2022-01-12 23:52:37 | INFO | train_inner | epoch 006:     32 / 50 loss=14.997, ppl=32702.2, wps=18633.8, ups=14.21, wpb=1311.7, bsz=62.7, num_updates=280, lr=8.4e-07, gnorm=19.077, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=52
2022-01-12 23:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:39 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 14.581 | ppl 24503.1 | wps 27435.2 | wpb 556.6 | bsz 30.3 | num_updates 298 | best_loss 14.581
2022-01-12 23:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 298 updates
2022-01-12 23:52:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 298 updates, score 14.581) (writing took 4.1451350569259375 seconds)
2022-01-12 23:52:43 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-12 23:52:43 | INFO | train | epoch 006 | loss 15.033 | ppl 33520 | wps 7267.8 | ups 5.9 | wpb 1232.9 | bsz 63.5 | num_updates 298 | lr 8.94e-07 | gnorm 18.987 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 58
2022-01-12 23:52:43 | INFO | fairseq.trainer | begin training epoch 7
2022-01-12 23:52:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:43 | INFO | train_inner | epoch 007:      2 / 50 loss=14.896, ppl=30486.8, wps=3582.8, ups=3.15, wpb=1136.9, bsz=64, num_updates=300, lr=9e-07, gnorm=18.768, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=58
2022-01-12 23:52:45 | INFO | train_inner | epoch 007:     22 / 50 loss=14.558, ppl=24115.6, wps=18272.6, ups=13.89, wpb=1315.3, bsz=64, num_updates=320, lr=9.6e-07, gnorm=17.902, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=60
2022-01-12 23:52:46 | INFO | train_inner | epoch 007:     42 / 50 loss=14.268, ppl=19731.4, wps=15200.6, ups=14.45, wpb=1052.3, bsz=62.7, num_updates=340, lr=1.02e-06, gnorm=19.044, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=61
2022-01-12 23:52:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:48 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 13.939 | ppl 15700.3 | wps 26097.4 | wpb 556.6 | bsz 30.3 | num_updates 348 | best_loss 13.939
2022-01-12 23:52:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 348 updates
2022-01-12 23:52:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 348 updates, score 13.939) (writing took 4.287074175197631 seconds)
2022-01-12 23:52:52 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-12 23:52:52 | INFO | train | epoch 007 | loss 14.362 | ppl 21061.2 | wps 6958.9 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 348 | lr 1.044e-06 | gnorm 18.136 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 67
2022-01-12 23:52:52 | INFO | fairseq.trainer | begin training epoch 8
2022-01-12 23:52:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:53 | INFO | train_inner | epoch 008:     12 / 50 loss=13.982, ppl=16177, wps=4133.3, ups=2.95, wpb=1399.5, bsz=64, num_updates=360, lr=1.08e-06, gnorm=18.713, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=68
2022-01-12 23:52:54 | INFO | train_inner | epoch 008:     32 / 50 loss=13.673, ppl=13065.5, wps=17796, ups=14.28, wpb=1246.2, bsz=64, num_updates=380, lr=1.14e-06, gnorm=16.927, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=69
2022-01-12 23:52:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 13.325 | ppl 10265.3 | wps 26425.9 | wpb 556.6 | bsz 30.3 | num_updates 398 | best_loss 13.325
2022-01-12 23:52:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 398 updates
2022-01-12 23:52:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 398 updates, score 13.325) (writing took 4.099818815011531 seconds)
2022-01-12 23:53:00 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-12 23:53:00 | INFO | train | epoch 008 | loss 13.671 | ppl 13041.4 | wps 7217.3 | ups 5.85 | wpb 1232.9 | bsz 63.5 | num_updates 398 | lr 1.194e-06 | gnorm 17.703 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 76
2022-01-12 23:53:00 | INFO | fairseq.trainer | begin training epoch 9
2022-01-12 23:53:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:01 | INFO | train_inner | epoch 009:      2 / 50 loss=13.445, ppl=11152.4, wps=3693.6, ups=3.12, wpb=1185.2, bsz=62.7, num_updates=400, lr=1.2e-06, gnorm=16.722, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=76
2022-01-12 23:53:02 | INFO | train_inner | epoch 009:     22 / 50 loss=13.342, ppl=10380.7, wps=16812.5, ups=13.58, wpb=1238, bsz=62.7, num_updates=420, lr=1.26e-06, gnorm=17.348, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=77
2022-01-12 23:53:04 | INFO | train_inner | epoch 009:     42 / 50 loss=13.019, ppl=8299.91, wps=16697.7, ups=13.69, wpb=1220, bsz=64, num_updates=440, lr=1.32e-06, gnorm=14.824, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=79
2022-01-12 23:53:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 12.838 | ppl 7324.38 | wps 26997 | wpb 556.6 | bsz 30.3 | num_updates 448 | best_loss 12.838
2022-01-12 23:53:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 448 updates
2022-01-12 23:53:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 448 updates, score 12.838) (writing took 4.447679174831137 seconds)
2022-01-12 23:53:09 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-12 23:53:09 | INFO | train | epoch 009 | loss 13.134 | ppl 8992.19 | wps 6858.8 | ups 5.56 | wpb 1232.9 | bsz 63.5 | num_updates 448 | lr 1.344e-06 | gnorm 15.786 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 85
2022-01-12 23:53:09 | INFO | fairseq.trainer | begin training epoch 10
2022-01-12 23:53:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:10 | INFO | train_inner | epoch 010:     12 / 50 loss=12.786, ppl=7061.32, wps=3611, ups=2.97, wpb=1217.8, bsz=62.7, num_updates=460, lr=1.38e-06, gnorm=14.703, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=85
2022-01-12 23:53:12 | INFO | train_inner | epoch 010:     32 / 50 loss=12.692, ppl=6618.46, wps=17043.9, ups=13.82, wpb=1232.8, bsz=64, num_updates=480, lr=1.44e-06, gnorm=14.378, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=87
2022-01-12 23:53:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:14 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 12.463 | ppl 5644.55 | wps 27276.5 | wpb 556.6 | bsz 30.3 | num_updates 498 | best_loss 12.463
2022-01-12 23:53:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 498 updates
2022-01-12 23:53:14 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 498 updates, score 12.463) (writing took 4.289617713075131 seconds)
2022-01-12 23:53:18 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-12 23:53:18 | INFO | train | epoch 010 | loss 12.642 | ppl 6391.42 | wps 6987.8 | ups 5.67 | wpb 1232.9 | bsz 63.5 | num_updates 498 | lr 1.494e-06 | gnorm 14.015 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 93
2022-01-12 23:53:18 | INFO | fairseq.trainer | begin training epoch 11
2022-01-12 23:53:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:19 | INFO | train_inner | epoch 011:      2 / 50 loss=12.533, ppl=5928.39, wps=3577.6, ups=2.99, wpb=1197, bsz=64, num_updates=500, lr=1.5e-06, gnorm=13.101, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=94
2022-01-12 23:53:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:19 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.415 | ppl 5460.19 | wps 26942 | wpb 556.6 | bsz 30.3 | num_updates 500 | best_loss 12.415
2022-01-12 23:53:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 500 updates
2022-01-12 23:53:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt
2022-01-12 23:53:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt
2022-01-12 23:53:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt (epoch 11 @ 500 updates, score 12.415) (writing took 7.498780622147024 seconds)
2022-01-12 23:53:28 | INFO | train_inner | epoch 011:     22 / 50 loss=12.392, ppl=5374.44, wps=2657, ups=2.03, wpb=1310, bsz=64, num_updates=520, lr=1.56e-06, gnorm=12.485, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=104
2022-01-12 23:53:30 | INFO | train_inner | epoch 011:     42 / 50 loss=12.215, ppl=4755.5, wps=15773.4, ups=13.26, wpb=1189.1, bsz=64, num_updates=540, lr=1.62e-06, gnorm=13.026, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=105
2022-01-12 23:53:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:31 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.077 | ppl 4321.61 | wps 30497.9 | wpb 556.6 | bsz 30.3 | num_updates 548 | best_loss 12.077
2022-01-12 23:53:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 548 updates
2022-01-12 23:53:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 548 updates, score 12.077) (writing took 4.4391100709326565 seconds)
2022-01-12 23:53:36 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-12 23:53:36 | INFO | train | epoch 011 | loss 12.282 | ppl 4980.5 | wps 3551.4 | ups 2.88 | wpb 1232.9 | bsz 63.5 | num_updates 548 | lr 1.644e-06 | gnorm 12.803 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 111
2022-01-12 23:53:36 | INFO | fairseq.trainer | begin training epoch 12
2022-01-12 23:53:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:37 | INFO | train_inner | epoch 012:     12 / 50 loss=12.166, ppl=4596.88, wps=3448.7, ups=2.84, wpb=1214.3, bsz=61.4, num_updates=560, lr=1.68e-06, gnorm=12.663, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=112
2022-01-12 23:53:38 | INFO | train_inner | epoch 012:     32 / 50 loss=12.011, ppl=4127.61, wps=16454.9, ups=13.47, wpb=1221.2, bsz=64, num_updates=580, lr=1.74e-06, gnorm=12.837, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=114
2022-01-12 23:53:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:40 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.796 | ppl 3555.78 | wps 28198.2 | wpb 556.6 | bsz 30.3 | num_updates 598 | best_loss 11.796
2022-01-12 23:53:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 598 updates
2022-01-12 23:53:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 598 updates, score 11.796) (writing took 4.454071433050558 seconds)
2022-01-12 23:53:45 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-12 23:53:45 | INFO | train | epoch 012 | loss 12.036 | ppl 4199.48 | wps 6988 | ups 5.67 | wpb 1232.9 | bsz 63.5 | num_updates 598 | lr 1.794e-06 | gnorm 12.537 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 120
2022-01-12 23:53:45 | INFO | fairseq.trainer | begin training epoch 13
2022-01-12 23:53:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:45 | INFO | train_inner | epoch 013:      2 / 50 loss=11.921, ppl=3878.97, wps=3726.4, ups=3.02, wpb=1232.8, bsz=64, num_updates=600, lr=1.8e-06, gnorm=12.194, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=120
2022-01-12 23:53:46 | INFO | train_inner | epoch 013:     22 / 50 loss=11.721, ppl=3376.6, wps=16894.1, ups=13.92, wpb=1213.5, bsz=62.7, num_updates=620, lr=1.86e-06, gnorm=11.924, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=122
2022-01-12 23:53:48 | INFO | train_inner | epoch 013:     42 / 50 loss=11.651, ppl=3215.5, wps=17250.3, ups=13.97, wpb=1234.5, bsz=64, num_updates=640, lr=1.92e-06, gnorm=11.082, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=123
2022-01-12 23:53:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:49 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.555 | ppl 3009.11 | wps 26811.2 | wpb 556.6 | bsz 30.3 | num_updates 648 | best_loss 11.555
2022-01-12 23:53:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 648 updates
2022-01-12 23:53:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 648 updates, score 11.555) (writing took 4.713082507951185 seconds)
2022-01-12 23:53:54 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-12 23:53:54 | INFO | train | epoch 013 | loss 11.741 | ppl 3423.89 | wps 6679 | ups 5.42 | wpb 1232.9 | bsz 63.5 | num_updates 648 | lr 1.944e-06 | gnorm 11.452 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 129
2022-01-12 23:53:54 | INFO | fairseq.trainer | begin training epoch 14
2022-01-12 23:53:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:55 | INFO | train_inner | epoch 014:     12 / 50 loss=11.813, ppl=3596.98, wps=3568.5, ups=2.83, wpb=1259.2, bsz=64, num_updates=660, lr=1.98e-06, gnorm=11.379, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=130
2022-01-12 23:53:56 | INFO | train_inner | epoch 014:     32 / 50 loss=11.503, ppl=2902.06, wps=18879.4, ups=14.21, wpb=1329, bsz=62.7, num_updates=680, lr=2.04e-06, gnorm=10.932, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=132
2022-01-12 23:53:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:58 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.387 | ppl 2678 | wps 25868 | wpb 556.6 | bsz 30.3 | num_updates 698 | best_loss 11.387
2022-01-12 23:53:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 698 updates
2022-01-12 23:53:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 698 updates, score 11.387) (writing took 4.417004777817056 seconds)
2022-01-12 23:54:03 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-12 23:54:03 | INFO | train | epoch 014 | loss 11.52 | ppl 2937.44 | wps 6941.2 | ups 5.63 | wpb 1232.9 | bsz 63.5 | num_updates 698 | lr 2.094e-06 | gnorm 11.088 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 138
2022-01-12 23:54:03 | INFO | fairseq.trainer | begin training epoch 15
2022-01-12 23:54:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:03 | INFO | train_inner | epoch 015:      2 / 50 loss=11.432, ppl=2762.96, wps=3347.2, ups=2.95, wpb=1136.4, bsz=64, num_updates=700, lr=2.1e-06, gnorm=10.866, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=138
2022-01-12 23:54:05 | INFO | train_inner | epoch 015:     22 / 50 loss=11.372, ppl=2650.52, wps=17312.9, ups=14.56, wpb=1188.8, bsz=64, num_updates=720, lr=2.16e-06, gnorm=11.018, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=140
2022-01-12 23:54:06 | INFO | train_inner | epoch 015:     42 / 50 loss=11.221, ppl=2387.45, wps=19193.5, ups=14.02, wpb=1368.7, bsz=64, num_updates=740, lr=2.22e-06, gnorm=10.342, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=141
2022-01-12 23:54:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:07 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.136 | ppl 2250.2 | wps 29516.2 | wpb 556.6 | bsz 30.3 | num_updates 748 | best_loss 11.136
2022-01-12 23:54:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 748 updates
2022-01-12 23:54:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 748 updates, score 11.136) (writing took 4.124612756073475 seconds)
2022-01-12 23:54:11 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-12 23:54:11 | INFO | train | epoch 015 | loss 11.292 | ppl 2507.03 | wps 7200.7 | ups 5.84 | wpb 1232.9 | bsz 63.5 | num_updates 748 | lr 2.244e-06 | gnorm 10.679 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 147
2022-01-12 23:54:12 | INFO | fairseq.trainer | begin training epoch 16
2022-01-12 23:54:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:12 | INFO | train_inner | epoch 016:     12 / 50 loss=11.361, ppl=2629.88, wps=3515.4, ups=3.09, wpb=1138.8, bsz=62.7, num_updates=760, lr=2.28e-06, gnorm=10.219, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=148
2022-01-12 23:54:14 | INFO | train_inner | epoch 016:     32 / 50 loss=11.075, ppl=2156.92, wps=15435.3, ups=13.67, wpb=1129.2, bsz=64, num_updates=780, lr=2.34e-06, gnorm=11.173, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=149
2022-01-12 23:54:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:16 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.99 | ppl 2033.53 | wps 26551.2 | wpb 556.6 | bsz 30.3 | num_updates 798 | best_loss 10.99
2022-01-12 23:54:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 798 updates
2022-01-12 23:54:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 798 updates, score 10.99) (writing took 4.637598752975464 seconds)
2022-01-12 23:54:21 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-12 23:54:21 | INFO | train | epoch 016 | loss 11.192 | ppl 2340.24 | wps 6766.1 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 798 | lr 2.394e-06 | gnorm 10.345 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 156
2022-01-12 23:54:21 | INFO | fairseq.trainer | begin training epoch 17
2022-01-12 23:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:21 | INFO | train_inner | epoch 017:      2 / 50 loss=11.151, ppl=2273.34, wps=3784.2, ups=2.88, wpb=1312.2, bsz=62.7, num_updates=800, lr=2.4e-06, gnorm=9.816, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=156
2022-01-12 23:54:22 | INFO | train_inner | epoch 017:     22 / 50 loss=10.972, ppl=2008.1, wps=15993, ups=13.78, wpb=1160.7, bsz=62.7, num_updates=820, lr=2.46e-06, gnorm=10.458, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=157
2022-01-12 23:54:24 | INFO | train_inner | epoch 017:     42 / 50 loss=11.179, ppl=2318.11, wps=19648.3, ups=13.37, wpb=1469.3, bsz=64, num_updates=840, lr=2.52e-06, gnorm=9.358, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=159
2022-01-12 23:54:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:25 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.807 | ppl 1791.87 | wps 25449.6 | wpb 556.6 | bsz 30.3 | num_updates 848 | best_loss 10.807
2022-01-12 23:54:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 848 updates
2022-01-12 23:54:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 848 updates, score 10.807) (writing took 4.269093121169135 seconds)
2022-01-12 23:54:29 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-12 23:54:29 | INFO | train | epoch 017 | loss 11.03 | ppl 2090.71 | wps 6977.1 | ups 5.66 | wpb 1232.9 | bsz 63.5 | num_updates 848 | lr 2.544e-06 | gnorm 9.918 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 165
2022-01-12 23:54:29 | INFO | fairseq.trainer | begin training epoch 18
2022-01-12 23:54:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:30 | INFO | train_inner | epoch 018:     12 / 50 loss=10.843, ppl=1836.91, wps=3308.3, ups=3.04, wpb=1087.5, bsz=64, num_updates=860, lr=2.58e-06, gnorm=9.365, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=166
2022-01-12 23:54:32 | INFO | train_inner | epoch 018:     32 / 50 loss=10.828, ppl=1818.15, wps=17520.8, ups=13.82, wpb=1268, bsz=62.7, num_updates=880, lr=2.64e-06, gnorm=10.213, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=167
2022-01-12 23:54:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:34 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.597 | ppl 1548.95 | wps 26539 | wpb 556.6 | bsz 30.3 | num_updates 898 | best_loss 10.597
2022-01-12 23:54:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 898 updates
2022-01-12 23:54:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 898 updates, score 10.597) (writing took 4.445276870159432 seconds)
2022-01-12 23:54:39 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-12 23:54:39 | INFO | train | epoch 018 | loss 10.846 | ppl 1840.12 | wps 6763.6 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 898 | lr 2.694e-06 | gnorm 9.592 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 174
2022-01-12 23:54:39 | INFO | fairseq.trainer | begin training epoch 19
2022-01-12 23:54:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:39 | INFO | train_inner | epoch 019:      2 / 50 loss=10.821, ppl=1808.99, wps=3363.1, ups=2.89, wpb=1164.9, bsz=64, num_updates=900, lr=2.7e-06, gnorm=9.536, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=174
2022-01-12 23:54:40 | INFO | train_inner | epoch 019:     22 / 50 loss=10.713, ppl=1678.08, wps=16347.1, ups=13.48, wpb=1212.3, bsz=62.7, num_updates=920, lr=2.76e-06, gnorm=9.34, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=175
2022-01-12 23:54:42 | INFO | train_inner | epoch 019:     42 / 50 loss=10.76, ppl=1734.71, wps=15970.7, ups=12.28, wpb=1300.9, bsz=64, num_updates=940, lr=2.82e-06, gnorm=9.489, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=177
2022-01-12 23:54:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:43 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 10.489 | ppl 1437.09 | wps 26452.6 | wpb 556.6 | bsz 30.3 | num_updates 948 | best_loss 10.489
2022-01-12 23:54:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 948 updates
2022-01-12 23:54:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 948 updates, score 10.489) (writing took 4.3387409390416 seconds)
2022-01-12 23:54:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-12 23:54:48 | INFO | train | epoch 019 | loss 10.694 | ppl 1657.03 | wps 6778.2 | ups 5.5 | wpb 1232.9 | bsz 63.5 | num_updates 948 | lr 2.844e-06 | gnorm 9.546 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 183
2022-01-12 23:54:48 | INFO | fairseq.trainer | begin training epoch 20
2022-01-12 23:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:49 | INFO | train_inner | epoch 020:     12 / 50 loss=10.489, ppl=1437.05, wps=3413.7, ups=2.98, wpb=1145.7, bsz=64, num_updates=960, lr=2.88e-06, gnorm=10.143, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=184
2022-01-12 23:54:50 | INFO | train_inner | epoch 020:     32 / 50 loss=10.624, ppl=1578.44, wps=17115, ups=13.8, wpb=1240, bsz=62.7, num_updates=980, lr=2.94e-06, gnorm=9.196, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=185
2022-01-12 23:54:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:52 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 10.395 | ppl 1346.34 | wps 27010.2 | wpb 556.6 | bsz 30.3 | num_updates 998 | best_loss 10.395
2022-01-12 23:54:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 998 updates
2022-01-12 23:54:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 998 updates, score 10.395) (writing took 4.420613712863997 seconds)
2022-01-12 23:54:57 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-12 23:54:57 | INFO | train | epoch 020 | loss 10.579 | ppl 1529.35 | wps 6888.6 | ups 5.59 | wpb 1232.9 | bsz 63.5 | num_updates 998 | lr 2.994e-06 | gnorm 9.368 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 192
2022-01-12 23:54:57 | INFO | fairseq.trainer | begin training epoch 21
2022-01-12 23:54:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:57 | INFO | train_inner | epoch 021:      2 / 50 loss=10.563, ppl=1513.22, wps=3660.2, ups=2.87, wpb=1273.9, bsz=64, num_updates=1000, lr=3e-06, gnorm=9.044, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=192
2022-01-12 23:54:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:58 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.327 | ppl 1284.77 | wps 26793.8 | wpb 556.6 | bsz 30.3 | num_updates 1000 | best_loss 10.327
2022-01-12 23:54:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1000 updates
2022-01-12 23:54:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt
2022-01-12 23:55:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt
2022-01-12 23:55:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt (epoch 21 @ 1000 updates, score 10.327) (writing took 6.069689427968115 seconds)
2022-01-12 23:55:06 | INFO | train_inner | epoch 021:     22 / 50 loss=10.557, ppl=1506.07, wps=3021.3, ups=2.32, wpb=1300.8, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=9.176, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=201
2022-01-12 23:55:07 | INFO | train_inner | epoch 021:     42 / 50 loss=10.38, ppl=1332.46, wps=16585.7, ups=13.6, wpb=1219.5, bsz=64, num_updates=1040, lr=3.12e-06, gnorm=8.935, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=202
2022-01-12 23:55:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:08 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.192 | ppl 1169.41 | wps 27626.5 | wpb 556.6 | bsz 30.3 | num_updates 1048 | best_loss 10.192
2022-01-12 23:55:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1048 updates
2022-01-12 23:55:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 1048 updates, score 10.192) (writing took 4.609479784034193 seconds)
2022-01-12 23:55:13 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-12 23:55:15 | INFO | train | epoch 021 | loss 10.483 | ppl 1431.35 | wps 3800.6 | ups 3.08 | wpb 1232.9 | bsz 63.5 | num_updates 1048 | lr 3.144e-06 | gnorm 9.147 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 208
2022-01-12 23:55:15 | INFO | fairseq.trainer | begin training epoch 22
2022-01-12 23:55:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:16 | INFO | train_inner | epoch 022:     12 / 50 loss=10.365, ppl=1318.47, wps=2851.5, ups=2.39, wpb=1195.2, bsz=62.7, num_updates=1060, lr=3.18e-06, gnorm=9.221, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=211
2022-01-12 23:55:17 | INFO | train_inner | epoch 022:     32 / 50 loss=10.429, ppl=1378.86, wps=18621.7, ups=14.71, wpb=1265.5, bsz=64, num_updates=1080, lr=3.24e-06, gnorm=8.68, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=212
2022-01-12 23:55:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:19 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 10.095 | ppl 1093.8 | wps 26662.4 | wpb 556.6 | bsz 30.3 | num_updates 1098 | best_loss 10.095
2022-01-12 23:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1098 updates
2022-01-12 23:55:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 1098 updates, score 10.095) (writing took 4.358964095823467 seconds)
2022-01-12 23:55:23 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-12 23:55:23 | INFO | train | epoch 022 | loss 10.341 | ppl 1297.09 | wps 7068.2 | ups 5.73 | wpb 1232.9 | bsz 63.5 | num_updates 1098 | lr 3.294e-06 | gnorm 9.153 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 218
2022-01-12 23:55:23 | INFO | fairseq.trainer | begin training epoch 23
2022-01-12 23:55:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:24 | INFO | train_inner | epoch 023:      2 / 50 loss=10.307, ppl=1266.61, wps=3506.3, ups=3.01, wpb=1164.5, bsz=62.7, num_updates=1100, lr=3.3e-06, gnorm=9.695, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=219
2022-01-12 23:55:25 | INFO | train_inner | epoch 023:     22 / 50 loss=10.193, ppl=1170.63, wps=16977, ups=13.89, wpb=1222.5, bsz=64, num_updates=1120, lr=3.36e-06, gnorm=8.713, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=220
2022-01-12 23:55:26 | INFO | train_inner | epoch 023:     42 / 50 loss=10.203, ppl=1178.85, wps=20223.5, ups=14.75, wpb=1370.7, bsz=64, num_updates=1140, lr=3.42e-06, gnorm=8.855, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=221
2022-01-12 23:55:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:28 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 10.011 | ppl 1031.65 | wps 26137.9 | wpb 556.6 | bsz 30.3 | num_updates 1148 | best_loss 10.011
2022-01-12 23:55:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1148 updates
2022-01-12 23:55:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 1148 updates, score 10.011) (writing took 4.447407083818689 seconds)
2022-01-12 23:55:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-12 23:55:32 | INFO | train | epoch 023 | loss 10.189 | ppl 1167.45 | wps 7030 | ups 5.7 | wpb 1232.9 | bsz 63.5 | num_updates 1148 | lr 3.444e-06 | gnorm 9.034 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 227
2022-01-12 23:55:32 | INFO | fairseq.trainer | begin training epoch 24
2022-01-12 23:55:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:33 | INFO | train_inner | epoch 024:     12 / 50 loss=10.111, ppl=1105.92, wps=2916.6, ups=2.93, wpb=994, bsz=62.7, num_updates=1160, lr=3.48e-06, gnorm=9.566, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=228
2022-01-12 23:55:35 | INFO | train_inner | epoch 024:     32 / 50 loss=10.131, ppl=1121.41, wps=17811.8, ups=12.5, wpb=1425.5, bsz=62.7, num_updates=1180, lr=3.54e-06, gnorm=8.593, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=230
2022-01-12 23:55:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:37 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.868 | ppl 934.23 | wps 26896.4 | wpb 556.6 | bsz 30.3 | num_updates 1198 | best_loss 9.868
2022-01-12 23:55:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1198 updates
2022-01-12 23:55:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 1198 updates, score 9.868) (writing took 4.69009534898214 seconds)
2022-01-12 23:55:42 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-12 23:55:42 | INFO | train | epoch 024 | loss 10.057 | ppl 1065.31 | wps 6488.8 | ups 5.26 | wpb 1232.9 | bsz 63.5 | num_updates 1198 | lr 3.594e-06 | gnorm 8.873 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 237
2022-01-12 23:55:42 | INFO | fairseq.trainer | begin training epoch 25
2022-01-12 23:55:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:42 | INFO | train_inner | epoch 025:      2 / 50 loss=9.954, ppl=991.84, wps=3271.9, ups=2.81, wpb=1164.8, bsz=64, num_updates=1200, lr=3.6e-06, gnorm=8.989, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=237
2022-01-12 23:55:43 | INFO | train_inner | epoch 025:     22 / 50 loss=9.978, ppl=1008.7, wps=18366.2, ups=13.79, wpb=1331.4, bsz=62.7, num_updates=1220, lr=3.66e-06, gnorm=9.158, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=238
2022-01-12 23:55:45 | INFO | train_inner | epoch 025:     42 / 50 loss=9.923, ppl=970.62, wps=16170.2, ups=13.64, wpb=1185.5, bsz=64, num_updates=1240, lr=3.72e-06, gnorm=8.794, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=240
2022-01-12 23:55:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:46 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.696 | ppl 829.18 | wps 26867.5 | wpb 556.6 | bsz 30.3 | num_updates 1248 | best_loss 9.696
2022-01-12 23:55:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1248 updates
2022-01-12 23:55:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 1248 updates, score 9.696) (writing took 4.413138160016388 seconds)
2022-01-12 23:55:51 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-12 23:55:51 | INFO | train | epoch 025 | loss 9.919 | ppl 968.37 | wps 6820.8 | ups 5.53 | wpb 1232.9 | bsz 63.5 | num_updates 1248 | lr 3.744e-06 | gnorm 8.967 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 246
2022-01-12 23:55:51 | INFO | fairseq.trainer | begin training epoch 26
2022-01-12 23:55:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:52 | INFO | train_inner | epoch 026:     12 / 50 loss=9.809, ppl=896.91, wps=3597, ups=2.92, wpb=1231.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=9.137, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=247
2022-01-12 23:55:53 | INFO | train_inner | epoch 026:     32 / 50 loss=9.827, ppl=908.3, wps=14846.7, ups=12.64, wpb=1174.3, bsz=64, num_updates=1280, lr=3.84e-06, gnorm=8.768, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=248
2022-01-12 23:55:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:55 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.581 | ppl 765.72 | wps 26388.5 | wpb 556.6 | bsz 30.3 | num_updates 1298 | best_loss 9.581
2022-01-12 23:55:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1298 updates
2022-01-12 23:55:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 1298 updates, score 9.581) (writing took 4.977432888932526 seconds)
2022-01-12 23:56:00 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-12 23:56:00 | INFO | train | epoch 026 | loss 9.811 | ppl 898.38 | wps 6337.7 | ups 5.14 | wpb 1232.9 | bsz 63.5 | num_updates 1298 | lr 3.894e-06 | gnorm 8.908 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 256
2022-01-12 23:56:00 | INFO | fairseq.trainer | begin training epoch 27
2022-01-12 23:56:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:01 | INFO | train_inner | epoch 027:      2 / 50 loss=9.732, ppl=850.61, wps=3375.5, ups=2.69, wpb=1252.9, bsz=64, num_updates=1300, lr=3.9e-06, gnorm=8.941, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=256
2022-01-12 23:56:02 | INFO | train_inner | epoch 027:     22 / 50 loss=9.839, ppl=916, wps=16485.7, ups=13.25, wpb=1244.4, bsz=64, num_updates=1320, lr=3.96e-06, gnorm=8.61, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=257
2022-01-12 23:56:04 | INFO | train_inner | epoch 027:     42 / 50 loss=9.762, ppl=868.56, wps=16674.8, ups=13.12, wpb=1271.2, bsz=62.7, num_updates=1340, lr=4.02e-06, gnorm=9.589, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=259
2022-01-12 23:56:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:05 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.511 | ppl 729.67 | wps 24503.1 | wpb 556.6 | bsz 30.3 | num_updates 1348 | best_loss 9.511
2022-01-12 23:56:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1348 updates
2022-01-12 23:56:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 1348 updates, score 9.511) (writing took 4.767022804124281 seconds)
2022-01-12 23:56:10 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-12 23:56:10 | INFO | train | epoch 027 | loss 9.714 | ppl 840.1 | wps 6437.5 | ups 5.22 | wpb 1232.9 | bsz 63.5 | num_updates 1348 | lr 4.044e-06 | gnorm 9.17 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 265
2022-01-12 23:56:10 | INFO | fairseq.trainer | begin training epoch 28
2022-01-12 23:56:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:11 | INFO | train_inner | epoch 028:     12 / 50 loss=9.468, ppl=707.99, wps=3060.6, ups=2.75, wpb=1113.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=9.25, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=266
2022-01-12 23:56:13 | INFO | train_inner | epoch 028:     32 / 50 loss=9.61, ppl=781.23, wps=13882.5, ups=11.36, wpb=1222.3, bsz=64, num_updates=1380, lr=4.14e-06, gnorm=8.775, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=268
2022-01-12 23:56:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:15 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.386 | ppl 668.91 | wps 25395.5 | wpb 556.6 | bsz 30.3 | num_updates 1398 | best_loss 9.386
2022-01-12 23:56:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1398 updates
2022-01-12 23:56:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 1398 updates, score 9.386) (writing took 4.761819014092907 seconds)
2022-01-12 23:56:20 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-12 23:56:20 | INFO | train | epoch 028 | loss 9.54 | ppl 744.64 | wps 6158.4 | ups 5 | wpb 1232.9 | bsz 63.5 | num_updates 1398 | lr 4.194e-06 | gnorm 9.601 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 275
2022-01-12 23:56:20 | INFO | fairseq.trainer | begin training epoch 29
2022-01-12 23:56:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:20 | INFO | train_inner | epoch 029:      2 / 50 loss=9.477, ppl=712.75, wps=3633.5, ups=2.66, wpb=1367.7, bsz=64, num_updates=1400, lr=4.2e-06, gnorm=10.426, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=275
2022-01-12 23:56:22 | INFO | train_inner | epoch 029:     22 / 50 loss=9.416, ppl=682.92, wps=14915.4, ups=12.79, wpb=1166, bsz=64, num_updates=1420, lr=4.26e-06, gnorm=8.797, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=277
2022-01-12 23:56:23 | INFO | train_inner | epoch 029:     42 / 50 loss=9.494, ppl=720.93, wps=17046.6, ups=13.03, wpb=1308.7, bsz=62.7, num_updates=1440, lr=4.32e-06, gnorm=9.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=279
2022-01-12 23:56:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:25 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.331 | ppl 643.99 | wps 25605.2 | wpb 556.6 | bsz 30.3 | num_updates 1448 | best_loss 9.331
2022-01-12 23:56:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1448 updates
2022-01-12 23:56:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 1448 updates, score 9.331) (writing took 4.545991942053661 seconds)
2022-01-12 23:56:29 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-12 23:56:29 | INFO | train | epoch 029 | loss 9.453 | ppl 700.95 | wps 6588.5 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 1448 | lr 4.344e-06 | gnorm 9.121 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 284
2022-01-12 23:56:29 | INFO | fairseq.trainer | begin training epoch 30
2022-01-12 23:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:30 | INFO | train_inner | epoch 030:     12 / 50 loss=9.476, ppl=712.32, wps=3644.8, ups=2.87, wpb=1269.3, bsz=64, num_updates=1460, lr=4.38e-06, gnorm=9.229, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=285
2022-01-12 23:56:32 | INFO | train_inner | epoch 030:     32 / 50 loss=9.288, ppl=625.25, wps=15289.5, ups=13.05, wpb=1171.5, bsz=64, num_updates=1480, lr=4.44e-06, gnorm=9.017, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=287
2022-01-12 23:56:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:34 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.253 | ppl 610.25 | wps 28299.3 | wpb 556.6 | bsz 30.3 | num_updates 1498 | best_loss 9.253
2022-01-12 23:56:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1498 updates
2022-01-12 23:56:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 30 @ 1498 updates, score 9.253) (writing took 4.596516771009192 seconds)
2022-01-12 23:56:39 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-12 23:56:39 | INFO | train | epoch 030 | loss 9.33 | ppl 643.78 | wps 6721.4 | ups 5.45 | wpb 1232.9 | bsz 63.5 | num_updates 1498 | lr 4.494e-06 | gnorm 8.929 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 294
2022-01-12 23:56:39 | INFO | fairseq.trainer | begin training epoch 31
2022-01-12 23:56:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:39 | INFO | train_inner | epoch 031:      2 / 50 loss=9.202, ppl=588.98, wps=3378.8, ups=2.84, wpb=1189.5, bsz=62.7, num_updates=1500, lr=4.5e-06, gnorm=9.095, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=294
2022-01-12 23:56:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:40 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.232 | ppl 601.18 | wps 23535 | wpb 556.6 | bsz 30.3 | num_updates 1500 | best_loss 9.232
2022-01-12 23:56:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1500 updates
2022-01-12 23:56:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt
2022-01-12 23:56:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt
2022-01-12 23:56:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt (epoch 31 @ 1500 updates, score 9.232) (writing took 9.790959219215438 seconds)
2022-01-12 23:56:51 | INFO | train_inner | epoch 031:     22 / 50 loss=9.321, ppl=639.69, wps=2117.9, ups=1.6, wpb=1321.7, bsz=62.7, num_updates=1520, lr=4.56e-06, gnorm=9.707, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=307
2022-01-12 23:56:53 | INFO | train_inner | epoch 031:     42 / 50 loss=9.174, ppl=577.5, wps=14784.7, ups=12.44, wpb=1188.6, bsz=64, num_updates=1540, lr=4.62e-06, gnorm=9.061, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=308
2022-01-12 23:56:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:54 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.117 | ppl 555.3 | wps 27764.4 | wpb 556.6 | bsz 30.3 | num_updates 1548 | best_loss 9.117
2022-01-12 23:56:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1548 updates
2022-01-12 23:56:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 1548 updates, score 9.117) (writing took 4.195408324943855 seconds)
2022-01-12 23:56:59 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-12 23:56:59 | INFO | train | epoch 031 | loss 9.198 | ppl 587.19 | wps 3091.7 | ups 2.51 | wpb 1232.9 | bsz 63.5 | num_updates 1548 | lr 4.644e-06 | gnorm 9.24 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 314
2022-01-12 23:56:59 | INFO | fairseq.trainer | begin training epoch 32
2022-01-12 23:56:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:00 | INFO | train_inner | epoch 032:     12 / 50 loss=9.169, ppl=575.58, wps=3594.8, ups=3.06, wpb=1174, bsz=64, num_updates=1560, lr=4.68e-06, gnorm=8.772, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=315
2022-01-12 23:57:01 | INFO | train_inner | epoch 032:     32 / 50 loss=9.072, ppl=538.37, wps=18087.3, ups=14.18, wpb=1275.7, bsz=64, num_updates=1580, lr=4.74e-06, gnorm=8.731, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=316
2022-01-12 23:57:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:03 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.027 | ppl 521.78 | wps 28324.7 | wpb 556.6 | bsz 30.3 | num_updates 1598 | best_loss 9.027
2022-01-12 23:57:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1598 updates
2022-01-12 23:57:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 32 @ 1598 updates, score 9.027) (writing took 4.552312697982416 seconds)
2022-01-12 23:57:07 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-12 23:57:07 | INFO | train | epoch 032 | loss 9.134 | ppl 562.01 | wps 6984.5 | ups 5.67 | wpb 1232.9 | bsz 63.5 | num_updates 1598 | lr 4.794e-06 | gnorm 8.798 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 323
2022-01-12 23:57:07 | INFO | fairseq.trainer | begin training epoch 33
2022-01-12 23:57:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:08 | INFO | train_inner | epoch 033:      2 / 50 loss=9.083, ppl=542.28, wps=3466.3, ups=2.97, wpb=1168.8, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=8.898, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=323
2022-01-12 23:57:09 | INFO | train_inner | epoch 033:     22 / 50 loss=9.16, ppl=572.2, wps=17745, ups=14.51, wpb=1223.3, bsz=62.7, num_updates=1620, lr=4.86e-06, gnorm=8.639, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=324
2022-01-12 23:57:11 | INFO | train_inner | epoch 033:     42 / 50 loss=8.97, ppl=501.37, wps=18239.8, ups=14.09, wpb=1294.6, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=8.891, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=326
2022-01-12 23:57:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:12 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.96 | ppl 498.03 | wps 27182 | wpb 556.6 | bsz 30.3 | num_updates 1648 | best_loss 8.96
2022-01-12 23:57:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1648 updates
2022-01-12 23:57:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1648 updates, score 8.96) (writing took 4.40379764395766 seconds)
2022-01-12 23:57:16 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-12 23:57:16 | INFO | train | epoch 033 | loss 9.046 | ppl 528.63 | wps 6999.9 | ups 5.68 | wpb 1232.9 | bsz 63.5 | num_updates 1648 | lr 4.944e-06 | gnorm 8.853 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 331
2022-01-12 23:57:16 | INFO | fairseq.trainer | begin training epoch 34
2022-01-12 23:57:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:17 | INFO | train_inner | epoch 034:     12 / 50 loss=8.992, ppl=509.02, wps=3243.3, ups=2.95, wpb=1098.5, bsz=64, num_updates=1660, lr=4.98e-06, gnorm=9.15, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=332
2022-01-12 23:57:19 | INFO | train_inner | epoch 034:     32 / 50 loss=8.89, ppl=474.37, wps=16938.6, ups=14.18, wpb=1194.3, bsz=64, num_updates=1680, lr=5.04e-06, gnorm=8.716, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=334
2022-01-12 23:57:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:21 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.847 | ppl 460.56 | wps 27702.4 | wpb 556.6 | bsz 30.3 | num_updates 1698 | best_loss 8.847
2022-01-12 23:57:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1698 updates
2022-01-12 23:57:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1698 updates, score 8.847) (writing took 4.3369038458913565 seconds)
2022-01-12 23:57:25 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-12 23:57:25 | INFO | train | epoch 034 | loss 8.964 | ppl 499.34 | wps 7080.6 | ups 5.74 | wpb 1232.9 | bsz 63.5 | num_updates 1698 | lr 5.094e-06 | gnorm 8.822 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 340
2022-01-12 23:57:25 | INFO | fairseq.trainer | begin training epoch 35
2022-01-12 23:57:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:25 | INFO | train_inner | epoch 035:      2 / 50 loss=9.058, ppl=532.94, wps=4522.8, ups=3.06, wpb=1478.3, bsz=62.7, num_updates=1700, lr=5.1e-06, gnorm=8.623, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=340
2022-01-12 23:57:27 | INFO | train_inner | epoch 035:     22 / 50 loss=8.871, ppl=468.19, wps=16268.2, ups=14.23, wpb=1143.2, bsz=64, num_updates=1720, lr=5.16e-06, gnorm=8.842, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=342
2022-01-12 23:57:28 | INFO | train_inner | epoch 035:     42 / 50 loss=8.779, ppl=439.19, wps=18255.3, ups=13.68, wpb=1334.7, bsz=62.7, num_updates=1740, lr=5.22e-06, gnorm=8.948, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=343
2022-01-12 23:57:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:29 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.728 | ppl 424.15 | wps 28176.1 | wpb 556.6 | bsz 30.3 | num_updates 1748 | best_loss 8.728
2022-01-12 23:57:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1748 updates
2022-01-12 23:57:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1748 updates, score 8.728) (writing took 4.49892555992119 seconds)
2022-01-12 23:57:34 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-12 23:57:34 | INFO | train | epoch 035 | loss 8.816 | ppl 450.69 | wps 6926.2 | ups 5.62 | wpb 1232.9 | bsz 63.5 | num_updates 1748 | lr 5.244e-06 | gnorm 8.869 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 349
2022-01-12 23:57:34 | INFO | fairseq.trainer | begin training epoch 36
2022-01-12 23:57:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:35 | INFO | train_inner | epoch 036:     12 / 50 loss=8.727, ppl=423.83, wps=3625.6, ups=2.92, wpb=1242.2, bsz=64, num_updates=1760, lr=5.28e-06, gnorm=8.865, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=350
2022-01-12 23:57:36 | INFO | train_inner | epoch 036:     32 / 50 loss=8.629, ppl=395.97, wps=18633.1, ups=14.4, wpb=1293.7, bsz=62.7, num_updates=1780, lr=5.34e-06, gnorm=8.793, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=352
2022-01-12 23:57:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:38 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.65 | ppl 401.69 | wps 27451.5 | wpb 556.6 | bsz 30.3 | num_updates 1798 | best_loss 8.65
2022-01-12 23:57:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1798 updates
2022-01-12 23:57:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1798 updates, score 8.65) (writing took 5.092594149056822 seconds)
2022-01-12 23:57:43 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-12 23:57:43 | INFO | train | epoch 036 | loss 8.673 | ppl 408.06 | wps 6446.3 | ups 5.23 | wpb 1232.9 | bsz 63.5 | num_updates 1798 | lr 5.394e-06 | gnorm 8.941 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 359
2022-01-12 23:57:44 | INFO | fairseq.trainer | begin training epoch 37
2022-01-12 23:57:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:44 | INFO | train_inner | epoch 037:      2 / 50 loss=8.652, ppl=402.3, wps=3091.1, ups=2.72, wpb=1136.7, bsz=64, num_updates=1800, lr=5.4e-06, gnorm=9.128, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=359
2022-01-12 23:57:45 | INFO | train_inner | epoch 037:     22 / 50 loss=8.584, ppl=383.85, wps=16061.9, ups=13.49, wpb=1191, bsz=62.7, num_updates=1820, lr=5.46e-06, gnorm=9.037, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=360
2022-01-12 23:57:47 | INFO | train_inner | epoch 037:     42 / 50 loss=8.691, ppl=413.41, wps=15540.4, ups=12.57, wpb=1236.2, bsz=64, num_updates=1840, lr=5.52e-06, gnorm=8.902, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=362
2022-01-12 23:57:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:48 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.519 | ppl 366.92 | wps 27076.3 | wpb 556.6 | bsz 30.3 | num_updates 1848 | best_loss 8.519
2022-01-12 23:57:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1848 updates
2022-01-12 23:57:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1848 updates, score 8.519) (writing took 4.238679522881284 seconds)
2022-01-12 23:57:52 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-12 23:57:53 | INFO | train | epoch 037 | loss 8.641 | ppl 399.21 | wps 6837.3 | ups 5.55 | wpb 1232.9 | bsz 63.5 | num_updates 1848 | lr 5.544e-06 | gnorm 9.062 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 368
2022-01-12 23:57:53 | INFO | fairseq.trainer | begin training epoch 38
2022-01-12 23:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:53 | INFO | train_inner | epoch 038:     12 / 50 loss=8.59, ppl=385.25, wps=3697.6, ups=3, wpb=1231.5, bsz=64, num_updates=1860, lr=5.58e-06, gnorm=9.033, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=369
2022-01-12 23:57:55 | INFO | train_inner | epoch 038:     32 / 50 loss=8.532, ppl=370.25, wps=15065.4, ups=13.6, wpb=1108.1, bsz=64, num_updates=1880, lr=5.64e-06, gnorm=10.302, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=370
2022-01-12 23:57:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:57 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.463 | ppl 352.84 | wps 27608.5 | wpb 556.6 | bsz 30.3 | num_updates 1898 | best_loss 8.463
2022-01-12 23:57:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1898 updates
2022-01-12 23:57:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 38 @ 1898 updates, score 8.463) (writing took 4.939729632111266 seconds)
2022-01-12 23:58:02 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-12 23:58:02 | INFO | train | epoch 038 | loss 8.496 | ppl 360.92 | wps 6408.3 | ups 5.2 | wpb 1232.9 | bsz 63.5 | num_updates 1898 | lr 5.694e-06 | gnorm 9.393 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 377
2022-01-12 23:58:02 | INFO | fairseq.trainer | begin training epoch 39
2022-01-12 23:58:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:02 | INFO | train_inner | epoch 039:      2 / 50 loss=8.431, ppl=345.12, wps=3629.9, ups=2.67, wpb=1357.8, bsz=62.7, num_updates=1900, lr=5.7e-06, gnorm=8.871, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=378
2022-01-12 23:58:04 | INFO | train_inner | epoch 039:     22 / 50 loss=8.31, ppl=317.3, wps=18391, ups=13.19, wpb=1394.3, bsz=62.7, num_updates=1920, lr=5.76e-06, gnorm=8.856, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=379
2022-01-12 23:58:06 | INFO | train_inner | epoch 039:     42 / 50 loss=8.6, ppl=388.07, wps=14391.7, ups=13.04, wpb=1103.3, bsz=64, num_updates=1940, lr=5.82e-06, gnorm=8.831, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=381
2022-01-12 23:58:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:07 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.338 | ppl 323.55 | wps 26376.4 | wpb 556.6 | bsz 30.3 | num_updates 1948 | best_loss 8.338
2022-01-12 23:58:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1948 updates
2022-01-12 23:58:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1948 updates, score 8.338) (writing took 4.365342371165752 seconds)
2022-01-12 23:58:11 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-12 23:58:11 | INFO | train | epoch 039 | loss 8.412 | ppl 340.57 | wps 6727.9 | ups 5.46 | wpb 1232.9 | bsz 63.5 | num_updates 1948 | lr 5.844e-06 | gnorm 8.833 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 386
2022-01-12 23:58:11 | INFO | fairseq.trainer | begin training epoch 40
2022-01-12 23:58:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:12 | INFO | train_inner | epoch 040:     12 / 50 loss=8.234, ppl=301.06, wps=3408.9, ups=2.94, wpb=1158.2, bsz=62.7, num_updates=1960, lr=5.88e-06, gnorm=8.929, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=387
2022-01-12 23:58:14 | INFO | train_inner | epoch 040:     32 / 50 loss=8.445, ppl=348.56, wps=16243, ups=13.6, wpb=1193.9, bsz=64, num_updates=1980, lr=5.94e-06, gnorm=8.674, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=389
2022-01-12 23:58:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:16 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.301 | ppl 315.3 | wps 26733.9 | wpb 556.6 | bsz 30.3 | num_updates 1998 | best_loss 8.301
2022-01-12 23:58:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1998 updates
2022-01-12 23:58:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1998 updates, score 8.301) (writing took 4.93078504386358 seconds)
2022-01-12 23:58:21 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-12 23:58:21 | INFO | train | epoch 040 | loss 8.351 | ppl 326.42 | wps 6578.5 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 1998 | lr 5.994e-06 | gnorm 8.829 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 396
2022-01-12 23:58:21 | INFO | fairseq.trainer | begin training epoch 41
2022-01-12 23:58:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:21 | INFO | train_inner | epoch 041:      2 / 50 loss=8.301, ppl=315.32, wps=3697.6, ups=2.79, wpb=1325.2, bsz=64, num_updates=2000, lr=6e-06, gnorm=8.798, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=396
2022-01-12 23:58:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:22 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.232 | ppl 300.58 | wps 25796.3 | wpb 556.6 | bsz 30.3 | num_updates 2000 | best_loss 8.232
2022-01-12 23:58:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2000 updates
2022-01-12 23:58:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt
2022-01-12 23:58:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt
2022-01-12 23:58:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt (epoch 41 @ 2000 updates, score 8.232) (writing took 9.888902221107855 seconds)
2022-01-12 23:58:33 | INFO | train_inner | epoch 041:     22 / 50 loss=8.306, ppl=316.57, wps=2124.6, ups=1.62, wpb=1308.8, bsz=64, num_updates=2020, lr=6.06e-06, gnorm=8.913, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=408
2022-01-12 23:58:35 | INFO | train_inner | epoch 041:     42 / 50 loss=8.255, ppl=305.54, wps=15575.6, ups=13.04, wpb=1194.7, bsz=62.7, num_updates=2040, lr=6.12e-06, gnorm=8.872, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=410
2022-01-12 23:58:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:36 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.254 | ppl 305.35 | wps 26718.4 | wpb 556.6 | bsz 30.3 | num_updates 2048 | best_loss 8.232
2022-01-12 23:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2048 updates
2022-01-12 23:58:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 2048 updates, score 8.254) (writing took 2.7794300818350166 seconds)
2022-01-12 23:58:39 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-12 23:58:39 | INFO | train | epoch 041 | loss 8.275 | ppl 309.75 | wps 3353.6 | ups 2.72 | wpb 1232.9 | bsz 63.5 | num_updates 2048 | lr 6.144e-06 | gnorm 8.832 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 414
2022-01-12 23:58:39 | INFO | fairseq.trainer | begin training epoch 42
2022-01-12 23:58:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:40 | INFO | train_inner | epoch 042:     12 / 50 loss=8.202, ppl=294.42, wps=4650.6, ups=3.82, wpb=1217.2, bsz=64, num_updates=2060, lr=6.18e-06, gnorm=8.577, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=415
2022-01-12 23:58:41 | INFO | train_inner | epoch 042:     32 / 50 loss=8.245, ppl=303.39, wps=17383.4, ups=13.94, wpb=1247.2, bsz=64, num_updates=2080, lr=6.24e-06, gnorm=8.729, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=417
2022-01-12 23:58:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:44 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.09 | ppl 272.57 | wps 25664.9 | wpb 556.6 | bsz 30.3 | num_updates 2098 | best_loss 8.09
2022-01-12 23:58:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2098 updates
2022-01-12 23:58:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 2098 updates, score 8.09) (writing took 4.342212746152654 seconds)
2022-01-12 23:58:48 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-12 23:58:48 | INFO | train | epoch 042 | loss 8.209 | ppl 295.91 | wps 6966.1 | ups 5.65 | wpb 1232.9 | bsz 63.5 | num_updates 2098 | lr 6.294e-06 | gnorm 8.768 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 423
2022-01-12 23:58:48 | INFO | fairseq.trainer | begin training epoch 43
2022-01-12 23:58:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:48 | INFO | train_inner | epoch 043:      2 / 50 loss=8.117, ppl=277.53, wps=3522.9, ups=2.98, wpb=1182.5, bsz=62.7, num_updates=2100, lr=6.3e-06, gnorm=8.905, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=423
2022-01-12 23:58:50 | INFO | train_inner | epoch 043:     22 / 50 loss=7.966, ppl=250.03, wps=17217.8, ups=14.07, wpb=1223.8, bsz=62.7, num_updates=2120, lr=6.36e-06, gnorm=8.955, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=425
2022-01-12 23:58:51 | INFO | train_inner | epoch 043:     42 / 50 loss=8.312, ppl=317.78, wps=16118.4, ups=12.53, wpb=1286.2, bsz=64, num_updates=2140, lr=6.42e-06, gnorm=8.445, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=426
2022-01-12 23:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:53 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.113 | ppl 276.8 | wps 25381.2 | wpb 556.6 | bsz 30.3 | num_updates 2148 | best_loss 8.09
2022-01-12 23:58:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2148 updates
2022-01-12 23:58:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 43 @ 2148 updates, score 8.113) (writing took 2.8221724259201437 seconds)
2022-01-12 23:58:55 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-12 23:58:55 | INFO | train | epoch 043 | loss 8.081 | ppl 270.87 | wps 8190.8 | ups 6.64 | wpb 1232.9 | bsz 63.5 | num_updates 2148 | lr 6.444e-06 | gnorm 8.733 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 431
2022-01-12 23:58:56 | INFO | fairseq.trainer | begin training epoch 44
2022-01-12 23:58:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:56 | INFO | train_inner | epoch 044:     12 / 50 loss=7.9, ppl=238.79, wps=4185.8, ups=3.86, wpb=1083.7, bsz=62.7, num_updates=2160, lr=6.48e-06, gnorm=8.953, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=432
2022-01-12 23:58:58 | INFO | train_inner | epoch 044:     32 / 50 loss=7.977, ppl=251.93, wps=16719.6, ups=13.21, wpb=1265.7, bsz=64, num_updates=2180, lr=6.54e-06, gnorm=8.731, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=433
2022-01-12 23:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:00 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.037 | ppl 262.58 | wps 26798.8 | wpb 556.6 | bsz 30.3 | num_updates 2198 | best_loss 8.037
2022-01-12 23:59:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2198 updates
2022-01-12 23:59:00 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 2198 updates, score 8.037) (writing took 5.0613508629612625 seconds)
2022-01-12 23:59:05 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-12 23:59:05 | INFO | train | epoch 044 | loss 7.974 | ppl 251.45 | wps 6452.4 | ups 5.23 | wpb 1232.9 | bsz 63.5 | num_updates 2198 | lr 6.594e-06 | gnorm 8.718 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 440
2022-01-12 23:59:05 | INFO | fairseq.trainer | begin training epoch 45
2022-01-12 23:59:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:05 | INFO | train_inner | epoch 045:      2 / 50 loss=7.975, ppl=251.59, wps=3463.6, ups=2.71, wpb=1277.9, bsz=64, num_updates=2200, lr=6.6e-06, gnorm=8.586, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=440
2022-01-12 23:59:07 | INFO | train_inner | epoch 045:     22 / 50 loss=7.894, ppl=237.79, wps=17351.7, ups=14.22, wpb=1220.5, bsz=62.7, num_updates=2220, lr=6.66e-06, gnorm=8.658, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=442
2022-01-12 23:59:08 | INFO | train_inner | epoch 045:     42 / 50 loss=7.856, ppl=231.67, wps=16552.2, ups=13.52, wpb=1224.2, bsz=64, num_updates=2240, lr=6.72e-06, gnorm=8.725, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=443
2022-01-12 23:59:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:10 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.94 | ppl 245.65 | wps 28042.6 | wpb 556.6 | bsz 30.3 | num_updates 2248 | best_loss 7.94
2022-01-12 23:59:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2248 updates
2022-01-12 23:59:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 2248 updates, score 7.94) (writing took 4.605860479874536 seconds)
2022-01-12 23:59:14 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-12 23:59:14 | INFO | train | epoch 045 | loss 7.891 | ppl 237.32 | wps 6729.4 | ups 5.46 | wpb 1232.9 | bsz 63.5 | num_updates 2248 | lr 6.744e-06 | gnorm 8.697 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 449
2022-01-12 23:59:14 | INFO | fairseq.trainer | begin training epoch 46
2022-01-12 23:59:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:15 | INFO | train_inner | epoch 046:     12 / 50 loss=7.994, ppl=254.98, wps=3714.2, ups=2.87, wpb=1296.3, bsz=62.7, num_updates=2260, lr=6.78e-06, gnorm=8.626, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=450
2022-01-12 23:59:17 | INFO | train_inner | epoch 046:     32 / 50 loss=7.749, ppl=215.1, wps=15671.5, ups=13.17, wpb=1189.8, bsz=64, num_updates=2280, lr=6.84e-06, gnorm=8.917, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=452
2022-01-12 23:59:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:19 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.904 | ppl 239.51 | wps 26257.2 | wpb 556.6 | bsz 30.3 | num_updates 2298 | best_loss 7.904
2022-01-12 23:59:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2298 updates
2022-01-12 23:59:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 46 @ 2298 updates, score 7.904) (writing took 4.576131146866828 seconds)
2022-01-12 23:59:23 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-12 23:59:23 | INFO | train | epoch 046 | loss 7.837 | ppl 228.63 | wps 6706.1 | ups 5.44 | wpb 1232.9 | bsz 63.5 | num_updates 2298 | lr 6.894e-06 | gnorm 8.75 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 459
2022-01-12 23:59:23 | INFO | fairseq.trainer | begin training epoch 47
2022-01-12 23:59:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:24 | INFO | train_inner | epoch 047:      2 / 50 loss=7.835, ppl=228.34, wps=3669.3, ups=2.88, wpb=1275.9, bsz=64, num_updates=2300, lr=6.9e-06, gnorm=9.032, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=459
2022-01-12 23:59:25 | INFO | train_inner | epoch 047:     22 / 50 loss=7.848, ppl=230.47, wps=17697.2, ups=14.21, wpb=1245.6, bsz=64, num_updates=2320, lr=6.96e-06, gnorm=8.976, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=460
2022-01-12 23:59:27 | INFO | train_inner | epoch 047:     42 / 50 loss=7.73, ppl=212.26, wps=17404.8, ups=13.66, wpb=1274.3, bsz=62.7, num_updates=2340, lr=7.02e-06, gnorm=8.723, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=462
2022-01-12 23:59:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:28 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.786 | ppl 220.69 | wps 24253.9 | wpb 556.6 | bsz 30.3 | num_updates 2348 | best_loss 7.786
2022-01-12 23:59:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 2348 updates
2022-01-12 23:59:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 2348 updates, score 7.786) (writing took 4.3219620999880135 seconds)
2022-01-12 23:59:32 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-12 23:59:32 | INFO | train | epoch 047 | loss 7.767 | ppl 217.86 | wps 6944.2 | ups 5.63 | wpb 1232.9 | bsz 63.5 | num_updates 2348 | lr 7.044e-06 | gnorm 8.984 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 467
2022-01-12 23:59:32 | INFO | fairseq.trainer | begin training epoch 48
2022-01-12 23:59:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:33 | INFO | train_inner | epoch 048:     12 / 50 loss=7.671, ppl=203.85, wps=3673.4, ups=2.96, wpb=1239.3, bsz=62.7, num_updates=2360, lr=7.08e-06, gnorm=8.791, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=468
2022-01-12 23:59:35 | INFO | train_inner | epoch 048:     32 / 50 loss=7.678, ppl=204.72, wps=16620.8, ups=14.11, wpb=1177.8, bsz=64, num_updates=2380, lr=7.14e-06, gnorm=8.765, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=470
2022-01-12 23:59:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:37 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.707 | ppl 208.95 | wps 25456.3 | wpb 556.6 | bsz 30.3 | num_updates 2398 | best_loss 7.707
2022-01-12 23:59:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 2398 updates
2022-01-12 23:59:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 48 @ 2398 updates, score 7.707) (writing took 4.2475571180693805 seconds)
2022-01-12 23:59:41 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-12 23:59:41 | INFO | train | epoch 048 | loss 7.747 | ppl 214.77 | wps 7075 | ups 5.74 | wpb 1232.9 | bsz 63.5 | num_updates 2398 | lr 7.194e-06 | gnorm 8.696 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 476
2022-01-12 23:59:41 | INFO | fairseq.trainer | begin training epoch 49
2022-01-12 23:59:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:41 | INFO | train_inner | epoch 049:      2 / 50 loss=7.856, ppl=231.67, wps=3639.7, ups=3.05, wpb=1192.6, bsz=64, num_updates=2400, lr=7.2e-06, gnorm=8.574, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=476
2022-01-12 23:59:43 | INFO | train_inner | epoch 049:     22 / 50 loss=7.437, ppl=173.24, wps=18505.6, ups=13.87, wpb=1334.5, bsz=64, num_updates=2420, lr=7.26e-06, gnorm=8.444, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=478
2022-01-12 23:59:44 | INFO | train_inner | epoch 049:     42 / 50 loss=7.571, ppl=190.18, wps=16310, ups=14.33, wpb=1138.3, bsz=62.7, num_updates=2440, lr=7.32e-06, gnorm=9.063, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=479
2022-01-12 23:59:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:45 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.68 | ppl 205.05 | wps 23843.5 | wpb 556.6 | bsz 30.3 | num_updates 2448 | best_loss 7.68
2022-01-12 23:59:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 2448 updates
2022-01-12 23:59:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 2448 updates, score 7.68) (writing took 4.621699879877269 seconds)
2022-01-12 23:59:50 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-12 23:59:50 | INFO | train | epoch 049 | loss 7.559 | ppl 188.55 | wps 6798.7 | ups 5.51 | wpb 1232.9 | bsz 63.5 | num_updates 2448 | lr 7.344e-06 | gnorm 8.73 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 485
2022-01-12 23:59:50 | INFO | fairseq.trainer | begin training epoch 50
2022-01-12 23:59:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:53 | INFO | train_inner | epoch 050:     12 / 50 loss=7.856, ppl=231.71, wps=3024.6, ups=2.37, wpb=1276.5, bsz=62.7, num_updates=2460, lr=7.38e-06, gnorm=8.075, clip=100, loss_scale=32, train_wall=3, gb_free=20.9, wall=488
2022-01-12 23:59:54 | INFO | train_inner | epoch 050:     32 / 50 loss=7.992, ppl=254.5, wps=17201.1, ups=13.51, wpb=1273.2, bsz=64, num_updates=2480, lr=7.44e-06, gnorm=8.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=489
2022-01-12 23:59:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:56 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.607 | ppl 194.93 | wps 26515.6 | wpb 556.6 | bsz 30.3 | num_updates 2498 | best_loss 7.607
2022-01-12 23:59:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2498 updates
2022-01-12 23:59:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 2498 updates, score 7.607) (writing took 4.406413435935974 seconds)
2022-01-13 00:00:01 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-13 00:00:01 | INFO | train | epoch 050 | loss 7.954 | ppl 247.89 | wps 5911.6 | ups 4.79 | wpb 1232.9 | bsz 63.5 | num_updates 2498 | lr 7.494e-06 | gnorm 7.901 | clip 100 | loss_scale 32 | train_wall 5 | gb_free 20.9 | wall 496
2022-01-13 00:00:01 | INFO | fairseq.trainer | begin training epoch 51
2022-01-13 00:00:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:01 | INFO | train_inner | epoch 051:      2 / 50 loss=7.965, ppl=249.94, wps=3527.3, ups=2.97, wpb=1188, bsz=64, num_updates=2500, lr=7.5e-06, gnorm=7.812, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=496
2022-01-13 00:00:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:02 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.556 | ppl 188.12 | wps 23617.3 | wpb 556.6 | bsz 30.3 | num_updates 2500 | best_loss 7.556
2022-01-13 00:00:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2500 updates
2022-01-13 00:00:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt
2022-01-13 00:00:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt
2022-01-13 00:00:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt (epoch 51 @ 2500 updates, score 7.556) (writing took 9.116204982856289 seconds)
2022-01-13 00:00:13 | INFO | train_inner | epoch 051:     22 / 50 loss=7.947, ppl=246.79, wps=2113.4, ups=1.7, wpb=1242.9, bsz=64, num_updates=2520, lr=7.56e-06, gnorm=7.705, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=508
2022-01-13 00:00:14 | INFO | train_inner | epoch 051:     42 / 50 loss=7.815, ppl=225.15, wps=16295.3, ups=13.01, wpb=1252.8, bsz=62.7, num_updates=2540, lr=7.62e-06, gnorm=7.548, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=509
2022-01-13 00:00:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:16 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.582 | ppl 191.56 | wps 24534.5 | wpb 556.6 | bsz 30.3 | num_updates 2548 | best_loss 7.556
2022-01-13 00:00:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2548 updates
2022-01-13 00:00:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 51 @ 2548 updates, score 7.582) (writing took 3.3303923478815705 seconds)
2022-01-13 00:00:19 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-13 00:00:19 | INFO | train | epoch 051 | loss 7.866 | ppl 233.29 | wps 3360.1 | ups 2.73 | wpb 1232.9 | bsz 63.5 | num_updates 2548 | lr 7.644e-06 | gnorm 7.713 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 514
2022-01-13 00:00:19 | INFO | fairseq.trainer | begin training epoch 52
2022-01-13 00:00:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:20 | INFO | train_inner | epoch 052:     12 / 50 loss=7.753, ppl=215.67, wps=4079.6, ups=3.47, wpb=1176.3, bsz=62.7, num_updates=2560, lr=7.68e-06, gnorm=8.2, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=515
2022-01-13 00:00:21 | INFO | train_inner | epoch 052:     32 / 50 loss=7.812, ppl=224.76, wps=15926.7, ups=13.37, wpb=1191, bsz=64, num_updates=2580, lr=7.74e-06, gnorm=7.874, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=517
2022-01-13 00:00:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:24 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.559 | ppl 188.59 | wps 25404.3 | wpb 556.6 | bsz 30.3 | num_updates 2598 | best_loss 7.556
2022-01-13 00:00:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 2598 updates
2022-01-13 00:00:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 52 @ 2598 updates, score 7.559) (writing took 3.254428870975971 seconds)
2022-01-13 00:00:27 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-13 00:00:27 | INFO | train | epoch 052 | loss 7.796 | ppl 222.19 | wps 7595.6 | ups 6.16 | wpb 1232.9 | bsz 63.5 | num_updates 2598 | lr 7.794e-06 | gnorm 7.966 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 522
2022-01-13 00:00:27 | INFO | fairseq.trainer | begin training epoch 53
2022-01-13 00:00:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:27 | INFO | train_inner | epoch 053:      2 / 50 loss=7.772, ppl=218.64, wps=4332.3, ups=3.41, wpb=1271.5, bsz=64, num_updates=2600, lr=7.8e-06, gnorm=7.908, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=522
2022-01-13 00:00:29 | INFO | train_inner | epoch 053:     22 / 50 loss=7.807, ppl=223.88, wps=17655, ups=13.44, wpb=1313.8, bsz=64, num_updates=2620, lr=7.86e-06, gnorm=7.316, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=524
2022-01-13 00:00:30 | INFO | train_inner | epoch 053:     42 / 50 loss=7.632, ppl=198.38, wps=13717.1, ups=11.88, wpb=1154.8, bsz=62.7, num_updates=2640, lr=7.92e-06, gnorm=8.024, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=526
2022-01-13 00:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:32 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.412 | ppl 170.27 | wps 27303.1 | wpb 556.6 | bsz 30.3 | num_updates 2648 | best_loss 7.412
2022-01-13 00:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 2648 updates
2022-01-13 00:00:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 2648 updates, score 7.412) (writing took 5.089505098061636 seconds)
2022-01-13 00:00:37 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-13 00:00:37 | INFO | train | epoch 053 | loss 7.707 | ppl 209.01 | wps 6233.1 | ups 5.06 | wpb 1232.9 | bsz 63.5 | num_updates 2648 | lr 7.944e-06 | gnorm 7.65 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 532
2022-01-13 00:00:37 | INFO | fairseq.trainer | begin training epoch 54
2022-01-13 00:00:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:38 | INFO | train_inner | epoch 054:     12 / 50 loss=7.668, ppl=203.31, wps=3287.3, ups=2.65, wpb=1242.8, bsz=64, num_updates=2660, lr=7.98e-06, gnorm=7.521, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=533
2022-01-13 00:00:39 | INFO | train_inner | epoch 054:     32 / 50 loss=7.665, ppl=202.93, wps=17643.7, ups=14.19, wpb=1243.4, bsz=64, num_updates=2680, lr=8.04e-06, gnorm=7.633, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=535
2022-01-13 00:00:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:41 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.433 | ppl 172.86 | wps 26600.4 | wpb 556.6 | bsz 30.3 | num_updates 2698 | best_loss 7.412
2022-01-13 00:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 2698 updates
2022-01-13 00:00:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 54 @ 2698 updates, score 7.433) (writing took 3.989198715193197 seconds)
2022-01-13 00:00:45 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-13 00:00:45 | INFO | train | epoch 054 | loss 7.627 | ppl 197.69 | wps 7212.2 | ups 5.85 | wpb 1232.9 | bsz 63.5 | num_updates 2698 | lr 8.094e-06 | gnorm 7.588 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 541
2022-01-13 00:00:45 | INFO | fairseq.trainer | begin training epoch 55
2022-01-13 00:00:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:46 | INFO | train_inner | epoch 055:      2 / 50 loss=7.49, ppl=179.72, wps=3767.4, ups=3.19, wpb=1182.4, bsz=62.7, num_updates=2700, lr=8.1e-06, gnorm=7.608, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=541
2022-01-13 00:00:47 | INFO | train_inner | epoch 055:     22 / 50 loss=7.332, ppl=161.16, wps=14790.9, ups=13.87, wpb=1066.3, bsz=62.7, num_updates=2720, lr=8.16e-06, gnorm=9.287, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=542
2022-01-13 00:00:49 | INFO | train_inner | epoch 055:     42 / 50 loss=7.628, ppl=197.85, wps=17417.2, ups=13.48, wpb=1291.8, bsz=64, num_updates=2740, lr=8.22e-06, gnorm=7.261, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=544
2022-01-13 00:00:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:50 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.352 | ppl 163.41 | wps 26194.6 | wpb 556.6 | bsz 30.3 | num_updates 2748 | best_loss 7.352
2022-01-13 00:00:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 2748 updates
2022-01-13 00:00:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 55 @ 2748 updates, score 7.352) (writing took 4.29561490402557 seconds)
2022-01-13 00:00:54 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-13 00:00:54 | INFO | train | epoch 055 | loss 7.528 | ppl 184.52 | wps 6895.5 | ups 5.59 | wpb 1232.9 | bsz 63.5 | num_updates 2748 | lr 8.244e-06 | gnorm 8.049 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 550
2022-01-13 00:00:54 | INFO | fairseq.trainer | begin training epoch 56
2022-01-13 00:00:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:55 | INFO | train_inner | epoch 056:     12 / 50 loss=7.617, ppl=196.36, wps=3931.7, ups=2.95, wpb=1332, bsz=64, num_updates=2760, lr=8.28e-06, gnorm=7.379, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=551
2022-01-13 00:00:57 | INFO | train_inner | epoch 056:     32 / 50 loss=7.455, ppl=175.47, wps=17931.7, ups=13.83, wpb=1297, bsz=62.7, num_updates=2780, lr=8.34e-06, gnorm=7.609, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=552
2022-01-13 00:00:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:59 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.268 | ppl 154.13 | wps 26234.9 | wpb 556.6 | bsz 30.3 | num_updates 2798 | best_loss 7.268
2022-01-13 00:00:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 2798 updates
2022-01-13 00:00:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 2798 updates, score 7.268) (writing took 4.465936854016036 seconds)
2022-01-13 00:01:03 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-13 00:01:03 | INFO | train | epoch 056 | loss 7.522 | ppl 183.82 | wps 6816.9 | ups 5.53 | wpb 1232.9 | bsz 63.5 | num_updates 2798 | lr 8.394e-06 | gnorm 7.608 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 559
2022-01-13 00:01:03 | INFO | fairseq.trainer | begin training epoch 57
2022-01-13 00:01:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:04 | INFO | train_inner | epoch 057:      2 / 50 loss=7.603, ppl=194.4, wps=3479.8, ups=2.9, wpb=1200.4, bsz=64, num_updates=2800, lr=8.4e-06, gnorm=7.566, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=559
2022-01-13 00:01:05 | INFO | train_inner | epoch 057:     22 / 50 loss=7.343, ppl=162.36, wps=18166.1, ups=13.92, wpb=1304.7, bsz=64, num_updates=2820, lr=8.46e-06, gnorm=7.346, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=560
2022-01-13 00:01:07 | INFO | train_inner | epoch 057:     42 / 50 loss=7.482, ppl=178.81, wps=14792.5, ups=12.62, wpb=1172.3, bsz=62.7, num_updates=2840, lr=8.52e-06, gnorm=7.784, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=562
2022-01-13 00:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:08 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.213 | ppl 148.41 | wps 26417.6 | wpb 556.6 | bsz 30.3 | num_updates 2848 | best_loss 7.213
2022-01-13 00:01:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 2848 updates
2022-01-13 00:01:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 57 @ 2848 updates, score 7.213) (writing took 4.1772787710651755 seconds)
2022-01-13 00:01:12 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-13 00:01:12 | INFO | train | epoch 057 | loss 7.442 | ppl 173.9 | wps 6952 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 2848 | lr 8.544e-06 | gnorm 7.599 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 567
2022-01-13 00:01:12 | INFO | fairseq.trainer | begin training epoch 58
2022-01-13 00:01:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:13 | INFO | train_inner | epoch 058:     12 / 50 loss=7.471, ppl=177.43, wps=3762.3, ups=3.07, wpb=1224.3, bsz=64, num_updates=2860, lr=8.58e-06, gnorm=7.622, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=568
2022-01-13 00:01:15 | INFO | train_inner | epoch 058:     32 / 50 loss=7.347, ppl=162.82, wps=13996.8, ups=11.97, wpb=1169.8, bsz=62.7, num_updates=2880, lr=8.64e-06, gnorm=7.758, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=570
2022-01-13 00:01:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:17 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.06 | ppl 133.46 | wps 25253.9 | wpb 556.6 | bsz 30.3 | num_updates 2898 | best_loss 7.06
2022-01-13 00:01:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 2898 updates
2022-01-13 00:01:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 58 @ 2898 updates, score 7.06) (writing took 4.362164995865896 seconds)
2022-01-13 00:01:21 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-13 00:01:22 | INFO | train | epoch 058 | loss 7.386 | ppl 167.28 | wps 6767.3 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 2898 | lr 8.694e-06 | gnorm 7.642 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 577
2022-01-13 00:01:22 | INFO | fairseq.trainer | begin training epoch 59
2022-01-13 00:01:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:22 | INFO | train_inner | epoch 059:      2 / 50 loss=7.393, ppl=168.03, wps=3606, ups=2.84, wpb=1271, bsz=62.7, num_updates=2900, lr=8.7e-06, gnorm=7.68, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=577
2022-01-13 00:01:24 | INFO | train_inner | epoch 059:     22 / 50 loss=7.228, ppl=149.9, wps=15681.9, ups=12.92, wpb=1213.8, bsz=64, num_updates=2920, lr=8.76e-06, gnorm=7.561, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=579
2022-01-13 00:01:25 | INFO | train_inner | epoch 059:     42 / 50 loss=7.287, ppl=156.23, wps=16704.3, ups=13.86, wpb=1205.3, bsz=64, num_updates=2940, lr=8.82e-06, gnorm=7.534, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=580
2022-01-13 00:01:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:26 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.04 | ppl 131.59 | wps 26928.6 | wpb 556.6 | bsz 30.3 | num_updates 2948 | best_loss 7.04
2022-01-13 00:01:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 2948 updates
2022-01-13 00:01:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 59 @ 2948 updates, score 7.04) (writing took 4.292766103055328 seconds)
2022-01-13 00:01:31 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-13 00:01:31 | INFO | train | epoch 059 | loss 7.284 | ppl 155.88 | wps 6850.3 | ups 5.56 | wpb 1232.9 | bsz 63.5 | num_updates 2948 | lr 8.844e-06 | gnorm 7.553 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 586
2022-01-13 00:01:31 | INFO | fairseq.trainer | begin training epoch 60
2022-01-13 00:01:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:32 | INFO | train_inner | epoch 060:     12 / 50 loss=7.297, ppl=157.28, wps=4063.7, ups=2.97, wpb=1369, bsz=64, num_updates=2960, lr=8.88e-06, gnorm=7.687, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=587
2022-01-13 00:01:33 | INFO | train_inner | epoch 060:     32 / 50 loss=7.322, ppl=160, wps=16409.5, ups=12.81, wpb=1281.2, bsz=62.7, num_updates=2980, lr=8.94e-06, gnorm=7.751, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=588
2022-01-13 00:01:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:35 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.034 | ppl 131.08 | wps 28028.6 | wpb 556.6 | bsz 30.3 | num_updates 2998 | best_loss 7.034
2022-01-13 00:01:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 2998 updates
2022-01-13 00:01:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 60 @ 2998 updates, score 7.034) (writing took 5.088655893923715 seconds)
2022-01-13 00:01:40 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-13 00:01:40 | INFO | train | epoch 060 | loss 7.224 | ppl 149.47 | wps 6355.5 | ups 5.15 | wpb 1232.9 | bsz 63.5 | num_updates 2998 | lr 8.994e-06 | gnorm 7.765 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 596
2022-01-13 00:01:40 | INFO | fairseq.trainer | begin training epoch 61
2022-01-13 00:01:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:41 | INFO | train_inner | epoch 061:      2 / 50 loss=7.147, ppl=141.71, wps=2947.8, ups=2.71, wpb=1087.5, bsz=64, num_updates=3000, lr=9e-06, gnorm=7.685, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=596
2022-01-13 00:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:42 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.08 | ppl 135.29 | wps 28111.9 | wpb 556.6 | bsz 30.3 | num_updates 3000 | best_loss 7.034
2022-01-13 00:01:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3000 updates
2022-01-13 00:01:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt
2022-01-13 00:01:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt
2022-01-13 00:01:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt (epoch 61 @ 3000 updates, score 7.08) (writing took 5.089682440040633 seconds)
2022-01-13 00:01:48 | INFO | train_inner | epoch 061:     22 / 50 loss=7.18, ppl=145, wps=3129.5, ups=2.65, wpb=1181.2, bsz=62.7, num_updates=3020, lr=9.06e-06, gnorm=7.701, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=603
2022-01-13 00:01:50 | INFO | train_inner | epoch 061:     42 / 50 loss=7.218, ppl=148.87, wps=16939.1, ups=13.17, wpb=1286.7, bsz=64, num_updates=3040, lr=9.12e-06, gnorm=7.684, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=605
2022-01-13 00:01:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:51 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.068 | ppl 134.22 | wps 25034.2 | wpb 556.6 | bsz 30.3 | num_updates 3048 | best_loss 7.034
2022-01-13 00:01:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3048 updates
2022-01-13 00:01:51 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:01:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:01:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 61 @ 3048 updates, score 7.068) (writing took 3.1684718979522586 seconds)
2022-01-13 00:01:54 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-13 00:01:54 | INFO | train | epoch 061 | loss 7.189 | ppl 145.9 | wps 4447.5 | ups 3.61 | wpb 1232.9 | bsz 63.5 | num_updates 3048 | lr 9.144e-06 | gnorm 7.641 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 609
2022-01-13 00:01:54 | INFO | fairseq.trainer | begin training epoch 62
2022-01-13 00:01:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:55 | INFO | train_inner | epoch 062:     12 / 50 loss=7.247, ppl=151.87, wps=4768.9, ups=3.63, wpb=1314.8, bsz=64, num_updates=3060, lr=9.18e-06, gnorm=7.253, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=610
2022-01-13 00:01:57 | INFO | train_inner | epoch 062:     32 / 50 loss=6.903, ppl=119.71, wps=14414.4, ups=13.22, wpb=1090.7, bsz=64, num_updates=3080, lr=9.24e-06, gnorm=7.925, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=612
2022-01-13 00:01:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:59 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.997 | ppl 127.75 | wps 25642.7 | wpb 556.6 | bsz 30.3 | num_updates 3098 | best_loss 6.997
2022-01-13 00:01:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 3098 updates
2022-01-13 00:01:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 62 @ 3098 updates, score 6.997) (writing took 4.368456893134862 seconds)
2022-01-13 00:02:03 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-13 00:02:03 | INFO | train | epoch 062 | loss 7.123 | ppl 139.36 | wps 6768.3 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 3098 | lr 9.294e-06 | gnorm 7.606 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 619
2022-01-13 00:02:03 | INFO | fairseq.trainer | begin training epoch 63
2022-01-13 00:02:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:04 | INFO | train_inner | epoch 063:      2 / 50 loss=7.231, ppl=150.2, wps=3953.2, ups=2.91, wpb=1357.5, bsz=62.7, num_updates=3100, lr=9.3e-06, gnorm=7.648, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=619
2022-01-13 00:02:05 | INFO | train_inner | epoch 063:     22 / 50 loss=7.031, ppl=130.81, wps=17231.3, ups=13.6, wpb=1267, bsz=64, num_updates=3120, lr=9.36e-06, gnorm=7.415, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=620
2022-01-13 00:02:07 | INFO | train_inner | epoch 063:     42 / 50 loss=7.028, ppl=130.51, wps=16932.6, ups=14.56, wpb=1162.7, bsz=62.7, num_updates=3140, lr=9.42e-06, gnorm=7.81, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=622
2022-01-13 00:02:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:08 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.847 | ppl 115.13 | wps 26909.2 | wpb 556.6 | bsz 30.3 | num_updates 3148 | best_loss 6.847
2022-01-13 00:02:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 3148 updates
2022-01-13 00:02:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 63 @ 3148 updates, score 6.847) (writing took 5.7117693750187755 seconds)
2022-01-13 00:02:14 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-13 00:02:14 | INFO | train | epoch 063 | loss 7.043 | ppl 131.85 | wps 6090.9 | ups 4.94 | wpb 1232.9 | bsz 63.5 | num_updates 3148 | lr 9.444e-06 | gnorm 7.687 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 629
2022-01-13 00:02:14 | INFO | fairseq.trainer | begin training epoch 64
2022-01-13 00:02:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:15 | INFO | train_inner | epoch 064:     12 / 50 loss=6.993, ppl=127.4, wps=2939.8, ups=2.48, wpb=1186.6, bsz=64, num_updates=3160, lr=9.48e-06, gnorm=7.72, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=630
2022-01-13 00:02:16 | INFO | train_inner | epoch 064:     32 / 50 loss=7.146, ppl=141.66, wps=16954.1, ups=13.54, wpb=1252.2, bsz=64, num_updates=3180, lr=9.54e-06, gnorm=7.821, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=631
2022-01-13 00:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:18 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.863 | ppl 116.38 | wps 26723.1 | wpb 556.6 | bsz 30.3 | num_updates 3198 | best_loss 6.847
2022-01-13 00:02:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 3198 updates
2022-01-13 00:02:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 64 @ 3198 updates, score 6.863) (writing took 2.9973171290475875 seconds)
2022-01-13 00:02:21 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-13 00:02:21 | INFO | train | epoch 064 | loss 7.015 | ppl 129.3 | wps 8209.7 | ups 6.66 | wpb 1232.9 | bsz 63.5 | num_updates 3198 | lr 9.594e-06 | gnorm 7.69 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 636
2022-01-13 00:02:21 | INFO | fairseq.trainer | begin training epoch 65
2022-01-13 00:02:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:21 | INFO | train_inner | epoch 065:      2 / 50 loss=6.916, ppl=120.75, wps=4731, ups=3.83, wpb=1234.3, bsz=62.7, num_updates=3200, lr=9.6e-06, gnorm=7.598, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=636
2022-01-13 00:02:23 | INFO | train_inner | epoch 065:     22 / 50 loss=6.815, ppl=112.58, wps=17373.8, ups=13.56, wpb=1281.6, bsz=64, num_updates=3220, lr=9.66e-06, gnorm=7.468, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=638
2022-01-13 00:02:24 | INFO | train_inner | epoch 065:     42 / 50 loss=6.897, ppl=119.19, wps=17407.9, ups=13.92, wpb=1250.8, bsz=64, num_updates=3240, lr=9.72e-06, gnorm=7.956, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=639
2022-01-13 00:02:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:26 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.754 | ppl 107.96 | wps 27435.4 | wpb 556.6 | bsz 30.3 | num_updates 3248 | best_loss 6.754
2022-01-13 00:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 3248 updates
2022-01-13 00:02:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 65 @ 3248 updates, score 6.754) (writing took 5.165929597103968 seconds)
2022-01-13 00:02:31 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-01-13 00:02:31 | INFO | train | epoch 065 | loss 6.907 | ppl 119.99 | wps 6332.7 | ups 5.14 | wpb 1232.9 | bsz 63.5 | num_updates 3248 | lr 9.744e-06 | gnorm 7.732 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 646
2022-01-13 00:02:31 | INFO | fairseq.trainer | begin training epoch 66
2022-01-13 00:02:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:32 | INFO | train_inner | epoch 066:     12 / 50 loss=6.969, ppl=125.26, wps=3262.8, ups=2.63, wpb=1240.5, bsz=62.7, num_updates=3260, lr=9.78e-06, gnorm=7.703, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=647
2022-01-13 00:02:33 | INFO | train_inner | epoch 066:     32 / 50 loss=6.83, ppl=113.75, wps=17104.4, ups=13.29, wpb=1286.8, bsz=64, num_updates=3280, lr=9.84e-06, gnorm=7.987, clip=100, loss_scale=32, train_wall=1, gb_free=20.8, wall=648
2022-01-13 00:02:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:36 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.703 | ppl 104.17 | wps 28140.3 | wpb 556.6 | bsz 30.3 | num_updates 3298 | best_loss 6.703
2022-01-13 00:02:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 3298 updates
2022-01-13 00:02:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 66 @ 3298 updates, score 6.703) (writing took 5.57637187698856 seconds)
2022-01-13 00:02:41 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-01-13 00:02:41 | INFO | train | epoch 066 | loss 6.911 | ppl 120.32 | wps 5936 | ups 4.81 | wpb 1232.9 | bsz 63.5 | num_updates 3298 | lr 9.894e-06 | gnorm 7.846 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 656
2022-01-13 00:02:41 | INFO | fairseq.trainer | begin training epoch 67
2022-01-13 00:02:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:41 | INFO | train_inner | epoch 067:      2 / 50 loss=7.013, ppl=129.17, wps=2863.2, ups=2.46, wpb=1165.1, bsz=62.7, num_updates=3300, lr=9.9e-06, gnorm=7.776, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=657
2022-01-13 00:02:43 | INFO | train_inner | epoch 067:     22 / 50 loss=6.888, ppl=118.45, wps=16588.8, ups=13.29, wpb=1248.2, bsz=64, num_updates=3320, lr=9.96e-06, gnorm=7.519, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=658
2022-01-13 00:02:45 | INFO | train_inner | epoch 067:     42 / 50 loss=6.84, ppl=114.56, wps=15103.3, ups=13.02, wpb=1160.2, bsz=62.7, num_updates=3340, lr=1.002e-05, gnorm=7.814, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=660
2022-01-13 00:02:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:46 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.727 | ppl 105.91 | wps 27156.8 | wpb 556.6 | bsz 30.3 | num_updates 3348 | best_loss 6.703
2022-01-13 00:02:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 3348 updates
2022-01-13 00:02:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 67 @ 3348 updates, score 6.727) (writing took 2.760889601893723 seconds)
2022-01-13 00:02:49 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-01-13 00:02:49 | INFO | train | epoch 067 | loss 6.829 | ppl 113.68 | wps 8100.9 | ups 6.57 | wpb 1232.9 | bsz 63.5 | num_updates 3348 | lr 1.0044e-05 | gnorm 7.7 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 664
2022-01-13 00:02:49 | INFO | fairseq.trainer | begin training epoch 68
2022-01-13 00:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:50 | INFO | train_inner | epoch 068:     12 / 50 loss=6.693, ppl=103.5, wps=5267.1, ups=3.77, wpb=1396.4, bsz=64, num_updates=3360, lr=1.008e-05, gnorm=7.661, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=665
2022-01-13 00:02:51 | INFO | train_inner | epoch 068:     32 / 50 loss=6.787, ppl=110.46, wps=16931.4, ups=13.57, wpb=1247.9, bsz=62.7, num_updates=3380, lr=1.014e-05, gnorm=7.696, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=666
2022-01-13 00:02:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:53 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.69 | ppl 103.24 | wps 25091.9 | wpb 556.6 | bsz 30.3 | num_updates 3398 | best_loss 6.69
2022-01-13 00:02:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 3398 updates
2022-01-13 00:02:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 68 @ 3398 updates, score 6.69) (writing took 3.931047295918688 seconds)
2022-01-13 00:02:57 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-01-13 00:02:57 | INFO | train | epoch 068 | loss 6.759 | ppl 108.32 | wps 7220.8 | ups 5.86 | wpb 1232.9 | bsz 63.5 | num_updates 3398 | lr 1.0194e-05 | gnorm 7.655 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 672
2022-01-13 00:02:57 | INFO | fairseq.trainer | begin training epoch 69
2022-01-13 00:02:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:58 | INFO | train_inner | epoch 069:      2 / 50 loss=6.71, ppl=104.71, wps=3431.3, ups=3.19, wpb=1076.8, bsz=64, num_updates=3400, lr=1.02e-05, gnorm=7.74, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=673
2022-01-13 00:02:59 | INFO | train_inner | epoch 069:     22 / 50 loss=6.817, ppl=112.74, wps=18982.9, ups=13.78, wpb=1377.8, bsz=64, num_updates=3420, lr=1.026e-05, gnorm=7.292, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=674
2022-01-13 00:03:01 | INFO | train_inner | epoch 069:     42 / 50 loss=6.604, ppl=97.29, wps=15050.8, ups=12.86, wpb=1170.5, bsz=62.7, num_updates=3440, lr=1.032e-05, gnorm=7.848, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=676
2022-01-13 00:03:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:02 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.554 | ppl 93.93 | wps 26835.2 | wpb 556.6 | bsz 30.3 | num_updates 3448 | best_loss 6.554
2022-01-13 00:03:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 3448 updates
2022-01-13 00:03:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:03:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 69 @ 3448 updates, score 6.554) (writing took 4.591499911854044 seconds)
2022-01-13 00:03:06 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-01-13 00:03:06 | INFO | train | epoch 069 | loss 6.691 | ppl 103.3 | wps 6762.5 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 3448 | lr 1.0344e-05 | gnorm 7.666 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 682
2022-01-13 00:03:07 | INFO | fairseq.trainer | begin training epoch 70
2022-01-13 00:03:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:03:07 | INFO | train_inner | epoch 070:     12 / 50 loss=6.726, ppl=105.89, wps=3573.3, ups=2.93, wpb=1221.6, bsz=62.7, num_updates=3460, lr=1.038e-05, gnorm=7.935, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=683
2022-01-13 00:03:09 | INFO | train_inner | epoch 070:     32 / 50 loss=6.528, ppl=92.26, wps=16406.3, ups=13.63, wpb=1203.3, bsz=64, num_updates=3480, lr=1.044e-05, gnorm=7.645, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=684
2022-01-13 00:03:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:11 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.655 | ppl 100.75 | wps 24804 | wpb 556.6 | bsz 30.3 | num_updates 3498 | best_loss 6.554
2022-01-13 00:03:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 3498 updates
2022-01-13 00:03:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 70 @ 3498 updates, score 6.655) (writing took 2.7075513841118664 seconds)
2022-01-13 00:03:14 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-01-13 00:03:14 | INFO | train | epoch 070 | loss 6.615 | ppl 98.03 | wps 8547.8 | ups 6.93 | wpb 1232.9 | bsz 63.5 | num_updates 3498 | lr 1.0494e-05 | gnorm 7.604 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 689
2022-01-13 00:03:14 | INFO | fairseq.trainer | begin training epoch 71
2022-01-13 00:03:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:03:14 | INFO | train_inner | epoch 071:      2 / 50 loss=6.691, ppl=103.3, wps=4979.5, ups=3.99, wpb=1248.8, bsz=64, num_updates=3500, lr=1.05e-05, gnorm=7.38, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=689
2022-01-13 00:03:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:15 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.669 | ppl 101.79 | wps 26710.3 | wpb 556.6 | bsz 30.3 | num_updates 3500 | best_loss 6.554
2022-01-13 00:03:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3500 updates
2022-01-13 00:03:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt
2022-01-13 00:03:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt
2022-01-13 00:03:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt (epoch 71 @ 3500 updates, score 6.669) (writing took 3.828785134013742 seconds)
2022-01-13 00:03:20 | INFO | train_inner | epoch 071:     22 / 50 loss=6.569, ppl=94.96, wps=3699.3, ups=3.16, wpb=1169.3, bsz=64, num_updates=3520, lr=1.056e-05, gnorm=7.909, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=695
2022-01-13 00:03:22 | INFO | train_inner | epoch 071:     42 / 50 loss=6.61, ppl=97.65, wps=14561.6, ups=12.03, wpb=1210.1, bsz=64, num_updates=3540, lr=1.062e-05, gnorm=7.789, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=697
2022-01-13 00:03:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:23 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.584 | ppl 95.94 | wps 24738 | wpb 556.6 | bsz 30.3 | num_updates 3548 | best_loss 6.554
2022-01-13 00:03:23 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-01-13 00:03:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3548 updates
2022-01-13 00:03:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 71 @ 3548 updates, score 6.584) (writing took 2.709606857970357 seconds)
2022-01-13 00:03:26 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-01-13 00:03:26 | INFO | train | epoch 071 | loss 6.637 | ppl 99.51 | wps 5003.6 | ups 4.06 | wpb 1232.9 | bsz 63.5 | num_updates 3548 | lr 1.0644e-05 | gnorm 7.79 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 701
2022-01-13 00:03:26 | INFO | fairseq_cli.train | done training in 696.7 seconds
2022-01-13 16:04:14 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-13 16:04:14 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-01-13 16:04:16 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-01-13 16:04:16 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-01-13 16:04:16 | INFO | fairseq_cli.train | model: BARTMLModel
2022-01-13 16:04:16 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-01-13 16:04:16 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-01-13 16:04:16 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-13 16:04:17 | INFO | bartabst.data.data_utils | loaded 908 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-01-13 16:04:20 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-01-13 16:04:20 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-13 16:04:20 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-01-13 16:04:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-13 16:04:20 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-01-13 16:04:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-13 16:04:20 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-13 16:04:20 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-01-13 16:04:20 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-01-13 16:04:22 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-01-13 16:04:22 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-01-13 16:04:22 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-13 16:04:25 | INFO | bartabst.data.data_utils | loaded 3,174 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-01-13 16:04:25 | INFO | fairseq.trainer | begin training epoch 1
2022-01-13 16:04:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:04:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-01-13 16:04:26 | INFO | train_inner | epoch 001:     21 / 50 loss=17.159, ppl=146394, wps=15982, ups=13.74, wpb=1196, bsz=62.7, num_updates=20, lr=6e-08, gnorm=23.615, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=6
2022-01-13 16:04:28 | INFO | train_inner | epoch 001:     41 / 50 loss=17.296, ppl=160960, wps=16788.3, ups=13.29, wpb=1263.2, bsz=64, num_updates=40, lr=1.2e-07, gnorm=23.468, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=8
2022-01-13 16:04:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:04:29 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.132 | ppl 143673 | wps 25542.1 | wpb 556.6 | bsz 30.3 | num_updates 49
2022-01-13 16:04:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 49 updates
2022-01-13 16:04:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 49 updates, score 17.132) (writing took 5.906699588987976 seconds)
2022-01-13 16:04:35 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-13 16:04:35 | INFO | train | epoch 001 | loss 17.245 | ppl 155295 | wps 5639.8 | ups 4.67 | wpb 1219.7 | bsz 63.5 | num_updates 49 | lr 1.47e-07 | gnorm 23.417 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 15
2022-01-13 16:04:35 | INFO | fairseq.trainer | begin training epoch 2
2022-01-13 16:04:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:04:36 | INFO | train_inner | epoch 002:     11 / 50 loss=17.221, ppl=152767, wps=2724.9, ups=2.41, wpb=1130.5, bsz=62.7, num_updates=60, lr=1.8e-07, gnorm=23.114, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=16
2022-01-13 16:04:38 | INFO | train_inner | epoch 002:     31 / 50 loss=17.199, ppl=150443, wps=18228.5, ups=13.95, wpb=1306.8, bsz=64, num_updates=80, lr=2.4e-07, gnorm=22.864, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=18
2022-01-13 16:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:04:40 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 16.922 | ppl 124169 | wps 26985.5 | wpb 556.6 | bsz 30.3 | num_updates 99 | best_loss 16.922
2022-01-13 16:04:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 99 updates
2022-01-13 16:04:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 99 updates, score 16.922) (writing took 4.311357039026916 seconds)
2022-01-13 16:04:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-13 16:04:44 | INFO | train | epoch 002 | loss 17.126 | ppl 143014 | wps 7050.7 | ups 5.72 | wpb 1232.9 | bsz 63.5 | num_updates 99 | lr 2.97e-07 | gnorm 22.899 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 24
2022-01-13 16:04:44 | INFO | fairseq.trainer | begin training epoch 3
2022-01-13 16:04:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:04:44 | INFO | train_inner | epoch 003:      1 / 50 loss=17.043, ppl=135068, wps=3761.6, ups=3.05, wpb=1233.1, bsz=64, num_updates=100, lr=3e-07, gnorm=22.704, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=24
2022-01-13 16:04:46 | INFO | train_inner | epoch 003:     21 / 50 loss=16.922, ppl=124168, wps=16640.3, ups=13.61, wpb=1222.8, bsz=62.7, num_updates=120, lr=3.6e-07, gnorm=22.369, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=26
2022-01-13 16:04:47 | INFO | train_inner | epoch 003:     41 / 50 loss=16.713, ppl=107421, wps=19230.2, ups=14.28, wpb=1347.1, bsz=64, num_updates=140, lr=4.2e-07, gnorm=21.783, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=27
2022-01-13 16:04:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:04:49 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.449 | ppl 89457.4 | wps 27430.9 | wpb 556.6 | bsz 30.3 | num_updates 149 | best_loss 16.449
2022-01-13 16:04:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 149 updates
2022-01-13 16:04:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 149 updates, score 16.449) (writing took 4.084286751924083 seconds)
2022-01-13 16:04:53 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-13 16:04:53 | INFO | train | epoch 003 | loss 16.775 | ppl 112183 | wps 7112.2 | ups 5.77 | wpb 1232.9 | bsz 63.5 | num_updates 149 | lr 4.47e-07 | gnorm 22.104 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 33
2022-01-13 16:04:53 | INFO | fairseq.trainer | begin training epoch 4
2022-01-13 16:04:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:04:54 | INFO | train_inner | epoch 004:     11 / 50 loss=16.552, ppl=96075.3, wps=3424.6, ups=3.09, wpb=1109.7, bsz=62.7, num_updates=160, lr=4.8e-07, gnorm=22.008, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=33
2022-01-13 16:04:55 | INFO | train_inner | epoch 004:     31 / 50 loss=16.283, ppl=79750.9, wps=16219, ups=12.86, wpb=1261, bsz=64, num_updates=180, lr=5.4e-07, gnorm=21.031, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=35
2022-01-13 16:04:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:04:58 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 15.807 | ppl 57342.1 | wps 27509.2 | wpb 556.6 | bsz 30.3 | num_updates 199 | best_loss 15.807
2022-01-13 16:04:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 199 updates
2022-01-13 16:04:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 199 updates, score 15.807) (writing took 4.109877816168591 seconds)
2022-01-13 16:05:02 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-13 16:05:02 | INFO | train | epoch 004 | loss 16.271 | ppl 79071.1 | wps 6888.1 | ups 5.59 | wpb 1232.9 | bsz 63.5 | num_updates 199 | lr 5.97e-07 | gnorm 21.383 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 42
2022-01-13 16:05:02 | INFO | fairseq.trainer | begin training epoch 5
2022-01-13 16:05:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:02 | INFO | train_inner | epoch 005:      1 / 50 loss=16.097, ppl=70117.1, wps=3609.4, ups=2.98, wpb=1210.8, bsz=64, num_updates=200, lr=6e-07, gnorm=21.412, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=42
2022-01-13 16:05:03 | INFO | train_inner | epoch 005:     21 / 50 loss=15.771, ppl=55917.4, wps=17116.2, ups=13.85, wpb=1235.9, bsz=64, num_updates=220, lr=6.6e-07, gnorm=20.211, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=43
2022-01-13 16:05:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-01-13 16:05:05 | INFO | train_inner | epoch 005:     42 / 50 loss=15.559, ppl=48277.4, wps=18462.5, ups=14.03, wpb=1316.2, bsz=64, num_updates=240, lr=7.2e-07, gnorm=19.435, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=45
2022-01-13 16:05:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:06 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 15.243 | ppl 38786.9 | wps 29118.5 | wpb 556.6 | bsz 30.3 | num_updates 248 | best_loss 15.243
2022-01-13 16:05:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 248 updates
2022-01-13 16:05:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 248 updates, score 15.243) (writing took 3.950551080983132 seconds)
2022-01-13 16:05:10 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-13 16:05:10 | INFO | train | epoch 005 | loss 15.636 | ppl 50916.4 | wps 7237.5 | ups 5.82 | wpb 1243 | bsz 63.5 | num_updates 248 | lr 7.44e-07 | gnorm 20.049 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 50
2022-01-13 16:05:10 | INFO | fairseq.trainer | begin training epoch 6
2022-01-13 16:05:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:11 | INFO | train_inner | epoch 006:     12 / 50 loss=15.337, ppl=41381, wps=3835.3, ups=3.15, wpb=1217.2, bsz=62.7, num_updates=260, lr=7.8e-07, gnorm=19.805, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=51
2022-01-13 16:05:12 | INFO | train_inner | epoch 006:     32 / 50 loss=14.997, ppl=32702.2, wps=18977.4, ups=14.47, wpb=1311.7, bsz=62.7, num_updates=280, lr=8.4e-07, gnorm=19.077, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=52
2022-01-13 16:05:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:14 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 14.581 | ppl 24503.1 | wps 28916.4 | wpb 556.6 | bsz 30.3 | num_updates 298 | best_loss 14.581
2022-01-13 16:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 298 updates
2022-01-13 16:05:14 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 298 updates, score 14.581) (writing took 4.286012321943417 seconds)
2022-01-13 16:05:19 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-13 16:05:19 | INFO | train | epoch 006 | loss 15.033 | ppl 33520 | wps 7152.8 | ups 5.8 | wpb 1232.9 | bsz 63.5 | num_updates 298 | lr 8.94e-07 | gnorm 18.987 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 59
2022-01-13 16:05:19 | INFO | fairseq.trainer | begin training epoch 7
2022-01-13 16:05:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:19 | INFO | train_inner | epoch 007:      2 / 50 loss=14.896, ppl=30486.8, wps=3494.6, ups=3.07, wpb=1136.9, bsz=64, num_updates=300, lr=9e-07, gnorm=18.768, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=59
2022-01-13 16:05:20 | INFO | train_inner | epoch 007:     22 / 50 loss=14.558, ppl=24115.6, wps=18699.9, ups=14.22, wpb=1315.3, bsz=64, num_updates=320, lr=9.6e-07, gnorm=17.902, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=60
2022-01-13 16:05:22 | INFO | train_inner | epoch 007:     42 / 50 loss=14.268, ppl=19731.4, wps=15578.2, ups=14.8, wpb=1052.3, bsz=62.7, num_updates=340, lr=1.02e-06, gnorm=19.044, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=62
2022-01-13 16:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:23 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 13.939 | ppl 15700.3 | wps 29150.7 | wpb 556.6 | bsz 30.3 | num_updates 348 | best_loss 13.939
2022-01-13 16:05:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 348 updates
2022-01-13 16:05:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 348 updates, score 13.939) (writing took 3.853371197823435 seconds)
2022-01-13 16:05:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-13 16:05:27 | INFO | train | epoch 007 | loss 14.362 | ppl 21061.2 | wps 7559 | ups 6.13 | wpb 1232.9 | bsz 63.5 | num_updates 348 | lr 1.044e-06 | gnorm 18.136 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 67
2022-01-13 16:05:27 | INFO | fairseq.trainer | begin training epoch 8
2022-01-13 16:05:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:28 | INFO | train_inner | epoch 008:     12 / 50 loss=13.982, ppl=16177, wps=4613.2, ups=3.3, wpb=1399.5, bsz=64, num_updates=360, lr=1.08e-06, gnorm=18.713, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=68
2022-01-13 16:05:29 | INFO | train_inner | epoch 008:     32 / 50 loss=13.673, ppl=13065.5, wps=18309.4, ups=14.69, wpb=1246.2, bsz=64, num_updates=380, lr=1.14e-06, gnorm=16.927, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=69
2022-01-13 16:05:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:31 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 13.325 | ppl 10265.3 | wps 30046.1 | wpb 556.6 | bsz 30.3 | num_updates 398 | best_loss 13.325
2022-01-13 16:05:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 398 updates
2022-01-13 16:05:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 398 updates, score 13.325) (writing took 4.423732958966866 seconds)
2022-01-13 16:05:36 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-13 16:05:36 | INFO | train | epoch 008 | loss 13.671 | ppl 13041.4 | wps 7070.2 | ups 5.73 | wpb 1232.9 | bsz 63.5 | num_updates 398 | lr 1.194e-06 | gnorm 17.703 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 76
2022-01-13 16:05:36 | INFO | fairseq.trainer | begin training epoch 9
2022-01-13 16:05:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:36 | INFO | train_inner | epoch 009:      2 / 50 loss=13.445, ppl=11152.4, wps=3388.7, ups=2.86, wpb=1185.2, bsz=62.7, num_updates=400, lr=1.2e-06, gnorm=16.722, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=76
2022-01-13 16:05:38 | INFO | train_inner | epoch 009:     22 / 50 loss=13.342, ppl=10380.7, wps=17530, ups=14.16, wpb=1238, bsz=62.7, num_updates=420, lr=1.26e-06, gnorm=17.348, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=77
2022-01-13 16:05:39 | INFO | train_inner | epoch 009:     42 / 50 loss=13.019, ppl=8299.91, wps=17288.2, ups=14.17, wpb=1220, bsz=64, num_updates=440, lr=1.32e-06, gnorm=14.824, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=79
2022-01-13 16:05:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:40 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 12.838 | ppl 7324.38 | wps 27468.9 | wpb 556.6 | bsz 30.3 | num_updates 448 | best_loss 12.838
2022-01-13 16:05:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 448 updates
2022-01-13 16:05:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 448 updates, score 12.838) (writing took 3.9607309179846197 seconds)
2022-01-13 16:05:44 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-13 16:05:44 | INFO | train | epoch 009 | loss 13.134 | ppl 8992.19 | wps 7398.4 | ups 6 | wpb 1232.9 | bsz 63.5 | num_updates 448 | lr 1.344e-06 | gnorm 15.786 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 84
2022-01-13 16:05:44 | INFO | fairseq.trainer | begin training epoch 10
2022-01-13 16:05:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:45 | INFO | train_inner | epoch 010:     12 / 50 loss=12.802, ppl=7139, wps=3932.3, ups=3.23, wpb=1217.8, bsz=62.7, num_updates=460, lr=1.38e-06, gnorm=13.664, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=85
2022-01-13 16:05:47 | INFO | train_inner | epoch 010:     32 / 50 loss=12.768, ppl=6972.94, wps=17250.1, ups=13.99, wpb=1232.8, bsz=64, num_updates=480, lr=1.44e-06, gnorm=12.989, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=87
2022-01-13 16:05:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:49 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 12.445 | ppl 5577.56 | wps 28296.4 | wpb 556.6 | bsz 30.3 | num_updates 498 | best_loss 12.445
2022-01-13 16:05:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 498 updates
2022-01-13 16:05:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 498 updates, score 12.445) (writing took 5.133760615019128 seconds)
2022-01-13 16:05:54 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-13 16:05:55 | INFO | train | epoch 010 | loss 12.653 | ppl 6442.06 | wps 6492.7 | ups 5.27 | wpb 1232.9 | bsz 63.5 | num_updates 498 | lr 1.494e-06 | gnorm 12.537 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 94
2022-01-13 16:05:55 | INFO | fairseq.trainer | begin training epoch 11
2022-01-13 16:05:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:55 | INFO | train_inner | epoch 011:      2 / 50 loss=12.5, ppl=5793.55, wps=2798.6, ups=2.34, wpb=1197, bsz=64, num_updates=500, lr=1.5e-06, gnorm=11.699, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=95
2022-01-13 16:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:56 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.396 | ppl 5389.64 | wps 27319.1 | wpb 556.6 | bsz 30.3 | num_updates 500 | best_loss 12.396
2022-01-13 16:05:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 500 updates
2022-01-13 16:05:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt
2022-01-13 16:05:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt
2022-01-13 16:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt (epoch 11 @ 500 updates, score 12.396) (writing took 6.879543789895251 seconds)
2022-01-13 16:06:05 | INFO | train_inner | epoch 011:     22 / 50 loss=12.339, ppl=5181.57, wps=2810.1, ups=2.15, wpb=1310, bsz=64, num_updates=520, lr=1.56e-06, gnorm=10.909, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=104
2022-01-13 16:06:06 | INFO | train_inner | epoch 011:     42 / 50 loss=12.22, ppl=4771.57, wps=15190.4, ups=12.77, wpb=1189.1, bsz=64, num_updates=540, lr=1.62e-06, gnorm=11.548, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=106
2022-01-13 16:06:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:07 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.046 | ppl 4227.29 | wps 26647.7 | wpb 556.6 | bsz 30.3 | num_updates 548 | best_loss 12.046
2022-01-13 16:06:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 548 updates
2022-01-13 16:06:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 548 updates, score 12.046) (writing took 4.2199249600525945 seconds)
2022-01-13 16:06:12 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-13 16:06:12 | INFO | train | epoch 011 | loss 12.283 | ppl 4984.74 | wps 3663.8 | ups 2.97 | wpb 1232.9 | bsz 63.5 | num_updates 548 | lr 1.644e-06 | gnorm 11.357 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 112
2022-01-13 16:06:12 | INFO | fairseq.trainer | begin training epoch 12
2022-01-13 16:06:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:13 | INFO | train_inner | epoch 012:     12 / 50 loss=12.14, ppl=4512.23, wps=3669.4, ups=3.02, wpb=1214.3, bsz=61.4, num_updates=560, lr=1.68e-06, gnorm=11.904, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=113
2022-01-13 16:06:14 | INFO | train_inner | epoch 012:     32 / 50 loss=11.944, ppl=3939.08, wps=16126.2, ups=13.21, wpb=1221.2, bsz=64, num_updates=580, lr=1.74e-06, gnorm=11.565, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=114
2022-01-13 16:06:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:16 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.751 | ppl 3447.89 | wps 28540 | wpb 556.6 | bsz 30.3 | num_updates 598 | best_loss 11.751
2022-01-13 16:06:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 598 updates
2022-01-13 16:06:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 598 updates, score 11.751) (writing took 4.101295632077381 seconds)
2022-01-13 16:06:20 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-13 16:06:20 | INFO | train | epoch 012 | loss 11.971 | ppl 4014.07 | wps 7091.9 | ups 5.75 | wpb 1232.9 | bsz 63.5 | num_updates 598 | lr 1.794e-06 | gnorm 12.637 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 120
2022-01-13 16:06:20 | INFO | fairseq.trainer | begin training epoch 13
2022-01-13 16:06:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:21 | INFO | train_inner | epoch 013:      2 / 50 loss=11.855, ppl=3705.11, wps=3836, ups=3.11, wpb=1232.8, bsz=64, num_updates=600, lr=1.8e-06, gnorm=13.871, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=121
2022-01-13 16:06:22 | INFO | train_inner | epoch 013:     22 / 50 loss=11.757, ppl=3459.88, wps=16800.1, ups=13.84, wpb=1213.5, bsz=62.7, num_updates=620, lr=1.86e-06, gnorm=10.483, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=122
2022-01-13 16:06:24 | INFO | train_inner | epoch 013:     42 / 50 loss=11.588, ppl=3078.23, wps=14068.7, ups=11.4, wpb=1234.5, bsz=64, num_updates=640, lr=1.92e-06, gnorm=9.733, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=124
2022-01-13 16:06:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:25 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.506 | ppl 2908.49 | wps 28868.6 | wpb 556.6 | bsz 30.3 | num_updates 648 | best_loss 11.506
2022-01-13 16:06:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 648 updates
2022-01-13 16:06:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 648 updates, score 11.506) (writing took 5.000021601095796 seconds)
2022-01-13 16:06:30 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-13 16:06:30 | INFO | train | epoch 013 | loss 11.719 | ppl 3371.76 | wps 6296 | ups 5.11 | wpb 1232.9 | bsz 63.5 | num_updates 648 | lr 1.944e-06 | gnorm 10.011 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 130
2022-01-13 16:06:30 | INFO | fairseq.trainer | begin training epoch 14
2022-01-13 16:06:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:31 | INFO | train_inner | epoch 014:     12 / 50 loss=11.784, ppl=3526.14, wps=3444.5, ups=2.74, wpb=1259.2, bsz=64, num_updates=660, lr=1.98e-06, gnorm=11.754, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=131
2022-01-13 16:06:33 | INFO | train_inner | epoch 014:     32 / 50 loss=11.538, ppl=2972.65, wps=18085.2, ups=13.61, wpb=1329, bsz=62.7, num_updates=680, lr=2.04e-06, gnorm=9.592, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=133
2022-01-13 16:06:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:35 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.335 | ppl 2583.71 | wps 26225.2 | wpb 556.6 | bsz 30.3 | num_updates 698 | best_loss 11.335
2022-01-13 16:06:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 698 updates
2022-01-13 16:06:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 698 updates, score 11.335) (writing took 4.299873220035806 seconds)
2022-01-13 16:06:39 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-13 16:06:39 | INFO | train | epoch 014 | loss 11.504 | ppl 2903.88 | wps 6949.9 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 698 | lr 2.094e-06 | gnorm 10.531 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 139
2022-01-13 16:06:39 | INFO | fairseq.trainer | begin training epoch 15
2022-01-13 16:06:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:39 | INFO | train_inner | epoch 015:      2 / 50 loss=11.364, ppl=2635.67, wps=3404.2, ups=3, wpb=1136.4, bsz=64, num_updates=700, lr=2.1e-06, gnorm=9.71, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=139
2022-01-13 16:06:41 | INFO | train_inner | epoch 015:     22 / 50 loss=11.366, ppl=2639.55, wps=17012.7, ups=14.31, wpb=1188.8, bsz=64, num_updates=720, lr=2.16e-06, gnorm=9.584, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=141
2022-01-13 16:06:42 | INFO | train_inner | epoch 015:     42 / 50 loss=11.254, ppl=2442.64, wps=18913, ups=13.82, wpb=1368.7, bsz=64, num_updates=740, lr=2.22e-06, gnorm=8.667, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=142
2022-01-13 16:06:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:44 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.099 | ppl 2193.77 | wps 26439.4 | wpb 556.6 | bsz 30.3 | num_updates 748 | best_loss 11.099
2022-01-13 16:06:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 748 updates
2022-01-13 16:06:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 748 updates, score 11.099) (writing took 4.298957463121042 seconds)
2022-01-13 16:06:48 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-13 16:06:48 | INFO | train | epoch 015 | loss 11.293 | ppl 2508.52 | wps 7034.6 | ups 5.71 | wpb 1232.9 | bsz 63.5 | num_updates 748 | lr 2.244e-06 | gnorm 9.173 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 148
2022-01-13 16:06:48 | INFO | fairseq.trainer | begin training epoch 16
2022-01-13 16:06:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:49 | INFO | train_inner | epoch 016:     12 / 50 loss=11.251, ppl=2436.61, wps=3436.8, ups=3.02, wpb=1138.8, bsz=62.7, num_updates=760, lr=2.28e-06, gnorm=8.97, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=149
2022-01-13 16:06:50 | INFO | train_inner | epoch 016:     32 / 50 loss=11.045, ppl=2112.64, wps=16052.3, ups=14.21, wpb=1129.2, bsz=64, num_updates=780, lr=2.34e-06, gnorm=9.986, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=150
2022-01-13 16:06:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:52 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.946 | ppl 1973.05 | wps 26753.4 | wpb 556.6 | bsz 30.3 | num_updates 798 | best_loss 10.946
2022-01-13 16:06:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 798 updates
2022-01-13 16:06:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 798 updates, score 10.946) (writing took 4.205898888874799 seconds)
2022-01-13 16:06:56 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-13 16:06:56 | INFO | train | epoch 016 | loss 11.149 | ppl 2271.5 | wps 7206.1 | ups 5.84 | wpb 1232.9 | bsz 63.5 | num_updates 798 | lr 2.394e-06 | gnorm 9.108 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 156
2022-01-13 16:06:56 | INFO | fairseq.trainer | begin training epoch 17
2022-01-13 16:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:57 | INFO | train_inner | epoch 017:      2 / 50 loss=11.146, ppl=2265.88, wps=4084.1, ups=3.11, wpb=1312.2, bsz=62.7, num_updates=800, lr=2.4e-06, gnorm=8.511, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=157
2022-01-13 16:06:58 | INFO | train_inner | epoch 017:     22 / 50 loss=10.91, ppl=1923.48, wps=16704.3, ups=14.39, wpb=1160.7, bsz=62.7, num_updates=820, lr=2.46e-06, gnorm=8.976, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=158
2022-01-13 16:06:59 | INFO | train_inner | epoch 017:     42 / 50 loss=11.13, ppl=2240.58, wps=20189.9, ups=13.74, wpb=1469.3, bsz=64, num_updates=840, lr=2.52e-06, gnorm=8.087, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=159
2022-01-13 16:07:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:01 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.77 | ppl 1745.71 | wps 26281.8 | wpb 556.6 | bsz 30.3 | num_updates 848 | best_loss 10.77
2022-01-13 16:07:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 848 updates
2022-01-13 16:07:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 848 updates, score 10.77) (writing took 4.110174420988187 seconds)
2022-01-13 16:07:05 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-13 16:07:05 | INFO | train | epoch 017 | loss 10.988 | ppl 2031.55 | wps 7250.3 | ups 5.88 | wpb 1232.9 | bsz 63.5 | num_updates 848 | lr 2.544e-06 | gnorm 8.635 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 165
2022-01-13 16:07:05 | INFO | fairseq.trainer | begin training epoch 18
2022-01-13 16:07:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:07:06 | INFO | train_inner | epoch 018:     12 / 50 loss=10.811, ppl=1796.58, wps=3415.6, ups=3.14, wpb=1087.5, bsz=64, num_updates=860, lr=2.58e-06, gnorm=8.306, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=166
2022-01-13 16:07:07 | INFO | train_inner | epoch 018:     32 / 50 loss=10.884, ppl=1889.38, wps=18001.4, ups=14.2, wpb=1268, bsz=62.7, num_updates=880, lr=2.64e-06, gnorm=9.078, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=167
2022-01-13 16:07:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:09 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.576 | ppl 1526.65 | wps 26991.8 | wpb 556.6 | bsz 30.3 | num_updates 898 | best_loss 10.576
2022-01-13 16:07:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 898 updates
2022-01-13 16:07:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 898 updates, score 10.576) (writing took 4.190769501030445 seconds)
2022-01-13 16:07:14 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-13 16:07:14 | INFO | train | epoch 018 | loss 10.831 | ppl 1821.56 | wps 7172.2 | ups 5.82 | wpb 1232.9 | bsz 63.5 | num_updates 898 | lr 2.694e-06 | gnorm 8.465 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 173
2022-01-13 16:07:14 | INFO | fairseq.trainer | begin training epoch 19
2022-01-13 16:07:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:07:14 | INFO | train_inner | epoch 019:      2 / 50 loss=10.767, ppl=1742.59, wps=3609.8, ups=3.1, wpb=1164.9, bsz=64, num_updates=900, lr=2.7e-06, gnorm=8.402, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=174
2022-01-13 16:07:15 | INFO | train_inner | epoch 019:     22 / 50 loss=10.706, ppl=1670.31, wps=17261.7, ups=14.24, wpb=1212.3, bsz=62.7, num_updates=920, lr=2.76e-06, gnorm=8.029, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=175
2022-01-13 16:07:17 | INFO | train_inner | epoch 019:     42 / 50 loss=10.779, ppl=1757.66, wps=18074.1, ups=13.89, wpb=1300.9, bsz=64, num_updates=940, lr=2.82e-06, gnorm=8.294, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=176
2022-01-13 16:07:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:18 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 10.468 | ppl 1416.62 | wps 26580.8 | wpb 556.6 | bsz 30.3 | num_updates 948 | best_loss 10.468
2022-01-13 16:07:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 948 updates
2022-01-13 16:07:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 948 updates, score 10.468) (writing took 4.056778114056215 seconds)
2022-01-13 16:07:22 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-13 16:07:22 | INFO | train | epoch 019 | loss 10.697 | ppl 1659.98 | wps 7313.8 | ups 5.93 | wpb 1232.9 | bsz 63.5 | num_updates 948 | lr 2.844e-06 | gnorm 8.25 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 182
2022-01-13 16:07:22 | INFO | fairseq.trainer | begin training epoch 20
2022-01-13 16:07:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:07:23 | INFO | train_inner | epoch 020:     12 / 50 loss=10.408, ppl=1358.86, wps=3650.6, ups=3.19, wpb=1145.7, bsz=64, num_updates=960, lr=2.88e-06, gnorm=8.498, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=183
2022-01-13 16:07:24 | INFO | train_inner | epoch 020:     32 / 50 loss=10.645, ppl=1601.75, wps=17553.3, ups=14.16, wpb=1240, bsz=62.7, num_updates=980, lr=2.94e-06, gnorm=7.943, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=184
2022-01-13 16:07:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:26 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 10.37 | ppl 1323 | wps 25591.8 | wpb 556.6 | bsz 30.3 | num_updates 998 | best_loss 10.37
2022-01-13 16:07:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 998 updates
2022-01-13 16:07:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 998 updates, score 10.37) (writing took 3.997481770813465 seconds)
2022-01-13 16:07:30 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-13 16:07:30 | INFO | train | epoch 020 | loss 10.568 | ppl 1517.83 | wps 7364.6 | ups 5.97 | wpb 1232.9 | bsz 63.5 | num_updates 998 | lr 2.994e-06 | gnorm 7.986 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 190
2022-01-13 16:07:30 | INFO | fairseq.trainer | begin training epoch 21
2022-01-13 16:07:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:07:31 | INFO | train_inner | epoch 021:      2 / 50 loss=10.55, ppl=1499.7, wps=4030.3, ups=3.16, wpb=1273.9, bsz=64, num_updates=1000, lr=3e-06, gnorm=7.78, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=191
2022-01-13 16:07:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:31 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.298 | ppl 1259.29 | wps 27011.2 | wpb 556.6 | bsz 30.3 | num_updates 1000 | best_loss 10.298
2022-01-13 16:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1000 updates
2022-01-13 16:07:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt
2022-01-13 16:07:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt
2022-01-13 16:07:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt (epoch 21 @ 1000 updates, score 10.298) (writing took 9.162504152860492 seconds)
2022-01-13 16:07:42 | INFO | train_inner | epoch 021:     22 / 50 loss=10.495, ppl=1443.49, wps=2214.3, ups=1.7, wpb=1300.8, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=7.772, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=202
2022-01-13 16:07:44 | INFO | train_inner | epoch 021:     42 / 50 loss=10.42, ppl=1370.16, wps=15273.6, ups=12.53, wpb=1219.5, bsz=64, num_updates=1040, lr=3.12e-06, gnorm=7.891, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=204
2022-01-13 16:07:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:45 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.162 | ppl 1145.6 | wps 27897.3 | wpb 556.6 | bsz 30.3 | num_updates 1048 | best_loss 10.162
2022-01-13 16:07:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1048 updates
2022-01-13 16:07:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 1048 updates, score 10.162) (writing took 4.235683298902586 seconds)
2022-01-13 16:07:50 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-13 16:07:50 | INFO | train | epoch 021 | loss 10.453 | ppl 1401.98 | wps 3207.5 | ups 2.6 | wpb 1232.9 | bsz 63.5 | num_updates 1048 | lr 3.144e-06 | gnorm 7.943 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 210
2022-01-13 16:07:50 | INFO | fairseq.trainer | begin training epoch 22
2022-01-13 16:07:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:07:51 | INFO | train_inner | epoch 022:     12 / 50 loss=10.37, ppl=1322.98, wps=3596.5, ups=3.01, wpb=1195.2, bsz=62.7, num_updates=1060, lr=3.18e-06, gnorm=8.135, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=211
2022-01-13 16:07:52 | INFO | train_inner | epoch 022:     32 / 50 loss=10.341, ppl=1297.05, wps=18189.6, ups=14.37, wpb=1265.5, bsz=64, num_updates=1080, lr=3.24e-06, gnorm=7.394, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=212
2022-01-13 16:07:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:54 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 10.077 | ppl 1080.39 | wps 27714.2 | wpb 556.6 | bsz 30.3 | num_updates 1098 | best_loss 10.077
2022-01-13 16:07:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1098 updates
2022-01-13 16:07:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 1098 updates, score 10.077) (writing took 5.392136902781203 seconds)
2022-01-13 16:07:59 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-13 16:07:59 | INFO | train | epoch 022 | loss 10.317 | ppl 1275.64 | wps 6308.4 | ups 5.12 | wpb 1232.9 | bsz 63.5 | num_updates 1098 | lr 3.294e-06 | gnorm 7.911 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 219
2022-01-13 16:07:59 | INFO | fairseq.trainer | begin training epoch 23
2022-01-13 16:07:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:00 | INFO | train_inner | epoch 023:      2 / 50 loss=10.312, ppl=1270.95, wps=3065.8, ups=2.63, wpb=1164.5, bsz=62.7, num_updates=1100, lr=3.3e-06, gnorm=8.39, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=220
2022-01-13 16:08:01 | INFO | train_inner | epoch 023:     22 / 50 loss=10.177, ppl=1157.55, wps=16238.3, ups=13.28, wpb=1222.5, bsz=64, num_updates=1120, lr=3.36e-06, gnorm=7.806, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=221
2022-01-13 16:08:03 | INFO | train_inner | epoch 023:     42 / 50 loss=10.255, ppl=1221.88, wps=19375.1, ups=14.14, wpb=1370.7, bsz=64, num_updates=1140, lr=3.42e-06, gnorm=7.533, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=222
2022-01-13 16:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:04 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 10.007 | ppl 1028.67 | wps 27370.5 | wpb 556.6 | bsz 30.3 | num_updates 1148 | best_loss 10.007
2022-01-13 16:08:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1148 updates
2022-01-13 16:08:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 1148 updates, score 10.007) (writing took 4.165350906085223 seconds)
2022-01-13 16:08:08 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-13 16:08:08 | INFO | train | epoch 023 | loss 10.205 | ppl 1180.3 | wps 7068.2 | ups 5.73 | wpb 1232.9 | bsz 63.5 | num_updates 1148 | lr 3.444e-06 | gnorm 7.872 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 228
2022-01-13 16:08:08 | INFO | fairseq.trainer | begin training epoch 24
2022-01-13 16:08:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:09 | INFO | train_inner | epoch 024:     12 / 50 loss=10.067, ppl=1072.99, wps=3057.7, ups=3.08, wpb=994, bsz=62.7, num_updates=1160, lr=3.48e-06, gnorm=8.31, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=229
2022-01-13 16:08:11 | INFO | train_inner | epoch 024:     32 / 50 loss=10.122, ppl=1114.56, wps=19354.5, ups=13.58, wpb=1425.5, bsz=62.7, num_updates=1180, lr=3.54e-06, gnorm=7.221, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=230
2022-01-13 16:08:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:13 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.844 | ppl 918.93 | wps 27018 | wpb 556.6 | bsz 30.3 | num_updates 1198 | best_loss 9.844
2022-01-13 16:08:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1198 updates
2022-01-13 16:08:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 1198 updates, score 9.844) (writing took 4.539449128089473 seconds)
2022-01-13 16:08:17 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-13 16:08:18 | INFO | train | epoch 024 | loss 10.053 | ppl 1062.49 | wps 6883 | ups 5.58 | wpb 1232.9 | bsz 63.5 | num_updates 1198 | lr 3.594e-06 | gnorm 7.597 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 237
2022-01-13 16:08:18 | INFO | fairseq.trainer | begin training epoch 25
2022-01-13 16:08:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:18 | INFO | train_inner | epoch 025:      2 / 50 loss=9.972, ppl=1004.14, wps=3160.9, ups=2.71, wpb=1164.8, bsz=64, num_updates=1200, lr=3.6e-06, gnorm=7.785, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=238
2022-01-13 16:08:19 | INFO | train_inner | epoch 025:     22 / 50 loss=10.077, ppl=1079.87, wps=18231.5, ups=13.69, wpb=1331.4, bsz=62.7, num_updates=1220, lr=3.66e-06, gnorm=7.686, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=239
2022-01-13 16:08:21 | INFO | train_inner | epoch 025:     42 / 50 loss=9.954, ppl=991.78, wps=15680.6, ups=13.23, wpb=1185.5, bsz=64, num_updates=1240, lr=3.72e-06, gnorm=7.555, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=241
2022-01-13 16:08:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:22 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.688 | ppl 824.62 | wps 27998.8 | wpb 556.6 | bsz 30.3 | num_updates 1248 | best_loss 9.688
2022-01-13 16:08:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1248 updates
2022-01-13 16:08:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 1248 updates, score 9.688) (writing took 4.02902780007571 seconds)
2022-01-13 16:08:26 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-13 16:08:26 | INFO | train | epoch 025 | loss 9.965 | ppl 999.15 | wps 7214.8 | ups 5.85 | wpb 1232.9 | bsz 63.5 | num_updates 1248 | lr 3.744e-06 | gnorm 7.66 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 246
2022-01-13 16:08:26 | INFO | fairseq.trainer | begin training epoch 26
2022-01-13 16:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:27 | INFO | train_inner | epoch 026:     12 / 50 loss=9.851, ppl=923.41, wps=3918.3, ups=3.18, wpb=1231.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=7.752, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=247
2022-01-13 16:08:29 | INFO | train_inner | epoch 026:     32 / 50 loss=9.812, ppl=899.17, wps=17144.9, ups=14.6, wpb=1174.3, bsz=64, num_updates=1280, lr=3.84e-06, gnorm=7.481, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=248
2022-01-13 16:08:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:31 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.575 | ppl 762.84 | wps 26759.6 | wpb 556.6 | bsz 30.3 | num_updates 1298 | best_loss 9.575
2022-01-13 16:08:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1298 updates
2022-01-13 16:08:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 1298 updates, score 9.575) (writing took 4.224266140954569 seconds)
2022-01-13 16:08:35 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-13 16:08:35 | INFO | train | epoch 026 | loss 9.85 | ppl 922.87 | wps 7164 | ups 5.81 | wpb 1232.9 | bsz 63.5 | num_updates 1298 | lr 3.894e-06 | gnorm 7.53 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 255
2022-01-13 16:08:35 | INFO | fairseq.trainer | begin training epoch 27
2022-01-13 16:08:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:35 | INFO | train_inner | epoch 027:      2 / 50 loss=9.789, ppl=884.85, wps=3855.1, ups=3.08, wpb=1252.9, bsz=64, num_updates=1300, lr=3.9e-06, gnorm=7.594, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=255
2022-01-13 16:08:37 | INFO | train_inner | epoch 027:     22 / 50 loss=9.88, ppl=942.3, wps=16714.2, ups=13.43, wpb=1244.4, bsz=64, num_updates=1320, lr=3.96e-06, gnorm=7.444, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=256
2022-01-13 16:08:38 | INFO | train_inner | epoch 027:     42 / 50 loss=9.866, ppl=933.26, wps=18015.5, ups=14.17, wpb=1271.2, bsz=62.7, num_updates=1340, lr=4.02e-06, gnorm=8.862, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=258
2022-01-13 16:08:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:39 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.503 | ppl 725.35 | wps 27187.1 | wpb 556.6 | bsz 30.3 | num_updates 1348 | best_loss 9.503
2022-01-13 16:08:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1348 updates
2022-01-13 16:08:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 1348 updates, score 9.503) (writing took 4.56431334791705 seconds)
2022-01-13 16:08:44 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-13 16:08:44 | INFO | train | epoch 027 | loss 9.78 | ppl 878.96 | wps 6836.1 | ups 5.54 | wpb 1232.9 | bsz 63.5 | num_updates 1348 | lr 4.044e-06 | gnorm 8.138 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 264
2022-01-13 16:08:44 | INFO | fairseq.trainer | begin training epoch 28
2022-01-13 16:08:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:45 | INFO | train_inner | epoch 028:     12 / 50 loss=9.496, ppl=722.02, wps=3279.4, ups=2.94, wpb=1113.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=8.056, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=265
2022-01-13 16:08:46 | INFO | train_inner | epoch 028:     32 / 50 loss=9.724, ppl=845.63, wps=16865.1, ups=13.8, wpb=1222.3, bsz=64, num_updates=1380, lr=4.14e-06, gnorm=7.729, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=266
2022-01-13 16:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:48 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.378 | ppl 665.44 | wps 26741.6 | wpb 556.6 | bsz 30.3 | num_updates 1398 | best_loss 9.378
2022-01-13 16:08:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1398 updates
2022-01-13 16:08:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 1398 updates, score 9.378) (writing took 4.184823209187016 seconds)
2022-01-13 16:08:52 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-13 16:08:52 | INFO | train | epoch 028 | loss 9.662 | ppl 810.1 | wps 7169.3 | ups 5.82 | wpb 1232.9 | bsz 63.5 | num_updates 1398 | lr 4.194e-06 | gnorm 7.946 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 272
2022-01-13 16:08:52 | INFO | fairseq.trainer | begin training epoch 29
2022-01-13 16:08:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:53 | INFO | train_inner | epoch 029:      2 / 50 loss=9.669, ppl=813.92, wps=4210.8, ups=3.08, wpb=1367.7, bsz=64, num_updates=1400, lr=4.2e-06, gnorm=7.827, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=273
2022-01-13 16:08:54 | INFO | train_inner | epoch 029:     22 / 50 loss=9.513, ppl=730.82, wps=16695.9, ups=14.32, wpb=1166, bsz=64, num_updates=1420, lr=4.26e-06, gnorm=7.468, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=274
2022-01-13 16:08:56 | INFO | train_inner | epoch 029:     42 / 50 loss=9.602, ppl=777.39, wps=17190.2, ups=13.14, wpb=1308.7, bsz=62.7, num_updates=1440, lr=4.32e-06, gnorm=7.71, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=276
2022-01-13 16:08:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:57 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.324 | ppl 640.96 | wps 25006 | wpb 556.6 | bsz 30.3 | num_updates 1448 | best_loss 9.324
2022-01-13 16:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1448 updates
2022-01-13 16:08:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 1448 updates, score 9.324) (writing took 4.171758497133851 seconds)
2022-01-13 16:09:01 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-13 16:09:01 | INFO | train | epoch 029 | loss 9.536 | ppl 742.55 | wps 7074.6 | ups 5.74 | wpb 1232.9 | bsz 63.5 | num_updates 1448 | lr 4.344e-06 | gnorm 7.551 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 281
2022-01-13 16:09:01 | INFO | fairseq.trainer | begin training epoch 30
2022-01-13 16:09:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:02 | INFO | train_inner | epoch 030:     12 / 50 loss=9.507, ppl=727.58, wps=3910.9, ups=3.08, wpb=1269.3, bsz=64, num_updates=1460, lr=4.38e-06, gnorm=7.389, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=282
2022-01-13 16:09:04 | INFO | train_inner | epoch 030:     32 / 50 loss=9.384, ppl=668.08, wps=15949.3, ups=13.61, wpb=1171.5, bsz=64, num_updates=1480, lr=4.44e-06, gnorm=7.701, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=284
2022-01-13 16:09:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:06 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.248 | ppl 608.12 | wps 28662.7 | wpb 556.6 | bsz 30.3 | num_updates 1498 | best_loss 9.248
2022-01-13 16:09:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1498 updates
2022-01-13 16:09:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 30 @ 1498 updates, score 9.248) (writing took 4.14149253978394 seconds)
2022-01-13 16:09:10 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-13 16:09:10 | INFO | train | epoch 030 | loss 9.409 | ppl 679.85 | wps 7225.8 | ups 5.86 | wpb 1232.9 | bsz 63.5 | num_updates 1498 | lr 4.494e-06 | gnorm 7.598 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 290
2022-01-13 16:09:10 | INFO | fairseq.trainer | begin training epoch 31
2022-01-13 16:09:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:10 | INFO | train_inner | epoch 031:      2 / 50 loss=9.278, ppl=620.82, wps=3751.6, ups=3.15, wpb=1189.5, bsz=62.7, num_updates=1500, lr=4.5e-06, gnorm=7.758, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=290
2022-01-13 16:09:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:11 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.218 | ppl 595.34 | wps 27220.4 | wpb 556.6 | bsz 30.3 | num_updates 1500 | best_loss 9.218
2022-01-13 16:09:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1500 updates
2022-01-13 16:09:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt
2022-01-13 16:09:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt
2022-01-13 16:09:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt (epoch 31 @ 1500 updates, score 9.218) (writing took 10.45356583292596 seconds)
2022-01-13 16:09:23 | INFO | train_inner | epoch 031:     22 / 50 loss=9.421, ppl=685.44, wps=2025, ups=1.53, wpb=1321.7, bsz=62.7, num_updates=1520, lr=4.56e-06, gnorm=7.377, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=303
2022-01-13 16:09:25 | INFO | train_inner | epoch 031:     42 / 50 loss=9.218, ppl=595.48, wps=14570.1, ups=12.26, wpb=1188.6, bsz=64, num_updates=1540, lr=4.62e-06, gnorm=7.754, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=305
2022-01-13 16:09:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:26 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.111 | ppl 552.86 | wps 27716.4 | wpb 556.6 | bsz 30.3 | num_updates 1548 | best_loss 9.111
2022-01-13 16:09:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1548 updates
2022-01-13 16:09:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 1548 updates, score 9.111) (writing took 4.13538451702334 seconds)
2022-01-13 16:09:30 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-13 16:09:30 | INFO | train | epoch 031 | loss 9.286 | ppl 624.08 | wps 3013.4 | ups 2.44 | wpb 1232.9 | bsz 63.5 | num_updates 1548 | lr 4.644e-06 | gnorm 7.536 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 310
2022-01-13 16:09:30 | INFO | fairseq.trainer | begin training epoch 32
2022-01-13 16:09:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:31 | INFO | train_inner | epoch 032:     12 / 50 loss=9.229, ppl=599.99, wps=3606.9, ups=3.07, wpb=1174, bsz=64, num_updates=1560, lr=4.68e-06, gnorm=7.511, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=311
2022-01-13 16:09:33 | INFO | train_inner | epoch 032:     32 / 50 loss=9.248, ppl=608.19, wps=18191.9, ups=14.26, wpb=1275.7, bsz=64, num_updates=1580, lr=4.74e-06, gnorm=7.446, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=312
2022-01-13 16:09:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:35 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.037 | ppl 525.23 | wps 26910.1 | wpb 556.6 | bsz 30.3 | num_updates 1598 | best_loss 9.037
2022-01-13 16:09:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1598 updates
2022-01-13 16:09:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 32 @ 1598 updates, score 9.037) (writing took 4.456599232042208 seconds)
2022-01-13 16:09:39 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-13 16:09:39 | INFO | train | epoch 032 | loss 9.223 | ppl 597.56 | wps 7006.5 | ups 5.68 | wpb 1232.9 | bsz 63.5 | num_updates 1598 | lr 4.794e-06 | gnorm 7.531 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 319
2022-01-13 16:09:39 | INFO | fairseq.trainer | begin training epoch 33
2022-01-13 16:09:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:39 | INFO | train_inner | epoch 033:      2 / 50 loss=9.115, ppl=554.63, wps=3508.5, ups=3, wpb=1168.8, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=7.641, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=319
2022-01-13 16:09:41 | INFO | train_inner | epoch 033:     22 / 50 loss=9.143, ppl=565.48, wps=17039.7, ups=13.93, wpb=1223.3, bsz=62.7, num_updates=1620, lr=4.86e-06, gnorm=7.254, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=321
2022-01-13 16:09:42 | INFO | train_inner | epoch 033:     42 / 50 loss=9.159, ppl=571.73, wps=17201.7, ups=13.29, wpb=1294.6, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=7.575, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=322
2022-01-13 16:09:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:44 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.956 | ppl 496.46 | wps 25895.8 | wpb 556.6 | bsz 30.3 | num_updates 1648 | best_loss 8.956
2022-01-13 16:09:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1648 updates
2022-01-13 16:09:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1648 updates, score 8.956) (writing took 4.015369780128822 seconds)
2022-01-13 16:09:48 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-13 16:09:48 | INFO | train | epoch 033 | loss 9.118 | ppl 555.59 | wps 7152.4 | ups 5.8 | wpb 1232.9 | bsz 63.5 | num_updates 1648 | lr 4.944e-06 | gnorm 7.498 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 327
2022-01-13 16:09:48 | INFO | fairseq.trainer | begin training epoch 34
2022-01-13 16:09:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:49 | INFO | train_inner | epoch 034:     12 / 50 loss=8.979, ppl=504.59, wps=3454.6, ups=3.14, wpb=1098.5, bsz=64, num_updates=1660, lr=4.98e-06, gnorm=7.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=328
2022-01-13 16:09:50 | INFO | train_inner | epoch 034:     32 / 50 loss=8.947, ppl=493.64, wps=16548.8, ups=13.86, wpb=1194.3, bsz=64, num_updates=1680, lr=5.04e-06, gnorm=7.478, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=330
2022-01-13 16:09:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:52 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.854 | ppl 462.74 | wps 26760.1 | wpb 556.6 | bsz 30.3 | num_updates 1698 | best_loss 8.854
2022-01-13 16:09:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1698 updates
2022-01-13 16:09:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1698 updates, score 8.854) (writing took 4.170731611782685 seconds)
2022-01-13 16:09:56 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-13 16:09:56 | INFO | train | epoch 034 | loss 9.039 | ppl 526.11 | wps 7200.2 | ups 5.84 | wpb 1232.9 | bsz 63.5 | num_updates 1698 | lr 5.094e-06 | gnorm 7.489 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 336
2022-01-13 16:09:56 | INFO | fairseq.trainer | begin training epoch 35
2022-01-13 16:09:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:56 | INFO | train_inner | epoch 035:      2 / 50 loss=9.221, ppl=596.9, wps=4602.8, ups=3.11, wpb=1478.3, bsz=62.7, num_updates=1700, lr=5.1e-06, gnorm=7.15, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=336
2022-01-13 16:09:58 | INFO | train_inner | epoch 035:     22 / 50 loss=8.91, ppl=481.01, wps=15877.5, ups=13.89, wpb=1143.2, bsz=64, num_updates=1720, lr=5.16e-06, gnorm=7.452, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=338
2022-01-13 16:09:59 | INFO | train_inner | epoch 035:     42 / 50 loss=8.951, ppl=494.93, wps=19175, ups=14.37, wpb=1334.7, bsz=62.7, num_updates=1740, lr=5.22e-06, gnorm=7.48, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=339
2022-01-13 16:10:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:01 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.746 | ppl 429.3 | wps 26160.1 | wpb 556.6 | bsz 30.3 | num_updates 1748 | best_loss 8.746
2022-01-13 16:10:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1748 updates
2022-01-13 16:10:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1748 updates, score 8.746) (writing took 4.151188254123554 seconds)
2022-01-13 16:10:05 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-13 16:10:05 | INFO | train | epoch 035 | loss 8.912 | ppl 481.56 | wps 7217.2 | ups 5.85 | wpb 1232.9 | bsz 63.5 | num_updates 1748 | lr 5.244e-06 | gnorm 7.451 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 345
2022-01-13 16:10:05 | INFO | fairseq.trainer | begin training epoch 36
2022-01-13 16:10:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:06 | INFO | train_inner | epoch 036:     12 / 50 loss=8.877, ppl=470.26, wps=3778.6, ups=3.04, wpb=1242.2, bsz=64, num_updates=1760, lr=5.28e-06, gnorm=7.571, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=346
2022-01-13 16:10:07 | INFO | train_inner | epoch 036:     32 / 50 loss=8.837, ppl=457.17, wps=18349.9, ups=14.18, wpb=1293.7, bsz=62.7, num_updates=1780, lr=5.34e-06, gnorm=7.468, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=347
2022-01-13 16:10:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:09 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.661 | ppl 404.91 | wps 25730 | wpb 556.6 | bsz 30.3 | num_updates 1798 | best_loss 8.661
2022-01-13 16:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1798 updates
2022-01-13 16:10:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1798 updates, score 8.661) (writing took 4.309992542024702 seconds)
2022-01-13 16:10:14 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-13 16:10:14 | INFO | train | epoch 036 | loss 8.866 | ppl 466.5 | wps 6961.3 | ups 5.65 | wpb 1232.9 | bsz 63.5 | num_updates 1798 | lr 5.394e-06 | gnorm 7.548 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 354
2022-01-13 16:10:14 | INFO | fairseq.trainer | begin training epoch 37
2022-01-13 16:10:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:14 | INFO | train_inner | epoch 037:      2 / 50 loss=8.84, ppl=458.23, wps=3427.3, ups=3.02, wpb=1136.7, bsz=64, num_updates=1800, lr=5.4e-06, gnorm=7.544, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=354
2022-01-13 16:10:15 | INFO | train_inner | epoch 037:     22 / 50 loss=8.691, ppl=413.28, wps=16389.6, ups=13.76, wpb=1191, bsz=62.7, num_updates=1820, lr=5.46e-06, gnorm=7.491, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=355
2022-01-13 16:10:17 | INFO | train_inner | epoch 037:     42 / 50 loss=8.782, ppl=440.22, wps=14674.6, ups=11.87, wpb=1236.2, bsz=64, num_updates=1840, lr=5.52e-06, gnorm=7.484, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=357
2022-01-13 16:10:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:18 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.541 | ppl 372.56 | wps 31255.1 | wpb 556.6 | bsz 30.3 | num_updates 1848 | best_loss 8.541
2022-01-13 16:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1848 updates
2022-01-13 16:10:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1848 updates, score 8.541) (writing took 4.009414136176929 seconds)
2022-01-13 16:10:22 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-13 16:10:22 | INFO | train | epoch 037 | loss 8.762 | ppl 434.03 | wps 7128.4 | ups 5.78 | wpb 1232.9 | bsz 63.5 | num_updates 1848 | lr 5.544e-06 | gnorm 7.596 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 362
2022-01-13 16:10:22 | INFO | fairseq.trainer | begin training epoch 38
2022-01-13 16:10:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:23 | INFO | train_inner | epoch 038:     12 / 50 loss=8.765, ppl=434.97, wps=3952.6, ups=3.21, wpb=1231.5, bsz=64, num_updates=1860, lr=5.58e-06, gnorm=7.787, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=363
2022-01-13 16:10:25 | INFO | train_inner | epoch 038:     32 / 50 loss=8.652, ppl=402.27, wps=15816.4, ups=14.27, wpb=1108.1, bsz=64, num_updates=1880, lr=5.64e-06, gnorm=7.524, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=365
2022-01-13 16:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:27 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.489 | ppl 359.29 | wps 26429.1 | wpb 556.6 | bsz 30.3 | num_updates 1898 | best_loss 8.489
2022-01-13 16:10:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1898 updates
2022-01-13 16:10:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 38 @ 1898 updates, score 8.489) (writing took 4.010337166022509 seconds)
2022-01-13 16:10:31 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-13 16:10:31 | INFO | train | epoch 038 | loss 8.72 | ppl 421.81 | wps 7324.5 | ups 5.94 | wpb 1232.9 | bsz 63.5 | num_updates 1898 | lr 5.694e-06 | gnorm 7.447 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 371
2022-01-13 16:10:31 | INFO | fairseq.trainer | begin training epoch 39
2022-01-13 16:10:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:31 | INFO | train_inner | epoch 039:      2 / 50 loss=8.76, ppl=433.48, wps=4321.9, ups=3.18, wpb=1357.8, bsz=62.7, num_updates=1900, lr=5.7e-06, gnorm=7.385, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=371
2022-01-13 16:10:32 | INFO | train_inner | epoch 039:     22 / 50 loss=8.66, ppl=404.52, wps=17956.5, ups=12.88, wpb=1394.3, bsz=62.7, num_updates=1920, lr=5.76e-06, gnorm=7.404, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=372
2022-01-13 16:10:34 | INFO | train_inner | epoch 039:     42 / 50 loss=8.679, ppl=409.79, wps=13311.2, ups=12.06, wpb=1103.3, bsz=64, num_updates=1940, lr=5.82e-06, gnorm=7.47, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=374
2022-01-13 16:10:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:36 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.371 | ppl 330.97 | wps 26976.6 | wpb 556.6 | bsz 30.3 | num_updates 1948 | best_loss 8.371
2022-01-13 16:10:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1948 updates
2022-01-13 16:10:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1948 updates, score 8.371) (writing took 4.564471570076421 seconds)
2022-01-13 16:10:40 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-13 16:10:40 | INFO | train | epoch 039 | loss 8.623 | ppl 394.21 | wps 6542 | ups 5.31 | wpb 1232.9 | bsz 63.5 | num_updates 1948 | lr 5.844e-06 | gnorm 7.454 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 380
2022-01-13 16:10:40 | INFO | fairseq.trainer | begin training epoch 40
2022-01-13 16:10:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:41 | INFO | train_inner | epoch 040:     12 / 50 loss=8.468, ppl=354.2, wps=3326, ups=2.87, wpb=1158.2, bsz=62.7, num_updates=1960, lr=5.88e-06, gnorm=7.667, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=381
2022-01-13 16:10:43 | INFO | train_inner | epoch 040:     32 / 50 loss=8.6, ppl=388.06, wps=16215.4, ups=13.58, wpb=1193.9, bsz=64, num_updates=1980, lr=5.94e-06, gnorm=7.286, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=383
2022-01-13 16:10:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:45 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.359 | ppl 328.42 | wps 25789.3 | wpb 556.6 | bsz 30.3 | num_updates 1998 | best_loss 8.359
2022-01-13 16:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1998 updates
2022-01-13 16:10:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1998 updates, score 8.359) (writing took 4.422414128202945 seconds)
2022-01-13 16:10:49 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-13 16:10:49 | INFO | train | epoch 040 | loss 8.554 | ppl 375.94 | wps 6908.7 | ups 5.6 | wpb 1232.9 | bsz 63.5 | num_updates 1998 | lr 5.994e-06 | gnorm 7.422 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 389
2022-01-13 16:10:49 | INFO | fairseq.trainer | begin training epoch 41
2022-01-13 16:10:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:49 | INFO | train_inner | epoch 041:      2 / 50 loss=8.543, ppl=373.01, wps=3903, ups=2.95, wpb=1325.2, bsz=64, num_updates=2000, lr=6e-06, gnorm=7.283, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=389
2022-01-13 16:10:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:50 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.283 | ppl 311.4 | wps 27538.8 | wpb 556.6 | bsz 30.3 | num_updates 2000 | best_loss 8.283
2022-01-13 16:10:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2000 updates
2022-01-13 16:10:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt
2022-01-13 16:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt
2022-01-13 16:11:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt (epoch 41 @ 2000 updates, score 8.283) (writing took 9.639275103108957 seconds)
2022-01-13 16:11:01 | INFO | train_inner | epoch 041:     22 / 50 loss=8.504, ppl=363.03, wps=2179.8, ups=1.67, wpb=1308.8, bsz=64, num_updates=2020, lr=6.06e-06, gnorm=7.23, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=401
2022-01-13 16:11:03 | INFO | train_inner | epoch 041:     42 / 50 loss=8.451, ppl=349.86, wps=15604.2, ups=13.06, wpb=1194.7, bsz=62.7, num_updates=2040, lr=6.12e-06, gnorm=7.374, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=403
2022-01-13 16:11:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:04 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.289 | ppl 312.71 | wps 26983.3 | wpb 556.6 | bsz 30.3 | num_updates 2048 | best_loss 8.283
2022-01-13 16:11:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2048 updates
2022-01-13 16:11:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:11:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:11:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 2048 updates, score 8.289) (writing took 2.782855710014701 seconds)
2022-01-13 16:11:07 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-13 16:11:07 | INFO | train | epoch 041 | loss 8.453 | ppl 350.35 | wps 3412.4 | ups 2.77 | wpb 1232.9 | bsz 63.5 | num_updates 2048 | lr 6.144e-06 | gnorm 7.295 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 407
2022-01-13 16:11:07 | INFO | fairseq.trainer | begin training epoch 42
2022-01-13 16:11:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:08 | INFO | train_inner | epoch 042:     12 / 50 loss=8.347, ppl=325.52, wps=4706.6, ups=3.87, wpb=1217.2, bsz=64, num_updates=2060, lr=6.18e-06, gnorm=7.363, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=408
2022-01-13 16:11:10 | INFO | train_inner | epoch 042:     32 / 50 loss=8.433, ppl=345.67, wps=16015.5, ups=12.84, wpb=1247.2, bsz=64, num_updates=2080, lr=6.24e-06, gnorm=7.501, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=410
2022-01-13 16:11:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:12 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.137 | ppl 281.45 | wps 30256.7 | wpb 556.6 | bsz 30.3 | num_updates 2098 | best_loss 8.137
2022-01-13 16:11:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2098 updates
2022-01-13 16:11:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 2098 updates, score 8.137) (writing took 4.345485378988087 seconds)
2022-01-13 16:11:16 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-13 16:11:16 | INFO | train | epoch 042 | loss 8.402 | ppl 338.38 | wps 6854.6 | ups 5.56 | wpb 1232.9 | bsz 63.5 | num_updates 2098 | lr 6.294e-06 | gnorm 7.467 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 416
2022-01-13 16:11:16 | INFO | fairseq.trainer | begin training epoch 43
2022-01-13 16:11:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:16 | INFO | train_inner | epoch 043:      2 / 50 loss=8.318, ppl=319.2, wps=3537.6, ups=2.99, wpb=1182.5, bsz=62.7, num_updates=2100, lr=6.3e-06, gnorm=7.432, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=416
2022-01-13 16:11:18 | INFO | train_inner | epoch 043:     22 / 50 loss=8.177, ppl=289.45, wps=16532.8, ups=13.51, wpb=1223.8, bsz=62.7, num_updates=2120, lr=6.36e-06, gnorm=7.531, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=418
2022-01-13 16:11:19 | INFO | train_inner | epoch 043:     42 / 50 loss=8.548, ppl=374.37, wps=16729.1, ups=13.01, wpb=1286.2, bsz=64, num_updates=2140, lr=6.42e-06, gnorm=7.07, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=419
2022-01-13 16:11:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:21 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.136 | ppl 281.38 | wps 27232.9 | wpb 556.6 | bsz 30.3 | num_updates 2148 | best_loss 8.136
2022-01-13 16:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2148 updates
2022-01-13 16:11:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 43 @ 2148 updates, score 8.136) (writing took 4.420745020965114 seconds)
2022-01-13 16:11:25 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-13 16:11:25 | INFO | train | epoch 043 | loss 8.313 | ppl 317.92 | wps 6704.3 | ups 5.44 | wpb 1232.9 | bsz 63.5 | num_updates 2148 | lr 6.444e-06 | gnorm 7.325 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 425
2022-01-13 16:11:25 | INFO | fairseq.trainer | begin training epoch 44
2022-01-13 16:11:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:26 | INFO | train_inner | epoch 044:     12 / 50 loss=8.121, ppl=278.34, wps=3163.3, ups=2.92, wpb=1083.7, bsz=62.7, num_updates=2160, lr=6.48e-06, gnorm=7.573, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=426
2022-01-13 16:11:28 | INFO | train_inner | epoch 044:     32 / 50 loss=8.235, ppl=301.22, wps=16333.8, ups=12.91, wpb=1265.7, bsz=64, num_updates=2180, lr=6.54e-06, gnorm=7.183, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=428
2022-01-13 16:11:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:30 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.064 | ppl 267.6 | wps 28635.2 | wpb 556.6 | bsz 30.3 | num_updates 2198 | best_loss 8.064
2022-01-13 16:11:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2198 updates
2022-01-13 16:11:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 2198 updates, score 8.064) (writing took 4.677682719193399 seconds)
2022-01-13 16:11:35 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-13 16:11:35 | INFO | train | epoch 044 | loss 8.246 | ppl 303.65 | wps 6578.2 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 2198 | lr 6.594e-06 | gnorm 7.281 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 435
2022-01-13 16:11:35 | INFO | fairseq.trainer | begin training epoch 45
2022-01-13 16:11:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:35 | INFO | train_inner | epoch 045:      2 / 50 loss=8.289, ppl=312.74, wps=3579.7, ups=2.8, wpb=1277.9, bsz=64, num_updates=2200, lr=6.6e-06, gnorm=7.232, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=435
2022-01-13 16:11:36 | INFO | train_inner | epoch 045:     22 / 50 loss=8.118, ppl=277.89, wps=16832.3, ups=13.79, wpb=1220.5, bsz=62.7, num_updates=2220, lr=6.66e-06, gnorm=7.418, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=436
2022-01-13 16:11:38 | INFO | train_inner | epoch 045:     42 / 50 loss=8.123, ppl=278.72, wps=16909.4, ups=13.81, wpb=1224.2, bsz=64, num_updates=2240, lr=6.72e-06, gnorm=7.486, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=438
2022-01-13 16:11:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:39 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.984 | ppl 253.12 | wps 26321.5 | wpb 556.6 | bsz 30.3 | num_updates 2248 | best_loss 7.984
2022-01-13 16:11:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2248 updates
2022-01-13 16:11:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 2248 updates, score 7.984) (writing took 4.313323424896225 seconds)
2022-01-13 16:11:44 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-13 16:11:44 | INFO | train | epoch 045 | loss 8.153 | ppl 284.65 | wps 7001.8 | ups 5.68 | wpb 1232.9 | bsz 63.5 | num_updates 2248 | lr 6.744e-06 | gnorm 7.412 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 443
2022-01-13 16:11:44 | INFO | fairseq.trainer | begin training epoch 46
2022-01-13 16:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:44 | INFO | train_inner | epoch 046:     12 / 50 loss=8.342, ppl=324.52, wps=3920.8, ups=3.02, wpb=1296.3, bsz=62.7, num_updates=2260, lr=6.78e-06, gnorm=7.174, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=444
2022-01-13 16:11:46 | INFO | train_inner | epoch 046:     32 / 50 loss=7.947, ppl=246.77, wps=16123.6, ups=13.55, wpb=1189.8, bsz=64, num_updates=2280, lr=6.84e-06, gnorm=7.652, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=446
2022-01-13 16:11:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:48 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.968 | ppl 250.4 | wps 27339.4 | wpb 556.6 | bsz 30.3 | num_updates 2298 | best_loss 7.968
2022-01-13 16:11:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2298 updates
2022-01-13 16:11:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 46 @ 2298 updates, score 7.968) (writing took 4.097854980966076 seconds)
2022-01-13 16:11:52 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-13 16:11:53 | INFO | train | epoch 046 | loss 8.086 | ppl 271.68 | wps 7038.4 | ups 5.71 | wpb 1232.9 | bsz 63.5 | num_updates 2298 | lr 6.894e-06 | gnorm 7.444 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 452
2022-01-13 16:11:54 | INFO | fairseq.trainer | begin training epoch 47
2022-01-13 16:11:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:54 | INFO | train_inner | epoch 047:      2 / 50 loss=8.117, ppl=277.68, wps=3279.6, ups=2.57, wpb=1275.9, bsz=64, num_updates=2300, lr=6.9e-06, gnorm=7.698, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=454
2022-01-13 16:11:55 | INFO | train_inner | epoch 047:     22 / 50 loss=8.119, ppl=278.04, wps=17783.4, ups=14.28, wpb=1245.6, bsz=64, num_updates=2320, lr=6.96e-06, gnorm=7.318, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=455
2022-01-13 16:11:57 | INFO | train_inner | epoch 047:     42 / 50 loss=7.939, ppl=245.34, wps=17622, ups=13.83, wpb=1274.3, bsz=62.7, num_updates=2340, lr=7.02e-06, gnorm=7.391, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=457
2022-01-13 16:11:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:58 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.843 | ppl 229.65 | wps 26060.8 | wpb 556.6 | bsz 30.3 | num_updates 2348 | best_loss 7.843
2022-01-13 16:11:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 2348 updates
2022-01-13 16:11:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 2348 updates, score 7.843) (writing took 4.281112919095904 seconds)
2022-01-13 16:12:02 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-13 16:12:02 | INFO | train | epoch 047 | loss 8.031 | ppl 261.61 | wps 7133.3 | ups 5.79 | wpb 1232.9 | bsz 63.5 | num_updates 2348 | lr 7.044e-06 | gnorm 7.469 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 462
2022-01-13 16:12:02 | INFO | fairseq.trainer | begin training epoch 48
2022-01-13 16:12:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:03 | INFO | train_inner | epoch 048:     12 / 50 loss=7.919, ppl=241.94, wps=3815.9, ups=3.08, wpb=1239.3, bsz=62.7, num_updates=2360, lr=7.08e-06, gnorm=9.644, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=463
2022-01-13 16:12:05 | INFO | train_inner | epoch 048:     32 / 50 loss=7.847, ppl=230.21, wps=16414.4, ups=13.94, wpb=1177.8, bsz=64, num_updates=2380, lr=7.14e-06, gnorm=8.052, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=464
2022-01-13 16:12:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:07 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.761 | ppl 216.96 | wps 26199.4 | wpb 556.6 | bsz 30.3 | num_updates 2398 | best_loss 7.761
2022-01-13 16:12:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 2398 updates
2022-01-13 16:12:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 48 @ 2398 updates, score 7.761) (writing took 5.027700481005013 seconds)
2022-01-13 16:12:12 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-13 16:12:12 | INFO | train | epoch 048 | loss 7.932 | ppl 244.17 | wps 6489.4 | ups 5.26 | wpb 1232.9 | bsz 63.5 | num_updates 2398 | lr 7.194e-06 | gnorm 8.748 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 472
2022-01-13 16:12:12 | INFO | fairseq.trainer | begin training epoch 49
2022-01-13 16:12:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:12 | INFO | train_inner | epoch 049:      2 / 50 loss=7.993, ppl=254.71, wps=3073.5, ups=2.58, wpb=1192.6, bsz=64, num_updates=2400, lr=7.2e-06, gnorm=7.797, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=472
2022-01-13 16:12:14 | INFO | train_inner | epoch 049:     22 / 50 loss=7.819, ppl=225.86, wps=18038.5, ups=13.52, wpb=1334.5, bsz=64, num_updates=2420, lr=7.26e-06, gnorm=7.082, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=474
2022-01-13 16:12:15 | INFO | train_inner | epoch 049:     42 / 50 loss=7.893, ppl=237.66, wps=15250.2, ups=13.4, wpb=1138.3, bsz=62.7, num_updates=2440, lr=7.32e-06, gnorm=10.43, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=475
2022-01-13 16:12:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:17 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.75 | ppl 215.26 | wps 27477.2 | wpb 556.6 | bsz 30.3 | num_updates 2448 | best_loss 7.75
2022-01-13 16:12:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 2448 updates
2022-01-13 16:12:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 2448 updates, score 7.75) (writing took 6.951026380993426 seconds)
2022-01-13 16:12:24 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-13 16:12:24 | INFO | train | epoch 049 | loss 7.901 | ppl 239.06 | wps 5250.4 | ups 4.26 | wpb 1232.9 | bsz 63.5 | num_updates 2448 | lr 7.344e-06 | gnorm 8.476 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 483
2022-01-13 16:12:24 | INFO | fairseq.trainer | begin training epoch 50
2022-01-13 16:12:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:25 | INFO | train_inner | epoch 050:     12 / 50 loss=7.935, ppl=244.74, wps=2738.1, ups=2.14, wpb=1276.5, bsz=62.7, num_updates=2460, lr=7.38e-06, gnorm=7.32, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=485
2022-01-13 16:12:26 | INFO | train_inner | epoch 050:     32 / 50 loss=7.874, ppl=234.54, wps=16877.8, ups=13.26, wpb=1273.2, bsz=64, num_updates=2480, lr=7.44e-06, gnorm=7.515, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=486
2022-01-13 16:12:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:28 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.689 | ppl 206.29 | wps 27047.5 | wpb 556.6 | bsz 30.3 | num_updates 2498 | best_loss 7.689
2022-01-13 16:12:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2498 updates
2022-01-13 16:12:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 2498 updates, score 7.689) (writing took 4.139003769960254 seconds)
2022-01-13 16:12:32 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-13 16:12:32 | INFO | train | epoch 050 | loss 7.832 | ppl 227.86 | wps 6958.1 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 2498 | lr 7.494e-06 | gnorm 7.37 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 492
2022-01-13 16:12:32 | INFO | fairseq.trainer | begin training epoch 51
2022-01-13 16:12:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:33 | INFO | train_inner | epoch 051:      2 / 50 loss=7.842, ppl=229.4, wps=3624.7, ups=3.05, wpb=1188, bsz=64, num_updates=2500, lr=7.5e-06, gnorm=7.233, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=493
2022-01-13 16:12:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:33 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.631 | ppl 198.27 | wps 27071.8 | wpb 556.6 | bsz 30.3 | num_updates 2500 | best_loss 7.631
2022-01-13 16:12:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2500 updates
2022-01-13 16:12:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt
2022-01-13 16:12:36 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt
2022-01-13 16:12:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt (epoch 51 @ 2500 updates, score 7.631) (writing took 8.39964364701882 seconds)
2022-01-13 16:12:44 | INFO | train_inner | epoch 051:     22 / 50 loss=7.839, ppl=229, wps=2271.9, ups=1.83, wpb=1242.9, bsz=64, num_updates=2520, lr=7.56e-06, gnorm=7.368, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=504
2022-01-13 16:12:45 | INFO | train_inner | epoch 051:     42 / 50 loss=7.727, ppl=211.93, wps=13503.6, ups=10.78, wpb=1252.8, bsz=62.7, num_updates=2540, lr=7.62e-06, gnorm=7.282, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=505
2022-01-13 16:12:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:47 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.639 | ppl 199.31 | wps 27398.3 | wpb 556.6 | bsz 30.3 | num_updates 2548 | best_loss 7.631
2022-01-13 16:12:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2548 updates
2022-01-13 16:12:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:12:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:12:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 51 @ 2548 updates, score 7.639) (writing took 2.8562494858633727 seconds)
2022-01-13 16:12:50 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-13 16:12:50 | INFO | train | epoch 051 | loss 7.765 | ppl 217.47 | wps 3560.3 | ups 2.89 | wpb 1232.9 | bsz 63.5 | num_updates 2548 | lr 7.644e-06 | gnorm 7.384 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 510
2022-01-13 16:12:50 | INFO | fairseq.trainer | begin training epoch 52
2022-01-13 16:12:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:51 | INFO | train_inner | epoch 052:     12 / 50 loss=7.654, ppl=201.47, wps=4490.8, ups=3.82, wpb=1176.3, bsz=62.7, num_updates=2560, lr=7.68e-06, gnorm=7.704, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=511
2022-01-13 16:12:52 | INFO | train_inner | epoch 052:     32 / 50 loss=7.736, ppl=213.22, wps=16047.8, ups=13.47, wpb=1191, bsz=64, num_updates=2580, lr=7.74e-06, gnorm=7.258, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=512
2022-01-13 16:12:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:54 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.614 | ppl 195.94 | wps 25819.1 | wpb 556.6 | bsz 30.3 | num_updates 2598 | best_loss 7.614
2022-01-13 16:12:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 2598 updates
2022-01-13 16:12:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 52 @ 2598 updates, score 7.614) (writing took 4.360743889817968 seconds)
2022-01-13 16:12:59 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-13 16:12:59 | INFO | train | epoch 052 | loss 7.712 | ppl 209.65 | wps 6971.8 | ups 5.65 | wpb 1232.9 | bsz 63.5 | num_updates 2598 | lr 7.794e-06 | gnorm 7.467 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 518
2022-01-13 16:12:59 | INFO | fairseq.trainer | begin training epoch 53
2022-01-13 16:12:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:59 | INFO | train_inner | epoch 053:      2 / 50 loss=7.686, ppl=205.88, wps=3851.5, ups=3.03, wpb=1271.5, bsz=64, num_updates=2600, lr=7.8e-06, gnorm=7.638, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=519
2022-01-13 16:13:00 | INFO | train_inner | epoch 053:     22 / 50 loss=7.714, ppl=210.03, wps=17902.4, ups=13.63, wpb=1313.8, bsz=64, num_updates=2620, lr=7.86e-06, gnorm=7.042, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=520
2022-01-13 16:13:02 | INFO | train_inner | epoch 053:     42 / 50 loss=7.551, ppl=187.55, wps=15547.8, ups=13.46, wpb=1154.8, bsz=62.7, num_updates=2640, lr=7.92e-06, gnorm=7.727, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=522
2022-01-13 16:13:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:03 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.473 | ppl 177.71 | wps 27355.1 | wpb 556.6 | bsz 30.3 | num_updates 2648 | best_loss 7.473
2022-01-13 16:13:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 2648 updates
2022-01-13 16:13:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 2648 updates, score 7.473) (writing took 3.8562265541404486 seconds)
2022-01-13 16:13:07 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-13 16:13:07 | INFO | train | epoch 053 | loss 7.625 | ppl 197.36 | wps 7384.9 | ups 5.99 | wpb 1232.9 | bsz 63.5 | num_updates 2648 | lr 7.944e-06 | gnorm 7.394 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 527
2022-01-13 16:13:07 | INFO | fairseq.trainer | begin training epoch 54
2022-01-13 16:13:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:08 | INFO | train_inner | epoch 054:     12 / 50 loss=7.592, ppl=192.88, wps=3972.9, ups=3.2, wpb=1242.8, bsz=64, num_updates=2660, lr=7.98e-06, gnorm=7.365, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=528
2022-01-13 16:13:09 | INFO | train_inner | epoch 054:     32 / 50 loss=7.59, ppl=192.72, wps=17855.5, ups=14.36, wpb=1243.4, bsz=64, num_updates=2680, lr=8.04e-06, gnorm=7.408, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=529
2022-01-13 16:13:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:11 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.484 | ppl 179 | wps 28413.3 | wpb 556.6 | bsz 30.3 | num_updates 2698 | best_loss 7.473
2022-01-13 16:13:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 2698 updates
2022-01-13 16:13:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:13:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:13:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 54 @ 2698 updates, score 7.484) (writing took 2.5603727689012885 seconds)
2022-01-13 16:13:14 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-13 16:13:14 | INFO | train | epoch 054 | loss 7.553 | ppl 187.85 | wps 8895 | ups 7.21 | wpb 1232.9 | bsz 63.5 | num_updates 2698 | lr 8.094e-06 | gnorm 7.379 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 534
2022-01-13 16:13:14 | INFO | fairseq.trainer | begin training epoch 55
2022-01-13 16:13:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:14 | INFO | train_inner | epoch 055:      2 / 50 loss=7.427, ppl=172.14, wps=4951.5, ups=4.19, wpb=1182.4, bsz=62.7, num_updates=2700, lr=8.1e-06, gnorm=7.373, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=534
2022-01-13 16:13:16 | INFO | train_inner | epoch 055:     22 / 50 loss=7.272, ppl=154.52, wps=14923.3, ups=13.99, wpb=1066.3, bsz=62.7, num_updates=2720, lr=8.16e-06, gnorm=7.8, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=536
2022-01-13 16:13:17 | INFO | train_inner | epoch 055:     42 / 50 loss=7.566, ppl=189.53, wps=17768.6, ups=13.75, wpb=1291.8, bsz=64, num_updates=2740, lr=8.22e-06, gnorm=7.07, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=537
2022-01-13 16:13:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:18 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.407 | ppl 169.77 | wps 26186.7 | wpb 556.6 | bsz 30.3 | num_updates 2748 | best_loss 7.407
2022-01-13 16:13:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 2748 updates
2022-01-13 16:13:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 55 @ 2748 updates, score 7.407) (writing took 4.511528666131198 seconds)
2022-01-13 16:13:23 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-13 16:13:23 | INFO | train | epoch 055 | loss 7.466 | ppl 176.84 | wps 6886.5 | ups 5.59 | wpb 1232.9 | bsz 63.5 | num_updates 2748 | lr 8.244e-06 | gnorm 7.343 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 543
2022-01-13 16:13:23 | INFO | fairseq.trainer | begin training epoch 56
2022-01-13 16:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:24 | INFO | train_inner | epoch 056:     12 / 50 loss=7.544, ppl=186.62, wps=3937.4, ups=2.96, wpb=1332, bsz=64, num_updates=2760, lr=8.28e-06, gnorm=7.184, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=544
2022-01-13 16:13:25 | INFO | train_inner | epoch 056:     32 / 50 loss=7.389, ppl=167.56, wps=17904.2, ups=13.8, wpb=1297, bsz=62.7, num_updates=2780, lr=8.34e-06, gnorm=7.493, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=545
2022-01-13 16:13:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:27 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.34 | ppl 162.03 | wps 26327.5 | wpb 556.6 | bsz 30.3 | num_updates 2798 | best_loss 7.34
2022-01-13 16:13:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 2798 updates
2022-01-13 16:13:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 2798 updates, score 7.34) (writing took 4.353646798990667 seconds)
2022-01-13 16:13:32 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-13 16:13:32 | INFO | train | epoch 056 | loss 7.451 | ppl 174.93 | wps 6955.9 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 2798 | lr 8.394e-06 | gnorm 7.433 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 552
2022-01-13 16:13:32 | INFO | fairseq.trainer | begin training epoch 57
2022-01-13 16:13:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:32 | INFO | train_inner | epoch 057:      2 / 50 loss=7.527, ppl=184.47, wps=3423.8, ups=2.85, wpb=1200.4, bsz=64, num_updates=2800, lr=8.4e-06, gnorm=7.354, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=552
2022-01-13 16:13:34 | INFO | train_inner | epoch 057:     22 / 50 loss=7.296, ppl=157.17, wps=18587, ups=14.25, wpb=1304.7, bsz=64, num_updates=2820, lr=8.46e-06, gnorm=7.394, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=554
2022-01-13 16:13:35 | INFO | train_inner | epoch 057:     42 / 50 loss=7.427, ppl=172.06, wps=16458.7, ups=14.04, wpb=1172.3, bsz=62.7, num_updates=2840, lr=8.52e-06, gnorm=7.617, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=555
2022-01-13 16:13:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:37 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.28 | ppl 155.41 | wps 27213.2 | wpb 556.6 | bsz 30.3 | num_updates 2848 | best_loss 7.28
2022-01-13 16:13:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 2848 updates
2022-01-13 16:13:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 57 @ 2848 updates, score 7.28) (writing took 4.000386480009183 seconds)
2022-01-13 16:13:41 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-13 16:13:41 | INFO | train | epoch 057 | loss 7.388 | ppl 167.51 | wps 7241.4 | ups 5.87 | wpb 1232.9 | bsz 63.5 | num_updates 2848 | lr 8.544e-06 | gnorm 7.517 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 561
2022-01-13 16:13:41 | INFO | fairseq.trainer | begin training epoch 58
2022-01-13 16:13:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:42 | INFO | train_inner | epoch 058:     12 / 50 loss=7.412, ppl=170.29, wps=3865, ups=3.16, wpb=1224.3, bsz=64, num_updates=2860, lr=8.58e-06, gnorm=7.526, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=562
2022-01-13 16:13:43 | INFO | train_inner | epoch 058:     32 / 50 loss=7.298, ppl=157.41, wps=16586.6, ups=14.18, wpb=1169.8, bsz=62.7, num_updates=2880, lr=8.64e-06, gnorm=7.549, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=563
2022-01-13 16:13:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:45 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.129 | ppl 140.02 | wps 24879.7 | wpb 556.6 | bsz 30.3 | num_updates 2898 | best_loss 7.129
2022-01-13 16:13:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 2898 updates
2022-01-13 16:13:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 58 @ 2898 updates, score 7.129) (writing took 3.902147063985467 seconds)
2022-01-13 16:13:49 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-13 16:13:49 | INFO | train | epoch 058 | loss 7.33 | ppl 160.9 | wps 7347.2 | ups 5.96 | wpb 1232.9 | bsz 63.5 | num_updates 2898 | lr 8.694e-06 | gnorm 7.467 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 569
2022-01-13 16:13:49 | INFO | fairseq.trainer | begin training epoch 59
2022-01-13 16:13:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:49 | INFO | train_inner | epoch 059:      2 / 50 loss=7.333, ppl=161.19, wps=4078.2, ups=3.21, wpb=1271, bsz=62.7, num_updates=2900, lr=8.7e-06, gnorm=7.482, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=569
2022-01-13 16:13:51 | INFO | train_inner | epoch 059:     22 / 50 loss=7.177, ppl=144.74, wps=17024.7, ups=14.03, wpb=1213.8, bsz=64, num_updates=2920, lr=8.76e-06, gnorm=7.498, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=571
2022-01-13 16:13:52 | INFO | train_inner | epoch 059:     42 / 50 loss=7.246, ppl=151.84, wps=16724.5, ups=13.88, wpb=1205.3, bsz=64, num_updates=2940, lr=8.82e-06, gnorm=7.388, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=572
2022-01-13 16:13:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:53 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.105 | ppl 137.7 | wps 25309.5 | wpb 556.6 | bsz 30.3 | num_updates 2948 | best_loss 7.105
2022-01-13 16:13:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 2948 updates
2022-01-13 16:13:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 59 @ 2948 updates, score 7.105) (writing took 3.966971711954102 seconds)
2022-01-13 16:13:57 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-13 16:13:57 | INFO | train | epoch 059 | loss 7.241 | ppl 151.23 | wps 7308.2 | ups 5.93 | wpb 1232.9 | bsz 63.5 | num_updates 2948 | lr 8.844e-06 | gnorm 7.427 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 577
2022-01-13 16:13:57 | INFO | fairseq.trainer | begin training epoch 60
2022-01-13 16:13:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:58 | INFO | train_inner | epoch 060:     12 / 50 loss=7.267, ppl=154, wps=4317.4, ups=3.15, wpb=1369, bsz=64, num_updates=2960, lr=8.88e-06, gnorm=7.296, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=578
2022-01-13 16:14:00 | INFO | train_inner | epoch 060:     32 / 50 loss=7.274, ppl=154.81, wps=17903, ups=13.97, wpb=1281.2, bsz=62.7, num_updates=2980, lr=8.94e-06, gnorm=7.473, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=580
2022-01-13 16:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:02 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.103 | ppl 137.51 | wps 25294.3 | wpb 556.6 | bsz 30.3 | num_updates 2998 | best_loss 7.103
2022-01-13 16:14:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 2998 updates
2022-01-13 16:14:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 60 @ 2998 updates, score 7.103) (writing took 4.383154954062775 seconds)
2022-01-13 16:14:06 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-13 16:14:06 | INFO | train | epoch 060 | loss 7.186 | ppl 145.57 | wps 6911.9 | ups 5.61 | wpb 1232.9 | bsz 63.5 | num_updates 2998 | lr 8.994e-06 | gnorm 7.489 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 586
2022-01-13 16:14:06 | INFO | fairseq.trainer | begin training epoch 61
2022-01-13 16:14:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:07 | INFO | train_inner | epoch 061:      2 / 50 loss=7.114, ppl=138.56, wps=3206.2, ups=2.95, wpb=1087.5, bsz=64, num_updates=3000, lr=9e-06, gnorm=7.546, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=587
2022-01-13 16:14:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:07 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.155 | ppl 142.48 | wps 28446.7 | wpb 556.6 | bsz 30.3 | num_updates 3000 | best_loss 7.103
2022-01-13 16:14:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3000 updates
2022-01-13 16:14:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt
2022-01-13 16:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt
2022-01-13 16:14:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt (epoch 61 @ 3000 updates, score 7.155) (writing took 4.007071870146319 seconds)
2022-01-13 16:14:13 | INFO | train_inner | epoch 061:     22 / 50 loss=7.134, ppl=140.5, wps=3663, ups=3.1, wpb=1181.2, bsz=62.7, num_updates=3020, lr=9.06e-06, gnorm=7.758, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=593
2022-01-13 16:14:15 | INFO | train_inner | epoch 061:     42 / 50 loss=7.174, ppl=144.38, wps=15567.3, ups=12.1, wpb=1286.7, bsz=64, num_updates=3040, lr=9.12e-06, gnorm=7.543, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=595
2022-01-13 16:14:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:16 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.13 | ppl 140.04 | wps 26763.2 | wpb 556.6 | bsz 30.3 | num_updates 3048 | best_loss 7.103
2022-01-13 16:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3048 updates
2022-01-13 16:14:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:14:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:14:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 61 @ 3048 updates, score 7.13) (writing took 2.425368594005704 seconds)
2022-01-13 16:14:19 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-13 16:14:19 | INFO | train | epoch 061 | loss 7.147 | ppl 141.77 | wps 5032.4 | ups 4.08 | wpb 1232.9 | bsz 63.5 | num_updates 3048 | lr 9.144e-06 | gnorm 7.58 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 599
2022-01-13 16:14:19 | INFO | fairseq.trainer | begin training epoch 62
2022-01-13 16:14:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:20 | INFO | train_inner | epoch 062:     12 / 50 loss=7.211, ppl=148.12, wps=5454.1, ups=4.15, wpb=1314.8, bsz=64, num_updates=3060, lr=9.18e-06, gnorm=7.136, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=600
2022-01-13 16:14:21 | INFO | train_inner | epoch 062:     32 / 50 loss=6.871, ppl=117.06, wps=15097, ups=13.84, wpb=1090.7, bsz=64, num_updates=3080, lr=9.24e-06, gnorm=7.801, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=601
2022-01-13 16:14:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:23 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 7.051 | ppl 132.58 | wps 27969 | wpb 556.6 | bsz 30.3 | num_updates 3098 | best_loss 7.051
2022-01-13 16:14:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 3098 updates
2022-01-13 16:14:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 62 @ 3098 updates, score 7.051) (writing took 3.8703499040566385 seconds)
2022-01-13 16:14:27 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-13 16:14:27 | INFO | train | epoch 062 | loss 7.091 | ppl 136.32 | wps 7467 | ups 6.06 | wpb 1232.9 | bsz 63.5 | num_updates 3098 | lr 9.294e-06 | gnorm 7.458 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 607
2022-01-13 16:14:27 | INFO | fairseq.trainer | begin training epoch 63
2022-01-13 16:14:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:27 | INFO | train_inner | epoch 063:      2 / 50 loss=7.208, ppl=147.8, wps=4457.8, ups=3.28, wpb=1357.5, bsz=62.7, num_updates=3100, lr=9.3e-06, gnorm=7.551, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=607
2022-01-13 16:14:29 | INFO | train_inner | epoch 063:     22 / 50 loss=7.018, ppl=129.61, wps=17364, ups=13.71, wpb=1267, bsz=64, num_updates=3120, lr=9.36e-06, gnorm=7.317, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=609
2022-01-13 16:14:30 | INFO | train_inner | epoch 063:     42 / 50 loss=7, ppl=127.96, wps=16197.2, ups=13.93, wpb=1162.7, bsz=62.7, num_updates=3140, lr=9.42e-06, gnorm=7.997, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=610
2022-01-13 16:14:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:31 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.924 | ppl 121.46 | wps 26643.7 | wpb 556.6 | bsz 30.3 | num_updates 3148 | best_loss 6.924
2022-01-13 16:14:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 3148 updates
2022-01-13 16:14:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 63 @ 3148 updates, score 6.924) (writing took 5.0752614790108055 seconds)
2022-01-13 16:14:37 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-13 16:14:37 | INFO | train | epoch 063 | loss 7.02 | ppl 129.79 | wps 6412.6 | ups 5.2 | wpb 1232.9 | bsz 63.5 | num_updates 3148 | lr 9.444e-06 | gnorm 7.741 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 616
2022-01-13 16:14:37 | INFO | fairseq.trainer | begin training epoch 64
2022-01-13 16:14:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:38 | INFO | train_inner | epoch 064:     12 / 50 loss=6.956, ppl=124.17, wps=3181.1, ups=2.68, wpb=1186.6, bsz=64, num_updates=3160, lr=9.48e-06, gnorm=7.598, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=617
2022-01-13 16:14:39 | INFO | train_inner | epoch 064:     32 / 50 loss=7.119, ppl=138.97, wps=17852.6, ups=14.26, wpb=1252.2, bsz=64, num_updates=3180, lr=9.54e-06, gnorm=7.707, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=619
2022-01-13 16:14:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:41 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.933 | ppl 122.18 | wps 27222.8 | wpb 556.6 | bsz 30.3 | num_updates 3198 | best_loss 6.924
2022-01-13 16:14:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 3198 updates
2022-01-13 16:14:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:14:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:14:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 64 @ 3198 updates, score 6.933) (writing took 2.620714506134391 seconds)
2022-01-13 16:14:44 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-13 16:14:44 | INFO | train | epoch 064 | loss 6.989 | ppl 127 | wps 8742.7 | ups 7.09 | wpb 1232.9 | bsz 63.5 | num_updates 3198 | lr 9.594e-06 | gnorm 7.571 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 623
2022-01-13 16:14:44 | INFO | fairseq.trainer | begin training epoch 65
2022-01-13 16:14:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:44 | INFO | train_inner | epoch 065:      2 / 50 loss=6.893, ppl=118.87, wps=5045.2, ups=4.09, wpb=1234.3, bsz=62.7, num_updates=3200, lr=9.6e-06, gnorm=7.485, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=624
2022-01-13 16:14:45 | INFO | train_inner | epoch 065:     22 / 50 loss=6.801, ppl=111.49, wps=17900.8, ups=13.97, wpb=1281.6, bsz=64, num_updates=3220, lr=9.66e-06, gnorm=7.353, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=625
2022-01-13 16:14:47 | INFO | train_inner | epoch 065:     42 / 50 loss=6.883, ppl=118.04, wps=16901.7, ups=13.51, wpb=1250.8, bsz=64, num_updates=3240, lr=9.72e-06, gnorm=7.533, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=627
2022-01-13 16:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:48 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.826 | ppl 113.47 | wps 26266.4 | wpb 556.6 | bsz 30.3 | num_updates 3248 | best_loss 6.826
2022-01-13 16:14:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 3248 updates
2022-01-13 16:14:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 65 @ 3248 updates, score 6.826) (writing took 4.1442303659860045 seconds)
2022-01-13 16:14:52 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-01-13 16:14:52 | INFO | train | epoch 065 | loss 6.89 | ppl 118.57 | wps 7175.4 | ups 5.82 | wpb 1232.9 | bsz 63.5 | num_updates 3248 | lr 9.744e-06 | gnorm 7.496 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 632
2022-01-13 16:14:52 | INFO | fairseq.trainer | begin training epoch 66
2022-01-13 16:14:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:53 | INFO | train_inner | epoch 066:     12 / 50 loss=6.948, ppl=123.49, wps=3881, ups=3.13, wpb=1240.5, bsz=62.7, num_updates=3260, lr=9.78e-06, gnorm=7.594, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=633
2022-01-13 16:14:55 | INFO | train_inner | epoch 066:     32 / 50 loss=6.802, ppl=111.62, wps=18003.3, ups=13.99, wpb=1286.8, bsz=64, num_updates=3280, lr=9.84e-06, gnorm=7.914, clip=100, loss_scale=32, train_wall=1, gb_free=20.8, wall=634
2022-01-13 16:14:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:57 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.769 | ppl 109.03 | wps 25176.8 | wpb 556.6 | bsz 30.3 | num_updates 3298 | best_loss 6.769
2022-01-13 16:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 3298 updates
2022-01-13 16:14:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:15:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 66 @ 3298 updates, score 6.769) (writing took 3.8774605200160295 seconds)
2022-01-13 16:15:00 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-01-13 16:15:00 | INFO | train | epoch 066 | loss 6.885 | ppl 118.16 | wps 7420.2 | ups 6.02 | wpb 1232.9 | bsz 63.5 | num_updates 3298 | lr 9.894e-06 | gnorm 7.755 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 640
2022-01-13 16:15:01 | INFO | fairseq.trainer | begin training epoch 67
2022-01-13 16:15:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:15:01 | INFO | train_inner | epoch 067:      2 / 50 loss=6.981, ppl=126.33, wps=3786.6, ups=3.25, wpb=1165.1, bsz=62.7, num_updates=3300, lr=9.9e-06, gnorm=7.689, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=641
2022-01-13 16:15:02 | INFO | train_inner | epoch 067:     22 / 50 loss=6.867, ppl=116.77, wps=17056.3, ups=13.66, wpb=1248.2, bsz=64, num_updates=3320, lr=9.96e-06, gnorm=7.413, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=642
2022-01-13 16:15:04 | INFO | train_inner | epoch 067:     42 / 50 loss=6.827, ppl=113.54, wps=16466.4, ups=14.19, wpb=1160.2, bsz=62.7, num_updates=3340, lr=1.002e-05, gnorm=7.797, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=644
2022-01-13 16:15:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:05 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.801 | ppl 111.5 | wps 26137.7 | wpb 556.6 | bsz 30.3 | num_updates 3348 | best_loss 6.769
2022-01-13 16:15:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 3348 updates
2022-01-13 16:15:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 67 @ 3348 updates, score 6.801) (writing took 2.5771543020382524 seconds)
2022-01-13 16:15:07 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-01-13 16:15:07 | INFO | train | epoch 067 | loss 6.809 | ppl 112.15 | wps 8808.9 | ups 7.15 | wpb 1232.9 | bsz 63.5 | num_updates 3348 | lr 1.0044e-05 | gnorm 7.633 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 647
2022-01-13 16:15:08 | INFO | fairseq.trainer | begin training epoch 68
2022-01-13 16:15:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:15:08 | INFO | train_inner | epoch 068:     12 / 50 loss=6.672, ppl=101.97, wps=5768, ups=4.13, wpb=1396.4, bsz=64, num_updates=3360, lr=1.008e-05, gnorm=7.473, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=648
2022-01-13 16:15:10 | INFO | train_inner | epoch 068:     32 / 50 loss=6.765, ppl=108.76, wps=16973.8, ups=13.6, wpb=1247.9, bsz=62.7, num_updates=3380, lr=1.014e-05, gnorm=7.725, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=650
2022-01-13 16:15:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:12 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.755 | ppl 108 | wps 26198.4 | wpb 556.6 | bsz 30.3 | num_updates 3398 | best_loss 6.755
2022-01-13 16:15:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 3398 updates
2022-01-13 16:15:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:15:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:15:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 68 @ 3398 updates, score 6.755) (writing took 3.872889523860067 seconds)
2022-01-13 16:15:16 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-01-13 16:15:16 | INFO | train | epoch 068 | loss 6.74 | ppl 106.92 | wps 7393.7 | ups 6 | wpb 1232.9 | bsz 63.5 | num_updates 3398 | lr 1.0194e-05 | gnorm 7.578 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 656
2022-01-13 16:15:16 | INFO | fairseq.trainer | begin training epoch 69
2022-01-13 16:15:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:15:16 | INFO | train_inner | epoch 069:      2 / 50 loss=6.698, ppl=103.81, wps=3486.5, ups=3.24, wpb=1076.8, bsz=64, num_updates=3400, lr=1.02e-05, gnorm=7.644, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=656
2022-01-13 16:15:18 | INFO | train_inner | epoch 069:     22 / 50 loss=6.801, ppl=111.49, wps=17688.4, ups=12.84, wpb=1377.8, bsz=64, num_updates=3420, lr=1.026e-05, gnorm=7.17, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=658
2022-01-13 16:15:19 | INFO | train_inner | epoch 069:     42 / 50 loss=6.594, ppl=96.63, wps=16251.7, ups=13.88, wpb=1170.5, bsz=62.7, num_updates=3440, lr=1.032e-05, gnorm=7.768, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=659
2022-01-13 16:15:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:21 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.625 | ppl 98.73 | wps 26391.5 | wpb 556.6 | bsz 30.3 | num_updates 3448 | best_loss 6.625
2022-01-13 16:15:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 3448 updates
2022-01-13 16:15:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:15:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:15:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 69 @ 3448 updates, score 6.625) (writing took 3.9896547191310674 seconds)
2022-01-13 16:15:25 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-01-13 16:15:25 | INFO | train | epoch 069 | loss 6.675 | ppl 102.22 | wps 7095.6 | ups 5.76 | wpb 1232.9 | bsz 63.5 | num_updates 3448 | lr 1.0344e-05 | gnorm 7.563 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 664
2022-01-13 16:15:25 | INFO | fairseq.trainer | begin training epoch 70
2022-01-13 16:15:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:15:26 | INFO | train_inner | epoch 070:     12 / 50 loss=6.707, ppl=104.47, wps=3812.4, ups=3.12, wpb=1221.6, bsz=62.7, num_updates=3460, lr=1.038e-05, gnorm=7.845, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=665
2022-01-13 16:15:27 | INFO | train_inner | epoch 070:     32 / 50 loss=6.524, ppl=92.03, wps=16901.9, ups=14.05, wpb=1203.3, bsz=64, num_updates=3480, lr=1.044e-05, gnorm=7.546, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=667
2022-01-13 16:15:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:29 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.717 | ppl 105.22 | wps 27867.3 | wpb 556.6 | bsz 30.3 | num_updates 3498 | best_loss 6.625
2022-01-13 16:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 3498 updates
2022-01-13 16:15:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 70 @ 3498 updates, score 6.717) (writing took 2.5344681250862777 seconds)
2022-01-13 16:15:32 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-01-13 16:15:32 | INFO | train | epoch 070 | loss 6.609 | ppl 97.6 | wps 8852.7 | ups 7.18 | wpb 1232.9 | bsz 63.5 | num_updates 3498 | lr 1.0494e-05 | gnorm 7.512 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 671
2022-01-13 16:15:32 | INFO | fairseq.trainer | begin training epoch 71
2022-01-13 16:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:15:32 | INFO | train_inner | epoch 071:      2 / 50 loss=6.685, ppl=102.91, wps=5199.8, ups=4.16, wpb=1248.8, bsz=64, num_updates=3500, lr=1.05e-05, gnorm=7.301, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=672
2022-01-13 16:15:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:32 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.722 | ppl 105.6 | wps 27954.9 | wpb 556.6 | bsz 30.3 | num_updates 3500 | best_loss 6.625
2022-01-13 16:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3500 updates
2022-01-13 16:15:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt
2022-01-13 16:15:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt
2022-01-13 16:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt (epoch 71 @ 3500 updates, score 6.722) (writing took 4.007647620048374 seconds)
2022-01-13 16:15:38 | INFO | train_inner | epoch 071:     22 / 50 loss=6.547, ppl=93.53, wps=3558, ups=3.04, wpb=1169.3, bsz=64, num_updates=3520, lr=1.056e-05, gnorm=7.828, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=678
2022-01-13 16:15:40 | INFO | train_inner | epoch 071:     42 / 50 loss=6.582, ppl=95.81, wps=16583.3, ups=13.7, wpb=1210.1, bsz=64, num_updates=3540, lr=1.062e-05, gnorm=7.678, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=680
2022-01-13 16:15:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:41 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.665 | ppl 101.48 | wps 27130 | wpb 556.6 | bsz 30.3 | num_updates 3548 | best_loss 6.625
2022-01-13 16:15:41 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-01-13 16:15:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3548 updates
2022-01-13 16:15:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 71 @ 3548 updates, score 6.665) (writing took 2.46215064288117 seconds)
2022-01-13 16:15:44 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-01-13 16:15:44 | INFO | train | epoch 071 | loss 6.616 | ppl 98.1 | wps 5065 | ups 4.11 | wpb 1232.9 | bsz 63.5 | num_updates 3548 | lr 1.0644e-05 | gnorm 7.696 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 684
2022-01-13 16:15:44 | INFO | fairseq_cli.train | done training in 679.1 seconds
2022-02-25 16:31:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-25 16:31:30 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-25 16:31:32 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-25 16:31:32 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-25 16:31:32 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-25 16:31:32 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-25 16:31:32 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-25 16:31:32 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-25 16:32:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-25 16:32:28 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-25 16:32:31 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-25 16:32:31 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-25 16:32:31 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-25 16:32:31 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-25 16:32:31 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-25 16:32:31 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-26 17:18:50 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-26 17:18:50 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-26 17:18:52 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-26 17:18:52 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-26 17:18:52 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-26 17:18:52 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-26 17:18:52 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-26 17:18:52 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-26 17:19:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-26 17:19:30 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-26 17:19:33 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-26 17:19:33 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-26 17:19:33 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-26 17:19:33 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-26 17:19:33 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-26 17:19:33 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
mmmm
2022-02-26 19:08:08 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-26 19:08:08 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-26 19:08:11 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-26 19:08:11 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-26 19:08:11 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-26 19:08:11 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-26 19:08:11 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-26 19:08:11 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-26 19:08:11 | INFO | bartabst.data.data_utils | loaded 598 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-02-26 19:08:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-02-26 19:08:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-26 19:08:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-02-26 19:08:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-26 19:08:16 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-02-26 19:08:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-26 19:08:16 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-26 19:08:16 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-02-26 19:08:16 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-02-26 19:08:19 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-02-26 19:08:20 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-02-26 19:08:20 | INFO | fairseq.trainer | loading train data for epoch 1
no aos file, no transfer aos used
2022-02-26 19:08:21 | INFO | bartabst.data.data_utils | loaded 1,910 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-02-26 19:08:21 | INFO | fairseq.trainer | begin training epoch 1
2022-02-26 19:08:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:08:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-26 19:08:23 | INFO | train_inner | epoch 001:     21 / 31 loss=17.267, ppl=157724, wps=16734.4, ups=13.81, wpb=1226.7, bsz=63.2, num_updates=20, lr=6e-08, gnorm=23.218, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=7
2022-02-26 19:08:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:08:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.127 | ppl 143116 | wps 27660.5 | wpb 591.2 | bsz 29.9 | num_updates 30
2022-02-26 19:08:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 30 updates
2022-02-26 19:08:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 30 updates, score 17.127) (writing took 11.127412697998807 seconds)
2022-02-26 19:08:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-26 19:08:36 | INFO | train | epoch 001 | loss 17.3 | ppl 161327 | wps 2577.3 | ups 2.1 | wpb 1238.7 | bsz 61.5 | num_updates 30 | lr 9e-08 | gnorm 23.531 | clip 100 | loss_scale 64 | train_wall 3 | gb_free 20.9 | wall 20
2022-02-26 19:08:36 | INFO | fairseq.trainer | begin training epoch 2
2022-02-26 19:08:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:08:36 | INFO | train_inner | epoch 002:     10 / 31 loss=17.275, ppl=158579, wps=1949.4, ups=1.51, wpb=1290.9, bsz=59.8, num_updates=40, lr=1.2e-07, gnorm=23.583, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=20
2022-02-26 19:08:38 | INFO | train_inner | epoch 002:     30 / 31 loss=17.298, ppl=161138, wps=18669.7, ups=15.35, wpb=1216.2, bsz=63.2, num_updates=60, lr=1.8e-07, gnorm=23.346, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=22
2022-02-26 19:08:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:08:38 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 17.054 | ppl 136065 | wps 31990.7 | wpb 591.2 | bsz 29.9 | num_updates 61 | best_loss 17.054
2022-02-26 19:08:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 61 updates
2022-02-26 19:08:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 61 updates, score 17.054) (writing took 6.530774657992879 seconds)
2022-02-26 19:08:45 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-26 19:08:45 | INFO | train | epoch 002 | loss 17.254 | ppl 156312 | wps 4169.8 | ups 3.4 | wpb 1227.6 | bsz 61.6 | num_updates 61 | lr 1.83e-07 | gnorm 23.324 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 29
2022-02-26 19:08:45 | INFO | fairseq.trainer | begin training epoch 3
2022-02-26 19:08:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:08:46 | INFO | train_inner | epoch 003:     19 / 31 loss=17.196, ppl=150139, wps=2790.8, ups=2.38, wpb=1173.7, bsz=60.3, num_updates=80, lr=2.4e-07, gnorm=23.684, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=30
2022-02-26 19:08:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:08:47 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.898 | ppl 122164 | wps 30591.3 | wpb 591.2 | bsz 29.9 | num_updates 92 | best_loss 16.898
2022-02-26 19:08:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 92 updates
2022-02-26 19:08:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 92 updates, score 16.898) (writing took 6.711596481996821 seconds)
2022-02-26 19:08:54 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-26 19:08:54 | INFO | train | epoch 003 | loss 17.118 | ppl 142264 | wps 4057.4 | ups 3.3 | wpb 1227.6 | bsz 61.6 | num_updates 92 | lr 2.76e-07 | gnorm 23.04 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 38
2022-02-26 19:08:54 | INFO | fairseq.trainer | begin training epoch 4
2022-02-26 19:08:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:08:55 | INFO | train_inner | epoch 004:      8 / 31 loss=16.977, ppl=129002, wps=2733.6, ups=2.28, wpb=1196.8, bsz=62.4, num_updates=100, lr=3e-07, gnorm=22.425, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=39
2022-02-26 19:08:56 | INFO | train_inner | epoch 004:     28 / 31 loss=16.92, ppl=124037, wps=20762.6, ups=15.27, wpb=1359.5, bsz=61.9, num_updates=120, lr=3.6e-07, gnorm=22.225, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=40
2022-02-26 19:08:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:08:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 16.547 | ppl 95749.3 | wps 31889.3 | wpb 591.2 | bsz 29.9 | num_updates 123 | best_loss 16.547
2022-02-26 19:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 123 updates
2022-02-26 19:08:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 123 updates, score 16.547) (writing took 5.007286517997272 seconds)
2022-02-26 19:09:02 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-26 19:09:02 | INFO | train | epoch 004 | loss 16.923 | ppl 124269 | wps 4930.9 | ups 4.02 | wpb 1227.6 | bsz 61.6 | num_updates 123 | lr 3.69e-07 | gnorm 22.883 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 46
2022-02-26 19:09:02 | INFO | fairseq.trainer | begin training epoch 5
2022-02-26 19:09:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:03 | INFO | train_inner | epoch 005:     17 / 31 loss=16.68, ppl=105006, wps=3596.8, ups=2.89, wpb=1244, bsz=62.4, num_updates=140, lr=4.2e-07, gnorm=22.544, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=47
2022-02-26 19:09:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:04 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 16.256 | ppl 78268.7 | wps 31397.1 | wpb 591.2 | bsz 29.9 | num_updates 154 | best_loss 16.256
2022-02-26 19:09:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 154 updates
2022-02-26 19:09:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 154 updates, score 16.256) (writing took 4.709121777996188 seconds)
2022-02-26 19:09:09 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-26 19:09:09 | INFO | train | epoch 005 | loss 16.632 | ppl 101583 | wps 5241.2 | ups 4.27 | wpb 1227.6 | bsz 61.6 | num_updates 154 | lr 4.62e-07 | gnorm 22.153 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 53
2022-02-26 19:09:09 | INFO | fairseq.trainer | begin training epoch 6
2022-02-26 19:09:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:10 | INFO | train_inner | epoch 006:      6 / 31 loss=16.541, ppl=95381.6, wps=3624.2, ups=3, wpb=1207.8, bsz=59.5, num_updates=160, lr=4.8e-07, gnorm=22.416, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=54
2022-02-26 19:09:11 | INFO | train_inner | epoch 006:     26 / 31 loss=16.35, ppl=83504.3, wps=17172.5, ups=14.57, wpb=1178.5, bsz=62.7, num_updates=180, lr=5.4e-07, gnorm=21.698, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=55
2022-02-26 19:09:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-26 19:09:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:12 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 16.015 | ppl 66201.8 | wps 31495.4 | wpb 591.2 | bsz 29.9 | num_updates 184 | best_loss 16.015
2022-02-26 19:09:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 184 updates
2022-02-26 19:09:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 184 updates, score 16.015) (writing took 3.9126116810075473 seconds)
2022-02-26 19:09:16 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-26 19:09:16 | INFO | train | epoch 006 | loss 16.342 | ppl 83077.8 | wps 5605.1 | ups 4.5 | wpb 1246.2 | bsz 61.5 | num_updates 184 | lr 5.52e-07 | gnorm 21.84 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 60
2022-02-26 19:09:16 | INFO | fairseq.trainer | begin training epoch 7
2022-02-26 19:09:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:17 | INFO | train_inner | epoch 007:     16 / 31 loss=16.094, ppl=69966.1, wps=4486.3, ups=3.38, wpb=1328.3, bsz=61.6, num_updates=200, lr=6e-07, gnorm=21.287, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=61
2022-02-26 19:09:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:18 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 15.6 | ppl 49658.7 | wps 31329.4 | wpb 591.2 | bsz 29.9 | num_updates 215 | best_loss 15.6
2022-02-26 19:09:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 215 updates
2022-02-26 19:09:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 215 updates, score 15.6) (writing took 3.949698372001876 seconds)
2022-02-26 19:09:22 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-26 19:09:22 | INFO | train | epoch 007 | loss 15.998 | ppl 65435.6 | wps 5724.5 | ups 4.66 | wpb 1227.6 | bsz 61.6 | num_updates 215 | lr 6.45e-07 | gnorm 20.749 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 67
2022-02-26 19:09:22 | INFO | fairseq.trainer | begin training epoch 8
2022-02-26 19:09:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:23 | INFO | train_inner | epoch 008:      5 / 31 loss=15.897, ppl=61030, wps=3694, ups=3.41, wpb=1083.2, bsz=61.1, num_updates=220, lr=6.6e-07, gnorm=20.573, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=67
2022-02-26 19:09:24 | INFO | train_inner | epoch 008:     25 / 31 loss=15.657, ppl=51683.7, wps=20923.1, ups=15.46, wpb=1353.2, bsz=63.2, num_updates=240, lr=7.2e-07, gnorm=19.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=68
2022-02-26 19:09:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:25 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 15.2 | ppl 37630.2 | wps 31294.9 | wpb 591.2 | bsz 29.9 | num_updates 246 | best_loss 15.2
2022-02-26 19:09:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 246 updates
2022-02-26 19:09:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 246 updates, score 15.2) (writing took 4.546259619994089 seconds)
2022-02-26 19:09:30 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-26 19:09:30 | INFO | train | epoch 008 | loss 15.657 | ppl 51682.3 | wps 5323.9 | ups 4.34 | wpb 1227.6 | bsz 61.6 | num_updates 246 | lr 7.38e-07 | gnorm 20.119 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 74
2022-02-26 19:09:30 | INFO | fairseq.trainer | begin training epoch 9
2022-02-26 19:09:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:31 | INFO | train_inner | epoch 009:     14 / 31 loss=15.318, ppl=40853.8, wps=3772.8, ups=3.1, wpb=1215.7, bsz=59.8, num_updates=260, lr=7.8e-07, gnorm=20.633, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=75
2022-02-26 19:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:32 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 14.784 | ppl 28201.9 | wps 30723.6 | wpb 591.2 | bsz 29.9 | num_updates 277 | best_loss 14.784
2022-02-26 19:09:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 277 updates
2022-02-26 19:09:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 277 updates, score 14.784) (writing took 3.9165174679947086 seconds)
2022-02-26 19:09:36 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-26 19:09:36 | INFO | train | epoch 009 | loss 15.208 | ppl 37842 | wps 5834.8 | ups 4.75 | wpb 1227.6 | bsz 61.6 | num_updates 277 | lr 8.31e-07 | gnorm 19.682 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 80
2022-02-26 19:09:36 | INFO | fairseq.trainer | begin training epoch 10
2022-02-26 19:09:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:36 | INFO | train_inner | epoch 010:      3 / 31 loss=15.111, ppl=35396.6, wps=3779.2, ups=3.46, wpb=1093.2, bsz=61.6, num_updates=280, lr=8.4e-07, gnorm=19.561, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=81
2022-02-26 19:09:38 | INFO | train_inner | epoch 010:     23 / 31 loss=14.767, ppl=27885.5, wps=19380, ups=14.41, wpb=1344.5, bsz=61.9, num_updates=300, lr=9e-07, gnorm=17.216, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=82
2022-02-26 19:09:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:39 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 14.438 | ppl 22195.1 | wps 32152 | wpb 591.2 | bsz 29.9 | num_updates 308 | best_loss 14.438
2022-02-26 19:09:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 308 updates
2022-02-26 19:09:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 308 updates, score 14.438) (writing took 3.778908355991007 seconds)
2022-02-26 19:09:43 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-26 19:09:43 | INFO | train | epoch 010 | loss 14.748 | ppl 27507.5 | wps 5887.4 | ups 4.8 | wpb 1227.6 | bsz 61.6 | num_updates 308 | lr 9.24e-07 | gnorm 17.533 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 87
2022-02-26 19:09:43 | INFO | fairseq.trainer | begin training epoch 11
2022-02-26 19:09:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:43 | INFO | train_inner | epoch 011:     12 / 31 loss=14.55, ppl=23989.2, wps=4322.3, ups=3.52, wpb=1227, bsz=62.4, num_updates=320, lr=9.6e-07, gnorm=17.459, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=88
2022-02-26 19:09:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:45 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 14.021 | ppl 16628.7 | wps 32804.8 | wpb 591.2 | bsz 29.9 | num_updates 339 | best_loss 14.021
2022-02-26 19:09:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 339 updates
2022-02-26 19:09:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 339 updates, score 14.021) (writing took 3.782653207992553 seconds)
2022-02-26 19:09:49 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-26 19:09:49 | INFO | train | epoch 011 | loss 14.325 | ppl 20528.7 | wps 5956.9 | ups 4.85 | wpb 1227.6 | bsz 61.6 | num_updates 339 | lr 1.017e-06 | gnorm 16.713 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 93
2022-02-26 19:09:49 | INFO | fairseq.trainer | begin training epoch 12
2022-02-26 19:09:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:49 | INFO | train_inner | epoch 012:      1 / 31 loss=14.189, ppl=18678.4, wps=4173.9, ups=3.54, wpb=1179.8, bsz=60.3, num_updates=340, lr=1.02e-06, gnorm=16.467, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=93
2022-02-26 19:09:50 | INFO | train_inner | epoch 012:     21 / 31 loss=13.987, ppl=16233.5, wps=18195.8, ups=15.01, wpb=1212.4, bsz=64, num_updates=360, lr=1.08e-06, gnorm=15.674, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=95
2022-02-26 19:09:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:52 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 13.544 | ppl 11946.6 | wps 30924.4 | wpb 591.2 | bsz 29.9 | num_updates 370 | best_loss 13.544
2022-02-26 19:09:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 370 updates
2022-02-26 19:09:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 370 updates, score 13.544) (writing took 3.8822677620046306 seconds)
2022-02-26 19:09:56 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-26 19:09:56 | INFO | train | epoch 012 | loss 13.923 | ppl 15532.8 | wps 5823.2 | ups 4.74 | wpb 1227.6 | bsz 61.6 | num_updates 370 | lr 1.11e-06 | gnorm 15.821 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 100
2022-02-26 19:09:56 | INFO | fairseq.trainer | begin training epoch 13
2022-02-26 19:09:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:56 | INFO | train_inner | epoch 013:     10 / 31 loss=13.682, ppl=13142.1, wps=4378.4, ups=3.41, wpb=1285.8, bsz=58.2, num_updates=380, lr=1.14e-06, gnorm=15.643, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=100
2022-02-26 19:09:58 | INFO | train_inner | epoch 013:     30 / 31 loss=13.378, ppl=10642.9, wps=17980.5, ups=14.78, wpb=1216.2, bsz=64, num_updates=400, lr=1.2e-06, gnorm=14.797, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=102
2022-02-26 19:09:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:58 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 13.118 | ppl 8889.96 | wps 26814.4 | wpb 591.2 | bsz 29.9 | num_updates 401 | best_loss 13.118
2022-02-26 19:09:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 401 updates
2022-02-26 19:09:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 401 updates, score 13.118) (writing took 3.7940042299887864 seconds)
2022-02-26 19:10:02 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-26 19:10:02 | INFO | train | epoch 013 | loss 13.449 | ppl 11185.6 | wps 5868.9 | ups 4.78 | wpb 1227.6 | bsz 61.6 | num_updates 401 | lr 1.203e-06 | gnorm 15.004 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 106
2022-02-26 19:10:02 | INFO | fairseq.trainer | begin training epoch 14
2022-02-26 19:10:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:03 | INFO | train_inner | epoch 014:     19 / 31 loss=13.181, ppl=9290.04, wps=4056.5, ups=3.46, wpb=1171.7, bsz=60.3, num_updates=420, lr=1.26e-06, gnorm=15.138, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=108
2022-02-26 19:10:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:05 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 12.85 | ppl 7380.64 | wps 32035 | wpb 591.2 | bsz 29.9 | num_updates 432 | best_loss 12.85
2022-02-26 19:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 432 updates
2022-02-26 19:10:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 432 updates, score 12.85) (writing took 3.7495829359977506 seconds)
2022-02-26 19:10:09 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-26 19:10:09 | INFO | train | epoch 014 | loss 13.122 | ppl 8915.98 | wps 5886.3 | ups 4.79 | wpb 1227.6 | bsz 61.6 | num_updates 432 | lr 1.296e-06 | gnorm 14.476 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 113
2022-02-26 19:10:09 | INFO | fairseq.trainer | begin training epoch 15
2022-02-26 19:10:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:09 | INFO | train_inner | epoch 015:      8 / 31 loss=12.983, ppl=8094.65, wps=4602, ups=3.5, wpb=1314.1, bsz=62.4, num_updates=440, lr=1.32e-06, gnorm=13.071, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=113
2022-02-26 19:10:11 | INFO | train_inner | epoch 015:     28 / 31 loss=12.839, ppl=7327.74, wps=18360.9, ups=15.29, wpb=1200.9, bsz=61.9, num_updates=460, lr=1.38e-06, gnorm=13.141, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=115
2022-02-26 19:10:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:11 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 12.678 | ppl 6552.71 | wps 32283.3 | wpb 591.2 | bsz 29.9 | num_updates 463 | best_loss 12.678
2022-02-26 19:10:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 463 updates
2022-02-26 19:10:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 463 updates, score 12.678) (writing took 3.7875990480097244 seconds)
2022-02-26 19:10:15 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-26 19:10:15 | INFO | train | epoch 015 | loss 12.854 | ppl 7404.58 | wps 5959.7 | ups 4.85 | wpb 1227.6 | bsz 61.6 | num_updates 463 | lr 1.389e-06 | gnorm 13.211 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 119
2022-02-26 19:10:15 | INFO | fairseq.trainer | begin training epoch 16
2022-02-26 19:10:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:16 | INFO | train_inner | epoch 016:     17 / 31 loss=12.654, ppl=6445.65, wps=4413, ups=3.54, wpb=1248.2, bsz=61.6, num_updates=480, lr=1.44e-06, gnorm=12.97, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=120
2022-02-26 19:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:18 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 12.382 | ppl 5337.52 | wps 31585 | wpb 591.2 | bsz 29.9 | num_updates 494 | best_loss 12.382
2022-02-26 19:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 494 updates
2022-02-26 19:10:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 494 updates, score 12.382) (writing took 4.13467990400386 seconds)
2022-02-26 19:10:22 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-26 19:10:22 | INFO | train | epoch 016 | loss 12.568 | ppl 6071.77 | wps 5613.4 | ups 4.57 | wpb 1227.6 | bsz 61.6 | num_updates 494 | lr 1.482e-06 | gnorm 12.295 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 126
2022-02-26 19:10:22 | INFO | fairseq.trainer | begin training epoch 17
2022-02-26 19:10:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:22 | INFO | train_inner | epoch 017:      6 / 31 loss=12.404, ppl=5417.92, wps=3887.8, ups=3.3, wpb=1179.1, bsz=59.8, num_updates=500, lr=1.5e-06, gnorm=12.802, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=126
2022-02-26 19:10:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:23 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.343 | ppl 5194.13 | wps 30918.9 | wpb 591.2 | bsz 29.9 | num_updates 500 | best_loss 12.343
2022-02-26 19:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 500 updates
2022-02-26 19:10:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-26 19:10:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-26 19:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt (epoch 17 @ 500 updates, score 12.343) (writing took 9.700520924990997 seconds)
2022-02-26 19:10:34 | INFO | train_inner | epoch 017:     26 / 31 loss=12.403, ppl=5414.69, wps=2238.4, ups=1.7, wpb=1319, bsz=63.2, num_updates=520, lr=1.56e-06, gnorm=11.284, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=138
2022-02-26 19:10:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:35 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.107 | ppl 4410.85 | wps 30057.2 | wpb 591.2 | bsz 29.9 | num_updates 525 | best_loss 12.107
2022-02-26 19:10:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 525 updates
2022-02-26 19:10:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 525 updates, score 12.107) (writing took 4.579516243000398 seconds)
2022-02-26 19:10:39 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-26 19:10:39 | INFO | train | epoch 017 | loss 12.334 | ppl 5163.78 | wps 2146.3 | ups 1.75 | wpb 1227.6 | bsz 61.6 | num_updates 525 | lr 1.575e-06 | gnorm 12.207 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 144
2022-02-26 19:10:39 | INFO | fairseq.trainer | begin training epoch 18
2022-02-26 19:10:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:41 | INFO | train_inner | epoch 018:     15 / 31 loss=12.154, ppl=4558.86, wps=3985.1, ups=3.07, wpb=1297.2, bsz=61.1, num_updates=540, lr=1.62e-06, gnorm=11.494, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=145
2022-02-26 19:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:42 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 12.041 | ppl 4213.69 | wps 30888.2 | wpb 591.2 | bsz 29.9 | num_updates 556 | best_loss 12.041
2022-02-26 19:10:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 556 updates
2022-02-26 19:10:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 556 updates, score 12.041) (writing took 5.406727541994769 seconds)
2022-02-26 19:10:48 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-26 19:10:48 | INFO | train | epoch 018 | loss 12.126 | ppl 4470.84 | wps 4715.9 | ups 3.84 | wpb 1227.6 | bsz 61.6 | num_updates 556 | lr 1.668e-06 | gnorm 11.134 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 152
2022-02-26 19:10:48 | INFO | fairseq.trainer | begin training epoch 19
2022-02-26 19:10:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:48 | INFO | train_inner | epoch 019:      4 / 31 loss=12.078, ppl=4324.56, wps=2770.6, ups=2.71, wpb=1023.3, bsz=61.6, num_updates=560, lr=1.68e-06, gnorm=11.391, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=152
2022-02-26 19:10:49 | INFO | train_inner | epoch 019:     24 / 31 loss=12.006, ppl=4113.69, wps=19849.2, ups=15.28, wpb=1298.8, bsz=63.2, num_updates=580, lr=1.74e-06, gnorm=10.387, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=153
2022-02-26 19:10:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:50 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.817 | ppl 3606.98 | wps 32030.5 | wpb 591.2 | bsz 29.9 | num_updates 587 | best_loss 11.817
2022-02-26 19:10:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 587 updates
2022-02-26 19:10:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 587 updates, score 11.817) (writing took 3.7990156619925983 seconds)
2022-02-26 19:10:54 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-26 19:10:54 | INFO | train | epoch 019 | loss 11.989 | ppl 4065.27 | wps 5941.9 | ups 4.84 | wpb 1227.6 | bsz 61.6 | num_updates 587 | lr 1.761e-06 | gnorm 10.621 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 158
2022-02-26 19:10:54 | INFO | fairseq.trainer | begin training epoch 20
2022-02-26 19:10:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:55 | INFO | train_inner | epoch 020:     13 / 31 loss=11.914, ppl=3858.43, wps=4315.6, ups=3.49, wpb=1235, bsz=61.1, num_updates=600, lr=1.8e-06, gnorm=10.549, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=159
2022-02-26 19:10:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:57 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.545 | ppl 2988.13 | wps 31427.2 | wpb 591.2 | bsz 29.9 | num_updates 618 | best_loss 11.545
2022-02-26 19:10:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 618 updates
2022-02-26 19:10:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 618 updates, score 11.545) (writing took 3.8354490509955212 seconds)
2022-02-26 19:11:01 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-26 19:11:01 | INFO | train | epoch 020 | loss 11.815 | ppl 3603.1 | wps 5735.7 | ups 4.67 | wpb 1227.6 | bsz 61.6 | num_updates 618 | lr 1.854e-06 | gnorm 11.158 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 165
2022-02-26 19:11:01 | INFO | fairseq.trainer | begin training epoch 21
2022-02-26 19:11:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:01 | INFO | train_inner | epoch 021:      2 / 31 loss=11.702, ppl=3332.18, wps=4048.3, ups=3.42, wpb=1183, bsz=60.3, num_updates=620, lr=1.86e-06, gnorm=11.543, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=165
2022-02-26 19:11:02 | INFO | train_inner | epoch 021:     22 / 31 loss=11.691, ppl=3306.81, wps=17977.6, ups=14.66, wpb=1226.5, bsz=62.7, num_updates=640, lr=1.92e-06, gnorm=9.858, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=166
2022-02-26 19:11:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:03 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.508 | ppl 2912.61 | wps 32105.3 | wpb 591.2 | bsz 29.9 | num_updates 649 | best_loss 11.508
2022-02-26 19:11:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 649 updates
2022-02-26 19:11:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 649 updates, score 11.508) (writing took 4.798500946999411 seconds)
2022-02-26 19:11:08 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-26 19:11:08 | INFO | train | epoch 021 | loss 11.644 | ppl 3200.83 | wps 5121 | ups 4.17 | wpb 1227.6 | bsz 61.6 | num_updates 649 | lr 1.947e-06 | gnorm 9.871 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 172
2022-02-26 19:11:08 | INFO | fairseq.trainer | begin training epoch 22
2022-02-26 19:11:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:09 | INFO | train_inner | epoch 022:     11 / 31 loss=11.537, ppl=2971.62, wps=3589.4, ups=3, wpb=1198, bsz=61.6, num_updates=660, lr=1.98e-06, gnorm=9.992, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=173
2022-02-26 19:11:10 | INFO | train_inner | epoch 022:     31 / 31 loss=11.538, ppl=2974.03, wps=18382.4, ups=14.63, wpb=1256.2, bsz=60.3, num_updates=680, lr=2.04e-06, gnorm=9.724, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=174
2022-02-26 19:11:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:11 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.344 | ppl 2599.5 | wps 29212 | wpb 591.2 | bsz 29.9 | num_updates 680 | best_loss 11.344
2022-02-26 19:11:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 680 updates
2022-02-26 19:11:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 680 updates, score 11.344) (writing took 3.9118930459953845 seconds)
2022-02-26 19:11:15 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-26 19:11:15 | INFO | train | epoch 022 | loss 11.527 | ppl 2951.19 | wps 5766.3 | ups 4.7 | wpb 1227.6 | bsz 61.6 | num_updates 680 | lr 2.04e-06 | gnorm 9.833 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 179
2022-02-26 19:11:15 | INFO | fairseq.trainer | begin training epoch 23
2022-02-26 19:11:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:16 | INFO | train_inner | epoch 023:     20 / 31 loss=11.339, ppl=2590.56, wps=4263.1, ups=3.4, wpb=1255.1, bsz=61.9, num_updates=700, lr=2.1e-06, gnorm=9.217, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=180
2022-02-26 19:11:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:17 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.297 | ppl 2515.29 | wps 31052 | wpb 591.2 | bsz 29.9 | num_updates 711 | best_loss 11.297
2022-02-26 19:11:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 711 updates
2022-02-26 19:11:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 711 updates, score 11.297) (writing took 3.854022731000441 seconds)
2022-02-26 19:11:21 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-26 19:11:21 | INFO | train | epoch 023 | loss 11.353 | ppl 2616.2 | wps 5885.9 | ups 4.79 | wpb 1227.6 | bsz 61.6 | num_updates 711 | lr 2.133e-06 | gnorm 9.329 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 185
2022-02-26 19:11:21 | INFO | fairseq.trainer | begin training epoch 24
2022-02-26 19:11:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:22 | INFO | train_inner | epoch 024:      9 / 31 loss=11.395, ppl=2693.73, wps=4374.9, ups=3.5, wpb=1250.5, bsz=62.4, num_updates=720, lr=2.16e-06, gnorm=9.237, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=186
2022-02-26 19:11:23 | INFO | train_inner | epoch 024:     29 / 31 loss=11.32, ppl=2557.36, wps=18468.3, ups=15.05, wpb=1227.5, bsz=61.9, num_updates=740, lr=2.22e-06, gnorm=8.948, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=187
2022-02-26 19:11:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:24 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.086 | ppl 2173.92 | wps 30058.4 | wpb 591.2 | bsz 29.9 | num_updates 742 | best_loss 11.086
2022-02-26 19:11:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 742 updates
2022-02-26 19:11:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 742 updates, score 11.086) (writing took 3.908917416993063 seconds)
2022-02-26 19:11:28 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-26 19:11:28 | INFO | train | epoch 024 | loss 11.324 | ppl 2564.11 | wps 5822.6 | ups 4.74 | wpb 1227.6 | bsz 61.6 | num_updates 742 | lr 2.226e-06 | gnorm 9.045 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 192
2022-02-26 19:11:28 | INFO | fairseq.trainer | begin training epoch 25
2022-02-26 19:11:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:29 | INFO | train_inner | epoch 025:     18 / 31 loss=11.159, ppl=2287.17, wps=3909.8, ups=3.18, wpb=1231.2, bsz=61.6, num_updates=760, lr=2.28e-06, gnorm=8.99, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=194
2022-02-26 19:11:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:31 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.026 | ppl 2084.92 | wps 26860 | wpb 591.2 | bsz 29.9 | num_updates 773 | best_loss 11.026
2022-02-26 19:11:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 773 updates
2022-02-26 19:11:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 773 updates, score 11.026) (writing took 4.987794471002417 seconds)
2022-02-26 19:11:36 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-26 19:11:36 | INFO | train | epoch 025 | loss 11.197 | ppl 2348.43 | wps 4510.6 | ups 3.67 | wpb 1227.6 | bsz 61.6 | num_updates 773 | lr 2.319e-06 | gnorm 8.724 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 200
2022-02-26 19:11:36 | INFO | fairseq.trainer | begin training epoch 26
2022-02-26 19:11:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:37 | INFO | train_inner | epoch 026:      7 / 31 loss=11.19, ppl=2336.16, wps=3176.8, ups=2.77, wpb=1146.8, bsz=59.8, num_updates=780, lr=2.34e-06, gnorm=9.033, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=201
2022-02-26 19:11:38 | INFO | train_inner | epoch 026:     27 / 31 loss=11.029, ppl=2089.19, wps=16993.6, ups=12.9, wpb=1317.7, bsz=63.2, num_updates=800, lr=2.4e-06, gnorm=9.143, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=202
2022-02-26 19:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:39 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 10.851 | ppl 1846.91 | wps 29921.7 | wpb 591.2 | bsz 29.9 | num_updates 804 | best_loss 10.851
2022-02-26 19:11:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 804 updates
2022-02-26 19:11:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 804 updates, score 10.851) (writing took 4.511411895000492 seconds)
2022-02-26 19:11:44 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-26 19:11:44 | INFO | train | epoch 026 | loss 11.093 | ppl 2185.07 | wps 5126.7 | ups 4.18 | wpb 1227.6 | bsz 61.6 | num_updates 804 | lr 2.412e-06 | gnorm 9.281 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 208
2022-02-26 19:11:44 | INFO | fairseq.trainer | begin training epoch 27
2022-02-26 19:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:45 | INFO | train_inner | epoch 027:     16 / 31 loss=11.013, ppl=2065.9, wps=3741.3, ups=3.06, wpb=1221, bsz=61.1, num_updates=820, lr=2.46e-06, gnorm=8.62, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=209
2022-02-26 19:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:46 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.76 | ppl 1733.8 | wps 30987.6 | wpb 591.2 | bsz 29.9 | num_updates 835 | best_loss 10.76
2022-02-26 19:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 835 updates
2022-02-26 19:11:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 835 updates, score 10.76) (writing took 4.025151114008622 seconds)
2022-02-26 19:11:50 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-26 19:11:50 | INFO | train | epoch 027 | loss 10.966 | ppl 1999.96 | wps 5596.6 | ups 4.56 | wpb 1227.6 | bsz 61.6 | num_updates 835 | lr 2.505e-06 | gnorm 8.672 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 215
2022-02-26 19:11:50 | INFO | fairseq.trainer | begin training epoch 28
2022-02-26 19:11:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:51 | INFO | train_inner | epoch 028:      5 / 31 loss=10.988, ppl=2031.63, wps=3748.1, ups=3.34, wpb=1122.6, bsz=60.8, num_updates=840, lr=2.52e-06, gnorm=8.795, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=215
2022-02-26 19:11:52 | INFO | train_inner | epoch 028:     25 / 31 loss=10.788, ppl=1767.82, wps=17598.1, ups=14.49, wpb=1214.5, bsz=62.7, num_updates=860, lr=2.58e-06, gnorm=10.262, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=216
2022-02-26 19:11:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:53 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.691 | ppl 1652.8 | wps 32068.7 | wpb 591.2 | bsz 29.9 | num_updates 866 | best_loss 10.691
2022-02-26 19:11:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 866 updates
2022-02-26 19:11:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 866 updates, score 10.691) (writing took 4.2252347379981074 seconds)
2022-02-26 19:11:57 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-26 19:11:57 | INFO | train | epoch 028 | loss 10.855 | ppl 1852.78 | wps 5546.8 | ups 4.52 | wpb 1227.6 | bsz 61.6 | num_updates 866 | lr 2.598e-06 | gnorm 9.585 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 221
2022-02-26 19:11:57 | INFO | fairseq.trainer | begin training epoch 29
2022-02-26 19:11:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:58 | INFO | train_inner | epoch 029:     14 / 31 loss=10.899, ppl=1909.23, wps=4019.5, ups=3.27, wpb=1228.5, bsz=60.3, num_updates=880, lr=2.64e-06, gnorm=8.296, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=222
2022-02-26 19:11:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:00 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.618 | ppl 1571.74 | wps 30675.9 | wpb 591.2 | bsz 29.9 | num_updates 897 | best_loss 10.618
2022-02-26 19:12:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 897 updates
2022-02-26 19:12:00 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 897 updates, score 10.618) (writing took 3.971856525997282 seconds)
2022-02-26 19:12:04 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-26 19:12:04 | INFO | train | epoch 029 | loss 10.806 | ppl 1790.76 | wps 5707 | ups 4.65 | wpb 1227.6 | bsz 61.6 | num_updates 897 | lr 2.691e-06 | gnorm 8.207 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 228
2022-02-26 19:12:04 | INFO | fairseq.trainer | begin training epoch 30
2022-02-26 19:12:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:04 | INFO | train_inner | epoch 030:      3 / 31 loss=10.743, ppl=1713.4, wps=4390.5, ups=3.37, wpb=1303.6, bsz=62.4, num_updates=900, lr=2.7e-06, gnorm=8.117, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=228
2022-02-26 19:12:06 | INFO | train_inner | epoch 030:     23 / 31 loss=10.682, ppl=1643.05, wps=17442.3, ups=14.9, wpb=1170.5, bsz=61.9, num_updates=920, lr=2.76e-06, gnorm=8.06, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=230
2022-02-26 19:12:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:07 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.647 | ppl 1603.19 | wps 31159.7 | wpb 591.2 | bsz 29.9 | num_updates 928 | best_loss 10.618
2022-02-26 19:12:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 928 updates
2022-02-26 19:12:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:12:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:12:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 30 @ 928 updates, score 10.647) (writing took 2.567257739006891 seconds)
2022-02-26 19:12:09 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-26 19:12:09 | INFO | train | epoch 030 | loss 10.674 | ppl 1633.26 | wps 7310.9 | ups 5.96 | wpb 1227.6 | bsz 61.6 | num_updates 928 | lr 2.784e-06 | gnorm 8.007 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 233
2022-02-26 19:12:09 | INFO | fairseq.trainer | begin training epoch 31
2022-02-26 19:12:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:10 | INFO | train_inner | epoch 031:     12 / 31 loss=10.632, ppl=1587.05, wps=5672, ups=4.45, wpb=1273.7, bsz=62.4, num_updates=940, lr=2.82e-06, gnorm=8.049, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=234
2022-02-26 19:12:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:12 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.457 | ppl 1405.16 | wps 26767.1 | wpb 591.2 | bsz 29.9 | num_updates 959 | best_loss 10.457
2022-02-26 19:12:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 959 updates
2022-02-26 19:12:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 959 updates, score 10.457) (writing took 4.0277933399920585 seconds)
2022-02-26 19:12:16 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-26 19:12:16 | INFO | train | epoch 031 | loss 10.615 | ppl 1568.13 | wps 5621.5 | ups 4.58 | wpb 1227.6 | bsz 61.6 | num_updates 959 | lr 2.877e-06 | gnorm 8.346 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 240
2022-02-26 19:12:16 | INFO | fairseq.trainer | begin training epoch 32
2022-02-26 19:12:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:16 | INFO | train_inner | epoch 032:      1 / 31 loss=10.614, ppl=1567.61, wps=4077, ups=3.33, wpb=1222.8, bsz=60.3, num_updates=960, lr=2.88e-06, gnorm=8.383, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=240
2022-02-26 19:12:17 | INFO | train_inner | epoch 032:     21 / 31 loss=10.572, ppl=1522.22, wps=17911.2, ups=14.52, wpb=1234, bsz=63.2, num_updates=980, lr=2.94e-06, gnorm=7.93, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=242
2022-02-26 19:12:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:19 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 10.473 | ppl 1421.34 | wps 29350.3 | wpb 591.2 | bsz 29.9 | num_updates 990 | best_loss 10.457
2022-02-26 19:12:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 990 updates
2022-02-26 19:12:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:12:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:12:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 32 @ 990 updates, score 10.473) (writing took 2.668422298011137 seconds)
2022-02-26 19:12:21 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-26 19:12:21 | INFO | train | epoch 032 | loss 10.551 | ppl 1499.93 | wps 7144.7 | ups 5.82 | wpb 1227.6 | bsz 61.6 | num_updates 990 | lr 2.97e-06 | gnorm 8.047 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 245
2022-02-26 19:12:21 | INFO | fairseq.trainer | begin training epoch 33
2022-02-26 19:12:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:22 | INFO | train_inner | epoch 033:     10 / 31 loss=10.603, ppl=1554.93, wps=5584.6, ups=4.42, wpb=1262.8, bsz=59, num_updates=1000, lr=3e-06, gnorm=7.961, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=246
2022-02-26 19:12:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:22 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.448 | ppl 1397.11 | wps 30748.5 | wpb 591.2 | bsz 29.9 | num_updates 1000 | best_loss 10.448
2022-02-26 19:12:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1000 updates
2022-02-26 19:12:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-26 19:12:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-26 19:12:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt (epoch 33 @ 1000 updates, score 10.448) (writing took 6.152512922999449 seconds)
2022-02-26 19:12:30 | INFO | train_inner | epoch 033:     30 / 31 loss=10.363, ppl=1317.27, wps=2849.8, ups=2.38, wpb=1198.3, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=7.931, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=255
2022-02-26 19:12:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:31 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.381 | ppl 1333.83 | wps 30042 | wpb 591.2 | bsz 29.9 | num_updates 1021 | best_loss 10.381
2022-02-26 19:12:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1021 updates
2022-02-26 19:12:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1021 updates, score 10.381) (writing took 4.063839041991741 seconds)
2022-02-26 19:12:35 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-26 19:12:35 | INFO | train | epoch 033 | loss 10.473 | ppl 1421.1 | wps 2768.2 | ups 2.25 | wpb 1227.6 | bsz 61.6 | num_updates 1021 | lr 3.063e-06 | gnorm 7.876 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 259
2022-02-26 19:12:35 | INFO | fairseq.trainer | begin training epoch 34
2022-02-26 19:12:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:37 | INFO | train_inner | epoch 034:     19 / 31 loss=10.387, ppl=1339.03, wps=3748.8, ups=3.09, wpb=1211.8, bsz=62.4, num_updates=1040, lr=3.12e-06, gnorm=8.232, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=261
2022-02-26 19:12:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:38 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 10.37 | ppl 1323.46 | wps 27523.2 | wpb 591.2 | bsz 29.9 | num_updates 1052 | best_loss 10.37
2022-02-26 19:12:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1052 updates
2022-02-26 19:12:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1052 updates, score 10.37) (writing took 4.3764925110008335 seconds)
2022-02-26 19:12:43 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-26 19:12:43 | INFO | train | epoch 034 | loss 10.416 | ppl 1366.01 | wps 5179.7 | ups 4.22 | wpb 1227.6 | bsz 61.6 | num_updates 1052 | lr 3.156e-06 | gnorm 8.081 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 267
2022-02-26 19:12:43 | INFO | fairseq.trainer | begin training epoch 35
2022-02-26 19:12:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:43 | INFO | train_inner | epoch 035:      8 / 31 loss=10.464, ppl=1412.53, wps=3767.2, ups=3.07, wpb=1227.2, bsz=60.3, num_updates=1060, lr=3.18e-06, gnorm=7.755, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=268
2022-02-26 19:12:45 | INFO | train_inner | epoch 035:     28 / 31 loss=10.297, ppl=1258.35, wps=19919.6, ups=15.37, wpb=1296, bsz=61.9, num_updates=1080, lr=3.24e-06, gnorm=7.923, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=269
2022-02-26 19:12:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:45 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 10.186 | ppl 1164.52 | wps 31484.2 | wpb 591.2 | bsz 29.9 | num_updates 1083 | best_loss 10.186
2022-02-26 19:12:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1083 updates
2022-02-26 19:12:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1083 updates, score 10.186) (writing took 3.9469108719931683 seconds)
2022-02-26 19:12:49 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-26 19:12:49 | INFO | train | epoch 035 | loss 10.339 | ppl 1294.92 | wps 5802.9 | ups 4.73 | wpb 1227.6 | bsz 61.6 | num_updates 1083 | lr 3.249e-06 | gnorm 7.947 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 273
2022-02-26 19:12:49 | INFO | fairseq.trainer | begin training epoch 36
2022-02-26 19:12:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:51 | INFO | train_inner | epoch 036:     17 / 31 loss=10.323, ppl=1281.35, wps=4354.9, ups=3.44, wpb=1265.6, bsz=61.1, num_updates=1100, lr=3.3e-06, gnorm=8.524, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=275
2022-02-26 19:12:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:52 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 10.132 | ppl 1121.9 | wps 27443 | wpb 591.2 | bsz 29.9 | num_updates 1114 | best_loss 10.132
2022-02-26 19:12:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1114 updates
2022-02-26 19:12:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1114 updates, score 10.132) (writing took 3.9177867389953462 seconds)
2022-02-26 19:12:56 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-26 19:12:56 | INFO | train | epoch 036 | loss 10.227 | ppl 1198.54 | wps 5742.9 | ups 4.68 | wpb 1227.6 | bsz 61.6 | num_updates 1114 | lr 3.342e-06 | gnorm 8.36 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 280
2022-02-26 19:12:56 | INFO | fairseq.trainer | begin training epoch 37
2022-02-26 19:12:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:56 | INFO | train_inner | epoch 037:      6 / 31 loss=10.092, ppl=1091.07, wps=3797.7, ups=3.35, wpb=1132.5, bsz=61.6, num_updates=1120, lr=3.36e-06, gnorm=7.921, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=281
2022-02-26 19:12:58 | INFO | train_inner | epoch 037:     26 / 31 loss=10.244, ppl=1212.5, wps=17669.7, ups=14.65, wpb=1205.9, bsz=61.9, num_updates=1140, lr=3.42e-06, gnorm=7.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=282
2022-02-26 19:12:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:59 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.967 | ppl 1000.76 | wps 31446.7 | wpb 591.2 | bsz 29.9 | num_updates 1145 | best_loss 9.967
2022-02-26 19:12:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1145 updates
2022-02-26 19:12:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1145 updates, score 9.967) (writing took 3.8547752570011653 seconds)
2022-02-26 19:13:03 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-26 19:13:03 | INFO | train | epoch 037 | loss 10.189 | ppl 1167.65 | wps 5759.8 | ups 4.69 | wpb 1227.6 | bsz 61.6 | num_updates 1145 | lr 3.435e-06 | gnorm 7.595 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 287
2022-02-26 19:13:03 | INFO | fairseq.trainer | begin training epoch 38
2022-02-26 19:13:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:04 | INFO | train_inner | epoch 038:     15 / 31 loss=10.075, ppl=1078.35, wps=4016.2, ups=3.4, wpb=1181, bsz=62.4, num_updates=1160, lr=3.48e-06, gnorm=7.75, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=288
2022-02-26 19:13:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:05 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 10.041 | ppl 1053.41 | wps 32089.2 | wpb 591.2 | bsz 29.9 | num_updates 1176 | best_loss 9.967
2022-02-26 19:13:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1176 updates
2022-02-26 19:13:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 38 @ 1176 updates, score 10.041) (writing took 2.951611247000983 seconds)
2022-02-26 19:13:08 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-26 19:13:08 | INFO | train | epoch 038 | loss 10.103 | ppl 1099.7 | wps 6641.8 | ups 5.41 | wpb 1227.6 | bsz 61.6 | num_updates 1176 | lr 3.528e-06 | gnorm 7.73 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 292
2022-02-26 19:13:08 | INFO | fairseq.trainer | begin training epoch 39
2022-02-26 19:13:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:09 | INFO | train_inner | epoch 039:      4 / 31 loss=10.085, ppl=1086.02, wps=5269.2, ups=4.07, wpb=1293.3, bsz=60.3, num_updates=1180, lr=3.54e-06, gnorm=7.527, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=293
2022-02-26 19:13:10 | INFO | train_inner | epoch 039:     24 / 31 loss=10.01, ppl=1031.02, wps=19044.3, ups=14.91, wpb=1277.4, bsz=62.7, num_updates=1200, lr=3.6e-06, gnorm=7.835, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=294
2022-02-26 19:13:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:11 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.903 | ppl 957.5 | wps 31190.5 | wpb 591.2 | bsz 29.9 | num_updates 1207 | best_loss 9.903
2022-02-26 19:13:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1207 updates
2022-02-26 19:13:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1207 updates, score 9.903) (writing took 3.9628922779957065 seconds)
2022-02-26 19:13:15 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-26 19:13:15 | INFO | train | epoch 039 | loss 9.996 | ppl 1020.98 | wps 5740.1 | ups 4.68 | wpb 1227.6 | bsz 61.6 | num_updates 1207 | lr 3.621e-06 | gnorm 7.737 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 299
2022-02-26 19:13:15 | INFO | fairseq.trainer | begin training epoch 40
2022-02-26 19:13:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:16 | INFO | train_inner | epoch 040:     13 / 31 loss=9.877, ppl=940.12, wps=3676.8, ups=3.38, wpb=1086.4, bsz=60.8, num_updates=1220, lr=3.66e-06, gnorm=7.614, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=300
2022-02-26 19:13:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:18 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.728 | ppl 848.31 | wps 32799.7 | wpb 591.2 | bsz 29.9 | num_updates 1238 | best_loss 9.728
2022-02-26 19:13:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1238 updates
2022-02-26 19:13:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1238 updates, score 9.728) (writing took 3.793460450004204 seconds)
2022-02-26 19:13:21 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-26 19:13:21 | INFO | train | epoch 040 | loss 9.915 | ppl 965.51 | wps 5923.4 | ups 4.83 | wpb 1227.6 | bsz 61.6 | num_updates 1238 | lr 3.714e-06 | gnorm 7.746 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 305
2022-02-26 19:13:21 | INFO | fairseq.trainer | begin training epoch 41
2022-02-26 19:13:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:22 | INFO | train_inner | epoch 041:      2 / 31 loss=10.012, ppl=1032.3, wps=4507.7, ups=3.5, wpb=1288, bsz=61.1, num_updates=1240, lr=3.72e-06, gnorm=8.023, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=306
2022-02-26 19:13:23 | INFO | train_inner | epoch 041:     22 / 31 loss=9.886, ppl=946.16, wps=19167.6, ups=14.39, wpb=1331.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=7.616, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=307
2022-02-26 19:13:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:24 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.767 | ppl 871.4 | wps 33886.6 | wpb 591.2 | bsz 29.9 | num_updates 1269 | best_loss 9.728
2022-02-26 19:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1269 updates
2022-02-26 19:13:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 1269 updates, score 9.767) (writing took 2.5023338650062215 seconds)
2022-02-26 19:13:27 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-26 19:13:27 | INFO | train | epoch 041 | loss 9.918 | ppl 967.22 | wps 7342.6 | ups 5.98 | wpb 1227.6 | bsz 61.6 | num_updates 1269 | lr 3.807e-06 | gnorm 7.718 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 311
2022-02-26 19:13:27 | INFO | fairseq.trainer | begin training epoch 42
2022-02-26 19:13:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:27 | INFO | train_inner | epoch 042:     11 / 31 loss=9.921, ppl=969.71, wps=5277.8, ups=4.53, wpb=1164.5, bsz=61.6, num_updates=1280, lr=3.84e-06, gnorm=7.611, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=312
2022-02-26 19:13:29 | INFO | train_inner | epoch 042:     31 / 31 loss=9.716, ppl=841.31, wps=18316.9, ups=15.08, wpb=1214.5, bsz=60.3, num_updates=1300, lr=3.9e-06, gnorm=7.924, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=313
2022-02-26 19:13:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:29 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.678 | ppl 819.2 | wps 32341.9 | wpb 591.2 | bsz 29.9 | num_updates 1300 | best_loss 9.678
2022-02-26 19:13:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1300 updates
2022-02-26 19:13:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 1300 updates, score 9.678) (writing took 3.7048442980012624 seconds)
2022-02-26 19:13:33 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-02-26 19:13:33 | INFO | train | epoch 042 | loss 9.786 | ppl 882.61 | wps 6013.7 | ups 4.9 | wpb 1227.6 | bsz 61.6 | num_updates 1300 | lr 3.9e-06 | gnorm 7.775 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 317
2022-02-26 19:13:33 | INFO | fairseq.trainer | begin training epoch 43
2022-02-26 19:13:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:34 | INFO | train_inner | epoch 043:     20 / 31 loss=9.795, ppl=888.08, wps=4541.5, ups=3.55, wpb=1280.2, bsz=61.9, num_updates=1320, lr=3.96e-06, gnorm=7.558, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=319
2022-02-26 19:13:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:36 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.652 | ppl 804.28 | wps 31285 | wpb 591.2 | bsz 29.9 | num_updates 1331 | best_loss 9.652
2022-02-26 19:13:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1331 updates
2022-02-26 19:13:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 43 @ 1331 updates, score 9.652) (writing took 4.120866194003611 seconds)
2022-02-26 19:13:40 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-02-26 19:13:40 | INFO | train | epoch 043 | loss 9.771 | ppl 873.56 | wps 5618.3 | ups 4.58 | wpb 1227.6 | bsz 61.6 | num_updates 1331 | lr 3.993e-06 | gnorm 7.659 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 324
2022-02-26 19:13:40 | INFO | fairseq.trainer | begin training epoch 44
2022-02-26 19:13:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:40 | INFO | train_inner | epoch 044:      9 / 31 loss=9.687, ppl=824.56, wps=3909.7, ups=3.3, wpb=1184.5, bsz=62.4, num_updates=1340, lr=4.02e-06, gnorm=8.028, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=325
2022-02-26 19:13:42 | INFO | train_inner | epoch 044:     29 / 31 loss=9.612, ppl=782.46, wps=17721.3, ups=14.51, wpb=1221.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=7.454, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=326
2022-02-26 19:13:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:42 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.624 | ppl 788.9 | wps 29326.1 | wpb 591.2 | bsz 29.9 | num_updates 1362 | best_loss 9.624
2022-02-26 19:13:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1362 updates
2022-02-26 19:13:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 1362 updates, score 9.624) (writing took 3.82695534999948 seconds)
2022-02-26 19:13:46 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-02-26 19:13:46 | INFO | train | epoch 044 | loss 9.66 | ppl 808.89 | wps 5797.2 | ups 4.72 | wpb 1227.6 | bsz 61.6 | num_updates 1362 | lr 4.086e-06 | gnorm 7.703 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 330
2022-02-26 19:13:46 | INFO | fairseq.trainer | begin training epoch 45
2022-02-26 19:13:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:48 | INFO | train_inner | epoch 045:     18 / 31 loss=9.634, ppl=794.66, wps=4131.2, ups=3.46, wpb=1195.7, bsz=60.3, num_updates=1380, lr=4.14e-06, gnorm=7.457, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=332
2022-02-26 19:13:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:49 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.52 | ppl 734.3 | wps 31101.7 | wpb 591.2 | bsz 29.9 | num_updates 1393 | best_loss 9.52
2022-02-26 19:13:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1393 updates
2022-02-26 19:13:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 1393 updates, score 9.52) (writing took 3.9072922019986436 seconds)
2022-02-26 19:13:53 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-02-26 19:13:53 | INFO | train | epoch 045 | loss 9.561 | ppl 755.44 | wps 5756.1 | ups 4.69 | wpb 1227.6 | bsz 61.6 | num_updates 1393 | lr 4.179e-06 | gnorm 7.69 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 337
2022-02-26 19:13:53 | INFO | fairseq.trainer | begin training epoch 46
2022-02-26 19:13:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:53 | INFO | train_inner | epoch 046:      7 / 31 loss=9.535, ppl=741.64, wps=4325.6, ups=3.44, wpb=1258.5, bsz=61.6, num_updates=1400, lr=4.2e-06, gnorm=8.11, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=338
2022-02-26 19:13:55 | INFO | train_inner | epoch 046:     27 / 31 loss=9.51, ppl=729.21, wps=17443.7, ups=14.31, wpb=1219.4, bsz=61.9, num_updates=1420, lr=4.26e-06, gnorm=7.706, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=339
2022-02-26 19:13:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:56 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.591 | ppl 771.02 | wps 30384.3 | wpb 591.2 | bsz 29.9 | num_updates 1424 | best_loss 9.52
2022-02-26 19:13:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1424 updates
2022-02-26 19:13:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 46 @ 1424 updates, score 9.591) (writing took 3.0463681620021816 seconds)
2022-02-26 19:13:59 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-02-26 19:13:59 | INFO | train | epoch 046 | loss 9.523 | ppl 735.7 | wps 6627.6 | ups 5.4 | wpb 1227.6 | bsz 61.6 | num_updates 1424 | lr 4.272e-06 | gnorm 7.805 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 343
2022-02-26 19:13:59 | INFO | fairseq.trainer | begin training epoch 47
2022-02-26 19:13:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:00 | INFO | train_inner | epoch 047:     16 / 31 loss=9.529, ppl=738.55, wps=4865.4, ups=4.03, wpb=1206, bsz=61.6, num_updates=1440, lr=4.32e-06, gnorm=7.821, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=344
2022-02-26 19:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:01 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.38 | ppl 666.3 | wps 31909.5 | wpb 591.2 | bsz 29.9 | num_updates 1455 | best_loss 9.38
2022-02-26 19:14:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1455 updates
2022-02-26 19:14:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 1455 updates, score 9.38) (writing took 3.9880868659965927 seconds)
2022-02-26 19:14:05 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-02-26 19:14:05 | INFO | train | epoch 047 | loss 9.443 | ppl 695.87 | wps 5750.6 | ups 4.68 | wpb 1227.6 | bsz 61.6 | num_updates 1455 | lr 4.365e-06 | gnorm 7.908 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 349
2022-02-26 19:14:05 | INFO | fairseq.trainer | begin training epoch 48
2022-02-26 19:14:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:06 | INFO | train_inner | epoch 048:      5 / 31 loss=9.411, ppl=680.69, wps=4242.3, ups=3.41, wpb=1244.2, bsz=59.8, num_updates=1460, lr=4.38e-06, gnorm=7.895, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=350
2022-02-26 19:14:07 | INFO | train_inner | epoch 048:     25 / 31 loss=9.321, ppl=639.61, wps=18914.5, ups=14.72, wpb=1285.3, bsz=63.2, num_updates=1480, lr=4.44e-06, gnorm=7.138, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=351
2022-02-26 19:14:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:08 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.397 | ppl 674.27 | wps 32077.8 | wpb 591.2 | bsz 29.9 | num_updates 1486 | best_loss 9.38
2022-02-26 19:14:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1486 updates
2022-02-26 19:14:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:14:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 48 @ 1486 updates, score 9.397) (writing took 2.5376265599916223 seconds)
2022-02-26 19:14:10 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-02-26 19:14:10 | INFO | train | epoch 048 | loss 9.37 | ppl 661.52 | wps 7350.4 | ups 5.99 | wpb 1227.6 | bsz 61.6 | num_updates 1486 | lr 4.458e-06 | gnorm 7.353 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 355
2022-02-26 19:14:10 | INFO | fairseq.trainer | begin training epoch 49
2022-02-26 19:14:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:11 | INFO | train_inner | epoch 049:     14 / 31 loss=9.437, ppl=693.28, wps=5702.6, ups=4.51, wpb=1265.4, bsz=62.4, num_updates=1500, lr=4.5e-06, gnorm=7.318, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=356
2022-02-26 19:14:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:12 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.329 | ppl 643.22 | wps 31269 | wpb 591.2 | bsz 29.9 | num_updates 1500 | best_loss 9.329
2022-02-26 19:14:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1500 updates
2022-02-26 19:14:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-26 19:14:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-26 19:14:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt (epoch 49 @ 1500 updates, score 9.329) (writing took 7.330665714995121 seconds)
2022-02-26 19:14:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:21 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.28 | ppl 621.66 | wps 30719.2 | wpb 591.2 | bsz 29.9 | num_updates 1517 | best_loss 9.28
2022-02-26 19:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1517 updates
2022-02-26 19:14:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 1517 updates, score 9.28) (writing took 3.9229340650053928 seconds)
2022-02-26 19:14:25 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-02-26 19:14:25 | INFO | train | epoch 049 | loss 9.284 | ppl 623.57 | wps 2606.3 | ups 2.12 | wpb 1227.6 | bsz 61.6 | num_updates 1517 | lr 4.551e-06 | gnorm 7.73 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 369
2022-02-26 19:14:25 | INFO | fairseq.trainer | begin training epoch 50
2022-02-26 19:14:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:25 | INFO | train_inner | epoch 050:      3 / 31 loss=9.082, ppl=541.94, wps=1585, ups=1.44, wpb=1100.2, bsz=60.3, num_updates=1520, lr=4.56e-06, gnorm=8.202, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=370
2022-02-26 19:14:27 | INFO | train_inner | epoch 050:     23 / 31 loss=9.277, ppl=620.56, wps=19169.4, ups=15.28, wpb=1254.2, bsz=61.9, num_updates=1540, lr=4.62e-06, gnorm=7.504, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=371
2022-02-26 19:14:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:28 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.271 | ppl 617.75 | wps 31290.7 | wpb 591.2 | bsz 29.9 | num_updates 1548 | best_loss 9.271
2022-02-26 19:14:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1548 updates
2022-02-26 19:14:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 1548 updates, score 9.271) (writing took 3.9350640530028613 seconds)
2022-02-26 19:14:32 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-02-26 19:14:32 | INFO | train | epoch 050 | loss 9.25 | ppl 608.87 | wps 5836.2 | ups 4.75 | wpb 1227.6 | bsz 61.6 | num_updates 1548 | lr 4.644e-06 | gnorm 7.543 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 376
2022-02-26 19:14:32 | INFO | fairseq.trainer | begin training epoch 51
2022-02-26 19:14:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:32 | INFO | train_inner | epoch 051:     12 / 31 loss=9.361, ppl=657.67, wps=4494, ups=3.46, wpb=1299.3, bsz=60.3, num_updates=1560, lr=4.68e-06, gnorm=7.962, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=377
2022-02-26 19:14:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:34 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.257 | ppl 611.94 | wps 30578.3 | wpb 591.2 | bsz 29.9 | num_updates 1579 | best_loss 9.257
2022-02-26 19:14:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1579 updates
2022-02-26 19:14:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 51 @ 1579 updates, score 9.257) (writing took 3.8273983910039533 seconds)
2022-02-26 19:14:38 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-02-26 19:14:38 | INFO | train | epoch 051 | loss 9.225 | ppl 598.25 | wps 5871.8 | ups 4.78 | wpb 1227.6 | bsz 61.6 | num_updates 1579 | lr 4.737e-06 | gnorm 7.8 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 382
2022-02-26 19:14:38 | INFO | fairseq.trainer | begin training epoch 52
2022-02-26 19:14:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:38 | INFO | train_inner | epoch 052:      1 / 31 loss=9.133, ppl=561.34, wps=4024.7, ups=3.48, wpb=1157.5, bsz=62.4, num_updates=1580, lr=4.74e-06, gnorm=7.452, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=382
2022-02-26 19:14:40 | INFO | train_inner | epoch 052:     21 / 31 loss=9.101, ppl=549.15, wps=17632.9, ups=14.95, wpb=1179.5, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=7.438, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=384
2022-02-26 19:14:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:41 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.17 | ppl 576.18 | wps 30014.6 | wpb 591.2 | bsz 29.9 | num_updates 1610 | best_loss 9.17
2022-02-26 19:14:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1610 updates
2022-02-26 19:14:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 52 @ 1610 updates, score 9.17) (writing took 3.831199442007346 seconds)
2022-02-26 19:14:45 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-02-26 19:14:45 | INFO | train | epoch 052 | loss 9.172 | ppl 577.02 | wps 5854.2 | ups 4.77 | wpb 1227.6 | bsz 61.6 | num_updates 1610 | lr 4.83e-06 | gnorm 7.504 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 389
2022-02-26 19:14:45 | INFO | fairseq.trainer | begin training epoch 53
2022-02-26 19:14:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:46 | INFO | train_inner | epoch 053:     10 / 31 loss=9.248, ppl=607.92, wps=4172, ups=3.29, wpb=1267.7, bsz=59.5, num_updates=1620, lr=4.86e-06, gnorm=7.46, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=390
2022-02-26 19:14:47 | INFO | train_inner | epoch 053:     30 / 31 loss=9.024, ppl=520.43, wps=18543.6, ups=14.4, wpb=1288.1, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=7.523, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=391
2022-02-26 19:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:48 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.057 | ppl 532.74 | wps 32095.7 | wpb 591.2 | bsz 29.9 | num_updates 1641 | best_loss 9.057
2022-02-26 19:14:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1641 updates
2022-02-26 19:14:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 1641 updates, score 9.057) (writing took 3.8129485249955906 seconds)
2022-02-26 19:14:51 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-02-26 19:14:51 | INFO | train | epoch 053 | loss 9.081 | ppl 541.46 | wps 5597.2 | ups 4.56 | wpb 1227.6 | bsz 61.6 | num_updates 1641 | lr 4.923e-06 | gnorm 7.519 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 395
2022-02-26 19:14:51 | INFO | fairseq.trainer | begin training epoch 54
2022-02-26 19:14:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:53 | INFO | train_inner | epoch 054:     19 / 31 loss=9.064, ppl=535.23, wps=4478.3, ups=3.5, wpb=1280.2, bsz=60.3, num_updates=1660, lr=4.98e-06, gnorm=7.437, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=397
2022-02-26 19:14:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:54 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.02 | ppl 519.27 | wps 27102.9 | wpb 591.2 | bsz 29.9 | num_updates 1672 | best_loss 9.02
2022-02-26 19:14:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1672 updates
2022-02-26 19:14:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 54 @ 1672 updates, score 9.02) (writing took 3.992239008002798 seconds)
2022-02-26 19:14:58 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-02-26 19:14:58 | INFO | train | epoch 054 | loss 9.038 | ppl 525.64 | wps 5686.9 | ups 4.63 | wpb 1227.6 | bsz 61.6 | num_updates 1672 | lr 5.016e-06 | gnorm 7.589 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 402
2022-02-26 19:14:58 | INFO | fairseq.trainer | begin training epoch 55
2022-02-26 19:14:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:59 | INFO | train_inner | epoch 055:      8 / 31 loss=9.001, ppl=512.28, wps=4060.9, ups=3.33, wpb=1220, bsz=62.4, num_updates=1680, lr=5.04e-06, gnorm=7.672, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=403
2022-02-26 19:15:00 | INFO | train_inner | epoch 055:     28 / 31 loss=8.935, ppl=489.36, wps=18705.3, ups=15.33, wpb=1219.8, bsz=61.9, num_updates=1700, lr=5.1e-06, gnorm=7.593, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=404
2022-02-26 19:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:01 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.04 | ppl 526.48 | wps 30292.6 | wpb 591.2 | bsz 29.9 | num_updates 1703 | best_loss 9.02
2022-02-26 19:15:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1703 updates
2022-02-26 19:15:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 55 @ 1703 updates, score 9.04) (writing took 3.0945771359984064 seconds)
2022-02-26 19:15:04 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-02-26 19:15:04 | INFO | train | epoch 055 | loss 8.944 | ppl 492.38 | wps 6538.1 | ups 5.33 | wpb 1227.6 | bsz 61.6 | num_updates 1703 | lr 5.109e-06 | gnorm 7.62 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 408
2022-02-26 19:15:04 | INFO | fairseq.trainer | begin training epoch 56
2022-02-26 19:15:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:05 | INFO | train_inner | epoch 056:     17 / 31 loss=8.831, ppl=455.37, wps=4492.2, ups=3.86, wpb=1164.4, bsz=61.1, num_updates=1720, lr=5.16e-06, gnorm=7.686, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=409
2022-02-26 19:15:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:07 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.985 | ppl 506.57 | wps 29202.3 | wpb 591.2 | bsz 29.9 | num_updates 1734 | best_loss 8.985
2022-02-26 19:15:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1734 updates
2022-02-26 19:15:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 1734 updates, score 8.985) (writing took 3.9883567150100134 seconds)
2022-02-26 19:15:11 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-02-26 19:15:11 | INFO | train | epoch 056 | loss 8.9 | ppl 477.76 | wps 5516.2 | ups 4.49 | wpb 1227.6 | bsz 61.6 | num_updates 1734 | lr 5.202e-06 | gnorm 7.488 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 415
2022-02-26 19:15:11 | INFO | fairseq.trainer | begin training epoch 57
2022-02-26 19:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:11 | INFO | train_inner | epoch 057:      6 / 31 loss=8.975, ppl=503.13, wps=3930.2, ups=3.31, wpb=1186.1, bsz=60.3, num_updates=1740, lr=5.22e-06, gnorm=7.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=415
2022-02-26 19:15:13 | INFO | train_inner | epoch 057:     26 / 31 loss=8.748, ppl=429.85, wps=17424.9, ups=14.45, wpb=1205.9, bsz=63.2, num_updates=1760, lr=5.28e-06, gnorm=7.639, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=417
2022-02-26 19:15:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:13 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.917 | ppl 483.34 | wps 29920.8 | wpb 591.2 | bsz 29.9 | num_updates 1765 | best_loss 8.917
2022-02-26 19:15:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1765 updates
2022-02-26 19:15:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 57 @ 1765 updates, score 8.917) (writing took 4.484520123995026 seconds)
2022-02-26 19:15:18 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-02-26 19:15:18 | INFO | train | epoch 057 | loss 8.831 | ppl 455.29 | wps 5310.5 | ups 4.33 | wpb 1227.6 | bsz 61.6 | num_updates 1765 | lr 5.295e-06 | gnorm 7.701 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 422
2022-02-26 19:15:18 | INFO | fairseq.trainer | begin training epoch 58
2022-02-26 19:15:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:19 | INFO | train_inner | epoch 058:     15 / 31 loss=8.879, ppl=470.76, wps=4073.6, ups=3.1, wpb=1316, bsz=61.1, num_updates=1780, lr=5.34e-06, gnorm=7.282, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=423
2022-02-26 19:15:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:21 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.929 | ppl 487.52 | wps 29114.2 | wpb 591.2 | bsz 29.9 | num_updates 1796 | best_loss 8.917
2022-02-26 19:15:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1796 updates
2022-02-26 19:15:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 58 @ 1796 updates, score 8.929) (writing took 2.663104753999505 seconds)
2022-02-26 19:15:24 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-02-26 19:15:24 | INFO | train | epoch 058 | loss 8.785 | ppl 441.11 | wps 6788.4 | ups 5.53 | wpb 1227.6 | bsz 61.6 | num_updates 1796 | lr 5.388e-06 | gnorm 8.058 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 428
2022-02-26 19:15:24 | INFO | fairseq.trainer | begin training epoch 59
2022-02-26 19:15:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:24 | INFO | train_inner | epoch 059:      4 / 31 loss=8.758, ppl=432.86, wps=5130, ups=4.19, wpb=1223.8, bsz=61.6, num_updates=1800, lr=5.4e-06, gnorm=8.515, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=428
2022-02-26 19:15:25 | INFO | train_inner | epoch 059:     24 / 31 loss=8.653, ppl=402.6, wps=18568.2, ups=15.4, wpb=1205.6, bsz=61.9, num_updates=1820, lr=5.46e-06, gnorm=7.411, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=429
2022-02-26 19:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:26 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.931 | ppl 488.15 | wps 31450.7 | wpb 591.2 | bsz 29.9 | num_updates 1827 | best_loss 8.917
2022-02-26 19:15:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1827 updates
2022-02-26 19:15:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 59 @ 1827 updates, score 8.931) (writing took 2.9940937159990426 seconds)
2022-02-26 19:15:29 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-02-26 19:15:29 | INFO | train | epoch 059 | loss 8.739 | ppl 427.18 | wps 6683.9 | ups 5.44 | wpb 1227.6 | bsz 61.6 | num_updates 1827 | lr 5.481e-06 | gnorm 7.532 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 433
2022-02-26 19:15:29 | INFO | fairseq.trainer | begin training epoch 60
2022-02-26 19:15:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:30 | INFO | train_inner | epoch 060:     13 / 31 loss=8.761, ppl=433.7, wps=4889.4, ups=4.02, wpb=1217.8, bsz=60.3, num_updates=1840, lr=5.52e-06, gnorm=7.47, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=434
2022-02-26 19:15:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:32 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.915 | ppl 482.77 | wps 28254.1 | wpb 591.2 | bsz 29.9 | num_updates 1858 | best_loss 8.915
2022-02-26 19:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1858 updates
2022-02-26 19:15:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 60 @ 1858 updates, score 8.915) (writing took 4.247326691998751 seconds)
2022-02-26 19:15:36 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-02-26 19:15:36 | INFO | train | epoch 060 | loss 8.646 | ppl 400.62 | wps 5503.5 | ups 4.48 | wpb 1227.6 | bsz 61.6 | num_updates 1858 | lr 5.574e-06 | gnorm 7.534 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 440
2022-02-26 19:15:36 | INFO | fairseq.trainer | begin training epoch 61
2022-02-26 19:15:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:36 | INFO | train_inner | epoch 061:      2 / 31 loss=8.623, ppl=394.25, wps=3831.6, ups=3.23, wpb=1186.7, bsz=62.4, num_updates=1860, lr=5.58e-06, gnorm=7.731, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=441
2022-02-26 19:15:38 | INFO | train_inner | epoch 061:     22 / 31 loss=8.635, ppl=397.55, wps=18901, ups=15.32, wpb=1234, bsz=61.9, num_updates=1880, lr=5.64e-06, gnorm=7.428, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=442
2022-02-26 19:15:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:39 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.764 | ppl 434.7 | wps 27905.9 | wpb 591.2 | bsz 29.9 | num_updates 1889 | best_loss 8.764
2022-02-26 19:15:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1889 updates
2022-02-26 19:15:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 61 @ 1889 updates, score 8.764) (writing took 3.942211846995633 seconds)
2022-02-26 19:15:43 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-02-26 19:15:43 | INFO | train | epoch 061 | loss 8.636 | ppl 397.74 | wps 5759.1 | ups 4.69 | wpb 1227.6 | bsz 61.6 | num_updates 1889 | lr 5.667e-06 | gnorm 7.483 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 447
2022-02-26 19:15:43 | INFO | fairseq.trainer | begin training epoch 62
2022-02-26 19:15:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:44 | INFO | train_inner | epoch 062:     11 / 31 loss=8.617, ppl=392.56, wps=4175.9, ups=3.35, wpb=1246.4, bsz=61.6, num_updates=1900, lr=5.7e-06, gnorm=7.402, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=448
2022-02-26 19:15:45 | INFO | train_inner | epoch 062:     31 / 31 loss=8.522, ppl=367.65, wps=19040, ups=15.32, wpb=1243, bsz=61.1, num_updates=1920, lr=5.76e-06, gnorm=7.999, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=449
2022-02-26 19:15:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:45 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8.769 | ppl 436.37 | wps 29402.5 | wpb 591.2 | bsz 29.9 | num_updates 1920 | best_loss 8.764
2022-02-26 19:15:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1920 updates
2022-02-26 19:15:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 62 @ 1920 updates, score 8.769) (writing took 2.6607420329964953 seconds)
2022-02-26 19:15:48 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-02-26 19:15:48 | INFO | train | epoch 062 | loss 8.556 | ppl 376.26 | wps 7113.7 | ups 5.79 | wpb 1227.6 | bsz 61.6 | num_updates 1920 | lr 5.76e-06 | gnorm 7.729 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 452
2022-02-26 19:15:48 | INFO | fairseq.trainer | begin training epoch 63
2022-02-26 19:15:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:50 | INFO | train_inner | epoch 063:     20 / 31 loss=8.55, ppl=374.8, wps=5330, ups=4.35, wpb=1224.7, bsz=63.2, num_updates=1940, lr=5.82e-06, gnorm=7.253, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=454
2022-02-26 19:15:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:51 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 8.569 | ppl 379.78 | wps 28915.8 | wpb 591.2 | bsz 29.9 | num_updates 1951 | best_loss 8.569
2022-02-26 19:15:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 1951 updates
2022-02-26 19:15:51 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 63 @ 1951 updates, score 8.569) (writing took 4.60081109899329 seconds)
2022-02-26 19:15:56 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-02-26 19:15:56 | INFO | train | epoch 063 | loss 8.53 | ppl 369.58 | wps 5154.7 | ups 4.2 | wpb 1227.6 | bsz 61.6 | num_updates 1951 | lr 5.853e-06 | gnorm 7.581 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 460
2022-02-26 19:15:56 | INFO | fairseq.trainer | begin training epoch 64
2022-02-26 19:15:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:56 | INFO | train_inner | epoch 064:      9 / 31 loss=8.421, ppl=342.78, wps=3404.5, ups=2.94, wpb=1158.3, bsz=61.1, num_updates=1960, lr=5.88e-06, gnorm=7.939, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=461
2022-02-26 19:15:58 | INFO | train_inner | epoch 064:     29 / 31 loss=8.559, ppl=377.07, wps=17901, ups=13.24, wpb=1352, bsz=61.9, num_updates=1980, lr=5.94e-06, gnorm=7.231, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=462
2022-02-26 19:15:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:59 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 8.677 | ppl 409.3 | wps 29792 | wpb 591.2 | bsz 29.9 | num_updates 1982 | best_loss 8.569
2022-02-26 19:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 1982 updates
2022-02-26 19:15:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:16:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:16:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 64 @ 1982 updates, score 8.677) (writing took 2.9681306069978746 seconds)
2022-02-26 19:16:02 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-02-26 19:16:02 | INFO | train | epoch 064 | loss 8.481 | ppl 357.42 | wps 6392.2 | ups 5.21 | wpb 1227.6 | bsz 61.6 | num_updates 1982 | lr 5.946e-06 | gnorm 7.468 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 466
2022-02-26 19:16:02 | INFO | fairseq.trainer | begin training epoch 65
2022-02-26 19:16:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:16:03 | INFO | train_inner | epoch 065:     18 / 31 loss=8.281, ppl=311.09, wps=4549.8, ups=3.93, wpb=1156.3, bsz=61.1, num_updates=2000, lr=6e-06, gnorm=7.597, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=467
2022-02-26 19:16:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:16:04 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.654 | ppl 402.69 | wps 30874.9 | wpb 591.2 | bsz 29.9 | num_updates 2000 | best_loss 8.569
2022-02-26 19:16:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 2000 updates
2022-02-26 19:16:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt
2022-02-26 19:16:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt
2022-02-26 19:16:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt (epoch 65 @ 2000 updates, score 8.654) (writing took 4.825617766007781 seconds)
2022-02-26 19:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:16:10 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.671 | ppl 407.55 | wps 26552.7 | wpb 591.2 | bsz 29.9 | num_updates 2013 | best_loss 8.569
2022-02-26 19:16:10 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-02-26 19:16:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 2013 updates
2022-02-26 19:16:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:16:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:16:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 65 @ 2013 updates, score 8.671) (writing took 2.389349050994497 seconds)
2022-02-26 19:16:12 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-02-26 19:16:12 | INFO | train | epoch 065 | loss 8.41 | ppl 340.17 | wps 3555.2 | ups 2.9 | wpb 1227.6 | bsz 61.6 | num_updates 2013 | lr 6.039e-06 | gnorm 7.413 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 476
2022-02-26 19:16:12 | INFO | fairseq_cli.train | done training in 471.2 seconds
2022-02-28 14:04:41 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 2, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-28 14:04:41 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-28 14:04:43 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-28 14:04:43 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-28 14:04:43 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-28 14:04:43 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-28 14:04:43 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-28 14:04:43 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-28 14:04:44 | INFO | bartabst.data.data_utils | loaded 598 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-02-28 14:04:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-02-28 14:04:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-28 14:04:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-02-28 14:04:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 14:04:48 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-02-28 14:04:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 14:04:48 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-28 14:04:48 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-02-28 14:04:48 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-02-28 14:04:49 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-02-28 14:04:49 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-02-28 14:04:49 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-02-28 14:04:49 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-02-28 14:04:49 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-02-28 14:04:49 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-02-28 14:04:49 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-02-28 14:04:50 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-02-28 14:04:50 | INFO | fairseq.trainer | loading train data for epoch 1
no aos file, no transfer aos used
2022-02-28 14:04:51 | INFO | bartabst.data.data_utils | loaded 1,910 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-02-28 14:04:51 | INFO | fairseq.trainer | begin training epoch 1
2022-02-28 14:04:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:04:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-28 14:04:53 | INFO | train_inner | epoch 001:     21 / 31 loss=17.267, ppl=157724, wps=17159.9, ups=14.15, wpb=1226.7, bsz=63.2, num_updates=20, lr=6e-08, gnorm=23.218, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=5
2022-02-28 14:04:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:04:54 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.127 | ppl 143116 | wps 31404.7 | wpb 591.2 | bsz 29.9 | num_updates 30
2022-02-28 14:04:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 30 updates
2022-02-28 14:04:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:04:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 30 updates, score 17.127) (writing took 11.303535271988949 seconds)
2022-02-28 14:05:06 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-28 14:05:06 | INFO | train | epoch 001 | loss 17.3 | ppl 161327 | wps 2560.2 | ups 2.08 | wpb 1238.7 | bsz 61.5 | num_updates 30 | lr 9e-08 | gnorm 23.531 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 18
2022-02-28 14:05:06 | INFO | fairseq.trainer | begin training epoch 2
2022-02-28 14:05:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:05:06 | INFO | train_inner | epoch 002:     10 / 31 loss=17.275, ppl=158579, wps=1921.8, ups=1.49, wpb=1290.9, bsz=59.8, num_updates=40, lr=1.2e-07, gnorm=23.583, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=19
2022-02-28 14:05:08 | INFO | train_inner | epoch 002:     30 / 31 loss=17.298, ppl=161138, wps=17895.5, ups=14.71, wpb=1216.2, bsz=63.2, num_updates=60, lr=1.8e-07, gnorm=23.346, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=20
2022-02-28 14:05:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:05:08 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 17.054 | ppl 136065 | wps 32242.9 | wpb 591.2 | bsz 29.9 | num_updates 61 | best_loss 17.054
2022-02-28 14:05:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 61 updates
2022-02-28 14:05:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 61 updates, score 17.054) (writing took 8.060145375988213 seconds)
2022-02-28 14:05:16 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-28 14:05:16 | INFO | train | epoch 002 | loss 17.254 | ppl 156312 | wps 3530.8 | ups 2.88 | wpb 1227.6 | bsz 61.6 | num_updates 61 | lr 1.83e-07 | gnorm 23.324 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 29
2022-02-28 14:05:16 | INFO | fairseq.trainer | begin training epoch 3
2022-02-28 14:05:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:05:18 | INFO | train_inner | epoch 003:     19 / 31 loss=17.196, ppl=150139, wps=2355.6, ups=2.01, wpb=1173.7, bsz=60.3, num_updates=80, lr=2.4e-07, gnorm=23.684, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=30
2022-02-28 14:05:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:05:19 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.898 | ppl 122164 | wps 31054.6 | wpb 591.2 | bsz 29.9 | num_updates 92 | best_loss 16.898
2022-02-28 14:05:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 92 updates
2022-02-28 14:05:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 92 updates, score 16.898) (writing took 6.550476650998462 seconds)
2022-02-28 14:05:26 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-28 14:05:26 | INFO | train | epoch 003 | loss 17.118 | ppl 142264 | wps 4155.7 | ups 3.39 | wpb 1227.6 | bsz 61.6 | num_updates 92 | lr 2.76e-07 | gnorm 23.04 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 38
2022-02-28 14:05:26 | INFO | fairseq.trainer | begin training epoch 4
2022-02-28 14:05:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:05:26 | INFO | train_inner | epoch 004:      8 / 31 loss=16.977, ppl=129002, wps=2818.1, ups=2.35, wpb=1196.8, bsz=62.4, num_updates=100, lr=3e-07, gnorm=22.425, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=39
2022-02-28 14:05:28 | INFO | train_inner | epoch 004:     28 / 31 loss=16.92, ppl=124037, wps=20187.7, ups=14.85, wpb=1359.5, bsz=61.9, num_updates=120, lr=3.6e-07, gnorm=22.225, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=40
2022-02-28 14:05:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:05:28 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 16.547 | ppl 95749.3 | wps 31757 | wpb 591.2 | bsz 29.9 | num_updates 123 | best_loss 16.547
2022-02-28 14:05:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 123 updates
2022-02-28 14:05:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 123 updates, score 16.547) (writing took 6.1073920560011175 seconds)
2022-02-28 14:05:34 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-28 14:05:34 | INFO | train | epoch 004 | loss 16.923 | ppl 124269 | wps 4286.5 | ups 3.49 | wpb 1227.6 | bsz 61.6 | num_updates 123 | lr 3.69e-07 | gnorm 22.883 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 47
2022-02-28 14:05:34 | INFO | fairseq.trainer | begin training epoch 5
2022-02-28 14:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:05:36 | INFO | train_inner | epoch 005:     17 / 31 loss=16.68, ppl=105006, wps=3073.3, ups=2.47, wpb=1244, bsz=62.4, num_updates=140, lr=4.2e-07, gnorm=22.544, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=48
2022-02-28 14:05:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:05:37 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 16.256 | ppl 78268.7 | wps 28689.4 | wpb 591.2 | bsz 29.9 | num_updates 154 | best_loss 16.256
2022-02-28 14:05:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 154 updates
2022-02-28 14:05:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 154 updates, score 16.256) (writing took 5.7474204000027385 seconds)
2022-02-28 14:05:43 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-28 14:05:43 | INFO | train | epoch 005 | loss 16.632 | ppl 101583 | wps 4482.5 | ups 3.65 | wpb 1227.6 | bsz 61.6 | num_updates 154 | lr 4.62e-07 | gnorm 22.153 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 55
2022-02-28 14:05:43 | INFO | fairseq.trainer | begin training epoch 6
2022-02-28 14:05:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:05:43 | INFO | train_inner | epoch 006:      6 / 31 loss=16.541, ppl=95381.6, wps=3131.4, ups=2.59, wpb=1207.8, bsz=59.5, num_updates=160, lr=4.8e-07, gnorm=22.416, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=56
2022-02-28 14:05:45 | INFO | train_inner | epoch 006:     26 / 31 loss=16.35, ppl=83504.3, wps=17855.3, ups=15.15, wpb=1178.5, bsz=62.7, num_updates=180, lr=5.4e-07, gnorm=21.698, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=57
2022-02-28 14:05:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-28 14:05:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:05:46 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 16.015 | ppl 66201.8 | wps 31668.7 | wpb 591.2 | bsz 29.9 | num_updates 184 | best_loss 16.015
2022-02-28 14:05:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 184 updates
2022-02-28 14:05:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:05:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 184 updates, score 16.015) (writing took 8.150140399986412 seconds)
2022-02-28 14:05:54 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-28 14:05:54 | INFO | train | epoch 006 | loss 16.342 | ppl 83077.8 | wps 3480.6 | ups 2.79 | wpb 1246.2 | bsz 61.5 | num_updates 184 | lr 5.52e-07 | gnorm 21.84 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 66
2022-02-28 14:05:54 | INFO | fairseq.trainer | begin training epoch 7
2022-02-28 14:05:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:05:55 | INFO | train_inner | epoch 007:     16 / 31 loss=16.094, ppl=69966.1, wps=2600.3, ups=1.96, wpb=1328.3, bsz=61.6, num_updates=200, lr=6e-07, gnorm=21.287, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=67
2022-02-28 14:05:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:05:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 15.6 | ppl 49658.7 | wps 30761.1 | wpb 591.2 | bsz 29.9 | num_updates 215 | best_loss 15.6
2022-02-28 14:05:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 215 updates
2022-02-28 14:05:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 215 updates, score 15.6) (writing took 5.776664548990084 seconds)
2022-02-28 14:06:02 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-28 14:06:02 | INFO | train | epoch 007 | loss 15.998 | ppl 65435.6 | wps 4469.1 | ups 3.64 | wpb 1227.6 | bsz 61.6 | num_updates 215 | lr 6.45e-07 | gnorm 20.749 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 75
2022-02-28 14:06:02 | INFO | fairseq.trainer | begin training epoch 8
2022-02-28 14:06:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:06:03 | INFO | train_inner | epoch 008:      5 / 31 loss=15.897, ppl=61030, wps=2809, ups=2.59, wpb=1083.2, bsz=61.1, num_updates=220, lr=6.6e-07, gnorm=20.573, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=75
2022-02-28 14:06:04 | INFO | train_inner | epoch 008:     25 / 31 loss=15.657, ppl=51683.7, wps=20652.3, ups=15.26, wpb=1353.2, bsz=63.2, num_updates=240, lr=7.2e-07, gnorm=19.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=76
2022-02-28 14:06:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:06:05 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 15.2 | ppl 37630.2 | wps 31913.5 | wpb 591.2 | bsz 29.9 | num_updates 246 | best_loss 15.2
2022-02-28 14:06:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 246 updates
2022-02-28 14:06:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 246 updates, score 15.2) (writing took 8.019961660000263 seconds)
2022-02-28 14:06:13 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-28 14:06:13 | INFO | train | epoch 008 | loss 15.657 | ppl 51682.3 | wps 3577.5 | ups 2.91 | wpb 1227.6 | bsz 61.6 | num_updates 246 | lr 7.38e-07 | gnorm 20.119 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 85
2022-02-28 14:06:13 | INFO | fairseq.trainer | begin training epoch 9
2022-02-28 14:06:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:06:14 | INFO | train_inner | epoch 009:     14 / 31 loss=15.318, ppl=40853.8, wps=2417.8, ups=1.99, wpb=1215.7, bsz=59.8, num_updates=260, lr=7.8e-07, gnorm=20.633, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=86
2022-02-28 14:06:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:06:16 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 14.784 | ppl 28201.9 | wps 30826.5 | wpb 591.2 | bsz 29.9 | num_updates 277 | best_loss 14.784
2022-02-28 14:06:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 277 updates
2022-02-28 14:06:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 277 updates, score 14.784) (writing took 7.044177266012412 seconds)
2022-02-28 14:06:23 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-28 14:06:23 | INFO | train | epoch 009 | loss 15.208 | ppl 37842 | wps 3875.8 | ups 3.16 | wpb 1227.6 | bsz 61.6 | num_updates 277 | lr 8.31e-07 | gnorm 19.682 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 95
2022-02-28 14:06:23 | INFO | fairseq.trainer | begin training epoch 10
2022-02-28 14:06:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:06:23 | INFO | train_inner | epoch 010:      3 / 31 loss=15.111, ppl=35396.6, wps=2435.3, ups=2.23, wpb=1093.2, bsz=61.6, num_updates=280, lr=8.4e-07, gnorm=19.561, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=95
2022-02-28 14:06:24 | INFO | train_inner | epoch 010:     23 / 31 loss=14.767, ppl=27885.5, wps=19607.8, ups=14.58, wpb=1344.5, bsz=61.9, num_updates=300, lr=9e-07, gnorm=17.216, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=97
2022-02-28 14:06:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:06:25 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 14.438 | ppl 22195.1 | wps 31644.2 | wpb 591.2 | bsz 29.9 | num_updates 308 | best_loss 14.438
2022-02-28 14:06:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 308 updates
2022-02-28 14:06:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 308 updates, score 14.438) (writing took 6.342502303014044 seconds)
2022-02-28 14:06:32 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-28 14:06:32 | INFO | train | epoch 010 | loss 14.748 | ppl 27507.5 | wps 4237.2 | ups 3.45 | wpb 1227.6 | bsz 61.6 | num_updates 308 | lr 9.24e-07 | gnorm 17.533 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 104
2022-02-28 14:06:32 | INFO | fairseq.trainer | begin training epoch 11
2022-02-28 14:06:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:06:33 | INFO | train_inner | epoch 011:     12 / 31 loss=14.55, ppl=23989.2, wps=2983, ups=2.43, wpb=1227, bsz=62.4, num_updates=320, lr=9.6e-07, gnorm=17.459, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=105
2022-02-28 14:06:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:06:34 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 14.021 | ppl 16628.7 | wps 31221.7 | wpb 591.2 | bsz 29.9 | num_updates 339 | best_loss 14.021
2022-02-28 14:06:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 339 updates
2022-02-28 14:06:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 339 updates, score 14.021) (writing took 5.84620307397563 seconds)
2022-02-28 14:06:40 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-28 14:06:40 | INFO | train | epoch 011 | loss 14.325 | ppl 20528.7 | wps 4455.9 | ups 3.63 | wpb 1227.6 | bsz 61.6 | num_updates 339 | lr 1.017e-06 | gnorm 16.713 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 113
2022-02-28 14:06:40 | INFO | fairseq.trainer | begin training epoch 12
2022-02-28 14:06:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:06:40 | INFO | train_inner | epoch 012:      1 / 31 loss=14.189, ppl=18678.4, wps=3019.2, ups=2.56, wpb=1179.8, bsz=60.3, num_updates=340, lr=1.02e-06, gnorm=16.467, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=113
2022-02-28 14:06:42 | INFO | train_inner | epoch 012:     21 / 31 loss=13.987, ppl=16233.5, wps=17868.8, ups=14.74, wpb=1212.4, bsz=64, num_updates=360, lr=1.08e-06, gnorm=15.674, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=114
2022-02-28 14:06:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:06:43 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 13.544 | ppl 11946.6 | wps 30692.3 | wpb 591.2 | bsz 29.9 | num_updates 370 | best_loss 13.544
2022-02-28 14:06:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 370 updates
2022-02-28 14:06:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 370 updates, score 13.544) (writing took 6.3266000210132916 seconds)
2022-02-28 14:06:49 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-28 14:06:49 | INFO | train | epoch 012 | loss 13.923 | ppl 15532.8 | wps 4231.6 | ups 3.45 | wpb 1227.6 | bsz 61.6 | num_updates 370 | lr 1.11e-06 | gnorm 15.821 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 122
2022-02-28 14:06:49 | INFO | fairseq.trainer | begin training epoch 13
2022-02-28 14:06:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:06:50 | INFO | train_inner | epoch 013:     10 / 31 loss=13.682, ppl=13142.1, wps=3117.1, ups=2.42, wpb=1285.8, bsz=58.2, num_updates=380, lr=1.14e-06, gnorm=15.643, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=122
2022-02-28 14:06:51 | INFO | train_inner | epoch 013:     30 / 31 loss=13.378, ppl=10642.9, wps=18725.4, ups=15.4, wpb=1216.2, bsz=64, num_updates=400, lr=1.2e-06, gnorm=14.797, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=124
2022-02-28 14:06:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:06:52 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 13.118 | ppl 8889.96 | wps 32577.7 | wpb 591.2 | bsz 29.9 | num_updates 401 | best_loss 13.118
2022-02-28 14:06:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 401 updates
2022-02-28 14:06:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:06:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:07:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 401 updates, score 13.118) (writing took 9.961652872996638 seconds)
2022-02-28 14:07:02 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-28 14:07:02 | INFO | train | epoch 013 | loss 13.449 | ppl 11185.6 | wps 3027.9 | ups 2.47 | wpb 1227.6 | bsz 61.6 | num_updates 401 | lr 1.203e-06 | gnorm 15.004 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 134
2022-02-28 14:07:02 | INFO | fairseq.trainer | begin training epoch 14
2022-02-28 14:07:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:07:03 | INFO | train_inner | epoch 014:     19 / 31 loss=13.181, ppl=9290.04, wps=1959.7, ups=1.67, wpb=1171.7, bsz=60.3, num_updates=420, lr=1.26e-06, gnorm=15.138, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=136
2022-02-28 14:07:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:07:05 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 12.85 | ppl 7380.64 | wps 31640.6 | wpb 591.2 | bsz 29.9 | num_updates 432 | best_loss 12.85
2022-02-28 14:07:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 432 updates
2022-02-28 14:07:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:07:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:07:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 432 updates, score 12.85) (writing took 5.973572761984542 seconds)
2022-02-28 14:07:11 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-28 14:07:11 | INFO | train | epoch 014 | loss 13.122 | ppl 8915.98 | wps 4291.7 | ups 3.5 | wpb 1227.6 | bsz 61.6 | num_updates 432 | lr 1.296e-06 | gnorm 14.476 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 143
2022-02-28 14:07:11 | INFO | fairseq.trainer | begin training epoch 15
2022-02-28 14:07:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:07:11 | INFO | train_inner | epoch 015:      8 / 31 loss=12.983, ppl=8094.65, wps=3241.4, ups=2.47, wpb=1314.1, bsz=62.4, num_updates=440, lr=1.32e-06, gnorm=13.071, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=144
2022-02-28 14:07:13 | INFO | train_inner | epoch 015:     28 / 31 loss=12.839, ppl=7327.74, wps=17870.3, ups=14.88, wpb=1200.9, bsz=61.9, num_updates=460, lr=1.38e-06, gnorm=13.141, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=145
2022-02-28 14:07:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:07:13 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 12.678 | ppl 6552.71 | wps 30228.6 | wpb 591.2 | bsz 29.9 | num_updates 463 | best_loss 12.678
2022-02-28 14:07:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 463 updates
2022-02-28 14:07:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:07:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:07:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 463 updates, score 12.678) (writing took 6.30094301601639 seconds)
2022-02-28 14:07:20 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-28 14:07:20 | INFO | train | epoch 015 | loss 12.854 | ppl 7404.58 | wps 4201.9 | ups 3.42 | wpb 1227.6 | bsz 61.6 | num_updates 463 | lr 1.389e-06 | gnorm 13.211 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 152
2022-02-28 14:07:20 | INFO | fairseq.trainer | begin training epoch 16
2022-02-28 14:07:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:07:21 | INFO | train_inner | epoch 016:     17 / 31 loss=12.654, ppl=6445.65, wps=3019.3, ups=2.42, wpb=1248.2, bsz=61.6, num_updates=480, lr=1.44e-06, gnorm=12.97, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=153
2022-02-28 14:07:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:07:22 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 12.382 | ppl 5337.52 | wps 31739.4 | wpb 591.2 | bsz 29.9 | num_updates 494 | best_loss 12.382
2022-02-28 14:07:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 494 updates
2022-02-28 14:07:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:07:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:07:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 494 updates, score 12.382) (writing took 5.725719313981244 seconds)
2022-02-28 14:07:28 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-28 14:07:28 | INFO | train | epoch 016 | loss 12.568 | ppl 6071.77 | wps 4515.3 | ups 3.68 | wpb 1227.6 | bsz 61.6 | num_updates 494 | lr 1.482e-06 | gnorm 12.295 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 161
2022-02-28 14:07:28 | INFO | fairseq.trainer | begin training epoch 17
2022-02-28 14:07:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:07:29 | INFO | train_inner | epoch 017:      6 / 31 loss=12.404, ppl=5417.92, wps=3076.7, ups=2.61, wpb=1179.1, bsz=59.8, num_updates=500, lr=1.5e-06, gnorm=12.802, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=161
2022-02-28 14:07:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:07:29 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.343 | ppl 5194.13 | wps 27153.8 | wpb 591.2 | bsz 29.9 | num_updates 500 | best_loss 12.343
2022-02-28 14:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 500 updates
2022-02-28 14:07:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-28 14:07:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-28 14:07:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt (epoch 17 @ 500 updates, score 12.343) (writing took 11.618865910015302 seconds)
2022-02-28 14:07:43 | INFO | train_inner | epoch 017:     26 / 31 loss=12.403, ppl=5414.69, wps=1835.9, ups=1.39, wpb=1319, bsz=63.2, num_updates=520, lr=1.56e-06, gnorm=11.284, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=175
2022-02-28 14:07:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:07:44 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.107 | ppl 4410.85 | wps 31811.7 | wpb 591.2 | bsz 29.9 | num_updates 525 | best_loss 12.107
2022-02-28 14:07:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 525 updates
2022-02-28 14:07:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:07:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:07:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 525 updates, score 12.107) (writing took 8.109874600020703 seconds)
2022-02-28 14:07:52 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-28 14:07:52 | INFO | train | epoch 017 | loss 12.334 | ppl 5163.78 | wps 1590.9 | ups 1.3 | wpb 1227.6 | bsz 61.6 | num_updates 525 | lr 1.575e-06 | gnorm 12.207 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 184
2022-02-28 14:07:52 | INFO | fairseq.trainer | begin training epoch 18
2022-02-28 14:07:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:07:53 | INFO | train_inner | epoch 018:     15 / 31 loss=12.154, ppl=4558.86, wps=2507, ups=1.93, wpb=1297.2, bsz=61.1, num_updates=540, lr=1.62e-06, gnorm=11.494, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=186
2022-02-28 14:07:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:07:55 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 12.041 | ppl 4213.69 | wps 26988.1 | wpb 591.2 | bsz 29.9 | num_updates 556 | best_loss 12.041
2022-02-28 14:07:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 556 updates
2022-02-28 14:07:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:07:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 556 updates, score 12.041) (writing took 6.540605737973237 seconds)
2022-02-28 14:08:02 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-28 14:08:02 | INFO | train | epoch 018 | loss 12.126 | ppl 4470.84 | wps 3973.7 | ups 3.24 | wpb 1227.6 | bsz 61.6 | num_updates 556 | lr 1.668e-06 | gnorm 11.134 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 194
2022-02-28 14:08:02 | INFO | fairseq.trainer | begin training epoch 19
2022-02-28 14:08:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:08:02 | INFO | train_inner | epoch 019:      4 / 31 loss=12.078, ppl=4324.56, wps=2340.3, ups=2.29, wpb=1023.3, bsz=61.6, num_updates=560, lr=1.68e-06, gnorm=11.391, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=195
2022-02-28 14:08:04 | INFO | train_inner | epoch 019:     24 / 31 loss=12.006, ppl=4113.69, wps=19712.6, ups=15.18, wpb=1298.8, bsz=63.2, num_updates=580, lr=1.74e-06, gnorm=10.387, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=196
2022-02-28 14:08:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:08:04 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.817 | ppl 3606.98 | wps 31411.6 | wpb 591.2 | bsz 29.9 | num_updates 587 | best_loss 11.817
2022-02-28 14:08:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 587 updates
2022-02-28 14:08:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 587 updates, score 11.817) (writing took 7.5592121689987835 seconds)
2022-02-28 14:08:12 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-28 14:08:12 | INFO | train | epoch 019 | loss 11.989 | ppl 4065.27 | wps 3722.4 | ups 3.03 | wpb 1227.6 | bsz 61.6 | num_updates 587 | lr 1.761e-06 | gnorm 10.621 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 204
2022-02-28 14:08:12 | INFO | fairseq.trainer | begin training epoch 20
2022-02-28 14:08:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:08:13 | INFO | train_inner | epoch 020:     13 / 31 loss=11.914, ppl=3858.43, wps=2572.2, ups=2.08, wpb=1235, bsz=61.1, num_updates=600, lr=1.8e-06, gnorm=10.549, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=205
2022-02-28 14:08:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:08:15 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.545 | ppl 2988.13 | wps 30065.1 | wpb 591.2 | bsz 29.9 | num_updates 618 | best_loss 11.545
2022-02-28 14:08:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 618 updates
2022-02-28 14:08:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 618 updates, score 11.545) (writing took 6.687083729979349 seconds)
2022-02-28 14:08:22 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-28 14:08:22 | INFO | train | epoch 020 | loss 11.815 | ppl 3603.1 | wps 3947.7 | ups 3.22 | wpb 1227.6 | bsz 61.6 | num_updates 618 | lr 1.854e-06 | gnorm 11.158 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 214
2022-02-28 14:08:22 | INFO | fairseq.trainer | begin training epoch 21
2022-02-28 14:08:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:08:22 | INFO | train_inner | epoch 021:      2 / 31 loss=11.702, ppl=3332.18, wps=2651.1, ups=2.24, wpb=1183, bsz=60.3, num_updates=620, lr=1.86e-06, gnorm=11.543, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=214
2022-02-28 14:08:23 | INFO | train_inner | epoch 021:     22 / 31 loss=11.691, ppl=3306.81, wps=17406.2, ups=14.19, wpb=1226.5, bsz=62.7, num_updates=640, lr=1.92e-06, gnorm=9.858, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=216
2022-02-28 14:08:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:08:25 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.508 | ppl 2912.61 | wps 30654.7 | wpb 591.2 | bsz 29.9 | num_updates 649 | best_loss 11.508
2022-02-28 14:08:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 649 updates
2022-02-28 14:08:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 649 updates, score 11.508) (writing took 6.508868246019119 seconds)
2022-02-28 14:08:31 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-28 14:08:31 | INFO | train | epoch 021 | loss 11.644 | ppl 3200.83 | wps 4031.3 | ups 3.28 | wpb 1227.6 | bsz 61.6 | num_updates 649 | lr 1.947e-06 | gnorm 9.871 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 224
2022-02-28 14:08:31 | INFO | fairseq.trainer | begin training epoch 22
2022-02-28 14:08:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:08:32 | INFO | train_inner | epoch 022:     11 / 31 loss=11.537, ppl=2971.62, wps=2799.8, ups=2.34, wpb=1198, bsz=61.6, num_updates=660, lr=1.98e-06, gnorm=9.992, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=224
2022-02-28 14:08:33 | INFO | train_inner | epoch 022:     31 / 31 loss=11.538, ppl=2974.03, wps=18869.6, ups=15.02, wpb=1256.2, bsz=60.3, num_updates=680, lr=2.04e-06, gnorm=9.724, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=226
2022-02-28 14:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:08:34 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.344 | ppl 2599.5 | wps 26514.2 | wpb 591.2 | bsz 29.9 | num_updates 680 | best_loss 11.344
2022-02-28 14:08:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 680 updates
2022-02-28 14:08:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 680 updates, score 11.344) (writing took 4.810758708015783 seconds)
2022-02-28 14:08:39 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-28 14:08:39 | INFO | train | epoch 022 | loss 11.527 | ppl 2951.19 | wps 5045.6 | ups 4.11 | wpb 1227.6 | bsz 61.6 | num_updates 680 | lr 2.04e-06 | gnorm 9.833 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 231
2022-02-28 14:08:39 | INFO | fairseq.trainer | begin training epoch 23
2022-02-28 14:08:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:08:40 | INFO | train_inner | epoch 023:     20 / 31 loss=11.339, ppl=2590.56, wps=3611.2, ups=2.88, wpb=1255.1, bsz=61.9, num_updates=700, lr=2.1e-06, gnorm=9.217, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=233
2022-02-28 14:08:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:08:42 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.297 | ppl 2515.29 | wps 31046.6 | wpb 591.2 | bsz 29.9 | num_updates 711 | best_loss 11.297
2022-02-28 14:08:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 711 updates
2022-02-28 14:08:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 711 updates, score 11.297) (writing took 4.529498424992198 seconds)
2022-02-28 14:08:46 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-28 14:08:46 | INFO | train | epoch 023 | loss 11.353 | ppl 2616.2 | wps 5253.7 | ups 4.28 | wpb 1227.6 | bsz 61.6 | num_updates 711 | lr 2.133e-06 | gnorm 9.329 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 238
2022-02-28 14:08:46 | INFO | fairseq.trainer | begin training epoch 24
2022-02-28 14:08:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:08:47 | INFO | train_inner | epoch 024:      9 / 31 loss=11.395, ppl=2693.73, wps=3861.6, ups=3.09, wpb=1250.5, bsz=62.4, num_updates=720, lr=2.16e-06, gnorm=9.237, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=239
2022-02-28 14:08:48 | INFO | train_inner | epoch 024:     29 / 31 loss=11.32, ppl=2557.36, wps=17475.7, ups=14.24, wpb=1227.5, bsz=61.9, num_updates=740, lr=2.22e-06, gnorm=8.948, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=241
2022-02-28 14:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:08:49 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.086 | ppl 2173.92 | wps 30131.3 | wpb 591.2 | bsz 29.9 | num_updates 742 | best_loss 11.086
2022-02-28 14:08:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 742 updates
2022-02-28 14:08:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:08:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 742 updates, score 11.086) (writing took 6.720834617008222 seconds)
2022-02-28 14:08:56 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-28 14:08:56 | INFO | train | epoch 024 | loss 11.324 | ppl 2564.11 | wps 4015.3 | ups 3.27 | wpb 1227.6 | bsz 61.6 | num_updates 742 | lr 2.226e-06 | gnorm 9.045 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 248
2022-02-28 14:08:56 | INFO | fairseq.trainer | begin training epoch 25
2022-02-28 14:08:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:08:57 | INFO | train_inner | epoch 025:     18 / 31 loss=11.159, ppl=2287.17, wps=2830.3, ups=2.3, wpb=1231.2, bsz=61.6, num_updates=760, lr=2.28e-06, gnorm=8.99, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=249
2022-02-28 14:08:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:08:58 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.026 | ppl 2084.92 | wps 31543.9 | wpb 591.2 | bsz 29.9 | num_updates 773 | best_loss 11.026
2022-02-28 14:08:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 773 updates
2022-02-28 14:08:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 773 updates, score 11.026) (writing took 5.772193659999175 seconds)
2022-02-28 14:09:04 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-28 14:09:04 | INFO | train | epoch 025 | loss 11.197 | ppl 2348.43 | wps 4438.5 | ups 3.62 | wpb 1227.6 | bsz 61.6 | num_updates 773 | lr 2.319e-06 | gnorm 8.724 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 256
2022-02-28 14:09:04 | INFO | fairseq.trainer | begin training epoch 26
2022-02-28 14:09:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:09:05 | INFO | train_inner | epoch 026:      7 / 31 loss=11.19, ppl=2336.16, wps=2933.4, ups=2.56, wpb=1146.8, bsz=59.8, num_updates=780, lr=2.34e-06, gnorm=9.033, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=257
2022-02-28 14:09:06 | INFO | train_inner | epoch 026:     27 / 31 loss=11.029, ppl=2089.19, wps=19611.6, ups=14.88, wpb=1317.7, bsz=63.2, num_updates=800, lr=2.4e-06, gnorm=9.143, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=258
2022-02-28 14:09:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:09:07 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 10.851 | ppl 1846.91 | wps 30253.9 | wpb 591.2 | bsz 29.9 | num_updates 804 | best_loss 10.851
2022-02-28 14:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 804 updates
2022-02-28 14:09:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 804 updates, score 10.851) (writing took 6.212893574003829 seconds)
2022-02-28 14:09:13 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-28 14:09:13 | INFO | train | epoch 026 | loss 11.093 | ppl 2185.07 | wps 4273.7 | ups 3.48 | wpb 1227.6 | bsz 61.6 | num_updates 804 | lr 2.412e-06 | gnorm 9.281 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 265
2022-02-28 14:09:13 | INFO | fairseq.trainer | begin training epoch 27
2022-02-28 14:09:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:09:14 | INFO | train_inner | epoch 027:     16 / 31 loss=11.013, ppl=2065.9, wps=2946.9, ups=2.41, wpb=1221, bsz=61.1, num_updates=820, lr=2.46e-06, gnorm=8.62, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=267
2022-02-28 14:09:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:09:16 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.76 | ppl 1733.8 | wps 31111.9 | wpb 591.2 | bsz 29.9 | num_updates 835 | best_loss 10.76
2022-02-28 14:09:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 835 updates
2022-02-28 14:09:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 835 updates, score 10.76) (writing took 7.749487682012841 seconds)
2022-02-28 14:09:24 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-28 14:09:24 | INFO | train | epoch 027 | loss 10.966 | ppl 1999.96 | wps 3609.8 | ups 2.94 | wpb 1227.6 | bsz 61.6 | num_updates 835 | lr 2.505e-06 | gnorm 8.672 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 276
2022-02-28 14:09:24 | INFO | fairseq.trainer | begin training epoch 28
2022-02-28 14:09:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:09:24 | INFO | train_inner | epoch 028:      5 / 31 loss=10.988, ppl=2031.63, wps=2321, ups=2.07, wpb=1122.6, bsz=60.8, num_updates=840, lr=2.52e-06, gnorm=8.795, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=276
2022-02-28 14:09:25 | INFO | train_inner | epoch 028:     25 / 31 loss=10.788, ppl=1767.82, wps=17524.9, ups=14.43, wpb=1214.5, bsz=62.7, num_updates=860, lr=2.58e-06, gnorm=10.262, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=278
2022-02-28 14:09:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:09:26 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.691 | ppl 1652.8 | wps 30178.8 | wpb 591.2 | bsz 29.9 | num_updates 866 | best_loss 10.691
2022-02-28 14:09:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 866 updates
2022-02-28 14:09:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 866 updates, score 10.691) (writing took 5.858175494999159 seconds)
2022-02-28 14:09:32 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-28 14:09:32 | INFO | train | epoch 028 | loss 10.855 | ppl 1852.78 | wps 4451.1 | ups 3.63 | wpb 1227.6 | bsz 61.6 | num_updates 866 | lr 2.598e-06 | gnorm 9.585 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 285
2022-02-28 14:09:32 | INFO | fairseq.trainer | begin training epoch 29
2022-02-28 14:09:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:09:33 | INFO | train_inner | epoch 029:     14 / 31 loss=10.899, ppl=1909.23, wps=3156.8, ups=2.57, wpb=1228.5, bsz=60.3, num_updates=880, lr=2.64e-06, gnorm=8.296, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=286
2022-02-28 14:09:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:09:35 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.618 | ppl 1571.74 | wps 30916.6 | wpb 591.2 | bsz 29.9 | num_updates 897 | best_loss 10.618
2022-02-28 14:09:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 897 updates
2022-02-28 14:09:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 897 updates, score 10.618) (writing took 5.50743893100298 seconds)
2022-02-28 14:09:40 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-28 14:09:40 | INFO | train | epoch 029 | loss 10.806 | ppl 1790.76 | wps 4637.7 | ups 3.78 | wpb 1227.6 | bsz 61.6 | num_updates 897 | lr 2.691e-06 | gnorm 8.207 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 293
2022-02-28 14:09:40 | INFO | fairseq.trainer | begin training epoch 30
2022-02-28 14:09:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:09:41 | INFO | train_inner | epoch 030:      3 / 31 loss=10.743, ppl=1713.4, wps=3444.7, ups=2.64, wpb=1303.6, bsz=62.4, num_updates=900, lr=2.7e-06, gnorm=8.117, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=293
2022-02-28 14:09:42 | INFO | train_inner | epoch 030:     23 / 31 loss=10.682, ppl=1643.05, wps=17320.8, ups=14.8, wpb=1170.5, bsz=61.9, num_updates=920, lr=2.76e-06, gnorm=8.06, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=295
2022-02-28 14:09:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:09:43 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.647 | ppl 1603.19 | wps 29415.9 | wpb 591.2 | bsz 29.9 | num_updates 928 | best_loss 10.618
2022-02-28 14:09:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 928 updates
2022-02-28 14:09:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:09:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:09:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 30 @ 928 updates, score 10.647) (writing took 2.759612786991056 seconds)
2022-02-28 14:09:46 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-28 14:09:46 | INFO | train | epoch 030 | loss 10.674 | ppl 1633.26 | wps 6912.8 | ups 5.63 | wpb 1227.6 | bsz 61.6 | num_updates 928 | lr 2.784e-06 | gnorm 8.007 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 298
2022-02-28 14:09:46 | INFO | fairseq.trainer | begin training epoch 31
2022-02-28 14:09:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:09:47 | INFO | train_inner | epoch 031:     12 / 31 loss=10.632, ppl=1587.05, wps=5425.9, ups=4.26, wpb=1273.7, bsz=62.4, num_updates=940, lr=2.82e-06, gnorm=8.049, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=299
2022-02-28 14:09:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:09:49 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.457 | ppl 1405.16 | wps 30885.6 | wpb 591.2 | bsz 29.9 | num_updates 959 | best_loss 10.457
2022-02-28 14:09:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 959 updates
2022-02-28 14:09:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:09:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 959 updates, score 10.457) (writing took 5.953607761999592 seconds)
2022-02-28 14:09:55 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-28 14:09:55 | INFO | train | epoch 031 | loss 10.615 | ppl 1568.13 | wps 4301 | ups 3.5 | wpb 1227.6 | bsz 61.6 | num_updates 959 | lr 2.877e-06 | gnorm 8.346 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 307
2022-02-28 14:09:55 | INFO | fairseq.trainer | begin training epoch 32
2022-02-28 14:09:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:09:55 | INFO | train_inner | epoch 032:      1 / 31 loss=10.614, ppl=1567.61, wps=3012.6, ups=2.46, wpb=1222.8, bsz=60.3, num_updates=960, lr=2.88e-06, gnorm=8.383, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=307
2022-02-28 14:09:56 | INFO | train_inner | epoch 032:     21 / 31 loss=10.572, ppl=1522.22, wps=17441.8, ups=14.13, wpb=1234, bsz=63.2, num_updates=980, lr=2.94e-06, gnorm=7.93, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=309
2022-02-28 14:09:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:09:58 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 10.473 | ppl 1421.34 | wps 30907.9 | wpb 591.2 | bsz 29.9 | num_updates 990 | best_loss 10.457
2022-02-28 14:09:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 990 updates
2022-02-28 14:09:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:10:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:10:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 32 @ 990 updates, score 10.473) (writing took 3.0090472610027064 seconds)
2022-02-28 14:10:01 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-28 14:10:01 | INFO | train | epoch 032 | loss 10.551 | ppl 1499.93 | wps 6655.9 | ups 5.42 | wpb 1227.6 | bsz 61.6 | num_updates 990 | lr 2.97e-06 | gnorm 8.047 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 313
2022-02-28 14:10:01 | INFO | fairseq.trainer | begin training epoch 33
2022-02-28 14:10:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:10:01 | INFO | train_inner | epoch 033:     10 / 31 loss=10.603, ppl=1554.93, wps=5072.6, ups=4.02, wpb=1262.8, bsz=59, num_updates=1000, lr=3e-06, gnorm=7.961, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=314
2022-02-28 14:10:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:10:02 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.448 | ppl 1397.11 | wps 30023.9 | wpb 591.2 | bsz 29.9 | num_updates 1000 | best_loss 10.448
2022-02-28 14:10:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1000 updates
2022-02-28 14:10:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-28 14:10:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-28 14:10:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt (epoch 33 @ 1000 updates, score 10.448) (writing took 8.110601521999342 seconds)
2022-02-28 14:10:12 | INFO | train_inner | epoch 033:     30 / 31 loss=10.363, ppl=1317.27, wps=2286.2, ups=1.91, wpb=1198.3, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=7.931, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=324
2022-02-28 14:10:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:10:12 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.381 | ppl 1333.83 | wps 30884.8 | wpb 591.2 | bsz 29.9 | num_updates 1021 | best_loss 10.381
2022-02-28 14:10:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1021 updates
2022-02-28 14:10:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:10:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:10:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1021 updates, score 10.381) (writing took 7.984039128001314 seconds)
2022-02-28 14:10:20 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-28 14:10:20 | INFO | train | epoch 033 | loss 10.473 | ppl 1421.1 | wps 1922.3 | ups 1.57 | wpb 1227.6 | bsz 61.6 | num_updates 1021 | lr 3.063e-06 | gnorm 7.876 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 333
2022-02-28 14:10:20 | INFO | fairseq.trainer | begin training epoch 34
2022-02-28 14:10:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:10:22 | INFO | train_inner | epoch 034:     19 / 31 loss=10.387, ppl=1339.03, wps=2439.6, ups=2.01, wpb=1211.8, bsz=62.4, num_updates=1040, lr=3.12e-06, gnorm=8.232, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=334
2022-02-28 14:10:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:10:23 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 10.37 | ppl 1323.46 | wps 30672.8 | wpb 591.2 | bsz 29.9 | num_updates 1052 | best_loss 10.37
2022-02-28 14:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1052 updates
2022-02-28 14:10:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:10:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1052 updates, score 10.37) (writing took 8.50254416099051 seconds)
2022-02-28 14:10:32 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-28 14:10:32 | INFO | train | epoch 034 | loss 10.416 | ppl 1366.01 | wps 3394 | ups 2.76 | wpb 1227.6 | bsz 61.6 | num_updates 1052 | lr 3.156e-06 | gnorm 8.081 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 344
2022-02-28 14:10:32 | INFO | fairseq.trainer | begin training epoch 35
2022-02-28 14:10:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:10:32 | INFO | train_inner | epoch 035:      8 / 31 loss=10.464, ppl=1412.53, wps=2346.7, ups=1.91, wpb=1227.2, bsz=60.3, num_updates=1060, lr=3.18e-06, gnorm=7.755, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=345
2022-02-28 14:10:34 | INFO | train_inner | epoch 035:     28 / 31 loss=10.297, ppl=1258.35, wps=17074.6, ups=13.18, wpb=1296, bsz=61.9, num_updates=1080, lr=3.24e-06, gnorm=7.923, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=346
2022-02-28 14:10:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:10:35 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 10.186 | ppl 1164.52 | wps 30984.6 | wpb 591.2 | bsz 29.9 | num_updates 1083 | best_loss 10.186
2022-02-28 14:10:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1083 updates
2022-02-28 14:10:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:10:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1083 updates, score 10.186) (writing took 7.269562159985071 seconds)
2022-02-28 14:10:42 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-28 14:10:42 | INFO | train | epoch 035 | loss 10.339 | ppl 1294.92 | wps 3756.5 | ups 3.06 | wpb 1227.6 | bsz 61.6 | num_updates 1083 | lr 3.249e-06 | gnorm 7.947 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 354
2022-02-28 14:10:42 | INFO | fairseq.trainer | begin training epoch 36
2022-02-28 14:10:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:10:43 | INFO | train_inner | epoch 036:     17 / 31 loss=10.323, ppl=1281.35, wps=2733.2, ups=2.16, wpb=1265.6, bsz=61.1, num_updates=1100, lr=3.3e-06, gnorm=8.524, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=355
2022-02-28 14:10:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:10:45 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 10.132 | ppl 1121.9 | wps 30322.4 | wpb 591.2 | bsz 29.9 | num_updates 1114 | best_loss 10.132
2022-02-28 14:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1114 updates
2022-02-28 14:10:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:10:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:10:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1114 updates, score 10.132) (writing took 5.123004071996547 seconds)
2022-02-28 14:10:50 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-28 14:10:50 | INFO | train | epoch 036 | loss 10.227 | ppl 1198.54 | wps 4836.5 | ups 3.94 | wpb 1227.6 | bsz 61.6 | num_updates 1114 | lr 3.342e-06 | gnorm 8.36 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 362
2022-02-28 14:10:50 | INFO | fairseq.trainer | begin training epoch 37
2022-02-28 14:10:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:10:50 | INFO | train_inner | epoch 037:      6 / 31 loss=10.092, ppl=1091.07, wps=3170.2, ups=2.8, wpb=1132.5, bsz=61.6, num_updates=1120, lr=3.36e-06, gnorm=7.921, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=363
2022-02-28 14:10:52 | INFO | train_inner | epoch 037:     26 / 31 loss=10.244, ppl=1212.5, wps=18085.7, ups=15, wpb=1205.9, bsz=61.9, num_updates=1140, lr=3.42e-06, gnorm=7.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=364
2022-02-28 14:10:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:10:52 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.967 | ppl 1000.76 | wps 30330.7 | wpb 591.2 | bsz 29.9 | num_updates 1145 | best_loss 9.967
2022-02-28 14:10:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1145 updates
2022-02-28 14:10:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:10:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:10:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1145 updates, score 9.967) (writing took 4.605722428008448 seconds)
2022-02-28 14:10:57 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-28 14:10:57 | INFO | train | epoch 037 | loss 10.189 | ppl 1167.65 | wps 5180.6 | ups 4.22 | wpb 1227.6 | bsz 61.6 | num_updates 1145 | lr 3.435e-06 | gnorm 7.595 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 369
2022-02-28 14:10:57 | INFO | fairseq.trainer | begin training epoch 38
2022-02-28 14:10:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:10:58 | INFO | train_inner | epoch 038:     15 / 31 loss=10.075, ppl=1078.35, wps=3561.3, ups=3.02, wpb=1181, bsz=62.4, num_updates=1160, lr=3.48e-06, gnorm=7.75, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=371
2022-02-28 14:10:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:11:00 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 10.041 | ppl 1053.41 | wps 30438.1 | wpb 591.2 | bsz 29.9 | num_updates 1176 | best_loss 9.967
2022-02-28 14:11:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1176 updates
2022-02-28 14:11:00 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:11:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:11:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 38 @ 1176 updates, score 10.041) (writing took 4.056687611999223 seconds)
2022-02-28 14:11:04 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-28 14:11:04 | INFO | train | epoch 038 | loss 10.103 | ppl 1099.7 | wps 5561.2 | ups 4.53 | wpb 1227.6 | bsz 61.6 | num_updates 1176 | lr 3.528e-06 | gnorm 7.73 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 376
2022-02-28 14:11:04 | INFO | fairseq.trainer | begin training epoch 39
2022-02-28 14:11:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:11:04 | INFO | train_inner | epoch 039:      4 / 31 loss=10.085, ppl=1086.02, wps=4255.8, ups=3.29, wpb=1293.3, bsz=60.3, num_updates=1180, lr=3.54e-06, gnorm=7.527, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=377
2022-02-28 14:11:06 | INFO | train_inner | epoch 039:     24 / 31 loss=10.01, ppl=1031.02, wps=18354.9, ups=14.37, wpb=1277.4, bsz=62.7, num_updates=1200, lr=3.6e-06, gnorm=7.835, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=378
2022-02-28 14:11:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:11:07 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.903 | ppl 957.5 | wps 31265.5 | wpb 591.2 | bsz 29.9 | num_updates 1207 | best_loss 9.903
2022-02-28 14:11:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1207 updates
2022-02-28 14:11:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:11:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:11:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1207 updates, score 9.903) (writing took 4.950237682991428 seconds)
2022-02-28 14:11:12 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-28 14:11:12 | INFO | train | epoch 039 | loss 9.996 | ppl 1020.98 | wps 4939.4 | ups 4.02 | wpb 1227.6 | bsz 61.6 | num_updates 1207 | lr 3.621e-06 | gnorm 7.737 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 384
2022-02-28 14:11:12 | INFO | fairseq.trainer | begin training epoch 40
2022-02-28 14:11:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:11:13 | INFO | train_inner | epoch 040:     13 / 31 loss=9.877, ppl=940.12, wps=3125.3, ups=2.88, wpb=1086.4, bsz=60.8, num_updates=1220, lr=3.66e-06, gnorm=7.614, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=385
2022-02-28 14:11:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:11:14 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.728 | ppl 848.31 | wps 28389.6 | wpb 591.2 | bsz 29.9 | num_updates 1238 | best_loss 9.728
2022-02-28 14:11:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1238 updates
2022-02-28 14:11:14 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:11:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:11:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1238 updates, score 9.728) (writing took 4.597096904995851 seconds)
2022-02-28 14:11:19 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-28 14:11:19 | INFO | train | epoch 040 | loss 9.915 | ppl 965.51 | wps 5178.4 | ups 4.22 | wpb 1227.6 | bsz 61.6 | num_updates 1238 | lr 3.714e-06 | gnorm 7.746 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 391
2022-02-28 14:11:19 | INFO | fairseq.trainer | begin training epoch 41
2022-02-28 14:11:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:11:19 | INFO | train_inner | epoch 041:      2 / 31 loss=10.012, ppl=1032.3, wps=3903.5, ups=3.03, wpb=1288, bsz=61.1, num_updates=1240, lr=3.72e-06, gnorm=8.023, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=392
2022-02-28 14:11:21 | INFO | train_inner | epoch 041:     22 / 31 loss=9.886, ppl=946.16, wps=19791.6, ups=14.86, wpb=1331.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=7.616, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=393
2022-02-28 14:11:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:11:22 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.767 | ppl 871.4 | wps 29501.4 | wpb 591.2 | bsz 29.9 | num_updates 1269 | best_loss 9.728
2022-02-28 14:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1269 updates
2022-02-28 14:11:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:11:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:11:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 1269 updates, score 9.767) (writing took 3.1095890529977623 seconds)
2022-02-28 14:11:25 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-28 14:11:25 | INFO | train | epoch 041 | loss 9.918 | ppl 967.22 | wps 6593.2 | ups 5.37 | wpb 1227.6 | bsz 61.6 | num_updates 1269 | lr 3.807e-06 | gnorm 7.718 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 397
2022-02-28 14:11:25 | INFO | fairseq.trainer | begin training epoch 42
2022-02-28 14:11:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:11:26 | INFO | train_inner | epoch 042:     11 / 31 loss=9.921, ppl=969.71, wps=4627.6, ups=3.97, wpb=1164.5, bsz=61.6, num_updates=1280, lr=3.84e-06, gnorm=7.611, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=398
2022-02-28 14:11:27 | INFO | train_inner | epoch 042:     31 / 31 loss=9.716, ppl=841.31, wps=17738, ups=14.61, wpb=1214.5, bsz=60.3, num_updates=1300, lr=3.9e-06, gnorm=7.924, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=399
2022-02-28 14:11:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:11:27 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.678 | ppl 819.2 | wps 31617.7 | wpb 591.2 | bsz 29.9 | num_updates 1300 | best_loss 9.678
2022-02-28 14:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1300 updates
2022-02-28 14:11:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:11:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:11:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 1300 updates, score 9.678) (writing took 5.073962534981547 seconds)
2022-02-28 14:11:33 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-02-28 14:11:33 | INFO | train | epoch 042 | loss 9.786 | ppl 882.61 | wps 4891.3 | ups 3.98 | wpb 1227.6 | bsz 61.6 | num_updates 1300 | lr 3.9e-06 | gnorm 7.775 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 405
2022-02-28 14:11:33 | INFO | fairseq.trainer | begin training epoch 43
2022-02-28 14:11:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:11:35 | INFO | train_inner | epoch 043:     20 / 31 loss=9.795, ppl=888.08, wps=3372.8, ups=2.63, wpb=1280.2, bsz=61.9, num_updates=1320, lr=3.96e-06, gnorm=7.558, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=407
2022-02-28 14:11:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:11:36 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.652 | ppl 804.28 | wps 30082.9 | wpb 591.2 | bsz 29.9 | num_updates 1331 | best_loss 9.652
2022-02-28 14:11:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1331 updates
2022-02-28 14:11:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:11:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:11:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 43 @ 1331 updates, score 9.652) (writing took 7.006134233000921 seconds)
2022-02-28 14:11:43 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-02-28 14:11:43 | INFO | train | epoch 043 | loss 9.771 | ppl 873.56 | wps 3730.9 | ups 3.04 | wpb 1227.6 | bsz 61.6 | num_updates 1331 | lr 3.993e-06 | gnorm 7.659 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 415
2022-02-28 14:11:43 | INFO | fairseq.trainer | begin training epoch 44
2022-02-28 14:11:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:11:44 | INFO | train_inner | epoch 044:      9 / 31 loss=9.687, ppl=824.56, wps=2646, ups=2.23, wpb=1184.5, bsz=62.4, num_updates=1340, lr=4.02e-06, gnorm=8.028, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=416
2022-02-28 14:11:45 | INFO | train_inner | epoch 044:     29 / 31 loss=9.612, ppl=782.46, wps=17146.6, ups=14.04, wpb=1221.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=7.454, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=417
2022-02-28 14:11:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:11:46 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.624 | ppl 788.9 | wps 30877.6 | wpb 591.2 | bsz 29.9 | num_updates 1362 | best_loss 9.624
2022-02-28 14:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1362 updates
2022-02-28 14:11:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:11:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:11:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 1362 updates, score 9.624) (writing took 8.521662431012373 seconds)
2022-02-28 14:11:54 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-02-28 14:11:54 | INFO | train | epoch 044 | loss 9.66 | ppl 808.89 | wps 3370.1 | ups 2.75 | wpb 1227.6 | bsz 61.6 | num_updates 1362 | lr 4.086e-06 | gnorm 7.703 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 426
2022-02-28 14:11:54 | INFO | fairseq.trainer | begin training epoch 45
2022-02-28 14:11:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:11:55 | INFO | train_inner | epoch 045:     18 / 31 loss=9.634, ppl=794.66, wps=2280.1, ups=1.91, wpb=1195.7, bsz=60.3, num_updates=1380, lr=4.14e-06, gnorm=7.457, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=428
2022-02-28 14:11:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:11:57 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.52 | ppl 734.3 | wps 30872.2 | wpb 591.2 | bsz 29.9 | num_updates 1393 | best_loss 9.52
2022-02-28 14:11:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1393 updates
2022-02-28 14:11:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:12:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:12:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 1393 updates, score 9.52) (writing took 4.561113103001844 seconds)
2022-02-28 14:12:01 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-02-28 14:12:01 | INFO | train | epoch 045 | loss 9.561 | ppl 755.44 | wps 5263.8 | ups 4.29 | wpb 1227.6 | bsz 61.6 | num_updates 1393 | lr 4.179e-06 | gnorm 7.69 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 434
2022-02-28 14:12:01 | INFO | fairseq.trainer | begin training epoch 46
2022-02-28 14:12:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:12:02 | INFO | train_inner | epoch 046:      7 / 31 loss=9.535, ppl=741.64, wps=3896, ups=3.1, wpb=1258.5, bsz=61.6, num_updates=1400, lr=4.2e-06, gnorm=8.11, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=434
2022-02-28 14:12:03 | INFO | train_inner | epoch 046:     27 / 31 loss=9.51, ppl=729.21, wps=17767.2, ups=14.57, wpb=1219.4, bsz=61.9, num_updates=1420, lr=4.26e-06, gnorm=7.706, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=436
2022-02-28 14:12:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:12:04 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.591 | ppl 771.02 | wps 30313.5 | wpb 591.2 | bsz 29.9 | num_updates 1424 | best_loss 9.52
2022-02-28 14:12:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1424 updates
2022-02-28 14:12:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:12:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:12:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 46 @ 1424 updates, score 9.591) (writing took 2.7209091639961116 seconds)
2022-02-28 14:12:07 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-02-28 14:12:07 | INFO | train | epoch 046 | loss 9.523 | ppl 735.7 | wps 7036.5 | ups 5.73 | wpb 1227.6 | bsz 61.6 | num_updates 1424 | lr 4.272e-06 | gnorm 7.805 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 439
2022-02-28 14:12:07 | INFO | fairseq.trainer | begin training epoch 47
2022-02-28 14:12:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:12:08 | INFO | train_inner | epoch 047:     16 / 31 loss=9.529, ppl=738.55, wps=5157, ups=4.28, wpb=1206, bsz=61.6, num_updates=1440, lr=4.32e-06, gnorm=7.821, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=440
2022-02-28 14:12:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:12:10 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.38 | ppl 666.3 | wps 32029.3 | wpb 591.2 | bsz 29.9 | num_updates 1455 | best_loss 9.38
2022-02-28 14:12:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1455 updates
2022-02-28 14:12:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:12:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:12:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 1455 updates, score 9.38) (writing took 5.648829462006688 seconds)
2022-02-28 14:12:15 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-02-28 14:12:15 | INFO | train | epoch 047 | loss 9.443 | ppl 695.87 | wps 4516.6 | ups 3.68 | wpb 1227.6 | bsz 61.6 | num_updates 1455 | lr 4.365e-06 | gnorm 7.908 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 448
2022-02-28 14:12:15 | INFO | fairseq.trainer | begin training epoch 48
2022-02-28 14:12:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:12:16 | INFO | train_inner | epoch 048:      5 / 31 loss=9.411, ppl=680.69, wps=3228.3, ups=2.59, wpb=1244.2, bsz=59.8, num_updates=1460, lr=4.38e-06, gnorm=7.895, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=448
2022-02-28 14:12:17 | INFO | train_inner | epoch 048:     25 / 31 loss=9.321, ppl=639.61, wps=18481.8, ups=14.38, wpb=1285.3, bsz=63.2, num_updates=1480, lr=4.44e-06, gnorm=7.138, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=449
2022-02-28 14:12:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:12:18 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.397 | ppl 674.27 | wps 31053.1 | wpb 591.2 | bsz 29.9 | num_updates 1486 | best_loss 9.38
2022-02-28 14:12:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1486 updates
2022-02-28 14:12:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:12:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:12:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 48 @ 1486 updates, score 9.397) (writing took 2.7814107599842828 seconds)
2022-02-28 14:12:21 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-02-28 14:12:21 | INFO | train | epoch 048 | loss 9.37 | ppl 661.52 | wps 6739.2 | ups 5.49 | wpb 1227.6 | bsz 61.6 | num_updates 1486 | lr 4.458e-06 | gnorm 7.353 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 453
2022-02-28 14:12:21 | INFO | fairseq.trainer | begin training epoch 49
2022-02-28 14:12:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:12:22 | INFO | train_inner | epoch 049:     14 / 31 loss=9.437, ppl=693.28, wps=5099.8, ups=4.03, wpb=1265.4, bsz=62.4, num_updates=1500, lr=4.5e-06, gnorm=7.318, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=454
2022-02-28 14:12:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:12:23 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.329 | ppl 643.22 | wps 32253.7 | wpb 591.2 | bsz 29.9 | num_updates 1500 | best_loss 9.329
2022-02-28 14:12:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1500 updates
2022-02-28 14:12:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-28 14:12:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-28 14:12:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt (epoch 49 @ 1500 updates, score 9.329) (writing took 9.113200540014077 seconds)
2022-02-28 14:12:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:12:34 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.28 | ppl 621.66 | wps 30442.6 | wpb 591.2 | bsz 29.9 | num_updates 1517 | best_loss 9.28
2022-02-28 14:12:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1517 updates
2022-02-28 14:12:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:12:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:12:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 1517 updates, score 9.28) (writing took 10.292796310997801 seconds)
2022-02-28 14:12:44 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-02-28 14:12:44 | INFO | train | epoch 049 | loss 9.284 | ppl 623.57 | wps 1651.1 | ups 1.34 | wpb 1227.6 | bsz 61.6 | num_updates 1517 | lr 4.551e-06 | gnorm 7.73 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 476
2022-02-28 14:12:44 | INFO | fairseq.trainer | begin training epoch 50
2022-02-28 14:12:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:12:44 | INFO | train_inner | epoch 050:      3 / 31 loss=9.082, ppl=541.94, wps=985.8, ups=0.9, wpb=1100.2, bsz=60.3, num_updates=1520, lr=4.56e-06, gnorm=8.202, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=477
2022-02-28 14:12:46 | INFO | train_inner | epoch 050:     23 / 31 loss=9.277, ppl=620.56, wps=18268.4, ups=14.57, wpb=1254.2, bsz=61.9, num_updates=1540, lr=4.62e-06, gnorm=7.504, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=478
2022-02-28 14:12:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:12:47 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.271 | ppl 617.75 | wps 30198.5 | wpb 591.2 | bsz 29.9 | num_updates 1548 | best_loss 9.271
2022-02-28 14:12:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1548 updates
2022-02-28 14:12:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:12:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:12:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 1548 updates, score 9.271) (writing took 6.8709635909763165 seconds)
2022-02-28 14:12:54 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-02-28 14:12:54 | INFO | train | epoch 050 | loss 9.25 | ppl 608.87 | wps 3914.9 | ups 3.19 | wpb 1227.6 | bsz 61.6 | num_updates 1548 | lr 4.644e-06 | gnorm 7.543 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 486
2022-02-28 14:12:54 | INFO | fairseq.trainer | begin training epoch 51
2022-02-28 14:12:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:12:55 | INFO | train_inner | epoch 051:     12 / 31 loss=9.361, ppl=657.67, wps=2937.8, ups=2.26, wpb=1299.3, bsz=60.3, num_updates=1560, lr=4.68e-06, gnorm=7.962, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=487
2022-02-28 14:12:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:12:56 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.257 | ppl 611.94 | wps 31532.9 | wpb 591.2 | bsz 29.9 | num_updates 1579 | best_loss 9.257
2022-02-28 14:12:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1579 updates
2022-02-28 14:12:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 51 @ 1579 updates, score 9.257) (writing took 7.79627203400014 seconds)
2022-02-28 14:13:04 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-02-28 14:13:04 | INFO | train | epoch 051 | loss 9.225 | ppl 598.25 | wps 3604.1 | ups 2.94 | wpb 1227.6 | bsz 61.6 | num_updates 1579 | lr 4.737e-06 | gnorm 7.8 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 497
2022-02-28 14:13:04 | INFO | fairseq.trainer | begin training epoch 52
2022-02-28 14:13:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:13:04 | INFO | train_inner | epoch 052:      1 / 31 loss=9.133, ppl=561.34, wps=2355.6, ups=2.04, wpb=1157.5, bsz=62.4, num_updates=1580, lr=4.74e-06, gnorm=7.452, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=497
2022-02-28 14:13:06 | INFO | train_inner | epoch 052:     21 / 31 loss=9.101, ppl=549.15, wps=17540.9, ups=14.87, wpb=1179.5, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=7.438, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=498
2022-02-28 14:13:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:13:07 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.17 | ppl 576.18 | wps 30772.5 | wpb 591.2 | bsz 29.9 | num_updates 1610 | best_loss 9.17
2022-02-28 14:13:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1610 updates
2022-02-28 14:13:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 52 @ 1610 updates, score 9.17) (writing took 5.745927304000361 seconds)
2022-02-28 14:13:13 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-02-28 14:13:13 | INFO | train | epoch 052 | loss 9.172 | ppl 577.02 | wps 4520.6 | ups 3.68 | wpb 1227.6 | bsz 61.6 | num_updates 1610 | lr 4.83e-06 | gnorm 7.504 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 505
2022-02-28 14:13:13 | INFO | fairseq.trainer | begin training epoch 53
2022-02-28 14:13:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:13:14 | INFO | train_inner | epoch 053:     10 / 31 loss=9.248, ppl=607.92, wps=3293.3, ups=2.6, wpb=1267.7, bsz=59.5, num_updates=1620, lr=4.86e-06, gnorm=7.46, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=506
2022-02-28 14:13:15 | INFO | train_inner | epoch 053:     30 / 31 loss=9.024, ppl=520.43, wps=17615.9, ups=13.68, wpb=1288.1, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=7.523, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=507
2022-02-28 14:13:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:13:15 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.057 | ppl 532.74 | wps 31305.6 | wpb 591.2 | bsz 29.9 | num_updates 1641 | best_loss 9.057
2022-02-28 14:13:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1641 updates
2022-02-28 14:13:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 1641 updates, score 9.057) (writing took 5.889950607001083 seconds)
2022-02-28 14:13:21 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-02-28 14:13:21 | INFO | train | epoch 053 | loss 9.081 | ppl 541.46 | wps 4396 | ups 3.58 | wpb 1227.6 | bsz 61.6 | num_updates 1641 | lr 4.923e-06 | gnorm 7.519 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 514
2022-02-28 14:13:21 | INFO | fairseq.trainer | begin training epoch 54
2022-02-28 14:13:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:13:23 | INFO | train_inner | epoch 054:     19 / 31 loss=9.064, ppl=535.23, wps=3265.5, ups=2.55, wpb=1280.2, bsz=60.3, num_updates=1660, lr=4.98e-06, gnorm=7.437, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=515
2022-02-28 14:13:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:13:24 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.02 | ppl 519.27 | wps 30284 | wpb 591.2 | bsz 29.9 | num_updates 1672 | best_loss 9.02
2022-02-28 14:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1672 updates
2022-02-28 14:13:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 54 @ 1672 updates, score 9.02) (writing took 4.32171219898737 seconds)
2022-02-28 14:13:28 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-02-28 14:13:28 | INFO | train | epoch 054 | loss 9.038 | ppl 525.64 | wps 5427.9 | ups 4.42 | wpb 1227.6 | bsz 61.6 | num_updates 1672 | lr 5.016e-06 | gnorm 7.589 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 521
2022-02-28 14:13:28 | INFO | fairseq.trainer | begin training epoch 55
2022-02-28 14:13:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:13:29 | INFO | train_inner | epoch 055:      8 / 31 loss=9.001, ppl=512.28, wps=3888.8, ups=3.19, wpb=1220, bsz=62.4, num_updates=1680, lr=5.04e-06, gnorm=7.672, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=521
2022-02-28 14:13:30 | INFO | train_inner | epoch 055:     28 / 31 loss=8.935, ppl=489.36, wps=18252, ups=14.96, wpb=1219.8, bsz=61.9, num_updates=1700, lr=5.1e-06, gnorm=7.593, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=523
2022-02-28 14:13:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:13:31 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.04 | ppl 526.48 | wps 30771.9 | wpb 591.2 | bsz 29.9 | num_updates 1703 | best_loss 9.02
2022-02-28 14:13:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1703 updates
2022-02-28 14:13:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:13:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:13:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 55 @ 1703 updates, score 9.04) (writing took 2.6744938869960606 seconds)
2022-02-28 14:13:34 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-02-28 14:13:34 | INFO | train | epoch 055 | loss 8.944 | ppl 492.38 | wps 7102.4 | ups 5.79 | wpb 1227.6 | bsz 61.6 | num_updates 1703 | lr 5.109e-06 | gnorm 7.62 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 526
2022-02-28 14:13:34 | INFO | fairseq.trainer | begin training epoch 56
2022-02-28 14:13:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:13:35 | INFO | train_inner | epoch 056:     17 / 31 loss=8.831, ppl=455.37, wps=5003.5, ups=4.3, wpb=1164.4, bsz=61.1, num_updates=1720, lr=5.16e-06, gnorm=7.686, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=527
2022-02-28 14:13:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:13:36 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.985 | ppl 506.57 | wps 30140.6 | wpb 591.2 | bsz 29.9 | num_updates 1734 | best_loss 8.985
2022-02-28 14:13:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1734 updates
2022-02-28 14:13:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 1734 updates, score 8.985) (writing took 6.501432585995644 seconds)
2022-02-28 14:13:43 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-02-28 14:13:43 | INFO | train | epoch 056 | loss 8.9 | ppl 477.76 | wps 4139.4 | ups 3.37 | wpb 1227.6 | bsz 61.6 | num_updates 1734 | lr 5.202e-06 | gnorm 7.488 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 535
2022-02-28 14:13:43 | INFO | fairseq.trainer | begin training epoch 57
2022-02-28 14:13:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:13:44 | INFO | train_inner | epoch 057:      6 / 31 loss=8.975, ppl=503.13, wps=2812.9, ups=2.37, wpb=1186.1, bsz=60.3, num_updates=1740, lr=5.22e-06, gnorm=7.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=536
2022-02-28 14:13:45 | INFO | train_inner | epoch 057:     26 / 31 loss=8.748, ppl=429.85, wps=18189.9, ups=15.08, wpb=1205.9, bsz=63.2, num_updates=1760, lr=5.28e-06, gnorm=7.639, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=537
2022-02-28 14:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:13:46 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.917 | ppl 483.34 | wps 30549.2 | wpb 591.2 | bsz 29.9 | num_updates 1765 | best_loss 8.917
2022-02-28 14:13:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1765 updates
2022-02-28 14:13:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:13:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 57 @ 1765 updates, score 8.917) (writing took 4.904080618987791 seconds)
2022-02-28 14:13:51 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-02-28 14:13:51 | INFO | train | epoch 057 | loss 8.831 | ppl 455.29 | wps 5044 | ups 4.11 | wpb 1227.6 | bsz 61.6 | num_updates 1765 | lr 5.295e-06 | gnorm 7.701 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 543
2022-02-28 14:13:51 | INFO | fairseq.trainer | begin training epoch 58
2022-02-28 14:13:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:13:52 | INFO | train_inner | epoch 058:     15 / 31 loss=8.879, ppl=470.76, wps=3835.5, ups=2.91, wpb=1316, bsz=61.1, num_updates=1780, lr=5.34e-06, gnorm=7.282, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=544
2022-02-28 14:13:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:13:53 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.929 | ppl 487.52 | wps 30609 | wpb 591.2 | bsz 29.9 | num_updates 1796 | best_loss 8.917
2022-02-28 14:13:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1796 updates
2022-02-28 14:13:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:13:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 58 @ 1796 updates, score 8.929) (writing took 2.7647947890218347 seconds)
2022-02-28 14:13:56 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-02-28 14:13:56 | INFO | train | epoch 058 | loss 8.785 | ppl 441.11 | wps 7010.6 | ups 5.71 | wpb 1227.6 | bsz 61.6 | num_updates 1796 | lr 5.388e-06 | gnorm 8.058 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 548
2022-02-28 14:13:56 | INFO | fairseq.trainer | begin training epoch 59
2022-02-28 14:13:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:13:57 | INFO | train_inner | epoch 059:      4 / 31 loss=8.758, ppl=432.86, wps=5126, ups=4.19, wpb=1223.8, bsz=61.6, num_updates=1800, lr=5.4e-06, gnorm=8.515, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=549
2022-02-28 14:13:58 | INFO | train_inner | epoch 059:     24 / 31 loss=8.653, ppl=402.6, wps=17211.2, ups=14.28, wpb=1205.6, bsz=61.9, num_updates=1820, lr=5.46e-06, gnorm=7.411, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=550
2022-02-28 14:13:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:13:59 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.931 | ppl 488.15 | wps 31964.2 | wpb 591.2 | bsz 29.9 | num_updates 1827 | best_loss 8.917
2022-02-28 14:13:59 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 2 runs
2022-02-28 14:13:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1827 updates
2022-02-28 14:13:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:14:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:14:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 59 @ 1827 updates, score 8.931) (writing took 3.1837682699842844 seconds)
2022-02-28 14:14:02 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-02-28 14:14:02 | INFO | train | epoch 059 | loss 8.739 | ppl 427.18 | wps 6402.2 | ups 5.21 | wpb 1227.6 | bsz 61.6 | num_updates 1827 | lr 5.481e-06 | gnorm 7.532 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 554
2022-02-28 14:14:02 | INFO | fairseq_cli.train | done training in 550.8 seconds
2022-02-28 14:26:46 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 2, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=0, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-28 14:26:46 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-28 14:26:48 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-28 14:26:48 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-28 14:26:48 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-28 14:26:48 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-28 14:26:48 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-28 14:26:48 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-28 14:26:49 | INFO | bartabst.data.data_utils | loaded 598 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-02-28 14:26:52 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-02-28 14:26:52 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-28 14:26:52 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-02-28 14:26:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 14:26:52 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-02-28 14:26:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 14:26:52 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-28 14:26:52 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-02-28 14:26:52 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-02-28 14:26:53 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-02-28 14:26:53 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-02-28 14:26:53 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-02-28 14:26:53 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-02-28 14:26:53 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-02-28 14:26:53 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-02-28 14:26:53 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-02-28 14:26:54 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-02-28 14:26:54 | INFO | fairseq.trainer | loading train data for epoch 1
no aos file, no transfer aos used
2022-02-28 14:26:55 | INFO | bartabst.data.data_utils | loaded 1,910 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-02-28 14:26:55 | INFO | fairseq.trainer | begin training epoch 1
2022-02-28 14:26:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:26:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-28 14:26:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-28 14:26:57 | INFO | train_inner | epoch 001:     22 / 31 loss=17.261, ppl=157033, wps=16452.6, ups=13.7, wpb=1166.5, bsz=63.2, num_updates=20, lr=6e-08, gnorm=22.558, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=5
2022-02-28 14:26:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:26:58 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.082 | ppl 138692 | wps 30384.7 | wpb 591.2 | bsz 29.9 | num_updates 29
2022-02-28 14:26:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 29 updates
2022-02-28 14:26:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 29 updates, score 17.082) (writing took 5.735888049995992 seconds)
2022-02-28 14:27:04 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-28 14:27:04 | INFO | train | epoch 001 | loss 17.28 | ppl 159182 | wps 4090.5 | ups 3.35 | wpb 1196.1 | bsz 61.4 | num_updates 29 | lr 8.7e-08 | gnorm 22.693 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 12
2022-02-28 14:27:04 | INFO | fairseq.trainer | begin training epoch 2
2022-02-28 14:27:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:27:05 | INFO | train_inner | epoch 002:     11 / 31 loss=17.297, ppl=160980, wps=3243.7, ups=2.55, wpb=1273, bsz=59.8, num_updates=40, lr=1.2e-07, gnorm=22.714, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=13
2022-02-28 14:27:06 | INFO | train_inner | epoch 002:     31 / 31 loss=17.243, ppl=155095, wps=16012.8, ups=13.37, wpb=1197.7, bsz=61.6, num_updates=60, lr=1.8e-07, gnorm=22.806, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=15
2022-02-28 14:27:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:27:07 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 17.036 | ppl 134352 | wps 29881.2 | wpb 591.2 | bsz 29.9 | num_updates 60 | best_loss 17.036
2022-02-28 14:27:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 60 updates
2022-02-28 14:27:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 60 updates, score 17.036) (writing took 8.329831804003334 seconds)
2022-02-28 14:27:15 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-28 14:27:15 | INFO | train | epoch 002 | loss 17.254 | ppl 156277 | wps 3402.2 | ups 2.77 | wpb 1227.6 | bsz 61.6 | num_updates 60 | lr 1.8e-07 | gnorm 22.693 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 23
2022-02-28 14:27:15 | INFO | fairseq.trainer | begin training epoch 3
2022-02-28 14:27:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:27:17 | INFO | train_inner | epoch 003:     20 / 31 loss=17.235, ppl=154224, wps=2368.1, ups=1.93, wpb=1228.4, bsz=61.9, num_updates=80, lr=2.4e-07, gnorm=22.397, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=25
2022-02-28 14:27:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:27:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.84 | ppl 117273 | wps 29515.4 | wpb 591.2 | bsz 29.9 | num_updates 91 | best_loss 16.84
2022-02-28 14:27:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 91 updates
2022-02-28 14:27:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 91 updates, score 16.84) (writing took 8.042643699009204 seconds)
2022-02-28 14:27:26 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-28 14:27:26 | INFO | train | epoch 003 | loss 17.151 | ppl 145523 | wps 3461.6 | ups 2.82 | wpb 1227.6 | bsz 61.6 | num_updates 91 | lr 2.73e-07 | gnorm 22.292 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 34
2022-02-28 14:27:26 | INFO | fairseq.trainer | begin training epoch 4
2022-02-28 14:27:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:27:27 | INFO | train_inner | epoch 004:      9 / 31 loss=16.965, ppl=127917, wps=2297.5, ups=1.96, wpb=1171.2, bsz=62.4, num_updates=100, lr=3e-07, gnorm=22.197, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=35
2022-02-28 14:27:28 | INFO | train_inner | epoch 004:     29 / 31 loss=16.896, ppl=121992, wps=20451.1, ups=15.06, wpb=1357.9, bsz=61.9, num_updates=120, lr=3.6e-07, gnorm=21.27, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=36
2022-02-28 14:27:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:27:29 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 16.574 | ppl 97573.9 | wps 30408.3 | wpb 591.2 | bsz 29.9 | num_updates 122 | best_loss 16.574
2022-02-28 14:27:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 122 updates
2022-02-28 14:27:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 122 updates, score 16.574) (writing took 5.099359711020952 seconds)
2022-02-28 14:27:34 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-28 14:27:34 | INFO | train | epoch 004 | loss 16.902 | ppl 122458 | wps 4921.4 | ups 4.01 | wpb 1227.6 | bsz 61.6 | num_updates 122 | lr 3.66e-07 | gnorm 22.149 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 42
2022-02-28 14:27:34 | INFO | fairseq.trainer | begin training epoch 5
2022-02-28 14:27:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:27:35 | INFO | train_inner | epoch 005:     18 / 31 loss=16.678, ppl=104835, wps=3581.8, ups=2.83, wpb=1263.7, bsz=62.4, num_updates=140, lr=4.2e-07, gnorm=21.583, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=44
2022-02-28 14:27:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:27:37 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 16.222 | ppl 76413 | wps 30225.9 | wpb 591.2 | bsz 29.9 | num_updates 153 | best_loss 16.222
2022-02-28 14:27:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 153 updates
2022-02-28 14:27:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 153 updates, score 16.222) (writing took 6.479010687995469 seconds)
2022-02-28 14:27:43 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-28 14:27:43 | INFO | train | epoch 005 | loss 16.62 | ppl 100702 | wps 4172.3 | ups 3.4 | wpb 1227.6 | bsz 61.6 | num_updates 153 | lr 4.59e-07 | gnorm 20.81 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 51
2022-02-28 14:27:43 | INFO | fairseq.trainer | begin training epoch 6
2022-02-28 14:27:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:27:44 | INFO | train_inner | epoch 006:      7 / 31 loss=16.465, ppl=90457.3, wps=2792.9, ups=2.39, wpb=1169.3, bsz=59.5, num_updates=160, lr=4.8e-07, gnorm=20.717, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=52
2022-02-28 14:27:45 | INFO | train_inner | epoch 006:     27 / 31 loss=16.294, ppl=80365.3, wps=16860.6, ups=14.34, wpb=1176.2, bsz=62.7, num_updates=180, lr=5.4e-07, gnorm=20.312, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=53
2022-02-28 14:27:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:27:46 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 15.826 | ppl 58097.1 | wps 29918.4 | wpb 591.2 | bsz 29.9 | num_updates 184 | best_loss 15.826
2022-02-28 14:27:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 184 updates
2022-02-28 14:27:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 184 updates, score 15.826) (writing took 6.719527410023147 seconds)
2022-02-28 14:27:53 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-28 14:27:53 | INFO | train | epoch 006 | loss 16.282 | ppl 79708.1 | wps 4003.7 | ups 3.26 | wpb 1227.6 | bsz 61.6 | num_updates 184 | lr 5.52e-07 | gnorm 20.492 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 61
2022-02-28 14:27:53 | INFO | fairseq.trainer | begin training epoch 7
2022-02-28 14:27:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:27:54 | INFO | train_inner | epoch 007:     16 / 31 loss=16.013, ppl=66132, wps=3032.1, ups=2.28, wpb=1328.3, bsz=61.6, num_updates=200, lr=6e-07, gnorm=20.262, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=62
2022-02-28 14:27:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:27:55 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 15.446 | ppl 44630.3 | wps 29654.6 | wpb 591.2 | bsz 29.9 | num_updates 215 | best_loss 15.446
2022-02-28 14:27:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 215 updates
2022-02-28 14:27:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:27:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 215 updates, score 15.446) (writing took 4.131947900983505 seconds)
2022-02-28 14:27:59 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-28 14:27:59 | INFO | train | epoch 007 | loss 15.881 | ppl 60361.4 | wps 5594.3 | ups 4.56 | wpb 1227.6 | bsz 61.6 | num_updates 215 | lr 6.45e-07 | gnorm 19.662 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 68
2022-02-28 14:27:59 | INFO | fairseq.trainer | begin training epoch 8
2022-02-28 14:27:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:28:00 | INFO | train_inner | epoch 008:      5 / 31 loss=15.712, ppl=53670.5, wps=3574.6, ups=3.3, wpb=1083.2, bsz=61.1, num_updates=220, lr=6.6e-07, gnorm=19.432, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=68
2022-02-28 14:28:01 | INFO | train_inner | epoch 008:     25 / 31 loss=15.512, ppl=46720.7, wps=19732.7, ups=14.58, wpb=1353.2, bsz=63.2, num_updates=240, lr=7.2e-07, gnorm=17.823, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=70
2022-02-28 14:28:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:28:02 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 15.107 | ppl 35285.2 | wps 28781.7 | wpb 591.2 | bsz 29.9 | num_updates 246 | best_loss 15.107
2022-02-28 14:28:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 246 updates
2022-02-28 14:28:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 246 updates, score 15.107) (writing took 6.297658297000453 seconds)
2022-02-28 14:28:08 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-28 14:28:08 | INFO | train | epoch 008 | loss 15.492 | ppl 46079.8 | wps 4211.2 | ups 3.43 | wpb 1227.6 | bsz 61.6 | num_updates 246 | lr 7.38e-07 | gnorm 18.928 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 77
2022-02-28 14:28:09 | INFO | fairseq.trainer | begin training epoch 9
2022-02-28 14:28:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:28:10 | INFO | train_inner | epoch 009:     14 / 31 loss=15.178, ppl=37070.1, wps=2937.5, ups=2.42, wpb=1215.7, bsz=59.8, num_updates=260, lr=7.8e-07, gnorm=19.228, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=78
2022-02-28 14:28:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:28:11 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 14.595 | ppl 24745.8 | wps 29596.8 | wpb 591.2 | bsz 29.9 | num_updates 277 | best_loss 14.595
2022-02-28 14:28:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 277 updates
2022-02-28 14:28:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 277 updates, score 14.595) (writing took 4.12660860500182 seconds)
2022-02-28 14:28:15 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-28 14:28:15 | INFO | train | epoch 009 | loss 15.053 | ppl 33996.4 | wps 5609.5 | ups 4.57 | wpb 1227.6 | bsz 61.6 | num_updates 277 | lr 8.31e-07 | gnorm 18.443 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 84
2022-02-28 14:28:15 | INFO | fairseq.trainer | begin training epoch 10
2022-02-28 14:28:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:28:16 | INFO | train_inner | epoch 010:      3 / 31 loss=14.951, ppl=31684.6, wps=3626.2, ups=3.32, wpb=1093.2, bsz=61.6, num_updates=280, lr=8.4e-07, gnorm=18.78, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=84
2022-02-28 14:28:17 | INFO | train_inner | epoch 010:     23 / 31 loss=14.623, ppl=25229, wps=19315.7, ups=14.37, wpb=1344.5, bsz=61.9, num_updates=300, lr=9e-07, gnorm=16.877, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=85
2022-02-28 14:28:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:28:18 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 14.152 | ppl 18203.1 | wps 29847 | wpb 591.2 | bsz 29.9 | num_updates 308 | best_loss 14.152
2022-02-28 14:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 308 updates
2022-02-28 14:28:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 308 updates, score 14.152) (writing took 4.946904103009729 seconds)
2022-02-28 14:28:23 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-28 14:28:23 | INFO | train | epoch 010 | loss 14.604 | ppl 24907.8 | wps 4973.9 | ups 4.05 | wpb 1227.6 | bsz 61.6 | num_updates 308 | lr 9.24e-07 | gnorm 17.192 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 91
2022-02-28 14:28:23 | INFO | fairseq.trainer | begin training epoch 11
2022-02-28 14:28:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:28:24 | INFO | train_inner | epoch 011:     12 / 31 loss=14.411, ppl=21787.5, wps=3524.2, ups=2.87, wpb=1227, bsz=62.4, num_updates=320, lr=9.6e-07, gnorm=17.009, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=92
2022-02-28 14:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:28:26 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 13.763 | ppl 13904.8 | wps 29429.8 | wpb 591.2 | bsz 29.9 | num_updates 339 | best_loss 13.763
2022-02-28 14:28:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 339 updates
2022-02-28 14:28:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 339 updates, score 13.763) (writing took 6.592899462993955 seconds)
2022-02-28 14:28:32 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-28 14:28:32 | INFO | train | epoch 011 | loss 14.193 | ppl 18733.8 | wps 4040.1 | ups 3.29 | wpb 1227.6 | bsz 61.6 | num_updates 339 | lr 1.017e-06 | gnorm 16.433 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 101
2022-02-28 14:28:32 | INFO | fairseq.trainer | begin training epoch 12
2022-02-28 14:28:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:28:33 | INFO | train_inner | epoch 012:      1 / 31 loss=14.06, ppl=17081.4, wps=2748.5, ups=2.33, wpb=1179.8, bsz=60.3, num_updates=340, lr=1.02e-06, gnorm=16.284, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=101
2022-02-28 14:28:34 | INFO | train_inner | epoch 012:     21 / 31 loss=13.867, ppl=14941.1, wps=17456.2, ups=14.4, wpb=1212.4, bsz=64, num_updates=360, lr=1.08e-06, gnorm=15.486, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=102
2022-02-28 14:28:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:28:35 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 13.315 | ppl 10190.5 | wps 29257.8 | wpb 591.2 | bsz 29.9 | num_updates 370 | best_loss 13.315
2022-02-28 14:28:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 370 updates
2022-02-28 14:28:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 370 updates, score 13.315) (writing took 4.347113724012161 seconds)
2022-02-28 14:28:39 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-28 14:28:39 | INFO | train | epoch 012 | loss 13.806 | ppl 14323.6 | wps 5365.3 | ups 4.37 | wpb 1227.6 | bsz 61.6 | num_updates 370 | lr 1.11e-06 | gnorm 15.629 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 108
2022-02-28 14:28:39 | INFO | fairseq.trainer | begin training epoch 13
2022-02-28 14:28:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:28:40 | INFO | train_inner | epoch 013:     10 / 31 loss=13.576, ppl=12213.5, wps=4057.7, ups=3.16, wpb=1285.8, bsz=58.2, num_updates=380, lr=1.14e-06, gnorm=15.464, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=109
2022-02-28 14:28:42 | INFO | train_inner | epoch 013:     30 / 31 loss=13.283, ppl=9965.69, wps=16949.1, ups=13.94, wpb=1216.2, bsz=64, num_updates=400, lr=1.2e-06, gnorm=14.561, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=110
2022-02-28 14:28:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:28:42 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 13.028 | ppl 8350.14 | wps 29193.4 | wpb 591.2 | bsz 29.9 | num_updates 401 | best_loss 13.028
2022-02-28 14:28:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 401 updates
2022-02-28 14:28:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 401 updates, score 13.028) (writing took 4.500605535984505 seconds)
2022-02-28 14:28:47 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-28 14:28:47 | INFO | train | epoch 013 | loss 13.352 | ppl 10453.5 | wps 5239.1 | ups 4.27 | wpb 1227.6 | bsz 61.6 | num_updates 401 | lr 1.203e-06 | gnorm 14.787 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 115
2022-02-28 14:28:47 | INFO | fairseq.trainer | begin training epoch 14
2022-02-28 14:28:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:28:48 | INFO | train_inner | epoch 014:     19 / 31 loss=13.1, ppl=8777.89, wps=3603, ups=3.08, wpb=1171.7, bsz=60.3, num_updates=420, lr=1.26e-06, gnorm=14.867, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=116
2022-02-28 14:28:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:28:50 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 12.734 | ppl 6814.87 | wps 30529.3 | wpb 591.2 | bsz 29.9 | num_updates 432 | best_loss 12.734
2022-02-28 14:28:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 432 updates
2022-02-28 14:28:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:28:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 432 updates, score 12.734) (writing took 6.669590267003514 seconds)
2022-02-28 14:28:56 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-28 14:28:56 | INFO | train | epoch 014 | loss 13.043 | ppl 8438.92 | wps 3974.8 | ups 3.24 | wpb 1227.6 | bsz 61.6 | num_updates 432 | lr 1.296e-06 | gnorm 14.158 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 125
2022-02-28 14:28:56 | INFO | fairseq.trainer | begin training epoch 15
2022-02-28 14:28:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:28:57 | INFO | train_inner | epoch 015:      8 / 31 loss=12.911, ppl=7703.27, wps=2995.2, ups=2.28, wpb=1314.1, bsz=62.4, num_updates=440, lr=1.32e-06, gnorm=12.729, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=125
2022-02-28 14:28:58 | INFO | train_inner | epoch 015:     28 / 31 loss=12.772, ppl=6995.16, wps=17266.3, ups=14.38, wpb=1200.9, bsz=61.9, num_updates=460, lr=1.38e-06, gnorm=12.986, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=127
2022-02-28 14:28:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:28:59 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 12.53 | ppl 5913.88 | wps 30439.7 | wpb 591.2 | bsz 29.9 | num_updates 463 | best_loss 12.53
2022-02-28 14:28:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 463 updates
2022-02-28 14:28:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 463 updates, score 12.53) (writing took 4.410416936996626 seconds)
2022-02-28 14:29:03 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-28 14:29:03 | INFO | train | epoch 015 | loss 12.788 | ppl 7072.32 | wps 5293.7 | ups 4.31 | wpb 1227.6 | bsz 61.6 | num_updates 463 | lr 1.389e-06 | gnorm 13.055 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 132
2022-02-28 14:29:04 | INFO | fairseq.trainer | begin training epoch 16
2022-02-28 14:29:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:29:05 | INFO | train_inner | epoch 016:     17 / 31 loss=12.597, ppl=6195.47, wps=3921, ups=3.14, wpb=1248.2, bsz=61.6, num_updates=480, lr=1.44e-06, gnorm=12.83, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=133
2022-02-28 14:29:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:29:06 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 12.27 | ppl 4937.49 | wps 30968.8 | wpb 591.2 | bsz 29.9 | num_updates 494 | best_loss 12.27
2022-02-28 14:29:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 494 updates
2022-02-28 14:29:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 494 updates, score 12.27) (writing took 4.114103644009447 seconds)
2022-02-28 14:29:10 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-28 14:29:10 | INFO | train | epoch 016 | loss 12.514 | ppl 5849.14 | wps 5609.5 | ups 4.57 | wpb 1227.6 | bsz 61.6 | num_updates 494 | lr 1.482e-06 | gnorm 12.435 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 139
2022-02-28 14:29:10 | INFO | fairseq.trainer | begin training epoch 17
2022-02-28 14:29:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:29:11 | INFO | train_inner | epoch 017:      6 / 31 loss=12.353, ppl=5232.88, wps=3807.2, ups=3.23, wpb=1179.1, bsz=59.8, num_updates=500, lr=1.5e-06, gnorm=12.781, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=139
2022-02-28 14:29:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:29:11 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.243 | ppl 4848.23 | wps 28566.9 | wpb 591.2 | bsz 29.9 | num_updates 500 | best_loss 12.243
2022-02-28 14:29:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 500 updates
2022-02-28 14:29:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-28 14:29:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-28 14:29:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt (epoch 17 @ 500 updates, score 12.243) (writing took 8.169878948974656 seconds)
2022-02-28 14:29:21 | INFO | train_inner | epoch 017:     26 / 31 loss=12.36, ppl=5257.1, wps=2589.4, ups=1.96, wpb=1319, bsz=63.2, num_updates=520, lr=1.56e-06, gnorm=11.088, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=149
2022-02-28 14:29:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:29:22 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.097 | ppl 4379.7 | wps 29595.3 | wpb 591.2 | bsz 29.9 | num_updates 525 | best_loss 12.097
2022-02-28 14:29:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 525 updates
2022-02-28 14:29:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 525 updates, score 12.097) (writing took 8.378683022019686 seconds)
2022-02-28 14:29:30 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-28 14:29:30 | INFO | train | epoch 017 | loss 12.29 | ppl 5006.82 | wps 1905.3 | ups 1.55 | wpb 1227.6 | bsz 61.6 | num_updates 525 | lr 1.575e-06 | gnorm 11.685 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 159
2022-02-28 14:29:30 | INFO | fairseq.trainer | begin training epoch 18
2022-02-28 14:29:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:29:32 | INFO | train_inner | epoch 018:     15 / 31 loss=12.114, ppl=4434, wps=2492.5, ups=1.92, wpb=1297.2, bsz=61.1, num_updates=540, lr=1.62e-06, gnorm=11.19, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=160
2022-02-28 14:29:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:29:33 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 11.931 | ppl 3903.7 | wps 29272.3 | wpb 591.2 | bsz 29.9 | num_updates 556 | best_loss 11.931
2022-02-28 14:29:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 556 updates
2022-02-28 14:29:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 556 updates, score 11.931) (writing took 6.019992924993858 seconds)
2022-02-28 14:29:39 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-28 14:29:39 | INFO | train | epoch 018 | loss 12.088 | ppl 4352.75 | wps 4376.7 | ups 3.57 | wpb 1227.6 | bsz 61.6 | num_updates 556 | lr 1.668e-06 | gnorm 11.016 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 167
2022-02-28 14:29:39 | INFO | fairseq.trainer | begin training epoch 19
2022-02-28 14:29:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:29:39 | INFO | train_inner | epoch 019:      4 / 31 loss=12.039, ppl=4208.22, wps=2580.3, ups=2.52, wpb=1023.3, bsz=61.6, num_updates=560, lr=1.68e-06, gnorm=11.267, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=168
2022-02-28 14:29:41 | INFO | train_inner | epoch 019:     24 / 31 loss=11.973, ppl=4020.71, wps=18952, ups=14.59, wpb=1298.8, bsz=63.2, num_updates=580, lr=1.74e-06, gnorm=10.284, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=169
2022-02-28 14:29:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:29:42 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.667 | ppl 3250.94 | wps 29257 | wpb 591.2 | bsz 29.9 | num_updates 587 | best_loss 11.667
2022-02-28 14:29:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 587 updates
2022-02-28 14:29:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 587 updates, score 11.667) (writing took 5.876293991022976 seconds)
2022-02-28 14:29:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-28 14:29:48 | INFO | train | epoch 019 | loss 11.956 | ppl 3971.72 | wps 4428.6 | ups 3.61 | wpb 1227.6 | bsz 61.6 | num_updates 587 | lr 1.761e-06 | gnorm 10.518 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 176
2022-02-28 14:29:48 | INFO | fairseq.trainer | begin training epoch 20
2022-02-28 14:29:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:29:49 | INFO | train_inner | epoch 020:     13 / 31 loss=11.883, ppl=3776.99, wps=3135.1, ups=2.54, wpb=1235, bsz=61.1, num_updates=600, lr=1.8e-06, gnorm=10.499, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=177
2022-02-28 14:29:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:29:50 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.577 | ppl 3055.42 | wps 29805 | wpb 591.2 | bsz 29.9 | num_updates 618 | best_loss 11.577
2022-02-28 14:29:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 618 updates
2022-02-28 14:29:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:29:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 618 updates, score 11.577) (writing took 6.8391732439922635 seconds)
2022-02-28 14:29:58 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-28 14:29:58 | INFO | train | epoch 020 | loss 11.786 | ppl 3531.31 | wps 3902 | ups 3.18 | wpb 1227.6 | bsz 61.6 | num_updates 618 | lr 1.854e-06 | gnorm 10.805 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 186
2022-02-28 14:29:58 | INFO | fairseq.trainer | begin training epoch 21
2022-02-28 14:29:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:29:58 | INFO | train_inner | epoch 021:      2 / 31 loss=11.675, ppl=3269.36, wps=2568.3, ups=2.17, wpb=1183, bsz=60.3, num_updates=620, lr=1.86e-06, gnorm=11.051, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=186
2022-02-28 14:30:00 | INFO | train_inner | epoch 021:     22 / 31 loss=11.668, ppl=3252.91, wps=15629.2, ups=12.74, wpb=1226.5, bsz=62.7, num_updates=640, lr=1.92e-06, gnorm=9.578, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=188
2022-02-28 14:30:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:30:01 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.491 | ppl 2878.73 | wps 27787.4 | wpb 591.2 | bsz 29.9 | num_updates 649 | best_loss 11.491
2022-02-28 14:30:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 649 updates
2022-02-28 14:30:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 649 updates, score 11.491) (writing took 5.496201946021756 seconds)
2022-02-28 14:30:06 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-28 14:30:06 | INFO | train | epoch 021 | loss 11.62 | ppl 3147.81 | wps 4352.9 | ups 3.55 | wpb 1227.6 | bsz 61.6 | num_updates 649 | lr 1.947e-06 | gnorm 9.714 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 195
2022-02-28 14:30:06 | INFO | fairseq.trainer | begin training epoch 22
2022-02-28 14:30:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:30:07 | INFO | train_inner | epoch 022:     11 / 31 loss=11.515, ppl=2926.27, wps=3120.8, ups=2.6, wpb=1198, bsz=61.6, num_updates=660, lr=1.98e-06, gnorm=9.964, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=195
2022-02-28 14:30:09 | INFO | train_inner | epoch 022:     31 / 31 loss=11.516, ppl=2928.37, wps=17628.1, ups=14.03, wpb=1256.2, bsz=60.3, num_updates=680, lr=2.04e-06, gnorm=9.207, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=197
2022-02-28 14:30:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:30:09 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.315 | ppl 2547.32 | wps 27863.7 | wpb 591.2 | bsz 29.9 | num_updates 680 | best_loss 11.315
2022-02-28 14:30:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 680 updates
2022-02-28 14:30:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 680 updates, score 11.315) (writing took 5.707496069982881 seconds)
2022-02-28 14:30:15 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-28 14:30:15 | INFO | train | epoch 022 | loss 11.506 | ppl 2907.57 | wps 4477.4 | ups 3.65 | wpb 1227.6 | bsz 61.6 | num_updates 680 | lr 2.04e-06 | gnorm 9.477 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 203
2022-02-28 14:30:15 | INFO | fairseq.trainer | begin training epoch 23
2022-02-28 14:30:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:30:16 | INFO | train_inner | epoch 023:     20 / 31 loss=11.318, ppl=2552.24, wps=3234.6, ups=2.58, wpb=1255.1, bsz=61.9, num_updates=700, lr=2.1e-06, gnorm=9.135, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=205
2022-02-28 14:30:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:30:18 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.211 | ppl 2370.55 | wps 29408.7 | wpb 591.2 | bsz 29.9 | num_updates 711 | best_loss 11.211
2022-02-28 14:30:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 711 updates
2022-02-28 14:30:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 711 updates, score 11.211) (writing took 4.874353466002503 seconds)
2022-02-28 14:30:23 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-28 14:30:23 | INFO | train | epoch 023 | loss 11.333 | ppl 2579.42 | wps 4908.2 | ups 4 | wpb 1227.6 | bsz 61.6 | num_updates 711 | lr 2.133e-06 | gnorm 9.256 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 211
2022-02-28 14:30:23 | INFO | fairseq.trainer | begin training epoch 24
2022-02-28 14:30:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:30:23 | INFO | train_inner | epoch 024:      9 / 31 loss=11.375, ppl=2655.87, wps=3601.3, ups=2.88, wpb=1250.5, bsz=62.4, num_updates=720, lr=2.16e-06, gnorm=9.19, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=212
2022-02-28 14:30:25 | INFO | train_inner | epoch 024:     29 / 31 loss=11.302, ppl=2524.64, wps=18683.9, ups=15.22, wpb=1227.5, bsz=61.9, num_updates=740, lr=2.22e-06, gnorm=8.893, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=213
2022-02-28 14:30:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:30:25 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.121 | ppl 2227.6 | wps 29372 | wpb 591.2 | bsz 29.9 | num_updates 742 | best_loss 11.121
2022-02-28 14:30:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 742 updates
2022-02-28 14:30:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 742 updates, score 11.121) (writing took 5.0395209940033965 seconds)
2022-02-28 14:30:30 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-28 14:30:30 | INFO | train | epoch 024 | loss 11.305 | ppl 2529.51 | wps 4930.4 | ups 4.02 | wpb 1227.6 | bsz 61.6 | num_updates 742 | lr 2.226e-06 | gnorm 9.002 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 219
2022-02-28 14:30:30 | INFO | fairseq.trainer | begin training epoch 25
2022-02-28 14:30:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:30:32 | INFO | train_inner | epoch 025:     18 / 31 loss=11.144, ppl=2262.33, wps=3510.9, ups=2.85, wpb=1231.2, bsz=61.6, num_updates=760, lr=2.28e-06, gnorm=9.008, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=220
2022-02-28 14:30:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:30:33 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 10.98 | ppl 2019.92 | wps 29892.7 | wpb 591.2 | bsz 29.9 | num_updates 773 | best_loss 10.98
2022-02-28 14:30:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 773 updates
2022-02-28 14:30:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:36 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 773 updates, score 10.98) (writing took 4.2497934739803895 seconds)
2022-02-28 14:30:37 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-28 14:30:37 | INFO | train | epoch 025 | loss 11.183 | ppl 2324.43 | wps 5373.7 | ups 4.38 | wpb 1227.6 | bsz 61.6 | num_updates 773 | lr 2.319e-06 | gnorm 8.717 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 226
2022-02-28 14:30:37 | INFO | fairseq.trainer | begin training epoch 26
2022-02-28 14:30:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:30:38 | INFO | train_inner | epoch 026:      7 / 31 loss=11.177, ppl=2314.8, wps=3624.7, ups=3.16, wpb=1146.8, bsz=59.8, num_updates=780, lr=2.34e-06, gnorm=8.997, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=226
2022-02-28 14:30:39 | INFO | train_inner | epoch 026:     27 / 31 loss=11.015, ppl=2069.17, wps=19215.5, ups=14.58, wpb=1317.7, bsz=63.2, num_updates=800, lr=2.4e-06, gnorm=9.061, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=228
2022-02-28 14:30:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:30:40 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 10.978 | ppl 2016.61 | wps 29927.3 | wpb 591.2 | bsz 29.9 | num_updates 804 | best_loss 10.978
2022-02-28 14:30:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 804 updates
2022-02-28 14:30:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 804 updates, score 10.978) (writing took 4.260484899976291 seconds)
2022-02-28 14:30:44 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-28 14:30:44 | INFO | train | epoch 026 | loss 11.079 | ppl 2164 | wps 5432.4 | ups 4.43 | wpb 1227.6 | bsz 61.6 | num_updates 804 | lr 2.412e-06 | gnorm 9.214 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 233
2022-02-28 14:30:44 | INFO | fairseq.trainer | begin training epoch 27
2022-02-28 14:30:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:30:46 | INFO | train_inner | epoch 027:     16 / 31 loss=10.999, ppl=2046.35, wps=3885.3, ups=3.18, wpb=1221, bsz=61.1, num_updates=820, lr=2.46e-06, gnorm=8.586, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=234
2022-02-28 14:30:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:30:47 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.874 | ppl 1876.53 | wps 29700.3 | wpb 591.2 | bsz 29.9 | num_updates 835 | best_loss 10.874
2022-02-28 14:30:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 835 updates
2022-02-28 14:30:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 835 updates, score 10.874) (writing took 5.4051342400489375 seconds)
2022-02-28 14:30:53 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-28 14:30:53 | INFO | train | epoch 027 | loss 10.954 | ppl 1983.14 | wps 4612.9 | ups 3.76 | wpb 1227.6 | bsz 61.6 | num_updates 835 | lr 2.505e-06 | gnorm 8.649 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 241
2022-02-28 14:30:53 | INFO | fairseq.trainer | begin training epoch 28
2022-02-28 14:30:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:30:53 | INFO | train_inner | epoch 028:      5 / 31 loss=10.978, ppl=2016.59, wps=3015.1, ups=2.69, wpb=1122.6, bsz=60.8, num_updates=840, lr=2.52e-06, gnorm=8.78, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=241
2022-02-28 14:30:54 | INFO | train_inner | epoch 028:     25 / 31 loss=10.778, ppl=1756.11, wps=18533.4, ups=15.26, wpb=1214.5, bsz=62.7, num_updates=860, lr=2.58e-06, gnorm=11.631, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=243
2022-02-28 14:30:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:30:55 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.721 | ppl 1687.39 | wps 28949.2 | wpb 591.2 | bsz 29.9 | num_updates 866 | best_loss 10.721
2022-02-28 14:30:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 866 updates
2022-02-28 14:30:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:30:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 866 updates, score 10.721) (writing took 4.328979037003592 seconds)
2022-02-28 14:31:00 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-28 14:31:00 | INFO | train | epoch 028 | loss 10.846 | ppl 1840.56 | wps 5457.7 | ups 4.45 | wpb 1227.6 | bsz 61.6 | num_updates 866 | lr 2.598e-06 | gnorm 10.462 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 248
2022-02-28 14:31:00 | INFO | fairseq.trainer | begin training epoch 29
2022-02-28 14:31:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:31:01 | INFO | train_inner | epoch 029:     14 / 31 loss=10.889, ppl=1896.35, wps=3885.7, ups=3.16, wpb=1228.5, bsz=60.3, num_updates=880, lr=2.64e-06, gnorm=8.278, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=249
2022-02-28 14:31:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:31:02 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.6 | ppl 1552.33 | wps 30098.1 | wpb 591.2 | bsz 29.9 | num_updates 897 | best_loss 10.6
2022-02-28 14:31:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 897 updates
2022-02-28 14:31:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 897 updates, score 10.6) (writing took 5.624009524995927 seconds)
2022-02-28 14:31:08 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-28 14:31:08 | INFO | train | epoch 029 | loss 10.796 | ppl 1778.06 | wps 4562.5 | ups 3.72 | wpb 1227.6 | bsz 61.6 | num_updates 897 | lr 2.691e-06 | gnorm 8.196 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 256
2022-02-28 14:31:08 | INFO | fairseq.trainer | begin training epoch 30
2022-02-28 14:31:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:31:08 | INFO | train_inner | epoch 030:      3 / 31 loss=10.733, ppl=1701.58, wps=3455.8, ups=2.65, wpb=1303.6, bsz=62.4, num_updates=900, lr=2.7e-06, gnorm=8.111, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=257
2022-02-28 14:31:10 | INFO | train_inner | epoch 030:     23 / 31 loss=10.673, ppl=1633.05, wps=17119.9, ups=14.63, wpb=1170.5, bsz=61.9, num_updates=920, lr=2.76e-06, gnorm=8.065, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=258
2022-02-28 14:31:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:31:11 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.617 | ppl 1570.59 | wps 29327.6 | wpb 591.2 | bsz 29.9 | num_updates 928 | best_loss 10.6
2022-02-28 14:31:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 928 updates
2022-02-28 14:31:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:31:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:31:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 30 @ 928 updates, score 10.617) (writing took 2.820068724977318 seconds)
2022-02-28 14:31:14 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-28 14:31:14 | INFO | train | epoch 030 | loss 10.665 | ppl 1623.98 | wps 6843.6 | ups 5.57 | wpb 1227.6 | bsz 61.6 | num_updates 928 | lr 2.784e-06 | gnorm 8.006 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 262
2022-02-28 14:31:14 | INFO | fairseq.trainer | begin training epoch 31
2022-02-28 14:31:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:31:15 | INFO | train_inner | epoch 031:     12 / 31 loss=10.626, ppl=1579.99, wps=5294.8, ups=4.16, wpb=1273.7, bsz=62.4, num_updates=940, lr=2.82e-06, gnorm=8.089, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=263
2022-02-28 14:31:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:31:16 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.508 | ppl 1456.16 | wps 31946.7 | wpb 591.2 | bsz 29.9 | num_updates 959 | best_loss 10.508
2022-02-28 14:31:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 959 updates
2022-02-28 14:31:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 959 updates, score 10.508) (writing took 4.248117938986979 seconds)
2022-02-28 14:31:21 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-28 14:31:21 | INFO | train | epoch 031 | loss 10.607 | ppl 1559.62 | wps 5483.1 | ups 4.47 | wpb 1227.6 | bsz 61.6 | num_updates 959 | lr 2.877e-06 | gnorm 8.367 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 269
2022-02-28 14:31:21 | INFO | fairseq.trainer | begin training epoch 32
2022-02-28 14:31:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:31:21 | INFO | train_inner | epoch 032:      1 / 31 loss=10.605, ppl=1557.75, wps=3954.6, ups=3.23, wpb=1222.8, bsz=60.3, num_updates=960, lr=2.88e-06, gnorm=8.371, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=269
2022-02-28 14:31:22 | INFO | train_inner | epoch 032:     21 / 31 loss=10.563, ppl=1513.25, wps=17836.9, ups=14.46, wpb=1234, bsz=63.2, num_updates=980, lr=2.94e-06, gnorm=7.924, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=270
2022-02-28 14:31:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:31:23 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 10.425 | ppl 1374.72 | wps 28066 | wpb 591.2 | bsz 29.9 | num_updates 990 | best_loss 10.425
2022-02-28 14:31:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 990 updates
2022-02-28 14:31:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 32 @ 990 updates, score 10.425) (writing took 4.781957378028892 seconds)
2022-02-28 14:31:28 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-28 14:31:28 | INFO | train | epoch 032 | loss 10.543 | ppl 1491.95 | wps 5068.2 | ups 4.13 | wpb 1227.6 | bsz 61.6 | num_updates 990 | lr 2.97e-06 | gnorm 8.039 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 276
2022-02-28 14:31:28 | INFO | fairseq.trainer | begin training epoch 33
2022-02-28 14:31:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:31:29 | INFO | train_inner | epoch 033:     10 / 31 loss=10.597, ppl=1548.47, wps=3720.5, ups=2.95, wpb=1262.8, bsz=59, num_updates=1000, lr=3e-06, gnorm=7.949, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=277
2022-02-28 14:31:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:31:29 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.427 | ppl 1376.52 | wps 31855 | wpb 591.2 | bsz 29.9 | num_updates 1000 | best_loss 10.425
2022-02-28 14:31:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1000 updates
2022-02-28 14:31:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-28 14:31:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-28 14:31:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt (epoch 33 @ 1000 updates, score 10.427) (writing took 4.659460992959794 seconds)
2022-02-28 14:31:35 | INFO | train_inner | epoch 033:     30 / 31 loss=10.356, ppl=1310.88, wps=3652.8, ups=3.05, wpb=1198.3, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=7.936, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=284
2022-02-28 14:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:31:36 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.416 | ppl 1366.34 | wps 28998.4 | wpb 591.2 | bsz 29.9 | num_updates 1021 | best_loss 10.416
2022-02-28 14:31:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1021 updates
2022-02-28 14:31:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1021 updates, score 10.416) (writing took 4.186580020003021 seconds)
2022-02-28 14:31:40 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-28 14:31:40 | INFO | train | epoch 033 | loss 10.466 | ppl 1414.34 | wps 3135.6 | ups 2.55 | wpb 1227.6 | bsz 61.6 | num_updates 1021 | lr 3.063e-06 | gnorm 7.876 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 288
2022-02-28 14:31:40 | INFO | fairseq.trainer | begin training epoch 34
2022-02-28 14:31:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:31:42 | INFO | train_inner | epoch 034:     19 / 31 loss=10.38, ppl=1332.23, wps=3943.2, ups=3.25, wpb=1211.8, bsz=62.4, num_updates=1040, lr=3.12e-06, gnorm=8.217, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=290
2022-02-28 14:31:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:31:43 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 10.349 | ppl 1304.05 | wps 25933.4 | wpb 591.2 | bsz 29.9 | num_updates 1052 | best_loss 10.349
2022-02-28 14:31:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1052 updates
2022-02-28 14:31:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1052 updates, score 10.349) (writing took 4.447771538980305 seconds)
2022-02-28 14:31:47 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-28 14:31:47 | INFO | train | epoch 034 | loss 10.409 | ppl 1359.59 | wps 5298 | ups 4.32 | wpb 1227.6 | bsz 61.6 | num_updates 1052 | lr 3.156e-06 | gnorm 8.079 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 296
2022-02-28 14:31:47 | INFO | fairseq.trainer | begin training epoch 35
2022-02-28 14:31:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:31:48 | INFO | train_inner | epoch 035:      8 / 31 loss=10.458, ppl=1406.63, wps=3803.6, ups=3.1, wpb=1227.2, bsz=60.3, num_updates=1060, lr=3.18e-06, gnorm=7.77, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=296
2022-02-28 14:31:49 | INFO | train_inner | epoch 035:     28 / 31 loss=10.291, ppl=1253.19, wps=18275.4, ups=14.1, wpb=1296, bsz=61.9, num_updates=1080, lr=3.24e-06, gnorm=7.919, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=298
2022-02-28 14:31:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:31:50 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 10.248 | ppl 1215.66 | wps 26725.7 | wpb 591.2 | bsz 29.9 | num_updates 1083 | best_loss 10.248
2022-02-28 14:31:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1083 updates
2022-02-28 14:31:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:31:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1083 updates, score 10.248) (writing took 5.007351749984082 seconds)
2022-02-28 14:31:55 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-28 14:31:55 | INFO | train | epoch 035 | loss 10.333 | ppl 1289.69 | wps 4852.7 | ups 3.95 | wpb 1227.6 | bsz 61.6 | num_updates 1083 | lr 3.249e-06 | gnorm 7.948 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 303
2022-02-28 14:31:55 | INFO | fairseq.trainer | begin training epoch 36
2022-02-28 14:31:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:31:57 | INFO | train_inner | epoch 036:     17 / 31 loss=10.317, ppl=1275.73, wps=3594.7, ups=2.84, wpb=1265.6, bsz=61.1, num_updates=1100, lr=3.3e-06, gnorm=8.525, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=305
2022-02-28 14:31:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:31:58 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 10.199 | ppl 1175.8 | wps 30465.1 | wpb 591.2 | bsz 29.9 | num_updates 1114 | best_loss 10.199
2022-02-28 14:31:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1114 updates
2022-02-28 14:31:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1114 updates, score 10.199) (writing took 4.338420928979758 seconds)
2022-02-28 14:32:02 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-28 14:32:02 | INFO | train | epoch 036 | loss 10.221 | ppl 1193.34 | wps 5318.6 | ups 4.33 | wpb 1227.6 | bsz 61.6 | num_updates 1114 | lr 3.342e-06 | gnorm 8.359 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 311
2022-02-28 14:32:02 | INFO | fairseq.trainer | begin training epoch 37
2022-02-28 14:32:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:32:03 | INFO | train_inner | epoch 037:      6 / 31 loss=10.086, ppl=1087.27, wps=3543.5, ups=3.13, wpb=1132.5, bsz=61.6, num_updates=1120, lr=3.36e-06, gnorm=7.919, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=311
2022-02-28 14:32:04 | INFO | train_inner | epoch 037:     26 / 31 loss=10.238, ppl=1207.72, wps=17599, ups=14.59, wpb=1205.9, bsz=61.9, num_updates=1140, lr=3.42e-06, gnorm=7.775, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=313
2022-02-28 14:32:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:32:05 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 10.103 | ppl 1099.68 | wps 28581.3 | wpb 591.2 | bsz 29.9 | num_updates 1145 | best_loss 10.103
2022-02-28 14:32:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1145 updates
2022-02-28 14:32:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1145 updates, score 10.103) (writing took 5.7743877960019745 seconds)
2022-02-28 14:32:11 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-28 14:32:11 | INFO | train | epoch 037 | loss 10.184 | ppl 1163.64 | wps 4476.2 | ups 3.65 | wpb 1227.6 | bsz 61.6 | num_updates 1145 | lr 3.435e-06 | gnorm 7.632 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 319
2022-02-28 14:32:11 | INFO | fairseq.trainer | begin training epoch 38
2022-02-28 14:32:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:32:12 | INFO | train_inner | epoch 038:     15 / 31 loss=10.071, ppl=1075.35, wps=3024.5, ups=2.56, wpb=1181, bsz=62.4, num_updates=1160, lr=3.48e-06, gnorm=7.805, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=320
2022-02-28 14:32:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:32:14 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 10.1 | ppl 1097.62 | wps 28789.7 | wpb 591.2 | bsz 29.9 | num_updates 1176 | best_loss 10.1
2022-02-28 14:32:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1176 updates
2022-02-28 14:32:14 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 38 @ 1176 updates, score 10.1) (writing took 4.956172301957849 seconds)
2022-02-28 14:32:19 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-28 14:32:19 | INFO | train | epoch 038 | loss 10.099 | ppl 1096.53 | wps 4864.6 | ups 3.96 | wpb 1227.6 | bsz 61.6 | num_updates 1176 | lr 3.528e-06 | gnorm 7.724 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 327
2022-02-28 14:32:19 | INFO | fairseq.trainer | begin training epoch 39
2022-02-28 14:32:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:32:19 | INFO | train_inner | epoch 039:      4 / 31 loss=10.081, ppl=1082.8, wps=3666.8, ups=2.84, wpb=1293.3, bsz=60.3, num_updates=1180, lr=3.54e-06, gnorm=7.524, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=327
2022-02-28 14:32:21 | INFO | train_inner | epoch 039:     24 / 31 loss=10.006, ppl=1028.31, wps=17911, ups=14.02, wpb=1277.4, bsz=62.7, num_updates=1200, lr=3.6e-06, gnorm=7.836, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=329
2022-02-28 14:32:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:32:22 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 10.017 | ppl 1035.81 | wps 30392.1 | wpb 591.2 | bsz 29.9 | num_updates 1207 | best_loss 10.017
2022-02-28 14:32:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1207 updates
2022-02-28 14:32:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1207 updates, score 10.017) (writing took 4.7135982980253175 seconds)
2022-02-28 14:32:26 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-28 14:32:26 | INFO | train | epoch 039 | loss 9.992 | ppl 1018.11 | wps 5013.8 | ups 4.08 | wpb 1227.6 | bsz 61.6 | num_updates 1207 | lr 3.621e-06 | gnorm 7.737 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 335
2022-02-28 14:32:26 | INFO | fairseq.trainer | begin training epoch 40
2022-02-28 14:32:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:32:27 | INFO | train_inner | epoch 040:     13 / 31 loss=9.872, ppl=937.01, wps=3161.6, ups=2.91, wpb=1086.4, bsz=60.8, num_updates=1220, lr=3.66e-06, gnorm=7.606, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=336
2022-02-28 14:32:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:32:29 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.916 | ppl 966.32 | wps 25723.6 | wpb 591.2 | bsz 29.9 | num_updates 1238 | best_loss 9.916
2022-02-28 14:32:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1238 updates
2022-02-28 14:32:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1238 updates, score 9.916) (writing took 4.693732679996174 seconds)
2022-02-28 14:32:34 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-28 14:32:34 | INFO | train | epoch 040 | loss 9.911 | ppl 962.9 | wps 5072.6 | ups 4.13 | wpb 1227.6 | bsz 61.6 | num_updates 1238 | lr 3.714e-06 | gnorm 7.75 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 342
2022-02-28 14:32:34 | INFO | fairseq.trainer | begin training epoch 41
2022-02-28 14:32:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:32:34 | INFO | train_inner | epoch 041:      2 / 31 loss=10.008, ppl=1029.96, wps=3825.5, ups=2.97, wpb=1288, bsz=61.1, num_updates=1240, lr=3.72e-06, gnorm=8.037, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=342
2022-02-28 14:32:36 | INFO | train_inner | epoch 041:     22 / 31 loss=9.882, ppl=943.85, wps=16963, ups=12.74, wpb=1331.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=7.604, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=344
2022-02-28 14:32:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:32:37 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.862 | ppl 930.3 | wps 27414.2 | wpb 591.2 | bsz 29.9 | num_updates 1269 | best_loss 9.862
2022-02-28 14:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1269 updates
2022-02-28 14:32:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 41 @ 1269 updates, score 9.862) (writing took 4.283574895001948 seconds)
2022-02-28 14:32:41 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-28 14:32:41 | INFO | train | epoch 041 | loss 9.914 | ppl 964.9 | wps 5258.1 | ups 4.28 | wpb 1227.6 | bsz 61.6 | num_updates 1269 | lr 3.807e-06 | gnorm 7.711 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 349
2022-02-28 14:32:41 | INFO | fairseq.trainer | begin training epoch 42
2022-02-28 14:32:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:32:42 | INFO | train_inner | epoch 042:     11 / 31 loss=9.917, ppl=966.76, wps=3689.2, ups=3.17, wpb=1164.5, bsz=61.6, num_updates=1280, lr=3.84e-06, gnorm=7.588, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=350
2022-02-28 14:32:43 | INFO | train_inner | epoch 042:     31 / 31 loss=9.715, ppl=840.15, wps=18541, ups=15.27, wpb=1214.5, bsz=60.3, num_updates=1300, lr=3.9e-06, gnorm=7.952, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=352
2022-02-28 14:32:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:32:44 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.764 | ppl 869.24 | wps 26349.2 | wpb 591.2 | bsz 29.9 | num_updates 1300 | best_loss 9.764
2022-02-28 14:32:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1300 updates
2022-02-28 14:32:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 1300 updates, score 9.764) (writing took 4.175434054050129 seconds)
2022-02-28 14:32:48 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-02-28 14:32:48 | INFO | train | epoch 042 | loss 9.782 | ppl 880.66 | wps 5526 | ups 4.5 | wpb 1227.6 | bsz 61.6 | num_updates 1300 | lr 3.9e-06 | gnorm 7.777 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 356
2022-02-28 14:32:48 | INFO | fairseq.trainer | begin training epoch 43
2022-02-28 14:32:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:32:50 | INFO | train_inner | epoch 043:     20 / 31 loss=9.792, ppl=886.62, wps=4152, ups=3.24, wpb=1280.2, bsz=61.9, num_updates=1320, lr=3.96e-06, gnorm=7.521, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=358
2022-02-28 14:32:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:32:51 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.759 | ppl 866.67 | wps 24674.6 | wpb 591.2 | bsz 29.9 | num_updates 1331 | best_loss 9.759
2022-02-28 14:32:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1331 updates
2022-02-28 14:32:51 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:32:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 43 @ 1331 updates, score 9.759) (writing took 4.623281051986851 seconds)
2022-02-28 14:32:56 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-02-28 14:32:56 | INFO | train | epoch 043 | loss 9.768 | ppl 871.87 | wps 5131.9 | ups 4.18 | wpb 1227.6 | bsz 61.6 | num_updates 1331 | lr 3.993e-06 | gnorm 8.033 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 364
2022-02-28 14:32:56 | INFO | fairseq.trainer | begin training epoch 44
2022-02-28 14:32:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:32:56 | INFO | train_inner | epoch 044:      9 / 31 loss=9.684, ppl=822.75, wps=3545.5, ups=2.99, wpb=1184.5, bsz=62.4, num_updates=1340, lr=4.02e-06, gnorm=8.524, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=365
2022-02-28 14:32:58 | INFO | train_inner | epoch 044:     29 / 31 loss=9.61, ppl=781.33, wps=17031.8, ups=13.94, wpb=1221.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=7.45, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=366
2022-02-28 14:32:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:32:58 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.684 | ppl 822.84 | wps 26146.9 | wpb 591.2 | bsz 29.9 | num_updates 1362 | best_loss 9.684
2022-02-28 14:32:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1362 updates
2022-02-28 14:32:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 1362 updates, score 9.684) (writing took 3.9853573159780353 seconds)
2022-02-28 14:33:02 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-02-28 14:33:02 | INFO | train | epoch 044 | loss 9.658 | ppl 807.63 | wps 5554.3 | ups 4.52 | wpb 1227.6 | bsz 61.6 | num_updates 1362 | lr 4.086e-06 | gnorm 7.622 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 371
2022-02-28 14:33:02 | INFO | fairseq.trainer | begin training epoch 45
2022-02-28 14:33:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:33:04 | INFO | train_inner | epoch 045:     18 / 31 loss=9.632, ppl=793.47, wps=3967.1, ups=3.32, wpb=1195.7, bsz=60.3, num_updates=1380, lr=4.14e-06, gnorm=7.459, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=372
2022-02-28 14:33:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:33:05 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.629 | ppl 791.91 | wps 24900.2 | wpb 591.2 | bsz 29.9 | num_updates 1393 | best_loss 9.629
2022-02-28 14:33:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1393 updates
2022-02-28 14:33:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 1393 updates, score 9.629) (writing took 4.033259761985391 seconds)
2022-02-28 14:33:09 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-02-28 14:33:09 | INFO | train | epoch 045 | loss 9.56 | ppl 754.8 | wps 5538.3 | ups 4.51 | wpb 1227.6 | bsz 61.6 | num_updates 1393 | lr 4.179e-06 | gnorm 7.686 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 378
2022-02-28 14:33:09 | INFO | fairseq.trainer | begin training epoch 46
2022-02-28 14:33:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:33:10 | INFO | train_inner | epoch 046:      7 / 31 loss=9.533, ppl=740.84, wps=4131.6, ups=3.28, wpb=1258.5, bsz=61.6, num_updates=1400, lr=4.2e-06, gnorm=7.885, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=378
2022-02-28 14:33:11 | INFO | train_inner | epoch 046:     27 / 31 loss=9.509, ppl=728.75, wps=17257.4, ups=14.15, wpb=1219.4, bsz=61.9, num_updates=1420, lr=4.26e-06, gnorm=7.708, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=380
2022-02-28 14:33:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:33:12 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.556 | ppl 752.48 | wps 26329.8 | wpb 591.2 | bsz 29.9 | num_updates 1424 | best_loss 9.556
2022-02-28 14:33:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1424 updates
2022-02-28 14:33:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 46 @ 1424 updates, score 9.556) (writing took 4.595336152997334 seconds)
2022-02-28 14:33:17 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-02-28 14:33:17 | INFO | train | epoch 046 | loss 9.521 | ppl 734.82 | wps 5156.6 | ups 4.2 | wpb 1227.6 | bsz 61.6 | num_updates 1424 | lr 4.272e-06 | gnorm 7.683 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 385
2022-02-28 14:33:17 | INFO | fairseq.trainer | begin training epoch 47
2022-02-28 14:33:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:33:18 | INFO | train_inner | epoch 047:     16 / 31 loss=9.527, ppl=737.68, wps=3637.1, ups=3.02, wpb=1206, bsz=61.6, num_updates=1440, lr=4.32e-06, gnorm=9.341, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=386
2022-02-28 14:33:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:33:19 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.54 | ppl 744.57 | wps 29101.8 | wpb 591.2 | bsz 29.9 | num_updates 1455 | best_loss 9.54
2022-02-28 14:33:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1455 updates
2022-02-28 14:33:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 1455 updates, score 9.54) (writing took 4.57436064700596 seconds)
2022-02-28 14:33:24 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-02-28 14:33:24 | INFO | train | epoch 047 | loss 9.441 | ppl 695.16 | wps 5141.4 | ups 4.19 | wpb 1227.6 | bsz 61.6 | num_updates 1455 | lr 4.365e-06 | gnorm 8.877 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 392
2022-02-28 14:33:24 | INFO | fairseq.trainer | begin training epoch 48
2022-02-28 14:33:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:33:25 | INFO | train_inner | epoch 048:      5 / 31 loss=9.41, ppl=680.46, wps=3756.5, ups=3.02, wpb=1244.2, bsz=59.8, num_updates=1460, lr=4.38e-06, gnorm=7.913, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=393
2022-02-28 14:33:26 | INFO | train_inner | epoch 048:     25 / 31 loss=9.322, ppl=639.96, wps=17891.8, ups=13.92, wpb=1285.3, bsz=63.2, num_updates=1480, lr=4.44e-06, gnorm=7.138, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=394
2022-02-28 14:33:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:33:27 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.479 | ppl 713.78 | wps 25958.3 | wpb 591.2 | bsz 29.9 | num_updates 1486 | best_loss 9.479
2022-02-28 14:33:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1486 updates
2022-02-28 14:33:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 48 @ 1486 updates, score 9.479) (writing took 4.909876686986536 seconds)
2022-02-28 14:33:32 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-02-28 14:33:32 | INFO | train | epoch 048 | loss 9.37 | ppl 661.84 | wps 4870.8 | ups 3.97 | wpb 1227.6 | bsz 61.6 | num_updates 1486 | lr 4.458e-06 | gnorm 7.358 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 400
2022-02-28 14:33:32 | INFO | fairseq.trainer | begin training epoch 49
2022-02-28 14:33:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:33:33 | INFO | train_inner | epoch 049:     14 / 31 loss=9.437, ppl=693.36, wps=3647.4, ups=2.88, wpb=1265.4, bsz=62.4, num_updates=1500, lr=4.5e-06, gnorm=7.326, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=401
2022-02-28 14:33:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:33:33 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.447 | ppl 697.96 | wps 26170.8 | wpb 591.2 | bsz 29.9 | num_updates 1500 | best_loss 9.447
2022-02-28 14:33:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1500 updates
2022-02-28 14:33:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-28 14:33:36 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-28 14:33:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt (epoch 49 @ 1500 updates, score 9.447) (writing took 10.110651839000639 seconds)
2022-02-28 14:33:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:33:45 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.38 | ppl 666.47 | wps 32086.8 | wpb 591.2 | bsz 29.9 | num_updates 1517 | best_loss 9.38
2022-02-28 14:33:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1517 updates
2022-02-28 14:33:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:33:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 1517 updates, score 9.38) (writing took 4.40558099799091 seconds)
2022-02-28 14:33:50 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-02-28 14:33:50 | INFO | train | epoch 049 | loss 9.285 | ppl 623.79 | wps 2116.4 | ups 1.72 | wpb 1227.6 | bsz 61.6 | num_updates 1517 | lr 4.551e-06 | gnorm 7.512 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 418
2022-02-28 14:33:50 | INFO | fairseq.trainer | begin training epoch 50
2022-02-28 14:33:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:33:50 | INFO | train_inner | epoch 050:      3 / 31 loss=9.082, ppl=541.88, wps=1274.4, ups=1.16, wpb=1100.2, bsz=60.3, num_updates=1520, lr=4.56e-06, gnorm=7.848, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=418
2022-02-28 14:33:52 | INFO | train_inner | epoch 050:     23 / 31 loss=9.278, ppl=620.71, wps=17338.5, ups=13.82, wpb=1254.2, bsz=61.9, num_updates=1540, lr=4.62e-06, gnorm=7.552, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=420
2022-02-28 14:33:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:33:53 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.431 | ppl 690.07 | wps 30209.3 | wpb 591.2 | bsz 29.9 | num_updates 1548 | best_loss 9.38
2022-02-28 14:33:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1548 updates
2022-02-28 14:33:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:33:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:33:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 50 @ 1548 updates, score 9.431) (writing took 2.84594259300502 seconds)
2022-02-28 14:33:55 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-02-28 14:33:55 | INFO | train | epoch 050 | loss 9.25 | ppl 609.07 | wps 6785.6 | ups 5.53 | wpb 1227.6 | bsz 61.6 | num_updates 1548 | lr 4.644e-06 | gnorm 7.569 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 424
2022-02-28 14:33:55 | INFO | fairseq.trainer | begin training epoch 51
2022-02-28 14:33:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:33:56 | INFO | train_inner | epoch 051:     12 / 31 loss=9.363, ppl=658.66, wps=5460.2, ups=4.2, wpb=1299.3, bsz=60.3, num_updates=1560, lr=4.68e-06, gnorm=7.529, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=425
2022-02-28 14:33:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:33:58 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.288 | ppl 625.25 | wps 29452.4 | wpb 591.2 | bsz 29.9 | num_updates 1579 | best_loss 9.288
2022-02-28 14:33:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1579 updates
2022-02-28 14:33:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 51 @ 1579 updates, score 9.288) (writing took 4.981869946001098 seconds)
2022-02-28 14:34:03 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-02-28 14:34:03 | INFO | train | epoch 051 | loss 9.225 | ppl 598.59 | wps 4958.4 | ups 4.04 | wpb 1227.6 | bsz 61.6 | num_updates 1579 | lr 4.737e-06 | gnorm 7.546 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 431
2022-02-28 14:34:03 | INFO | fairseq.trainer | begin training epoch 52
2022-02-28 14:34:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:34:03 | INFO | train_inner | epoch 052:      1 / 31 loss=9.133, ppl=561.39, wps=3342.7, ups=2.89, wpb=1157.5, bsz=62.4, num_updates=1580, lr=4.74e-06, gnorm=7.49, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=432
2022-02-28 14:34:05 | INFO | train_inner | epoch 052:     21 / 31 loss=9.102, ppl=549.34, wps=17511.8, ups=14.85, wpb=1179.5, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=7.406, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=433
2022-02-28 14:34:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:34:06 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.28 | ppl 621.56 | wps 29335 | wpb 591.2 | bsz 29.9 | num_updates 1610 | best_loss 9.28
2022-02-28 14:34:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1610 updates
2022-02-28 14:34:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 52 @ 1610 updates, score 9.28) (writing took 4.912559889955446 seconds)
2022-02-28 14:34:11 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-02-28 14:34:11 | INFO | train | epoch 052 | loss 9.171 | ppl 576.62 | wps 5033.4 | ups 4.1 | wpb 1227.6 | bsz 61.6 | num_updates 1610 | lr 4.83e-06 | gnorm 7.595 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 439
2022-02-28 14:34:11 | INFO | fairseq.trainer | begin training epoch 53
2022-02-28 14:34:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:34:12 | INFO | train_inner | epoch 053:     10 / 31 loss=9.246, ppl=607.22, wps=3689.6, ups=2.91, wpb=1267.7, bsz=59.5, num_updates=1620, lr=4.86e-06, gnorm=7.64, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=440
2022-02-28 14:34:13 | INFO | train_inner | epoch 053:     30 / 31 loss=9.024, ppl=520.76, wps=18275, ups=14.19, wpb=1288.1, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=7.515, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=441
2022-02-28 14:34:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:34:13 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.26 | ppl 612.91 | wps 29103.4 | wpb 591.2 | bsz 29.9 | num_updates 1641 | best_loss 9.26
2022-02-28 14:34:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1641 updates
2022-02-28 14:34:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 1641 updates, score 9.26) (writing took 6.555562349036336 seconds)
2022-02-28 14:34:20 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-02-28 14:34:20 | INFO | train | epoch 053 | loss 9.081 | ppl 541.69 | wps 4090 | ups 3.33 | wpb 1227.6 | bsz 61.6 | num_updates 1641 | lr 4.923e-06 | gnorm 7.517 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 448
2022-02-28 14:34:20 | INFO | fairseq.trainer | begin training epoch 54
2022-02-28 14:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:34:21 | INFO | train_inner | epoch 054:     19 / 31 loss=9.064, ppl=535.3, wps=3007.6, ups=2.35, wpb=1280.2, bsz=60.3, num_updates=1660, lr=4.98e-06, gnorm=7.408, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=450
2022-02-28 14:34:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:34:23 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.197 | ppl 586.99 | wps 30126.1 | wpb 591.2 | bsz 29.9 | num_updates 1672 | best_loss 9.197
2022-02-28 14:34:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1672 updates
2022-02-28 14:34:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 54 @ 1672 updates, score 9.197) (writing took 6.543036934977863 seconds)
2022-02-28 14:34:29 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-02-28 14:34:29 | INFO | train | epoch 054 | loss 9.038 | ppl 525.75 | wps 4114 | ups 3.35 | wpb 1227.6 | bsz 61.6 | num_updates 1672 | lr 5.016e-06 | gnorm 7.55 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 458
2022-02-28 14:34:29 | INFO | fairseq.trainer | begin training epoch 55
2022-02-28 14:34:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:34:30 | INFO | train_inner | epoch 055:      8 / 31 loss=9.003, ppl=513.13, wps=2869.7, ups=2.35, wpb=1220, bsz=62.4, num_updates=1680, lr=5.04e-06, gnorm=8.299, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=458
2022-02-28 14:34:31 | INFO | train_inner | epoch 055:     28 / 31 loss=8.935, ppl=489.6, wps=18269.2, ups=14.98, wpb=1219.8, bsz=61.9, num_updates=1700, lr=5.1e-06, gnorm=7.806, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=460
2022-02-28 14:34:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:34:32 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.196 | ppl 586.61 | wps 29372.9 | wpb 591.2 | bsz 29.9 | num_updates 1703 | best_loss 9.196
2022-02-28 14:34:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1703 updates
2022-02-28 14:34:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 55 @ 1703 updates, score 9.196) (writing took 8.178238658991177 seconds)
2022-02-28 14:34:40 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-02-28 14:34:40 | INFO | train | epoch 055 | loss 8.946 | ppl 493.11 | wps 3503.8 | ups 2.85 | wpb 1227.6 | bsz 61.6 | num_updates 1703 | lr 5.109e-06 | gnorm 8.182 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 468
2022-02-28 14:34:40 | INFO | fairseq.trainer | begin training epoch 56
2022-02-28 14:34:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:34:41 | INFO | train_inner | epoch 056:     17 / 31 loss=8.834, ppl=456.37, wps=2295.9, ups=1.97, wpb=1164.4, bsz=61.1, num_updates=1720, lr=5.16e-06, gnorm=7.694, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=470
2022-02-28 14:34:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:34:43 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.097 | ppl 547.47 | wps 28688.1 | wpb 591.2 | bsz 29.9 | num_updates 1734 | best_loss 9.097
2022-02-28 14:34:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1734 updates
2022-02-28 14:34:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:34:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 1734 updates, score 9.097) (writing took 6.120887109020259 seconds)
2022-02-28 14:34:49 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-02-28 14:34:49 | INFO | train | epoch 056 | loss 8.902 | ppl 478.44 | wps 4328.9 | ups 3.53 | wpb 1227.6 | bsz 61.6 | num_updates 1734 | lr 5.202e-06 | gnorm 7.489 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 477
2022-02-28 14:34:49 | INFO | fairseq.trainer | begin training epoch 57
2022-02-28 14:34:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:34:50 | INFO | train_inner | epoch 057:      6 / 31 loss=8.976, ppl=503.42, wps=2949, ups=2.49, wpb=1186.1, bsz=60.3, num_updates=1740, lr=5.22e-06, gnorm=7.628, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=478
2022-02-28 14:34:51 | INFO | train_inner | epoch 057:     26 / 31 loss=8.752, ppl=431.18, wps=17582.8, ups=14.58, wpb=1205.9, bsz=63.2, num_updates=1760, lr=5.28e-06, gnorm=7.712, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=479
2022-02-28 14:34:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:34:52 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.113 | ppl 553.68 | wps 29675.8 | wpb 591.2 | bsz 29.9 | num_updates 1765 | best_loss 9.097
2022-02-28 14:34:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1765 updates
2022-02-28 14:34:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:34:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:34:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 57 @ 1765 updates, score 9.113) (writing took 2.757278382021468 seconds)
2022-02-28 14:34:54 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-02-28 14:34:54 | INFO | train | epoch 057 | loss 8.835 | ppl 456.51 | wps 6990.4 | ups 5.69 | wpb 1227.6 | bsz 61.6 | num_updates 1765 | lr 5.295e-06 | gnorm 7.67 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 483
2022-02-28 14:34:54 | INFO | fairseq.trainer | begin training epoch 58
2022-02-28 14:34:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:34:56 | INFO | train_inner | epoch 058:     15 / 31 loss=8.882, ppl=471.63, wps=5519.7, ups=4.19, wpb=1316, bsz=61.1, num_updates=1780, lr=5.34e-06, gnorm=7.259, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=484
2022-02-28 14:34:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:34:57 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.988 | ppl 507.85 | wps 30772.2 | wpb 591.2 | bsz 29.9 | num_updates 1796 | best_loss 8.988
2022-02-28 14:34:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1796 updates
2022-02-28 14:34:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:35:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:35:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 58 @ 1796 updates, score 8.988) (writing took 5.722390465030912 seconds)
2022-02-28 14:35:03 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-02-28 14:35:03 | INFO | train | epoch 058 | loss 8.787 | ppl 441.74 | wps 4498.1 | ups 3.66 | wpb 1227.6 | bsz 61.6 | num_updates 1796 | lr 5.388e-06 | gnorm 7.808 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 491
2022-02-28 14:35:03 | INFO | fairseq.trainer | begin training epoch 59
2022-02-28 14:35:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:35:03 | INFO | train_inner | epoch 059:      4 / 31 loss=8.76, ppl=433.67, wps=3207.8, ups=2.62, wpb=1223.8, bsz=61.6, num_updates=1800, lr=5.4e-06, gnorm=8.168, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=492
2022-02-28 14:35:05 | INFO | train_inner | epoch 059:     24 / 31 loss=8.656, ppl=403.5, wps=17694.9, ups=14.68, wpb=1205.6, bsz=61.9, num_updates=1820, lr=5.46e-06, gnorm=7.421, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=493
2022-02-28 14:35:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:35:06 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.07 | ppl 537.41 | wps 30319.4 | wpb 591.2 | bsz 29.9 | num_updates 1827 | best_loss 8.988
2022-02-28 14:35:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1827 updates
2022-02-28 14:35:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:35:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:35:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 59 @ 1827 updates, score 9.07) (writing took 3.0387527749990113 seconds)
2022-02-28 14:35:09 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-02-28 14:35:09 | INFO | train | epoch 059 | loss 8.742 | ppl 428.24 | wps 6679.1 | ups 5.44 | wpb 1227.6 | bsz 61.6 | num_updates 1827 | lr 5.481e-06 | gnorm 7.524 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 497
2022-02-28 14:35:09 | INFO | fairseq.trainer | begin training epoch 60
2022-02-28 14:35:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:35:10 | INFO | train_inner | epoch 060:     13 / 31 loss=8.764, ppl=434.75, wps=4901.5, ups=4.03, wpb=1217.8, bsz=60.3, num_updates=1840, lr=5.52e-06, gnorm=7.45, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=498
2022-02-28 14:35:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:35:12 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.897 | ppl 476.6 | wps 28796.6 | wpb 591.2 | bsz 29.9 | num_updates 1858 | best_loss 8.897
2022-02-28 14:35:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1858 updates
2022-02-28 14:35:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:35:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:35:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 60 @ 1858 updates, score 8.897) (writing took 5.667013564961962 seconds)
2022-02-28 14:35:17 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-02-28 14:35:17 | INFO | train | epoch 060 | loss 8.648 | ppl 401.25 | wps 4437.1 | ups 3.61 | wpb 1227.6 | bsz 61.6 | num_updates 1858 | lr 5.574e-06 | gnorm 7.541 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 505
2022-02-28 14:35:17 | INFO | fairseq.trainer | begin training epoch 61
2022-02-28 14:35:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:35:17 | INFO | train_inner | epoch 061:      2 / 31 loss=8.625, ppl=394.82, wps=3042.7, ups=2.56, wpb=1186.7, bsz=62.4, num_updates=1860, lr=5.58e-06, gnorm=7.743, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=506
2022-02-28 14:35:19 | INFO | train_inner | epoch 061:     22 / 31 loss=8.639, ppl=398.59, wps=17644.3, ups=14.3, wpb=1234, bsz=61.9, num_updates=1880, lr=5.64e-06, gnorm=7.221, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=507
2022-02-28 14:35:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:35:20 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.891 | ppl 474.88 | wps 27575.3 | wpb 591.2 | bsz 29.9 | num_updates 1889 | best_loss 8.891
2022-02-28 14:35:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1889 updates
2022-02-28 14:35:20 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:35:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:35:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 61 @ 1889 updates, score 8.891) (writing took 5.467353221029043 seconds)
2022-02-28 14:35:26 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-02-28 14:35:26 | INFO | train | epoch 061 | loss 8.64 | ppl 398.87 | wps 4582.1 | ups 3.73 | wpb 1227.6 | bsz 61.6 | num_updates 1889 | lr 5.667e-06 | gnorm 7.336 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 514
2022-02-28 14:35:26 | INFO | fairseq.trainer | begin training epoch 62
2022-02-28 14:35:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:35:26 | INFO | train_inner | epoch 062:     11 / 31 loss=8.621, ppl=393.59, wps=3305, ups=2.65, wpb=1246.4, bsz=61.6, num_updates=1900, lr=5.7e-06, gnorm=7.377, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=515
2022-02-28 14:35:28 | INFO | train_inner | epoch 062:     31 / 31 loss=8.524, ppl=368.05, wps=17592.2, ups=14.15, wpb=1243, bsz=61.1, num_updates=1920, lr=5.76e-06, gnorm=7.675, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=516
2022-02-28 14:35:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:35:28 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8.935 | ppl 489.55 | wps 30634.4 | wpb 591.2 | bsz 29.9 | num_updates 1920 | best_loss 8.891
2022-02-28 14:35:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1920 updates
2022-02-28 14:35:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:35:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:35:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 62 @ 1920 updates, score 8.935) (writing took 2.5717000819859095 seconds)
2022-02-28 14:35:31 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-02-28 14:35:31 | INFO | train | epoch 062 | loss 8.558 | ppl 376.83 | wps 7193.4 | ups 5.86 | wpb 1227.6 | bsz 61.6 | num_updates 1920 | lr 5.76e-06 | gnorm 7.517 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 519
2022-02-28 14:35:31 | INFO | fairseq.trainer | begin training epoch 63
2022-02-28 14:35:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:35:32 | INFO | train_inner | epoch 063:     20 / 31 loss=8.551, ppl=375.02, wps=5430.3, ups=4.43, wpb=1224.7, bsz=63.2, num_updates=1940, lr=5.82e-06, gnorm=7.257, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=521
2022-02-28 14:35:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:35:34 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 8.867 | ppl 466.9 | wps 27663.4 | wpb 591.2 | bsz 29.9 | num_updates 1951 | best_loss 8.867
2022-02-28 14:35:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 1951 updates
2022-02-28 14:35:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:35:36 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:35:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 63 @ 1951 updates, score 8.867) (writing took 5.687619792006444 seconds)
2022-02-28 14:35:39 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-02-28 14:35:39 | INFO | train | epoch 063 | loss 8.53 | ppl 369.77 | wps 4513.3 | ups 3.68 | wpb 1227.6 | bsz 61.6 | num_updates 1951 | lr 5.853e-06 | gnorm 7.596 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 528
2022-02-28 14:35:39 | INFO | fairseq.trainer | begin training epoch 64
2022-02-28 14:35:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:35:40 | INFO | train_inner | epoch 064:      9 / 31 loss=8.422, ppl=343.1, wps=3023.9, ups=2.61, wpb=1158.3, bsz=61.1, num_updates=1960, lr=5.88e-06, gnorm=7.962, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=528
2022-02-28 14:35:41 | INFO | train_inner | epoch 064:     29 / 31 loss=8.56, ppl=377.39, wps=18928.9, ups=14, wpb=1352, bsz=61.9, num_updates=1980, lr=5.94e-06, gnorm=7.267, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=530
2022-02-28 14:35:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:35:42 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 8.819 | ppl 451.59 | wps 30897.6 | wpb 591.2 | bsz 29.9 | num_updates 1982 | best_loss 8.819
2022-02-28 14:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 1982 updates
2022-02-28 14:35:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:35:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:35:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 64 @ 1982 updates, score 8.819) (writing took 6.6321799120050855 seconds)
2022-02-28 14:35:49 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-02-28 14:35:49 | INFO | train | epoch 064 | loss 8.483 | ppl 357.77 | wps 4060.3 | ups 3.31 | wpb 1227.6 | bsz 61.6 | num_updates 1982 | lr 5.946e-06 | gnorm 7.492 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 537
2022-02-28 14:35:49 | INFO | fairseq.trainer | begin training epoch 65
2022-02-28 14:35:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:35:50 | INFO | train_inner | epoch 065:     18 / 31 loss=8.284, ppl=311.76, wps=2701.4, ups=2.34, wpb=1156.3, bsz=61.1, num_updates=2000, lr=6e-06, gnorm=7.6, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=538
2022-02-28 14:35:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:35:50 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.779 | ppl 439.39 | wps 29562.9 | wpb 591.2 | bsz 29.9 | num_updates 2000 | best_loss 8.779
2022-02-28 14:35:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 2000 updates
2022-02-28 14:35:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt
2022-02-28 14:35:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt
2022-02-28 14:36:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt (epoch 65 @ 2000 updates, score 8.779) (writing took 11.050310076971073 seconds)
2022-02-28 14:36:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:36:03 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.773 | ppl 437.46 | wps 31986.8 | wpb 591.2 | bsz 29.9 | num_updates 2013 | best_loss 8.773
2022-02-28 14:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 2013 updates
2022-02-28 14:36:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 65 @ 2013 updates, score 8.773) (writing took 5.697586541995406 seconds)
2022-02-28 14:36:09 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-02-28 14:36:09 | INFO | train | epoch 065 | loss 8.413 | ppl 340.82 | wps 1902.9 | ups 1.55 | wpb 1227.6 | bsz 61.6 | num_updates 2013 | lr 6.039e-06 | gnorm 7.444 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 557
2022-02-28 14:36:09 | INFO | fairseq.trainer | begin training epoch 66
2022-02-28 14:36:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:36:09 | INFO | train_inner | epoch 066:      7 / 31 loss=8.481, ppl=357.27, wps=1251.6, ups=1.04, wpb=1206.3, bsz=61.6, num_updates=2020, lr=6.06e-06, gnorm=7.556, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=558
2022-02-28 14:36:11 | INFO | train_inner | epoch 066:     27 / 31 loss=8.391, ppl=335.74, wps=19250, ups=14.62, wpb=1316.7, bsz=61.9, num_updates=2040, lr=6.12e-06, gnorm=7.315, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=559
2022-02-28 14:36:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:36:11 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 8.733 | ppl 425.41 | wps 30132.9 | wpb 591.2 | bsz 29.9 | num_updates 2044 | best_loss 8.733
2022-02-28 14:36:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 2044 updates
2022-02-28 14:36:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 66 @ 2044 updates, score 8.733) (writing took 5.818762104958296 seconds)
2022-02-28 14:36:17 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-02-28 14:36:17 | INFO | train | epoch 066 | loss 8.37 | ppl 330.76 | wps 4466.9 | ups 3.64 | wpb 1227.6 | bsz 61.6 | num_updates 2044 | lr 6.132e-06 | gnorm 7.455 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 566
2022-02-28 14:36:17 | INFO | fairseq.trainer | begin training epoch 67
2022-02-28 14:36:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:36:19 | INFO | train_inner | epoch 067:     16 / 31 loss=8.287, ppl=312.39, wps=3097.4, ups=2.52, wpb=1227, bsz=61.1, num_updates=2060, lr=6.18e-06, gnorm=7.194, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=567
2022-02-28 14:36:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:36:20 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 8.782 | ppl 440.14 | wps 28137.7 | wpb 591.2 | bsz 29.9 | num_updates 2075 | best_loss 8.733
2022-02-28 14:36:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 2075 updates
2022-02-28 14:36:20 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:36:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:36:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 67 @ 2075 updates, score 8.782) (writing took 3.0129759410046972 seconds)
2022-02-28 14:36:23 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-02-28 14:36:23 | INFO | train | epoch 067 | loss 8.302 | ppl 315.66 | wps 6219.1 | ups 5.07 | wpb 1227.6 | bsz 61.6 | num_updates 2075 | lr 6.225e-06 | gnorm 7.382 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 572
2022-02-28 14:36:23 | INFO | fairseq.trainer | begin training epoch 68
2022-02-28 14:36:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:36:24 | INFO | train_inner | epoch 068:      5 / 31 loss=8.373, ppl=331.52, wps=4895.7, ups=3.83, wpb=1276.7, bsz=59.5, num_updates=2080, lr=6.24e-06, gnorm=7.586, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=572
2022-02-28 14:36:25 | INFO | train_inner | epoch 068:     25 / 31 loss=8.214, ppl=296.91, wps=16953, ups=14.23, wpb=1191.1, bsz=64, num_updates=2100, lr=6.3e-06, gnorm=7.239, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=573
2022-02-28 14:36:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:36:26 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 8.697 | ppl 415.13 | wps 29954.8 | wpb 591.2 | bsz 29.9 | num_updates 2106 | best_loss 8.697
2022-02-28 14:36:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 2106 updates
2022-02-28 14:36:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 68 @ 2106 updates, score 8.697) (writing took 4.143503616971429 seconds)
2022-02-28 14:36:30 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-02-28 14:36:30 | INFO | train | epoch 068 | loss 8.255 | ppl 305.52 | wps 5545.5 | ups 4.52 | wpb 1227.6 | bsz 61.6 | num_updates 2106 | lr 6.318e-06 | gnorm 7.36 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 579
2022-02-28 14:36:30 | INFO | fairseq.trainer | begin training epoch 69
2022-02-28 14:36:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:36:31 | INFO | train_inner | epoch 069:     14 / 31 loss=8.321, ppl=319.79, wps=4250.4, ups=3.29, wpb=1291.7, bsz=61.6, num_updates=2120, lr=6.36e-06, gnorm=7.375, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=580
2022-02-28 14:36:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:36:33 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 8.646 | ppl 400.7 | wps 29806.1 | wpb 591.2 | bsz 29.9 | num_updates 2137 | best_loss 8.646
2022-02-28 14:36:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 2137 updates
2022-02-28 14:36:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:36 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 69 @ 2137 updates, score 8.646) (writing took 4.243694878008682 seconds)
2022-02-28 14:36:37 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-02-28 14:36:37 | INFO | train | epoch 069 | loss 8.227 | ppl 299.71 | wps 5493.4 | ups 4.47 | wpb 1227.6 | bsz 61.6 | num_updates 2137 | lr 6.411e-06 | gnorm 7.453 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 585
2022-02-28 14:36:37 | INFO | fairseq.trainer | begin training epoch 70
2022-02-28 14:36:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:36:37 | INFO | train_inner | epoch 070:      3 / 31 loss=8.132, ppl=280.52, wps=3703, ups=3.23, wpb=1145.5, bsz=60.3, num_updates=2140, lr=6.42e-06, gnorm=7.552, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=586
2022-02-28 14:36:39 | INFO | train_inner | epoch 070:     23 / 31 loss=8.135, ppl=281.11, wps=16150.5, ups=14.6, wpb=1106.3, bsz=62.7, num_updates=2160, lr=6.48e-06, gnorm=8.115, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=587
2022-02-28 14:36:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:36:40 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 8.636 | ppl 397.69 | wps 28919.5 | wpb 591.2 | bsz 29.9 | num_updates 2168 | best_loss 8.636
2022-02-28 14:36:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 2168 updates
2022-02-28 14:36:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 70 @ 2168 updates, score 8.636) (writing took 4.783432096010074 seconds)
2022-02-28 14:36:45 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-02-28 14:36:45 | INFO | train | epoch 070 | loss 8.184 | ppl 290.88 | wps 5075.2 | ups 4.13 | wpb 1227.6 | bsz 61.6 | num_updates 2168 | lr 6.504e-06 | gnorm 7.72 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 593
2022-02-28 14:36:45 | INFO | fairseq.trainer | begin training epoch 71
2022-02-28 14:36:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:36:46 | INFO | train_inner | epoch 071:     12 / 31 loss=8.164, ppl=286.85, wps=3930.8, ups=2.94, wpb=1338.5, bsz=62.4, num_updates=2180, lr=6.54e-06, gnorm=7.228, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=594
2022-02-28 14:36:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:36:48 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 8.571 | ppl 380.38 | wps 25237.8 | wpb 591.2 | bsz 29.9 | num_updates 2199 | best_loss 8.571
2022-02-28 14:36:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 2199 updates
2022-02-28 14:36:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:36:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 71 @ 2199 updates, score 8.571) (writing took 5.903520410996862 seconds)
2022-02-28 14:36:54 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-02-28 14:36:54 | INFO | train | epoch 071 | loss 8.157 | ppl 285.42 | wps 4219.7 | ups 3.44 | wpb 1227.6 | bsz 61.6 | num_updates 2199 | lr 6.597e-06 | gnorm 7.431 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 602
2022-02-28 14:36:54 | INFO | fairseq.trainer | begin training epoch 72
2022-02-28 14:36:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:36:54 | INFO | train_inner | epoch 072:      1 / 31 loss=8.202, ppl=294.53, wps=2879.4, ups=2.43, wpb=1185.5, bsz=60.3, num_updates=2200, lr=6.6e-06, gnorm=7.484, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=602
2022-02-28 14:36:55 | INFO | train_inner | epoch 072:     21 / 31 loss=8.11, ppl=276.28, wps=17222.3, ups=13.75, wpb=1252.2, bsz=61.9, num_updates=2220, lr=6.66e-06, gnorm=7.477, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=604
2022-02-28 14:36:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:36:57 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 8.548 | ppl 374.17 | wps 30091 | wpb 591.2 | bsz 29.9 | num_updates 2230 | best_loss 8.548
2022-02-28 14:36:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 2230 updates
2022-02-28 14:36:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:37:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:37:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 72 @ 2230 updates, score 8.548) (writing took 5.130958202003967 seconds)
2022-02-28 14:37:02 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-02-28 14:37:02 | INFO | train | epoch 072 | loss 8.087 | ppl 271.99 | wps 4796.2 | ups 3.91 | wpb 1227.6 | bsz 61.6 | num_updates 2230 | lr 6.69e-06 | gnorm 7.575 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 610
2022-02-28 14:37:02 | INFO | fairseq.trainer | begin training epoch 73
2022-02-28 14:37:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:37:02 | INFO | train_inner | epoch 073:     10 / 31 loss=7.874, ppl=234.63, wps=2840.6, ups=2.82, wpb=1009, bsz=62.4, num_updates=2240, lr=6.72e-06, gnorm=7.903, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=611
2022-02-28 14:37:04 | INFO | train_inner | epoch 073:     30 / 31 loss=8.185, ppl=291, wps=20016.1, ups=13.68, wpb=1463.7, bsz=61.9, num_updates=2260, lr=6.78e-06, gnorm=6.912, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=612
2022-02-28 14:37:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:37:04 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 8.615 | ppl 392.13 | wps 27512.9 | wpb 591.2 | bsz 29.9 | num_updates 2261 | best_loss 8.548
2022-02-28 14:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 2261 updates
2022-02-28 14:37:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:37:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:37:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 73 @ 2261 updates, score 8.615) (writing took 2.9107746240333654 seconds)
2022-02-28 14:37:07 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-02-28 14:37:07 | INFO | train | epoch 073 | loss 8.05 | ppl 265 | wps 6625.1 | ups 5.4 | wpb 1227.6 | bsz 61.6 | num_updates 2261 | lr 6.783e-06 | gnorm 7.383 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 616
2022-02-28 14:37:07 | INFO | fairseq.trainer | begin training epoch 74
2022-02-28 14:37:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:37:09 | INFO | train_inner | epoch 074:     19 / 31 loss=7.92, ppl=242.26, wps=4869.3, ups=4.07, wpb=1196.2, bsz=62.4, num_updates=2280, lr=6.84e-06, gnorm=7.72, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=617
2022-02-28 14:37:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:37:10 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 8.476 | ppl 356.08 | wps 30601.3 | wpb 591.2 | bsz 29.9 | num_updates 2292 | best_loss 8.476
2022-02-28 14:37:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 2292 updates
2022-02-28 14:37:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:37:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:37:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 74 @ 2292 updates, score 8.476) (writing took 4.581324402010068 seconds)
2022-02-28 14:37:15 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-02-28 14:37:15 | INFO | train | epoch 074 | loss 8.008 | ppl 257.46 | wps 5120.4 | ups 4.17 | wpb 1227.6 | bsz 61.6 | num_updates 2292 | lr 6.876e-06 | gnorm 7.525 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 623
2022-02-28 14:37:15 | INFO | fairseq.trainer | begin training epoch 75
2022-02-28 14:37:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:37:16 | INFO | train_inner | epoch 075:      8 / 31 loss=8.134, ppl=280.88, wps=3822, ups=3, wpb=1272.5, bsz=59, num_updates=2300, lr=6.9e-06, gnorm=7.339, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=624
2022-02-28 14:37:17 | INFO | train_inner | epoch 075:     28 / 31 loss=7.933, ppl=244.31, wps=16114.9, ups=13.52, wpb=1191.9, bsz=63.2, num_updates=2320, lr=6.96e-06, gnorm=7.283, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=625
2022-02-28 14:37:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:37:18 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 8.526 | ppl 368.63 | wps 25401.6 | wpb 591.2 | bsz 29.9 | num_updates 2323 | best_loss 8.476
2022-02-28 14:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 2323 updates
2022-02-28 14:37:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:37:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:37:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 75 @ 2323 updates, score 8.526) (writing took 2.984489233000204 seconds)
2022-02-28 14:37:21 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-02-28 14:37:21 | INFO | train | epoch 075 | loss 7.955 | ppl 248.06 | wps 6460.6 | ups 5.26 | wpb 1227.6 | bsz 61.6 | num_updates 2323 | lr 6.969e-06 | gnorm 7.418 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 629
2022-02-28 14:37:21 | INFO | fairseq.trainer | begin training epoch 76
2022-02-28 14:37:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:37:22 | INFO | train_inner | epoch 076:     17 / 31 loss=7.836, ppl=228.42, wps=4789.3, ups=4, wpb=1196.3, bsz=61.1, num_updates=2340, lr=7.02e-06, gnorm=7.545, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=630
2022-02-28 14:37:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:37:23 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 8.479 | ppl 356.91 | wps 27599.7 | wpb 591.2 | bsz 29.9 | num_updates 2354 | best_loss 8.476
2022-02-28 14:37:23 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 2 runs
2022-02-28 14:37:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 2354 updates
2022-02-28 14:37:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:37:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:37:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 76 @ 2354 updates, score 8.479) (writing took 2.7489286190248094 seconds)
2022-02-28 14:37:26 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-02-28 14:37:26 | INFO | train | epoch 076 | loss 7.899 | ppl 238.62 | wps 6897.2 | ups 5.62 | wpb 1227.6 | bsz 61.6 | num_updates 2354 | lr 7.062e-06 | gnorm 7.381 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 635
2022-02-28 14:37:26 | INFO | fairseq_cli.train | done training in 631.1 seconds
2022-02-28 14:51:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 2, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=999, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 999, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-28 14:51:06 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-28 14:51:08 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-28 14:51:08 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-28 14:51:08 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-28 14:51:08 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-28 14:51:08 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-28 14:51:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-28 14:51:09 | INFO | bartabst.data.data_utils | loaded 598 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-02-28 14:51:12 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-02-28 14:51:12 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-28 14:51:12 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-02-28 14:51:12 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 14:51:12 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-02-28 14:51:12 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 14:51:12 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-28 14:51:12 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-02-28 14:51:12 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-02-28 14:51:13 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-02-28 14:51:13 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-02-28 14:51:13 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-02-28 14:51:13 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-02-28 14:51:13 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-02-28 14:51:13 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-02-28 14:51:13 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-02-28 14:51:14 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-02-28 14:51:14 | INFO | fairseq.trainer | loading train data for epoch 1
no aos file, no transfer aos used
2022-02-28 14:51:15 | INFO | bartabst.data.data_utils | loaded 1,910 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-02-28 14:51:15 | INFO | fairseq.trainer | begin training epoch 1
2022-02-28 14:51:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:51:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-28 14:51:17 | INFO | train_inner | epoch 001:     21 / 31 loss=17.267, ppl=157724, wps=15785.5, ups=13.01, wpb=1226.7, bsz=63.2, num_updates=20, lr=6e-08, gnorm=23.218, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=5
2022-02-28 14:51:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:51:18 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.127 | ppl 143116 | wps 22108.8 | wpb 591.2 | bsz 29.9 | num_updates 30
2022-02-28 14:51:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 30 updates
2022-02-28 14:51:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:51:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:51:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 30 updates, score 17.127) (writing took 5.86646795499837 seconds)
2022-02-28 14:51:24 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-28 14:51:24 | INFO | train | epoch 001 | loss 17.3 | ppl 161327 | wps 4097.5 | ups 3.33 | wpb 1238.7 | bsz 61.5 | num_updates 30 | lr 9e-08 | gnorm 23.531 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 12
2022-02-28 14:51:24 | INFO | fairseq.trainer | begin training epoch 2
2022-02-28 14:51:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:51:25 | INFO | train_inner | epoch 002:     10 / 31 loss=17.275, ppl=158579, wps=3221, ups=2.5, wpb=1290.9, bsz=59.8, num_updates=40, lr=1.2e-07, gnorm=23.583, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=13
2022-02-28 14:51:26 | INFO | train_inner | epoch 002:     30 / 31 loss=17.298, ppl=161138, wps=17845.7, ups=14.67, wpb=1216.2, bsz=63.2, num_updates=60, lr=1.8e-07, gnorm=23.346, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=15
2022-02-28 14:51:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:51:27 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 17.054 | ppl 136065 | wps 29528.5 | wpb 591.2 | bsz 29.9 | num_updates 61 | best_loss 17.054
2022-02-28 14:51:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 61 updates
2022-02-28 14:51:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:51:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:51:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 61 updates, score 17.054) (writing took 5.42883118102327 seconds)
2022-02-28 14:51:32 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-28 14:51:32 | INFO | train | epoch 002 | loss 17.254 | ppl 156312 | wps 4697.7 | ups 3.83 | wpb 1227.6 | bsz 61.6 | num_updates 61 | lr 1.83e-07 | gnorm 23.324 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 21
2022-02-28 14:51:32 | INFO | fairseq.trainer | begin training epoch 3
2022-02-28 14:51:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:51:34 | INFO | train_inner | epoch 003:     19 / 31 loss=17.196, ppl=150139, wps=3201.1, ups=2.73, wpb=1173.7, bsz=60.3, num_updates=80, lr=2.4e-07, gnorm=23.684, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=22
2022-02-28 14:51:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:51:35 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.898 | ppl 122164 | wps 29074 | wpb 591.2 | bsz 29.9 | num_updates 92 | best_loss 16.898
2022-02-28 14:51:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 92 updates
2022-02-28 14:51:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:51:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:51:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 92 updates, score 16.898) (writing took 6.411543188034557 seconds)
2022-02-28 14:51:41 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-28 14:51:42 | INFO | train | epoch 003 | loss 17.118 | ppl 142264 | wps 4183 | ups 3.41 | wpb 1227.6 | bsz 61.6 | num_updates 92 | lr 2.76e-07 | gnorm 23.04 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 30
2022-02-28 14:51:42 | INFO | fairseq.trainer | begin training epoch 4
2022-02-28 14:51:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:51:42 | INFO | train_inner | epoch 004:      8 / 31 loss=16.977, ppl=129002, wps=2824.4, ups=2.36, wpb=1196.8, bsz=62.4, num_updates=100, lr=3e-07, gnorm=22.425, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=30
2022-02-28 14:51:44 | INFO | train_inner | epoch 004:     28 / 31 loss=16.92, ppl=124037, wps=20006.9, ups=14.72, wpb=1359.5, bsz=61.9, num_updates=120, lr=3.6e-07, gnorm=22.225, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=32
2022-02-28 14:51:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:51:44 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 16.547 | ppl 95749.3 | wps 30617.3 | wpb 591.2 | bsz 29.9 | num_updates 123 | best_loss 16.547
2022-02-28 14:51:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 123 updates
2022-02-28 14:51:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:51:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:51:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 123 updates, score 16.547) (writing took 7.036707370018121 seconds)
2022-02-28 14:51:51 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-28 14:51:51 | INFO | train | epoch 004 | loss 16.923 | ppl 124269 | wps 3889.3 | ups 3.17 | wpb 1227.6 | bsz 61.6 | num_updates 123 | lr 3.69e-07 | gnorm 22.883 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 40
2022-02-28 14:51:51 | INFO | fairseq.trainer | begin training epoch 5
2022-02-28 14:51:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:51:53 | INFO | train_inner | epoch 005:     17 / 31 loss=16.68, ppl=105006, wps=2768.2, ups=2.23, wpb=1244, bsz=62.4, num_updates=140, lr=4.2e-07, gnorm=22.544, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=41
2022-02-28 14:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:51:54 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 16.256 | ppl 78268.7 | wps 31941.5 | wpb 591.2 | bsz 29.9 | num_updates 154 | best_loss 16.256
2022-02-28 14:51:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 154 updates
2022-02-28 14:51:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 154 updates, score 16.256) (writing took 8.985846693976782 seconds)
2022-02-28 14:52:03 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-28 14:52:03 | INFO | train | epoch 005 | loss 16.632 | ppl 101583 | wps 3270.4 | ups 2.66 | wpb 1227.6 | bsz 61.6 | num_updates 154 | lr 4.62e-07 | gnorm 22.153 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 51
2022-02-28 14:52:03 | INFO | fairseq.trainer | begin training epoch 6
2022-02-28 14:52:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:52:04 | INFO | train_inner | epoch 006:      6 / 31 loss=16.541, ppl=95381.6, wps=2169.8, ups=1.8, wpb=1207.8, bsz=59.5, num_updates=160, lr=4.8e-07, gnorm=22.416, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=52
2022-02-28 14:52:05 | INFO | train_inner | epoch 006:     26 / 31 loss=16.35, ppl=83504.3, wps=14887, ups=12.63, wpb=1178.5, bsz=62.7, num_updates=180, lr=5.4e-07, gnorm=21.698, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=54
2022-02-28 14:52:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-28 14:52:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:52:06 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 16.015 | ppl 66201.8 | wps 30495.7 | wpb 591.2 | bsz 29.9 | num_updates 184 | best_loss 16.015
2022-02-28 14:52:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 184 updates
2022-02-28 14:52:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 184 updates, score 16.015) (writing took 6.0858670670422725 seconds)
2022-02-28 14:52:12 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-28 14:52:12 | INFO | train | epoch 006 | loss 16.342 | ppl 83077.8 | wps 4079.9 | ups 3.27 | wpb 1246.2 | bsz 61.5 | num_updates 184 | lr 5.52e-07 | gnorm 21.84 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 60
2022-02-28 14:52:12 | INFO | fairseq.trainer | begin training epoch 7
2022-02-28 14:52:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:52:14 | INFO | train_inner | epoch 007:     16 / 31 loss=16.094, ppl=69966.1, wps=3212.4, ups=2.42, wpb=1328.3, bsz=61.6, num_updates=200, lr=6e-07, gnorm=21.287, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=62
2022-02-28 14:52:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:52:15 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 15.6 | ppl 49658.7 | wps 29533.8 | wpb 591.2 | bsz 29.9 | num_updates 215 | best_loss 15.6
2022-02-28 14:52:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 215 updates
2022-02-28 14:52:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 215 updates, score 15.6) (writing took 6.027384263987187 seconds)
2022-02-28 14:52:21 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-28 14:52:21 | INFO | train | epoch 007 | loss 15.998 | ppl 65435.6 | wps 4254.7 | ups 3.47 | wpb 1227.6 | bsz 61.6 | num_updates 215 | lr 6.45e-07 | gnorm 20.749 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 69
2022-02-28 14:52:21 | INFO | fairseq.trainer | begin training epoch 8
2022-02-28 14:52:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:52:22 | INFO | train_inner | epoch 008:      5 / 31 loss=15.897, ppl=61030, wps=2687, ups=2.48, wpb=1083.2, bsz=61.1, num_updates=220, lr=6.6e-07, gnorm=20.573, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=70
2022-02-28 14:52:23 | INFO | train_inner | epoch 008:     25 / 31 loss=15.657, ppl=51683.7, wps=18149.5, ups=13.41, wpb=1353.2, bsz=63.2, num_updates=240, lr=7.2e-07, gnorm=19.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=71
2022-02-28 14:52:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:52:24 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 15.2 | ppl 37630.2 | wps 29979.7 | wpb 591.2 | bsz 29.9 | num_updates 246 | best_loss 15.2
2022-02-28 14:52:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 246 updates
2022-02-28 14:52:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 246 updates, score 15.2) (writing took 5.4965158679988235 seconds)
2022-02-28 14:52:30 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-28 14:52:30 | INFO | train | epoch 008 | loss 15.657 | ppl 51682.3 | wps 4541.8 | ups 3.7 | wpb 1227.6 | bsz 61.6 | num_updates 246 | lr 7.38e-07 | gnorm 20.119 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 78
2022-02-28 14:52:30 | INFO | fairseq.trainer | begin training epoch 9
2022-02-28 14:52:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:52:31 | INFO | train_inner | epoch 009:     14 / 31 loss=15.318, ppl=40853.8, wps=3225.3, ups=2.65, wpb=1215.7, bsz=59.8, num_updates=260, lr=7.8e-07, gnorm=20.633, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=79
2022-02-28 14:52:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:52:32 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 14.784 | ppl 28201.9 | wps 29989.7 | wpb 591.2 | bsz 29.9 | num_updates 277 | best_loss 14.784
2022-02-28 14:52:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 277 updates
2022-02-28 14:52:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 277 updates, score 14.784) (writing took 4.365728363976814 seconds)
2022-02-28 14:52:37 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-28 14:52:37 | INFO | train | epoch 009 | loss 15.208 | ppl 37842 | wps 5428.5 | ups 4.42 | wpb 1227.6 | bsz 61.6 | num_updates 277 | lr 8.31e-07 | gnorm 19.682 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 85
2022-02-28 14:52:37 | INFO | fairseq.trainer | begin training epoch 10
2022-02-28 14:52:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:52:37 | INFO | train_inner | epoch 010:      3 / 31 loss=15.104, ppl=35210.3, wps=3466.6, ups=3.17, wpb=1093.2, bsz=61.6, num_updates=280, lr=8.4e-07, gnorm=19.7, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=85
2022-02-28 14:52:38 | INFO | train_inner | epoch 010:     23 / 31 loss=14.765, ppl=27834.5, wps=19321, ups=14.37, wpb=1344.5, bsz=61.9, num_updates=300, lr=9e-07, gnorm=18.676, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=87
2022-02-28 14:52:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:52:39 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 14.464 | ppl 22595.8 | wps 30099.3 | wpb 591.2 | bsz 29.9 | num_updates 308 | best_loss 14.464
2022-02-28 14:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 308 updates
2022-02-28 14:52:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 308 updates, score 14.464) (writing took 4.2582471770001575 seconds)
2022-02-28 14:52:44 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-28 14:52:44 | INFO | train | epoch 010 | loss 14.759 | ppl 27720.4 | wps 5356.9 | ups 4.36 | wpb 1227.6 | bsz 61.6 | num_updates 308 | lr 9.24e-07 | gnorm 18.897 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 92
2022-02-28 14:52:44 | INFO | fairseq.trainer | begin training epoch 11
2022-02-28 14:52:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:52:45 | INFO | train_inner | epoch 011:     12 / 31 loss=14.6, ppl=24828.3, wps=3891.9, ups=3.17, wpb=1227, bsz=62.4, num_updates=320, lr=9.6e-07, gnorm=18.834, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=93
2022-02-28 14:52:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:52:46 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 14.073 | ppl 17235.3 | wps 30271.3 | wpb 591.2 | bsz 29.9 | num_updates 339 | best_loss 14.073
2022-02-28 14:52:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 339 updates
2022-02-28 14:52:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 339 updates, score 14.073) (writing took 5.400391620001756 seconds)
2022-02-28 14:52:52 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-28 14:52:52 | INFO | train | epoch 011 | loss 14.389 | ppl 21458.5 | wps 4718.1 | ups 3.84 | wpb 1227.6 | bsz 61.6 | num_updates 339 | lr 1.017e-06 | gnorm 18.183 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 100
2022-02-28 14:52:52 | INFO | fairseq.trainer | begin training epoch 12
2022-02-28 14:52:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:52:52 | INFO | train_inner | epoch 012:      1 / 31 loss=14.301, ppl=20191.7, wps=3225.4, ups=2.73, wpb=1179.8, bsz=60.3, num_updates=340, lr=1.02e-06, gnorm=17.967, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=100
2022-02-28 14:52:53 | INFO | train_inner | epoch 012:     21 / 31 loss=14.055, ppl=17025.1, wps=16302.6, ups=13.45, wpb=1212.4, bsz=64, num_updates=360, lr=1.08e-06, gnorm=17.003, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=102
2022-02-28 14:52:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:52:55 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 13.617 | ppl 12566.8 | wps 30546.6 | wpb 591.2 | bsz 29.9 | num_updates 370 | best_loss 13.617
2022-02-28 14:52:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 370 updates
2022-02-28 14:52:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:52:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 370 updates, score 13.617) (writing took 4.2440116190118715 seconds)
2022-02-28 14:52:59 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-28 14:52:59 | INFO | train | epoch 012 | loss 14.003 | ppl 16418 | wps 5401.7 | ups 4.4 | wpb 1227.6 | bsz 61.6 | num_updates 370 | lr 1.11e-06 | gnorm 17.37 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 107
2022-02-28 14:52:59 | INFO | fairseq.trainer | begin training epoch 13
2022-02-28 14:52:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:53:00 | INFO | train_inner | epoch 013:     10 / 31 loss=13.75, ppl=13778.1, wps=4164.2, ups=3.24, wpb=1285.8, bsz=58.2, num_updates=380, lr=1.14e-06, gnorm=17.654, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=108
2022-02-28 14:53:01 | INFO | train_inner | epoch 013:     30 / 31 loss=13.514, ppl=11696.1, wps=18089.2, ups=14.87, wpb=1216.2, bsz=64, num_updates=400, lr=1.2e-06, gnorm=16.288, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=109
2022-02-28 14:53:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:53:02 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 13.2 | ppl 9412.47 | wps 30382.6 | wpb 591.2 | bsz 29.9 | num_updates 401 | best_loss 13.2
2022-02-28 14:53:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 401 updates
2022-02-28 14:53:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 401 updates, score 13.2) (writing took 5.973174699989613 seconds)
2022-02-28 14:53:08 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-28 14:53:08 | INFO | train | epoch 013 | loss 13.566 | ppl 12124.8 | wps 4406.3 | ups 3.59 | wpb 1227.6 | bsz 61.6 | num_updates 401 | lr 1.203e-06 | gnorm 16.666 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 116
2022-02-28 14:53:08 | INFO | fairseq.trainer | begin training epoch 14
2022-02-28 14:53:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:53:09 | INFO | train_inner | epoch 014:     19 / 31 loss=13.283, ppl=9968.58, wps=2959.2, ups=2.53, wpb=1171.7, bsz=60.3, num_updates=420, lr=1.26e-06, gnorm=16.419, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=117
2022-02-28 14:53:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:53:10 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 12.94 | ppl 7858.52 | wps 30336.9 | wpb 591.2 | bsz 29.9 | num_updates 432 | best_loss 12.94
2022-02-28 14:53:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 432 updates
2022-02-28 14:53:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 432 updates, score 12.94) (writing took 6.045180675981101 seconds)
2022-02-28 14:53:16 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-28 14:53:16 | INFO | train | epoch 014 | loss 13.295 | ppl 10051.3 | wps 4373.3 | ups 3.56 | wpb 1227.6 | bsz 61.6 | num_updates 432 | lr 1.296e-06 | gnorm 15.822 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 124
2022-02-28 14:53:16 | INFO | fairseq.trainer | begin training epoch 15
2022-02-28 14:53:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:53:17 | INFO | train_inner | epoch 015:      8 / 31 loss=13.188, ppl=9333.61, wps=3291.8, ups=2.51, wpb=1314.1, bsz=62.4, num_updates=440, lr=1.32e-06, gnorm=14.642, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=125
2022-02-28 14:53:18 | INFO | train_inner | epoch 015:     28 / 31 loss=12.899, ppl=7637.98, wps=18415.2, ups=15.33, wpb=1200.9, bsz=61.9, num_updates=460, lr=1.38e-06, gnorm=14.871, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=126
2022-02-28 14:53:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:53:19 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 12.763 | ppl 6948.98 | wps 30372.5 | wpb 591.2 | bsz 29.9 | num_updates 463 | best_loss 12.763
2022-02-28 14:53:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 463 updates
2022-02-28 14:53:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 463 updates, score 12.763) (writing took 4.263287809037138 seconds)
2022-02-28 14:53:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-28 14:53:23 | INFO | train | epoch 015 | loss 12.917 | ppl 7733.12 | wps 5490.3 | ups 4.47 | wpb 1227.6 | bsz 61.6 | num_updates 463 | lr 1.389e-06 | gnorm 14.902 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 131
2022-02-28 14:53:23 | INFO | fairseq.trainer | begin training epoch 16
2022-02-28 14:53:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:53:25 | INFO | train_inner | epoch 016:     17 / 31 loss=12.743, ppl=6854.52, wps=3960.3, ups=3.17, wpb=1248.2, bsz=61.6, num_updates=480, lr=1.44e-06, gnorm=14.753, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=133
2022-02-28 14:53:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:53:26 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 12.475 | ppl 5694.42 | wps 31970.8 | wpb 591.2 | bsz 29.9 | num_updates 494 | best_loss 12.475
2022-02-28 14:53:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 494 updates
2022-02-28 14:53:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 494 updates, score 12.475) (writing took 4.074399831995834 seconds)
2022-02-28 14:53:30 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-28 14:53:30 | INFO | train | epoch 016 | loss 12.701 | ppl 6658.13 | wps 5711.2 | ups 4.65 | wpb 1227.6 | bsz 61.6 | num_updates 494 | lr 1.482e-06 | gnorm 14.154 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 138
2022-02-28 14:53:30 | INFO | fairseq.trainer | begin training epoch 17
2022-02-28 14:53:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:53:30 | INFO | train_inner | epoch 017:      6 / 31 loss=12.557, ppl=6026.34, wps=3989.3, ups=3.38, wpb=1179.1, bsz=59.8, num_updates=500, lr=1.5e-06, gnorm=14.616, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=139
2022-02-28 14:53:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:53:31 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.435 | ppl 5537.4 | wps 29756.2 | wpb 591.2 | bsz 29.9 | num_updates 500 | best_loss 12.435
2022-02-28 14:53:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 500 updates
2022-02-28 14:53:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-28 14:53:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-28 14:53:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt (epoch 17 @ 500 updates, score 12.435) (writing took 12.012368890980724 seconds)
2022-02-28 14:53:45 | INFO | train_inner | epoch 017:     26 / 31 loss=12.521, ppl=5879.5, wps=1817.9, ups=1.38, wpb=1319, bsz=63.2, num_updates=520, lr=1.56e-06, gnorm=13.085, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=153
2022-02-28 14:53:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:53:46 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.202 | ppl 4710.16 | wps 30002.3 | wpb 591.2 | bsz 29.9 | num_updates 525 | best_loss 12.202
2022-02-28 14:53:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 525 updates
2022-02-28 14:53:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 525 updates, score 12.202) (writing took 5.210123289027251 seconds)
2022-02-28 14:53:51 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-28 14:53:51 | INFO | train | epoch 017 | loss 12.457 | ppl 5622.64 | wps 1790.8 | ups 1.46 | wpb 1227.6 | bsz 61.6 | num_updates 525 | lr 1.575e-06 | gnorm 14.041 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 159
2022-02-28 14:53:51 | INFO | fairseq.trainer | begin training epoch 18
2022-02-28 14:53:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:53:52 | INFO | train_inner | epoch 018:     15 / 31 loss=12.301, ppl=5044.79, wps=3494.7, ups=2.69, wpb=1297.2, bsz=61.1, num_updates=540, lr=1.62e-06, gnorm=13.24, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=161
2022-02-28 14:53:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:53:54 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 12.124 | ppl 4463.44 | wps 30061.3 | wpb 591.2 | bsz 29.9 | num_updates 556 | best_loss 12.124
2022-02-28 14:53:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 556 updates
2022-02-28 14:53:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:53:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 556 updates, score 12.124) (writing took 5.298862287949305 seconds)
2022-02-28 14:53:59 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-28 14:53:59 | INFO | train | epoch 018 | loss 12.246 | ppl 4857.89 | wps 4669.4 | ups 3.8 | wpb 1227.6 | bsz 61.6 | num_updates 556 | lr 1.668e-06 | gnorm 12.715 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 168
2022-02-28 14:53:59 | INFO | fairseq.trainer | begin training epoch 19
2022-02-28 14:53:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:54:00 | INFO | train_inner | epoch 019:      4 / 31 loss=12.212, ppl=4744.04, wps=2774.6, ups=2.71, wpb=1023.3, bsz=61.6, num_updates=560, lr=1.68e-06, gnorm=12.843, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=168
2022-02-28 14:54:01 | INFO | train_inner | epoch 019:     24 / 31 loss=12.059, ppl=4267.42, wps=19206.9, ups=14.79, wpb=1298.8, bsz=63.2, num_updates=580, lr=1.74e-06, gnorm=11.984, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=169
2022-02-28 14:54:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:54:02 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.91 | ppl 3847.37 | wps 30889.3 | wpb 591.2 | bsz 29.9 | num_updates 587 | best_loss 11.91
2022-02-28 14:54:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 587 updates
2022-02-28 14:54:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 587 updates, score 11.91) (writing took 6.069035083986819 seconds)
2022-02-28 14:54:08 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-28 14:54:08 | INFO | train | epoch 019 | loss 12.077 | ppl 4321.18 | wps 4342.5 | ups 3.54 | wpb 1227.6 | bsz 61.6 | num_updates 587 | lr 1.761e-06 | gnorm 12.187 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 176
2022-02-28 14:54:08 | INFO | fairseq.trainer | begin training epoch 20
2022-02-28 14:54:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:54:09 | INFO | train_inner | epoch 020:     13 / 31 loss=12.034, ppl=4194.47, wps=3088.7, ups=2.5, wpb=1235, bsz=61.1, num_updates=600, lr=1.8e-06, gnorm=11.972, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=177
2022-02-28 14:54:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:54:11 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.623 | ppl 3154.84 | wps 31044.8 | wpb 591.2 | bsz 29.9 | num_updates 618 | best_loss 11.623
2022-02-28 14:54:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 618 updates
2022-02-28 14:54:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 618 updates, score 11.623) (writing took 5.419645450019743 seconds)
2022-02-28 14:54:16 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-28 14:54:16 | INFO | train | epoch 020 | loss 11.917 | ppl 3866.95 | wps 4752.3 | ups 3.87 | wpb 1227.6 | bsz 61.6 | num_updates 618 | lr 1.854e-06 | gnorm 11.963 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 184
2022-02-28 14:54:16 | INFO | fairseq.trainer | begin training epoch 21
2022-02-28 14:54:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:54:17 | INFO | train_inner | epoch 021:      2 / 31 loss=11.812, ppl=3596.7, wps=3202.6, ups=2.71, wpb=1183, bsz=60.3, num_updates=620, lr=1.86e-06, gnorm=12.006, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=185
2022-02-28 14:54:18 | INFO | train_inner | epoch 021:     22 / 31 loss=11.744, ppl=3429.55, wps=17415.8, ups=14.2, wpb=1226.5, bsz=62.7, num_updates=640, lr=1.92e-06, gnorm=11.093, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=186
2022-02-28 14:54:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:54:19 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.585 | ppl 3072.33 | wps 30121.4 | wpb 591.2 | bsz 29.9 | num_updates 649 | best_loss 11.585
2022-02-28 14:54:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 649 updates
2022-02-28 14:54:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 649 updates, score 11.585) (writing took 5.861216417979449 seconds)
2022-02-28 14:54:25 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-28 14:54:25 | INFO | train | epoch 021 | loss 11.721 | ppl 3376.38 | wps 4334.5 | ups 3.53 | wpb 1227.6 | bsz 61.6 | num_updates 649 | lr 1.947e-06 | gnorm 11.118 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 193
2022-02-28 14:54:25 | INFO | fairseq.trainer | begin training epoch 22
2022-02-28 14:54:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:54:26 | INFO | train_inner | epoch 022:     11 / 31 loss=11.643, ppl=3197.66, wps=3047.1, ups=2.54, wpb=1198, bsz=61.6, num_updates=660, lr=1.98e-06, gnorm=11.795, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=194
2022-02-28 14:54:27 | INFO | train_inner | epoch 022:     31 / 31 loss=11.666, ppl=3249.57, wps=17755.6, ups=14.13, wpb=1256.2, bsz=60.3, num_updates=680, lr=2.04e-06, gnorm=10.865, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=195
2022-02-28 14:54:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:54:28 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.425 | ppl 2750.37 | wps 28651.9 | wpb 591.2 | bsz 29.9 | num_updates 680 | best_loss 11.425
2022-02-28 14:54:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 680 updates
2022-02-28 14:54:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 680 updates, score 11.425) (writing took 5.12428104999708 seconds)
2022-02-28 14:54:33 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-28 14:54:33 | INFO | train | epoch 022 | loss 11.652 | ppl 3218.33 | wps 4831.7 | ups 3.94 | wpb 1227.6 | bsz 61.6 | num_updates 680 | lr 2.04e-06 | gnorm 11.308 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 201
2022-02-28 14:54:33 | INFO | fairseq.trainer | begin training epoch 23
2022-02-28 14:54:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:54:34 | INFO | train_inner | epoch 023:     20 / 31 loss=11.503, ppl=2901.78, wps=3534.7, ups=2.82, wpb=1255.1, bsz=61.9, num_updates=700, lr=2.1e-06, gnorm=10.673, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=203
2022-02-28 14:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:54:36 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.375 | ppl 2655.65 | wps 31804.5 | wpb 591.2 | bsz 29.9 | num_updates 711 | best_loss 11.375
2022-02-28 14:54:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 711 updates
2022-02-28 14:54:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 711 updates, score 11.375) (writing took 4.292496284993831 seconds)
2022-02-28 14:54:40 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-28 14:54:40 | INFO | train | epoch 023 | loss 11.502 | ppl 2900.95 | wps 5453 | ups 4.44 | wpb 1227.6 | bsz 61.6 | num_updates 711 | lr 2.133e-06 | gnorm 10.801 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 208
2022-02-28 14:54:40 | INFO | fairseq.trainer | begin training epoch 24
2022-02-28 14:54:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:54:41 | INFO | train_inner | epoch 024:      9 / 31 loss=11.495, ppl=2885.93, wps=4025.9, ups=3.22, wpb=1250.5, bsz=62.4, num_updates=720, lr=2.16e-06, gnorm=10.702, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=209
2022-02-28 14:54:42 | INFO | train_inner | epoch 024:     29 / 31 loss=11.405, ppl=2710.84, wps=16981.4, ups=13.83, wpb=1227.5, bsz=61.9, num_updates=740, lr=2.22e-06, gnorm=10.65, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=210
2022-02-28 14:54:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:54:43 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.158 | ppl 2284.93 | wps 30480.4 | wpb 591.2 | bsz 29.9 | num_updates 742 | best_loss 11.158
2022-02-28 14:54:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 742 updates
2022-02-28 14:54:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 742 updates, score 11.158) (writing took 4.9592513789539225 seconds)
2022-02-28 14:54:48 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-28 14:54:48 | INFO | train | epoch 024 | loss 11.395 | ppl 2692.6 | wps 4909.2 | ups 4 | wpb 1227.6 | bsz 61.6 | num_updates 742 | lr 2.226e-06 | gnorm 10.728 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 216
2022-02-28 14:54:48 | INFO | fairseq.trainer | begin training epoch 25
2022-02-28 14:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:54:49 | INFO | train_inner | epoch 025:     18 / 31 loss=11.151, ppl=2274.27, wps=3564.9, ups=2.9, wpb=1231.2, bsz=61.6, num_updates=760, lr=2.28e-06, gnorm=10.497, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=217
2022-02-28 14:54:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:54:50 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.088 | ppl 2176.13 | wps 30067.1 | wpb 591.2 | bsz 29.9 | num_updates 773 | best_loss 11.088
2022-02-28 14:54:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 773 updates
2022-02-28 14:54:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:54:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 773 updates, score 11.088) (writing took 4.403779610991478 seconds)
2022-02-28 14:54:55 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-28 14:54:55 | INFO | train | epoch 025 | loss 11.226 | ppl 2394.67 | wps 5340.9 | ups 4.35 | wpb 1227.6 | bsz 61.6 | num_updates 773 | lr 2.319e-06 | gnorm 10.1 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 223
2022-02-28 14:54:55 | INFO | fairseq.trainer | begin training epoch 26
2022-02-28 14:54:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:54:55 | INFO | train_inner | epoch 026:      7 / 31 loss=11.252, ppl=2438.9, wps=3592.9, ups=3.13, wpb=1146.8, bsz=59.8, num_updates=780, lr=2.34e-06, gnorm=10.522, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=224
2022-02-28 14:54:57 | INFO | train_inner | epoch 026:     27 / 31 loss=11.125, ppl=2232.91, wps=19740.3, ups=14.98, wpb=1317.7, bsz=63.2, num_updates=800, lr=2.4e-06, gnorm=9.936, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=225
2022-02-28 14:54:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:54:57 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 10.919 | ppl 1935.83 | wps 28732.4 | wpb 591.2 | bsz 29.9 | num_updates 804 | best_loss 10.919
2022-02-28 14:54:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 804 updates
2022-02-28 14:54:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 804 updates, score 10.919) (writing took 4.209851627005264 seconds)
2022-02-28 14:55:02 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-28 14:55:02 | INFO | train | epoch 026 | loss 11.168 | ppl 2300.33 | wps 5555.5 | ups 4.53 | wpb 1227.6 | bsz 61.6 | num_updates 804 | lr 2.412e-06 | gnorm 10.362 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 230
2022-02-28 14:55:02 | INFO | fairseq.trainer | begin training epoch 27
2022-02-28 14:55:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:55:03 | INFO | train_inner | epoch 027:     16 / 31 loss=11.169, ppl=2302.88, wps=3946, ups=3.23, wpb=1221, bsz=61.1, num_updates=820, lr=2.46e-06, gnorm=9.954, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=231
2022-02-28 14:55:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:55:04 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.806 | ppl 1790.59 | wps 27258.4 | wpb 591.2 | bsz 29.9 | num_updates 835 | best_loss 10.806
2022-02-28 14:55:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 835 updates
2022-02-28 14:55:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 835 updates, score 10.806) (writing took 3.990397567045875 seconds)
2022-02-28 14:55:08 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-28 14:55:08 | INFO | train | epoch 027 | loss 11.082 | ppl 2167.61 | wps 5608.4 | ups 4.57 | wpb 1227.6 | bsz 61.6 | num_updates 835 | lr 2.505e-06 | gnorm 10.002 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 237
2022-02-28 14:55:08 | INFO | fairseq.trainer | begin training epoch 28
2022-02-28 14:55:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:55:09 | INFO | train_inner | epoch 028:      5 / 31 loss=11.003, ppl=2052.15, wps=3751.6, ups=3.34, wpb=1122.6, bsz=60.8, num_updates=840, lr=2.52e-06, gnorm=10.147, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=237
2022-02-28 14:55:11 | INFO | train_inner | epoch 028:     25 / 31 loss=10.889, ppl=1896.19, wps=13776.9, ups=11.34, wpb=1214.5, bsz=62.7, num_updates=860, lr=2.58e-06, gnorm=10.387, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=239
2022-02-28 14:55:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:55:12 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.734 | ppl 1703.65 | wps 25957.1 | wpb 591.2 | bsz 29.9 | num_updates 866 | best_loss 10.734
2022-02-28 14:55:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 866 updates
2022-02-28 14:55:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 866 updates, score 10.734) (writing took 4.708166319003794 seconds)
2022-02-28 14:55:16 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-28 14:55:16 | INFO | train | epoch 028 | loss 10.902 | ppl 1913.74 | wps 4813.7 | ups 3.92 | wpb 1227.6 | bsz 61.6 | num_updates 866 | lr 2.598e-06 | gnorm 10.149 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 245
2022-02-28 14:55:16 | INFO | fairseq.trainer | begin training epoch 29
2022-02-28 14:55:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:55:17 | INFO | train_inner | epoch 029:     14 / 31 loss=10.951, ppl=1979.24, wps=3632.5, ups=2.96, wpb=1228.5, bsz=60.3, num_updates=880, lr=2.64e-06, gnorm=9.529, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=246
2022-02-28 14:55:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:55:19 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.669 | ppl 1627.72 | wps 29643.1 | wpb 591.2 | bsz 29.9 | num_updates 897 | best_loss 10.669
2022-02-28 14:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 897 updates
2022-02-28 14:55:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 897 updates, score 10.669) (writing took 4.044298931024969 seconds)
2022-02-28 14:55:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-28 14:55:23 | INFO | train | epoch 029 | loss 10.881 | ppl 1886.09 | wps 5680.6 | ups 4.63 | wpb 1227.6 | bsz 61.6 | num_updates 897 | lr 2.691e-06 | gnorm 9.502 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 251
2022-02-28 14:55:23 | INFO | fairseq.trainer | begin training epoch 30
2022-02-28 14:55:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:55:23 | INFO | train_inner | epoch 030:      3 / 31 loss=10.792, ppl=1772.93, wps=4384.8, ups=3.36, wpb=1303.6, bsz=62.4, num_updates=900, lr=2.7e-06, gnorm=9.456, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=252
2022-02-28 14:55:25 | INFO | train_inner | epoch 030:     23 / 31 loss=10.757, ppl=1730.08, wps=17008.4, ups=14.53, wpb=1170.5, bsz=61.9, num_updates=920, lr=2.76e-06, gnorm=9.407, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=253
2022-02-28 14:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:55:26 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.698 | ppl 1661.24 | wps 27476.3 | wpb 591.2 | bsz 29.9 | num_updates 928 | best_loss 10.669
2022-02-28 14:55:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 928 updates
2022-02-28 14:55:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:55:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:55:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 30 @ 928 updates, score 10.698) (writing took 2.675685999041889 seconds)
2022-02-28 14:55:28 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-28 14:55:28 | INFO | train | epoch 030 | loss 10.771 | ppl 1747.22 | wps 7001.5 | ups 5.7 | wpb 1227.6 | bsz 61.6 | num_updates 928 | lr 2.784e-06 | gnorm 9.357 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 257
2022-02-28 14:55:28 | INFO | fairseq.trainer | begin training epoch 31
2022-02-28 14:55:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:55:29 | INFO | train_inner | epoch 031:     12 / 31 loss=10.769, ppl=1745.52, wps=5463.6, ups=4.29, wpb=1273.7, bsz=62.4, num_updates=940, lr=2.82e-06, gnorm=9.045, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=258
2022-02-28 14:55:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:55:31 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.494 | ppl 1442.18 | wps 25937.7 | wpb 591.2 | bsz 29.9 | num_updates 959 | best_loss 10.494
2022-02-28 14:55:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 959 updates
2022-02-28 14:55:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 959 updates, score 10.494) (writing took 4.0344654389773495 seconds)
2022-02-28 14:55:35 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-28 14:55:35 | INFO | train | epoch 031 | loss 10.676 | ppl 1636.57 | wps 5601.9 | ups 4.56 | wpb 1227.6 | bsz 61.6 | num_updates 959 | lr 2.877e-06 | gnorm 9.344 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 263
2022-02-28 14:55:35 | INFO | fairseq.trainer | begin training epoch 32
2022-02-28 14:55:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:55:35 | INFO | train_inner | epoch 032:      1 / 31 loss=10.645, ppl=1601.31, wps=4024.2, ups=3.29, wpb=1222.8, bsz=60.3, num_updates=960, lr=2.88e-06, gnorm=9.598, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=264
2022-02-28 14:55:37 | INFO | train_inner | epoch 032:     21 / 31 loss=10.541, ppl=1490.16, wps=17859.6, ups=14.47, wpb=1234, bsz=63.2, num_updates=980, lr=2.94e-06, gnorm=9.085, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=265
2022-02-28 14:55:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:55:38 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 10.515 | ppl 1463.44 | wps 27627.5 | wpb 591.2 | bsz 29.9 | num_updates 990 | best_loss 10.494
2022-02-28 14:55:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 990 updates
2022-02-28 14:55:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:55:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:55:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 32 @ 990 updates, score 10.515) (writing took 2.7741036390070803 seconds)
2022-02-28 14:55:41 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-28 14:55:41 | INFO | train | epoch 032 | loss 10.561 | ppl 1510.8 | wps 6918.5 | ups 5.64 | wpb 1227.6 | bsz 61.6 | num_updates 990 | lr 2.97e-06 | gnorm 9.211 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 269
2022-02-28 14:55:41 | INFO | fairseq.trainer | begin training epoch 33
2022-02-28 14:55:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:55:42 | INFO | train_inner | epoch 033:     10 / 31 loss=10.687, ppl=1648.54, wps=5294.7, ups=4.19, wpb=1262.8, bsz=59, num_updates=1000, lr=3e-06, gnorm=9.105, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=270
2022-02-28 14:55:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:55:42 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.47 | ppl 1418.4 | wps 30826.6 | wpb 591.2 | bsz 29.9 | num_updates 1000 | best_loss 10.47
2022-02-28 14:55:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1000 updates
2022-02-28 14:55:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-28 14:55:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-28 14:55:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt (epoch 33 @ 1000 updates, score 10.47) (writing took 10.793660284020007 seconds)
2022-02-28 14:55:54 | INFO | train_inner | epoch 033:     30 / 31 loss=10.403, ppl=1354.36, wps=1863.6, ups=1.56, wpb=1198.3, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=9.284, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=283
2022-02-28 14:55:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:55:55 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.417 | ppl 1367.28 | wps 29105.5 | wpb 591.2 | bsz 29.9 | num_updates 1021 | best_loss 10.417
2022-02-28 14:55:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1021 updates
2022-02-28 14:55:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:55:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1021 updates, score 10.417) (writing took 6.071706201997586 seconds)
2022-02-28 14:56:02 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-28 14:56:02 | INFO | train | epoch 033 | loss 10.531 | ppl 1479.71 | wps 1810.2 | ups 1.47 | wpb 1227.6 | bsz 61.6 | num_updates 1021 | lr 3.063e-06 | gnorm 9.191 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 290
2022-02-28 14:56:02 | INFO | fairseq.trainer | begin training epoch 34
2022-02-28 14:56:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:56:03 | INFO | train_inner | epoch 034:     19 / 31 loss=10.476, ppl=1424.38, wps=2765.9, ups=2.28, wpb=1211.8, bsz=62.4, num_updates=1040, lr=3.12e-06, gnorm=9.515, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=291
2022-02-28 14:56:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:56:05 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 10.389 | ppl 1340.71 | wps 30478.2 | wpb 591.2 | bsz 29.9 | num_updates 1052 | best_loss 10.389
2022-02-28 14:56:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1052 updates
2022-02-28 14:56:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1052 updates, score 10.389) (writing took 4.809669237991329 seconds)
2022-02-28 14:56:09 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-28 14:56:09 | INFO | train | epoch 034 | loss 10.439 | ppl 1388.49 | wps 5056.3 | ups 4.12 | wpb 1227.6 | bsz 61.6 | num_updates 1052 | lr 3.156e-06 | gnorm 9.383 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 298
2022-02-28 14:56:09 | INFO | fairseq.trainer | begin training epoch 35
2022-02-28 14:56:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:56:10 | INFO | train_inner | epoch 035:      8 / 31 loss=10.444, ppl=1392.76, wps=3635.7, ups=2.96, wpb=1227.2, bsz=60.3, num_updates=1060, lr=3.18e-06, gnorm=9.007, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=298
2022-02-28 14:56:11 | INFO | train_inner | epoch 035:     28 / 31 loss=10.33, ppl=1287.38, wps=18613.8, ups=14.36, wpb=1296, bsz=61.9, num_updates=1080, lr=3.24e-06, gnorm=9.196, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=300
2022-02-28 14:56:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:56:12 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 10.208 | ppl 1182.94 | wps 29274.7 | wpb 591.2 | bsz 29.9 | num_updates 1083 | best_loss 10.208
2022-02-28 14:56:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1083 updates
2022-02-28 14:56:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1083 updates, score 10.208) (writing took 5.340076378954109 seconds)
2022-02-28 14:56:17 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-28 14:56:18 | INFO | train | epoch 035 | loss 10.378 | ppl 1330.5 | wps 4717.6 | ups 3.84 | wpb 1227.6 | bsz 61.6 | num_updates 1083 | lr 3.249e-06 | gnorm 9.121 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 306
2022-02-28 14:56:18 | INFO | fairseq.trainer | begin training epoch 36
2022-02-28 14:56:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:56:19 | INFO | train_inner | epoch 036:     17 / 31 loss=10.343, ppl=1299.26, wps=3346.3, ups=2.64, wpb=1265.6, bsz=61.1, num_updates=1100, lr=3.3e-06, gnorm=9.032, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=307
2022-02-28 14:56:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:56:20 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 10.138 | ppl 1126.96 | wps 30275.7 | wpb 591.2 | bsz 29.9 | num_updates 1114 | best_loss 10.138
2022-02-28 14:56:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1114 updates
2022-02-28 14:56:20 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1114 updates, score 10.138) (writing took 4.727652622037567 seconds)
2022-02-28 14:56:25 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-28 14:56:25 | INFO | train | epoch 036 | loss 10.269 | ppl 1233.67 | wps 5138.1 | ups 4.19 | wpb 1227.6 | bsz 61.6 | num_updates 1114 | lr 3.342e-06 | gnorm 9.116 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 313
2022-02-28 14:56:25 | INFO | fairseq.trainer | begin training epoch 37
2022-02-28 14:56:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:56:26 | INFO | train_inner | epoch 037:      6 / 31 loss=10.163, ppl=1146.21, wps=3391, ups=2.99, wpb=1132.5, bsz=61.6, num_updates=1120, lr=3.36e-06, gnorm=9.07, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=314
2022-02-28 14:56:27 | INFO | train_inner | epoch 037:     26 / 31 loss=10.241, ppl=1210.11, wps=16632.8, ups=13.79, wpb=1205.9, bsz=61.9, num_updates=1140, lr=3.42e-06, gnorm=9.183, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=315
2022-02-28 14:56:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:56:28 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.992 | ppl 1018.33 | wps 30581.2 | wpb 591.2 | bsz 29.9 | num_updates 1145 | best_loss 9.992
2022-02-28 14:56:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1145 updates
2022-02-28 14:56:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1145 updates, score 9.992) (writing took 6.81902022700524 seconds)
2022-02-28 14:56:35 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-28 14:56:35 | INFO | train | epoch 037 | loss 10.193 | ppl 1170.29 | wps 3928.8 | ups 3.2 | wpb 1227.6 | bsz 61.6 | num_updates 1145 | lr 3.435e-06 | gnorm 8.928 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 323
2022-02-28 14:56:35 | INFO | fairseq.trainer | begin training epoch 38
2022-02-28 14:56:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:56:36 | INFO | train_inner | epoch 038:     15 / 31 loss=10.11, ppl=1105.4, wps=2667.4, ups=2.26, wpb=1181, bsz=62.4, num_updates=1160, lr=3.48e-06, gnorm=8.913, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=324
2022-02-28 14:56:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:56:38 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 10.073 | ppl 1077.11 | wps 30179.9 | wpb 591.2 | bsz 29.9 | num_updates 1176 | best_loss 9.992
2022-02-28 14:56:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1176 updates
2022-02-28 14:56:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:56:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:56:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 38 @ 1176 updates, score 10.073) (writing took 2.6080262990435585 seconds)
2022-02-28 14:56:40 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-28 14:56:40 | INFO | train | epoch 038 | loss 10.127 | ppl 1117.98 | wps 7013.9 | ups 5.71 | wpb 1227.6 | bsz 61.6 | num_updates 1176 | lr 3.528e-06 | gnorm 9.013 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 329
2022-02-28 14:56:40 | INFO | fairseq.trainer | begin training epoch 39
2022-02-28 14:56:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:56:41 | INFO | train_inner | epoch 039:      4 / 31 loss=10.076, ppl=1079.36, wps=5577.5, ups=4.31, wpb=1293.3, bsz=60.3, num_updates=1180, lr=3.54e-06, gnorm=8.908, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=329
2022-02-28 14:56:42 | INFO | train_inner | epoch 039:     24 / 31 loss=10.06, ppl=1067.31, wps=17997.6, ups=14.09, wpb=1277.4, bsz=62.7, num_updates=1200, lr=3.6e-06, gnorm=9.015, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=330
2022-02-28 14:56:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:56:43 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.902 | ppl 956.82 | wps 29139.1 | wpb 591.2 | bsz 29.9 | num_updates 1207 | best_loss 9.902
2022-02-28 14:56:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1207 updates
2022-02-28 14:56:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1207 updates, score 9.902) (writing took 4.637657143990509 seconds)
2022-02-28 14:56:48 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-28 14:56:48 | INFO | train | epoch 039 | loss 10.033 | ppl 1047.59 | wps 5153 | ups 4.2 | wpb 1227.6 | bsz 61.6 | num_updates 1207 | lr 3.621e-06 | gnorm 8.955 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 336
2022-02-28 14:56:48 | INFO | fairseq.trainer | begin training epoch 40
2022-02-28 14:56:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:56:49 | INFO | train_inner | epoch 040:     13 / 31 loss=9.87, ppl=936.04, wps=3306.9, ups=3.04, wpb=1086.4, bsz=60.8, num_updates=1220, lr=3.66e-06, gnorm=9.091, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=337
2022-02-28 14:56:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:56:50 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.756 | ppl 864.86 | wps 31765.6 | wpb 591.2 | bsz 29.9 | num_updates 1238 | best_loss 9.756
2022-02-28 14:56:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1238 updates
2022-02-28 14:56:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:56:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1238 updates, score 9.756) (writing took 7.444089540047571 seconds)
2022-02-28 14:56:58 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-28 14:56:58 | INFO | train | epoch 040 | loss 9.93 | ppl 975.79 | wps 3792.7 | ups 3.09 | wpb 1227.6 | bsz 61.6 | num_updates 1238 | lr 3.714e-06 | gnorm 9.124 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 346
2022-02-28 14:56:58 | INFO | fairseq.trainer | begin training epoch 41
2022-02-28 14:56:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:56:58 | INFO | train_inner | epoch 041:      2 / 31 loss=10.049, ppl=1059.07, wps=2770.1, ups=2.15, wpb=1288, bsz=61.1, num_updates=1240, lr=3.72e-06, gnorm=9.202, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=346
2022-02-28 14:56:59 | INFO | train_inner | epoch 041:     22 / 31 loss=9.796, ppl=889.11, wps=19667, ups=14.77, wpb=1331.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=8.936, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=348
2022-02-28 14:57:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:57:00 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.784 | ppl 881.81 | wps 30273.4 | wpb 591.2 | bsz 29.9 | num_updates 1269 | best_loss 9.756
2022-02-28 14:57:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1269 updates
2022-02-28 14:57:00 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:57:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:57:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 1269 updates, score 9.784) (writing took 2.644978569005616 seconds)
2022-02-28 14:57:03 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-28 14:57:03 | INFO | train | epoch 041 | loss 9.862 | ppl 930.8 | wps 7177.9 | ups 5.85 | wpb 1227.6 | bsz 61.6 | num_updates 1269 | lr 3.807e-06 | gnorm 9.01 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 351
2022-02-28 14:57:03 | INFO | fairseq.trainer | begin training epoch 42
2022-02-28 14:57:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:57:04 | INFO | train_inner | epoch 042:     11 / 31 loss=9.887, ppl=946.53, wps=5129.8, ups=4.4, wpb=1164.5, bsz=61.6, num_updates=1280, lr=3.84e-06, gnorm=8.977, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=352
2022-02-28 14:57:05 | INFO | train_inner | epoch 042:     31 / 31 loss=9.803, ppl=893.37, wps=17907.2, ups=14.74, wpb=1214.5, bsz=60.3, num_updates=1300, lr=3.9e-06, gnorm=9.284, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=353
2022-02-28 14:57:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:57:06 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.689 | ppl 825.15 | wps 28831.4 | wpb 591.2 | bsz 29.9 | num_updates 1300 | best_loss 9.689
2022-02-28 14:57:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1300 updates
2022-02-28 14:57:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:57:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:57:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 1300 updates, score 9.689) (writing took 4.842917161993682 seconds)
2022-02-28 14:57:11 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-02-28 14:57:11 | INFO | train | epoch 042 | loss 9.819 | ppl 903.04 | wps 5063.2 | ups 4.12 | wpb 1227.6 | bsz 61.6 | num_updates 1300 | lr 3.9e-06 | gnorm 9.124 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 359
2022-02-28 14:57:11 | INFO | fairseq.trainer | begin training epoch 43
2022-02-28 14:57:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:57:12 | INFO | train_inner | epoch 043:     20 / 31 loss=9.725, ppl=846.29, wps=3756.3, ups=2.93, wpb=1280.2, bsz=61.9, num_updates=1320, lr=3.96e-06, gnorm=8.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=360
2022-02-28 14:57:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:57:13 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.675 | ppl 817.41 | wps 29647.3 | wpb 591.2 | bsz 29.9 | num_updates 1331 | best_loss 9.675
2022-02-28 14:57:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1331 updates
2022-02-28 14:57:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:57:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:57:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 43 @ 1331 updates, score 9.675) (writing took 5.870593187049963 seconds)
2022-02-28 14:57:19 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-02-28 14:57:19 | INFO | train | epoch 043 | loss 9.726 | ppl 847.12 | wps 4449.9 | ups 3.62 | wpb 1227.6 | bsz 61.6 | num_updates 1331 | lr 3.993e-06 | gnorm 8.799 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 367
2022-02-28 14:57:19 | INFO | fairseq.trainer | begin training epoch 44
2022-02-28 14:57:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:57:20 | INFO | train_inner | epoch 044:      9 / 31 loss=9.695, ppl=828.75, wps=3035.3, ups=2.56, wpb=1184.5, bsz=62.4, num_updates=1340, lr=4.02e-06, gnorm=9.112, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=368
2022-02-28 14:57:21 | INFO | train_inner | epoch 044:     29 / 31 loss=9.647, ppl=801.5, wps=17744.1, ups=14.52, wpb=1221.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=8.704, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=369
2022-02-28 14:57:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:57:22 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.64 | ppl 797.72 | wps 28994.3 | wpb 591.2 | bsz 29.9 | num_updates 1362 | best_loss 9.64
2022-02-28 14:57:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1362 updates
2022-02-28 14:57:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:57:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:57:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 1362 updates, score 9.64) (writing took 5.05449835798936 seconds)
2022-02-28 14:57:27 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-02-28 14:57:27 | INFO | train | epoch 044 | loss 9.671 | ppl 815.16 | wps 4895.3 | ups 3.99 | wpb 1227.6 | bsz 61.6 | num_updates 1362 | lr 4.086e-06 | gnorm 8.989 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 375
2022-02-28 14:57:27 | INFO | fairseq.trainer | begin training epoch 45
2022-02-28 14:57:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:57:28 | INFO | train_inner | epoch 045:     18 / 31 loss=9.59, ppl=770.78, wps=3410.2, ups=2.85, wpb=1195.7, bsz=60.3, num_updates=1380, lr=4.14e-06, gnorm=8.715, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=376
2022-02-28 14:57:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:57:30 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.543 | ppl 745.93 | wps 29101.7 | wpb 591.2 | bsz 29.9 | num_updates 1393 | best_loss 9.543
2022-02-28 14:57:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1393 updates
2022-02-28 14:57:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:57:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:57:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 1393 updates, score 9.543) (writing took 5.273731762019452 seconds)
2022-02-28 14:57:35 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-02-28 14:57:35 | INFO | train | epoch 045 | loss 9.513 | ppl 730.45 | wps 4767.2 | ups 3.88 | wpb 1227.6 | bsz 61.6 | num_updates 1393 | lr 4.179e-06 | gnorm 8.849 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 383
2022-02-28 14:57:35 | INFO | fairseq.trainer | begin training epoch 46
2022-02-28 14:57:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:57:35 | INFO | train_inner | epoch 046:      7 / 31 loss=9.469, ppl=708.63, wps=3464.5, ups=2.75, wpb=1258.5, bsz=61.6, num_updates=1400, lr=4.2e-06, gnorm=9.028, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=384
2022-02-28 14:57:37 | INFO | train_inner | epoch 046:     27 / 31 loss=9.442, ppl=695.5, wps=17986.4, ups=14.75, wpb=1219.4, bsz=61.9, num_updates=1420, lr=4.26e-06, gnorm=8.881, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=385
2022-02-28 14:57:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:57:38 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.593 | ppl 772.24 | wps 29179.7 | wpb 591.2 | bsz 29.9 | num_updates 1424 | best_loss 9.543
2022-02-28 14:57:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1424 updates
2022-02-28 14:57:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:57:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:57:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 46 @ 1424 updates, score 9.593) (writing took 2.8270620619878173 seconds)
2022-02-28 14:57:40 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-02-28 14:57:40 | INFO | train | epoch 046 | loss 9.47 | ppl 709.23 | wps 6855.1 | ups 5.58 | wpb 1227.6 | bsz 61.6 | num_updates 1424 | lr 4.272e-06 | gnorm 8.818 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 389
2022-02-28 14:57:40 | INFO | fairseq.trainer | begin training epoch 47
2022-02-28 14:57:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:57:42 | INFO | train_inner | epoch 047:     16 / 31 loss=9.532, ppl=740.17, wps=5061.4, ups=4.2, wpb=1206, bsz=61.6, num_updates=1440, lr=4.32e-06, gnorm=8.812, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=390
2022-02-28 14:57:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:57:43 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.386 | ppl 668.95 | wps 29772.4 | wpb 591.2 | bsz 29.9 | num_updates 1455 | best_loss 9.386
2022-02-28 14:57:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1455 updates
2022-02-28 14:57:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:57:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:57:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 1455 updates, score 9.386) (writing took 4.176890133006964 seconds)
2022-02-28 14:57:47 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-02-28 14:57:47 | INFO | train | epoch 047 | loss 9.415 | ppl 682.42 | wps 5496.2 | ups 4.48 | wpb 1227.6 | bsz 61.6 | num_updates 1455 | lr 4.365e-06 | gnorm 9.045 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 396
2022-02-28 14:57:47 | INFO | fairseq.trainer | begin training epoch 48
2022-02-28 14:57:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:57:48 | INFO | train_inner | epoch 048:      5 / 31 loss=9.387, ppl=669.75, wps=4041.5, ups=3.25, wpb=1244.2, bsz=59.8, num_updates=1460, lr=4.38e-06, gnorm=9.192, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=396
2022-02-28 14:57:49 | INFO | train_inner | epoch 048:     25 / 31 loss=9.245, ppl=606.97, wps=18763.5, ups=14.6, wpb=1285.3, bsz=63.2, num_updates=1480, lr=4.44e-06, gnorm=8.764, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=397
2022-02-28 14:57:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:57:50 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.425 | ppl 687.47 | wps 29803.8 | wpb 591.2 | bsz 29.9 | num_updates 1486 | best_loss 9.386
2022-02-28 14:57:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1486 updates
2022-02-28 14:57:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:57:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:57:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 48 @ 1486 updates, score 9.425) (writing took 2.703687174944207 seconds)
2022-02-28 14:57:53 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-02-28 14:57:53 | INFO | train | epoch 048 | loss 9.32 | ppl 638.94 | wps 7088.1 | ups 5.77 | wpb 1227.6 | bsz 61.6 | num_updates 1486 | lr 4.458e-06 | gnorm 8.837 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 401
2022-02-28 14:57:53 | INFO | fairseq.trainer | begin training epoch 49
2022-02-28 14:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:57:54 | INFO | train_inner | epoch 049:     14 / 31 loss=9.369, ppl=661.02, wps=5257.7, ups=4.15, wpb=1265.4, bsz=62.4, num_updates=1500, lr=4.5e-06, gnorm=8.554, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=402
2022-02-28 14:57:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:57:55 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.337 | ppl 646.58 | wps 26154.1 | wpb 591.2 | bsz 29.9 | num_updates 1500 | best_loss 9.337
2022-02-28 14:57:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1500 updates
2022-02-28 14:57:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-28 14:57:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-28 14:58:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt (epoch 49 @ 1500 updates, score 9.337) (writing took 33.13361250201706 seconds)
2022-02-28 14:58:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:58:29 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.283 | ppl 622.81 | wps 29086 | wpb 591.2 | bsz 29.9 | num_updates 1517 | best_loss 9.283
2022-02-28 14:58:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1517 updates
2022-02-28 14:58:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:58:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:58:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 1517 updates, score 9.283) (writing took 5.0545490449876525 seconds)
2022-02-28 14:58:35 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-02-28 14:58:35 | INFO | train | epoch 049 | loss 9.281 | ppl 622.22 | wps 909.6 | ups 0.74 | wpb 1227.6 | bsz 61.6 | num_updates 1517 | lr 4.551e-06 | gnorm 8.74 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 443
2022-02-28 14:58:35 | INFO | fairseq.trainer | begin training epoch 50
2022-02-28 14:58:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:58:35 | INFO | train_inner | epoch 050:      3 / 31 loss=9.103, ppl=550.03, wps=537.8, ups=0.49, wpb=1100.2, bsz=60.3, num_updates=1520, lr=4.56e-06, gnorm=9.078, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=443
2022-02-28 14:58:36 | INFO | train_inner | epoch 050:     23 / 31 loss=9.195, ppl=586.29, wps=19057.2, ups=15.19, wpb=1254.2, bsz=61.9, num_updates=1540, lr=4.62e-06, gnorm=8.838, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=444
2022-02-28 14:58:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:58:37 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.273 | ppl 618.79 | wps 29937.9 | wpb 591.2 | bsz 29.9 | num_updates 1548 | best_loss 9.273
2022-02-28 14:58:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1548 updates
2022-02-28 14:58:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:58:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:58:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 1548 updates, score 9.273) (writing took 6.711797930998728 seconds)
2022-02-28 14:58:44 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-02-28 14:58:44 | INFO | train | epoch 050 | loss 9.158 | ppl 571.45 | wps 4070.4 | ups 3.32 | wpb 1227.6 | bsz 61.6 | num_updates 1548 | lr 4.644e-06 | gnorm 8.807 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 452
2022-02-28 14:58:44 | INFO | fairseq.trainer | begin training epoch 51
2022-02-28 14:58:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:58:45 | INFO | train_inner | epoch 051:     12 / 31 loss=9.229, ppl=600.05, wps=2997.2, ups=2.31, wpb=1299.3, bsz=60.3, num_updates=1560, lr=4.68e-06, gnorm=8.878, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=453
2022-02-28 14:58:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:58:47 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.266 | ppl 615.6 | wps 30412.6 | wpb 591.2 | bsz 29.9 | num_updates 1579 | best_loss 9.266
2022-02-28 14:58:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1579 updates
2022-02-28 14:58:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:58:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:58:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 51 @ 1579 updates, score 9.266) (writing took 11.908004436001647 seconds)
2022-02-28 14:58:59 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-02-28 14:58:59 | INFO | train | epoch 051 | loss 9.095 | ppl 546.93 | wps 2592.4 | ups 2.11 | wpb 1227.6 | bsz 61.6 | num_updates 1579 | lr 4.737e-06 | gnorm 8.817 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 467
2022-02-28 14:58:59 | INFO | fairseq.trainer | begin training epoch 52
2022-02-28 14:58:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:58:59 | INFO | train_inner | epoch 052:      1 / 31 loss=9.02, ppl=518.97, wps=1647.4, ups=1.42, wpb=1157.5, bsz=62.4, num_updates=1580, lr=4.74e-06, gnorm=8.611, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=467
2022-02-28 14:59:00 | INFO | train_inner | epoch 052:     21 / 31 loss=9.15, ppl=567.93, wps=16362.4, ups=13.87, wpb=1179.5, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=8.545, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=469
2022-02-28 14:59:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:59:02 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.153 | ppl 569.11 | wps 31525.3 | wpb 591.2 | bsz 29.9 | num_updates 1610 | best_loss 9.153
2022-02-28 14:59:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1610 updates
2022-02-28 14:59:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:59:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:59:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 52 @ 1610 updates, score 9.153) (writing took 9.246192599006463 seconds)
2022-02-28 14:59:11 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-02-28 14:59:11 | INFO | train | epoch 052 | loss 9.072 | ppl 538.22 | wps 3137.8 | ups 2.56 | wpb 1227.6 | bsz 61.6 | num_updates 1610 | lr 4.83e-06 | gnorm 8.798 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 479
2022-02-28 14:59:11 | INFO | fairseq.trainer | begin training epoch 53
2022-02-28 14:59:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:59:12 | INFO | train_inner | epoch 053:     10 / 31 loss=9.017, ppl=518.02, wps=2224.9, ups=1.76, wpb=1267.7, bsz=59.5, num_updates=1620, lr=4.86e-06, gnorm=8.927, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=480
2022-02-28 14:59:13 | INFO | train_inner | epoch 053:     30 / 31 loss=8.93, ppl=487.9, wps=17611.7, ups=13.67, wpb=1288.1, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=9.251, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=481
2022-02-28 14:59:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:59:14 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.062 | ppl 534.65 | wps 30827.4 | wpb 591.2 | bsz 29.9 | num_updates 1641 | best_loss 9.062
2022-02-28 14:59:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1641 updates
2022-02-28 14:59:14 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:59:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:59:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 1641 updates, score 9.062) (writing took 18.142349532980006 seconds)
2022-02-28 14:59:32 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-02-28 14:59:32 | INFO | train | epoch 053 | loss 8.997 | ppl 510.96 | wps 1820.5 | ups 1.48 | wpb 1227.6 | bsz 61.6 | num_updates 1641 | lr 4.923e-06 | gnorm 9.064 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 500
2022-02-28 14:59:32 | INFO | fairseq.trainer | begin training epoch 54
2022-02-28 14:59:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:59:33 | INFO | train_inner | epoch 054:     19 / 31 loss=8.927, ppl=486.64, wps=1275.1, ups=1, wpb=1280.2, bsz=60.3, num_updates=1660, lr=4.98e-06, gnorm=8.733, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=502
2022-02-28 14:59:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:59:35 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.021 | ppl 519.61 | wps 30518.5 | wpb 591.2 | bsz 29.9 | num_updates 1672 | best_loss 9.021
2022-02-28 14:59:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1672 updates
2022-02-28 14:59:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:59:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:59:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 54 @ 1672 updates, score 9.021) (writing took 10.486139654007275 seconds)
2022-02-28 14:59:45 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-02-28 14:59:46 | INFO | train | epoch 054 | loss 8.932 | ppl 488.35 | wps 2882 | ups 2.35 | wpb 1227.6 | bsz 61.6 | num_updates 1672 | lr 5.016e-06 | gnorm 8.884 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 513
2022-02-28 14:59:46 | INFO | fairseq.trainer | begin training epoch 55
2022-02-28 14:59:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:59:46 | INFO | train_inner | epoch 055:      8 / 31 loss=8.925, ppl=486.21, wps=1885.8, ups=1.55, wpb=1220, bsz=62.4, num_updates=1680, lr=5.04e-06, gnorm=9.047, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=515
2022-02-28 14:59:48 | INFO | train_inner | epoch 055:     28 / 31 loss=8.826, ppl=453.82, wps=17121, ups=14.04, wpb=1219.8, bsz=61.9, num_updates=1700, lr=5.1e-06, gnorm=9.026, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=516
2022-02-28 14:59:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:59:48 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.051 | ppl 530.43 | wps 29162.4 | wpb 591.2 | bsz 29.9 | num_updates 1703 | best_loss 9.021
2022-02-28 14:59:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1703 updates
2022-02-28 14:59:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:59:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 14:59:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 55 @ 1703 updates, score 9.051) (writing took 2.935941413976252 seconds)
2022-02-28 14:59:51 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-02-28 14:59:51 | INFO | train | epoch 055 | loss 8.846 | ppl 460.26 | wps 6656.9 | ups 5.42 | wpb 1227.6 | bsz 61.6 | num_updates 1703 | lr 5.109e-06 | gnorm 9.023 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 520
2022-02-28 14:59:51 | INFO | fairseq.trainer | begin training epoch 56
2022-02-28 14:59:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 14:59:53 | INFO | train_inner | epoch 056:     17 / 31 loss=8.734, ppl=425.74, wps=4752.4, ups=4.08, wpb=1164.4, bsz=61.1, num_updates=1720, lr=5.16e-06, gnorm=8.862, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=521
2022-02-28 14:59:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 14:59:54 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.978 | ppl 504.22 | wps 29728.1 | wpb 591.2 | bsz 29.9 | num_updates 1734 | best_loss 8.978
2022-02-28 14:59:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1734 updates
2022-02-28 14:59:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:59:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 14:59:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 1734 updates, score 8.978) (writing took 5.42287850799039 seconds)
2022-02-28 14:59:59 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-02-28 14:59:59 | INFO | train | epoch 056 | loss 8.744 | ppl 428.62 | wps 4699.2 | ups 3.83 | wpb 1227.6 | bsz 61.6 | num_updates 1734 | lr 5.202e-06 | gnorm 8.856 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 528
2022-02-28 14:59:59 | INFO | fairseq.trainer | begin training epoch 57
2022-02-28 14:59:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:00:00 | INFO | train_inner | epoch 057:      6 / 31 loss=8.839, ppl=458.06, wps=3217.8, ups=2.71, wpb=1186.1, bsz=60.3, num_updates=1740, lr=5.22e-06, gnorm=8.949, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=528
2022-02-28 15:00:01 | INFO | train_inner | epoch 057:     26 / 31 loss=8.691, ppl=413.29, wps=18082.1, ups=14.99, wpb=1205.9, bsz=63.2, num_updates=1760, lr=5.28e-06, gnorm=14.108, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=530
2022-02-28 15:00:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:00:02 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.923 | ppl 485.39 | wps 29906.9 | wpb 591.2 | bsz 29.9 | num_updates 1765 | best_loss 8.923
2022-02-28 15:00:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1765 updates
2022-02-28 15:00:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 57 @ 1765 updates, score 8.923) (writing took 8.369274987024255 seconds)
2022-02-28 15:00:11 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-02-28 15:00:11 | INFO | train | epoch 057 | loss 8.765 | ppl 435.01 | wps 3434.7 | ups 2.8 | wpb 1227.6 | bsz 61.6 | num_updates 1765 | lr 5.295e-06 | gnorm 12.202 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 539
2022-02-28 15:00:11 | INFO | fairseq.trainer | begin training epoch 58
2022-02-28 15:00:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:00:12 | INFO | train_inner | epoch 058:     15 / 31 loss=8.765, ppl=435.04, wps=2538.7, ups=1.93, wpb=1316, bsz=61.1, num_updates=1780, lr=5.34e-06, gnorm=8.946, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=540
2022-02-28 15:00:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:00:13 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.907 | ppl 479.9 | wps 29124.9 | wpb 591.2 | bsz 29.9 | num_updates 1796 | best_loss 8.907
2022-02-28 15:00:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1796 updates
2022-02-28 15:00:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 58 @ 1796 updates, score 8.907) (writing took 6.481605375010986 seconds)
2022-02-28 15:00:20 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-02-28 15:00:20 | INFO | train | epoch 058 | loss 8.676 | ppl 408.97 | wps 4145.6 | ups 3.38 | wpb 1227.6 | bsz 61.6 | num_updates 1796 | lr 5.388e-06 | gnorm 9.114 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 548
2022-02-28 15:00:20 | INFO | fairseq.trainer | begin training epoch 59
2022-02-28 15:00:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:00:20 | INFO | train_inner | epoch 059:      4 / 31 loss=8.596, ppl=386.99, wps=2904.6, ups=2.37, wpb=1223.8, bsz=61.6, num_updates=1800, lr=5.4e-06, gnorm=9.207, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=548
2022-02-28 15:00:22 | INFO | train_inner | epoch 059:     24 / 31 loss=8.593, ppl=386.08, wps=17309.4, ups=14.36, wpb=1205.6, bsz=61.9, num_updates=1820, lr=5.46e-06, gnorm=8.846, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=550
2022-02-28 15:00:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:00:23 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.914 | ppl 482.51 | wps 31029.6 | wpb 591.2 | bsz 29.9 | num_updates 1827 | best_loss 8.907
2022-02-28 15:00:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1827 updates
2022-02-28 15:00:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:00:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:00:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 59 @ 1827 updates, score 8.914) (writing took 2.7060317309806123 seconds)
2022-02-28 15:00:25 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-02-28 15:00:25 | INFO | train | epoch 059 | loss 8.595 | ppl 386.79 | wps 6958 | ups 5.67 | wpb 1227.6 | bsz 61.6 | num_updates 1827 | lr 5.481e-06 | gnorm 8.882 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 553
2022-02-28 15:00:25 | INFO | fairseq.trainer | begin training epoch 60
2022-02-28 15:00:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:00:26 | INFO | train_inner | epoch 060:     13 / 31 loss=8.608, ppl=390.13, wps=5209.7, ups=4.28, wpb=1217.8, bsz=60.3, num_updates=1840, lr=5.52e-06, gnorm=8.742, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=554
2022-02-28 15:00:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:00:28 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.892 | ppl 475.07 | wps 30663.7 | wpb 591.2 | bsz 29.9 | num_updates 1858 | best_loss 8.892
2022-02-28 15:00:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1858 updates
2022-02-28 15:00:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 60 @ 1858 updates, score 8.892) (writing took 6.413673667004332 seconds)
2022-02-28 15:00:34 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-02-28 15:00:34 | INFO | train | epoch 060 | loss 8.525 | ppl 368.28 | wps 4201 | ups 3.42 | wpb 1227.6 | bsz 61.6 | num_updates 1858 | lr 5.574e-06 | gnorm 9.012 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 563
2022-02-28 15:00:34 | INFO | fairseq.trainer | begin training epoch 61
2022-02-28 15:00:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:00:35 | INFO | train_inner | epoch 061:      2 / 31 loss=8.512, ppl=365.02, wps=2854.8, ups=2.41, wpb=1186.7, bsz=62.4, num_updates=1860, lr=5.58e-06, gnorm=9.189, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=563
2022-02-28 15:00:36 | INFO | train_inner | epoch 061:     22 / 31 loss=8.523, ppl=367.73, wps=18208.3, ups=14.76, wpb=1234, bsz=61.9, num_updates=1880, lr=5.64e-06, gnorm=8.769, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=564
2022-02-28 15:00:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:00:37 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.741 | ppl 427.96 | wps 33063.1 | wpb 591.2 | bsz 29.9 | num_updates 1889 | best_loss 8.741
2022-02-28 15:00:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1889 updates
2022-02-28 15:00:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 61 @ 1889 updates, score 8.741) (writing took 5.261066653009038 seconds)
2022-02-28 15:00:42 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-02-28 15:00:42 | INFO | train | epoch 061 | loss 8.529 | ppl 369.36 | wps 4815.5 | ups 3.92 | wpb 1227.6 | bsz 61.6 | num_updates 1889 | lr 5.667e-06 | gnorm 8.837 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 570
2022-02-28 15:00:42 | INFO | fairseq.trainer | begin training epoch 62
2022-02-28 15:00:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:00:43 | INFO | train_inner | epoch 062:     11 / 31 loss=8.493, ppl=360.3, wps=3457.5, ups=2.77, wpb=1246.4, bsz=61.6, num_updates=1900, lr=5.7e-06, gnorm=8.798, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=571
2022-02-28 15:00:45 | INFO | train_inner | epoch 062:     31 / 31 loss=8.422, ppl=343.03, wps=17709.9, ups=14.25, wpb=1243, bsz=61.1, num_updates=1920, lr=5.76e-06, gnorm=8.915, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=573
2022-02-28 15:00:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:00:45 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8.714 | ppl 420.06 | wps 26979.3 | wpb 591.2 | bsz 29.9 | num_updates 1920 | best_loss 8.714
2022-02-28 15:00:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1920 updates
2022-02-28 15:00:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 62 @ 1920 updates, score 8.714) (writing took 4.266684365982655 seconds)
2022-02-28 15:00:49 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-02-28 15:00:49 | INFO | train | epoch 062 | loss 8.461 | ppl 352.33 | wps 5377.9 | ups 4.38 | wpb 1227.6 | bsz 61.6 | num_updates 1920 | lr 5.76e-06 | gnorm 8.792 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 578
2022-02-28 15:00:49 | INFO | fairseq.trainer | begin training epoch 63
2022-02-28 15:00:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:00:51 | INFO | train_inner | epoch 063:     20 / 31 loss=8.413, ppl=340.88, wps=3903.1, ups=3.19, wpb=1224.7, bsz=63.2, num_updates=1940, lr=5.82e-06, gnorm=8.687, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=579
2022-02-28 15:00:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:00:52 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 8.538 | ppl 371.73 | wps 26202.9 | wpb 591.2 | bsz 29.9 | num_updates 1951 | best_loss 8.538
2022-02-28 15:00:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 1951 updates
2022-02-28 15:00:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:00:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 63 @ 1951 updates, score 8.538) (writing took 4.184590826975182 seconds)
2022-02-28 15:00:56 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-02-28 15:00:56 | INFO | train | epoch 063 | loss 8.346 | ppl 325.33 | wps 5468 | ups 4.45 | wpb 1227.6 | bsz 61.6 | num_updates 1951 | lr 5.853e-06 | gnorm 9.053 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 584
2022-02-28 15:00:56 | INFO | fairseq.trainer | begin training epoch 64
2022-02-28 15:00:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:00:57 | INFO | train_inner | epoch 064:      9 / 31 loss=8.319, ppl=319.36, wps=3722, ups=3.21, wpb=1158.3, bsz=61.1, num_updates=1960, lr=5.88e-06, gnorm=9.276, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=585
2022-02-28 15:00:58 | INFO | train_inner | epoch 064:     29 / 31 loss=8.416, ppl=341.59, wps=20022.1, ups=14.81, wpb=1352, bsz=61.9, num_updates=1980, lr=5.94e-06, gnorm=8.764, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=587
2022-02-28 15:00:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:00:59 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 8.645 | ppl 400.29 | wps 26513.2 | wpb 591.2 | bsz 29.9 | num_updates 1982 | best_loss 8.538
2022-02-28 15:00:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 1982 updates
2022-02-28 15:00:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:01:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 64 @ 1982 updates, score 8.645) (writing took 2.782322307000868 seconds)
2022-02-28 15:01:02 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-02-28 15:01:02 | INFO | train | epoch 064 | loss 8.398 | ppl 337.21 | wps 6834.8 | ups 5.57 | wpb 1227.6 | bsz 61.6 | num_updates 1982 | lr 5.946e-06 | gnorm 8.846 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 590
2022-02-28 15:01:02 | INFO | fairseq.trainer | begin training epoch 65
2022-02-28 15:01:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:01:03 | INFO | train_inner | epoch 065:     18 / 31 loss=8.173, ppl=288.54, wps=4806.5, ups=4.16, wpb=1156.3, bsz=61.1, num_updates=2000, lr=6e-06, gnorm=8.874, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=591
2022-02-28 15:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:01:04 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.613 | ppl 391.59 | wps 31269.3 | wpb 591.2 | bsz 29.9 | num_updates 2000 | best_loss 8.538
2022-02-28 15:01:04 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 2 runs
2022-02-28 15:01:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 2000 updates
2022-02-28 15:01:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt
2022-02-28 15:01:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt
2022-02-28 15:01:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt (epoch 65 @ 2000 updates, score 8.613) (writing took 4.350082887976896 seconds)
2022-02-28 15:01:08 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-02-28 15:01:08 | INFO | train | epoch 065 | loss 8.197 | ppl 293.38 | wps 3518.9 | ups 2.92 | wpb 1206.2 | bsz 62.6 | num_updates 2000 | lr 6e-06 | gnorm 8.73 | clip 100 | loss_scale 32 | train_wall 1 | gb_free 20.9 | wall 596
2022-02-28 15:01:08 | INFO | fairseq_cli.train | done training in 592.8 seconds
2022-02-28 15:40:19 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 2, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=15, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 15, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-28 15:40:19 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-28 15:40:21 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-28 15:40:21 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-28 15:40:21 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-28 15:40:21 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-28 15:40:21 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-28 15:40:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-28 15:40:21 | INFO | bartabst.data.data_utils | loaded 598 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-02-28 15:40:24 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-02-28 15:40:24 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-28 15:40:24 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-02-28 15:40:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 15:40:24 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-02-28 15:40:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 15:40:24 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-28 15:40:24 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-02-28 15:40:24 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-02-28 15:40:26 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-02-28 15:40:26 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-02-28 15:40:26 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-02-28 15:40:26 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-02-28 15:40:26 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-02-28 15:40:26 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-02-28 15:40:26 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-02-28 15:40:26 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-02-28 15:40:26 | INFO | fairseq.trainer | loading train data for epoch 1
no aos file, no transfer aos used
2022-02-28 15:40:28 | INFO | bartabst.data.data_utils | loaded 1,910 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-02-28 15:40:28 | INFO | fairseq.trainer | begin training epoch 1
2022-02-28 15:40:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:40:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-28 15:40:29 | INFO | train_inner | epoch 001:     21 / 31 loss=17.267, ppl=157724, wps=16978.4, ups=14, wpb=1226.7, bsz=63.2, num_updates=20, lr=6e-08, gnorm=23.218, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=5
2022-02-28 15:40:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:40:31 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.127 | ppl 143116 | wps 29487.9 | wpb 591.2 | bsz 29.9 | num_updates 30
2022-02-28 15:40:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 30 updates
2022-02-28 15:40:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:40:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:40:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 30 updates, score 17.127) (writing took 5.254480239993427 seconds)
2022-02-28 15:40:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-28 15:40:36 | INFO | train | epoch 001 | loss 17.3 | ppl 161327 | wps 4603.6 | ups 3.75 | wpb 1238.7 | bsz 61.5 | num_updates 30 | lr 9e-08 | gnorm 23.531 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 11
2022-02-28 15:40:36 | INFO | fairseq.trainer | begin training epoch 2
2022-02-28 15:40:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:40:37 | INFO | train_inner | epoch 002:     10 / 31 loss=17.275, ppl=158579, wps=3609.8, ups=2.8, wpb=1290.9, bsz=59.8, num_updates=40, lr=1.2e-07, gnorm=23.583, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=12
2022-02-28 15:40:38 | INFO | train_inner | epoch 002:     30 / 31 loss=17.298, ppl=161138, wps=16832.8, ups=13.84, wpb=1216.2, bsz=63.2, num_updates=60, lr=1.8e-07, gnorm=23.346, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=14
2022-02-28 15:40:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:40:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 17.054 | ppl 136065 | wps 30548.6 | wpb 591.2 | bsz 29.9 | num_updates 61 | best_loss 17.054
2022-02-28 15:40:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 61 updates
2022-02-28 15:40:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:40:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:40:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 61 updates, score 17.054) (writing took 5.061310566961765 seconds)
2022-02-28 15:40:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-28 15:40:44 | INFO | train | epoch 002 | loss 17.254 | ppl 156312 | wps 4871.8 | ups 3.97 | wpb 1227.6 | bsz 61.6 | num_updates 61 | lr 1.83e-07 | gnorm 23.324 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 19
2022-02-28 15:40:44 | INFO | fairseq.trainer | begin training epoch 3
2022-02-28 15:40:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:40:45 | INFO | train_inner | epoch 003:     19 / 31 loss=17.196, ppl=150139, wps=3341.2, ups=2.85, wpb=1173.7, bsz=60.3, num_updates=80, lr=2.4e-07, gnorm=23.684, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=21
2022-02-28 15:40:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:40:46 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.898 | ppl 122164 | wps 30013.8 | wpb 591.2 | bsz 29.9 | num_updates 92 | best_loss 16.898
2022-02-28 15:40:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 92 updates
2022-02-28 15:40:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:40:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:40:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 92 updates, score 16.898) (writing took 5.901085843972396 seconds)
2022-02-28 15:40:52 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-28 15:40:52 | INFO | train | epoch 003 | loss 17.118 | ppl 142264 | wps 4414.9 | ups 3.6 | wpb 1227.6 | bsz 61.6 | num_updates 92 | lr 2.76e-07 | gnorm 23.04 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 28
2022-02-28 15:40:52 | INFO | fairseq.trainer | begin training epoch 4
2022-02-28 15:40:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:40:53 | INFO | train_inner | epoch 004:      8 / 31 loss=16.977, ppl=129002, wps=3016.8, ups=2.52, wpb=1196.8, bsz=62.4, num_updates=100, lr=3e-07, gnorm=22.425, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=29
2022-02-28 15:40:55 | INFO | train_inner | epoch 004:     28 / 31 loss=16.92, ppl=124037, wps=15956.1, ups=11.74, wpb=1359.5, bsz=61.9, num_updates=120, lr=3.6e-07, gnorm=22.225, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=30
2022-02-28 15:40:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:40:55 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 16.547 | ppl 95749.3 | wps 34530.1 | wpb 591.2 | bsz 29.9 | num_updates 123 | best_loss 16.547
2022-02-28 15:40:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 123 updates
2022-02-28 15:40:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:40:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 123 updates, score 16.547) (writing took 4.667123049031943 seconds)
2022-02-28 15:41:00 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-28 15:41:00 | INFO | train | epoch 004 | loss 16.923 | ppl 124269 | wps 4923.7 | ups 4.01 | wpb 1227.6 | bsz 61.6 | num_updates 123 | lr 3.69e-07 | gnorm 22.883 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 36
2022-02-28 15:41:00 | INFO | fairseq.trainer | begin training epoch 5
2022-02-28 15:41:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:41:01 | INFO | train_inner | epoch 005:     17 / 31 loss=16.68, ppl=105006, wps=3721.1, ups=2.99, wpb=1244, bsz=62.4, num_updates=140, lr=4.2e-07, gnorm=22.544, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=37
2022-02-28 15:41:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:41:03 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 16.256 | ppl 78268.7 | wps 30284.4 | wpb 591.2 | bsz 29.9 | num_updates 154 | best_loss 16.256
2022-02-28 15:41:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 154 updates
2022-02-28 15:41:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 154 updates, score 16.256) (writing took 4.429908318037633 seconds)
2022-02-28 15:41:08 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-28 15:41:08 | INFO | train | epoch 005 | loss 16.632 | ppl 101583 | wps 5070.3 | ups 4.13 | wpb 1227.6 | bsz 61.6 | num_updates 154 | lr 4.62e-07 | gnorm 22.153 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 43
2022-02-28 15:41:08 | INFO | fairseq.trainer | begin training epoch 6
2022-02-28 15:41:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:41:08 | INFO | train_inner | epoch 006:      6 / 31 loss=16.541, ppl=95381.6, wps=3648, ups=3.02, wpb=1207.8, bsz=59.5, num_updates=160, lr=4.8e-07, gnorm=22.416, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=44
2022-02-28 15:41:09 | INFO | train_inner | epoch 006:     26 / 31 loss=16.35, ppl=83504.3, wps=17925.8, ups=15.21, wpb=1178.5, bsz=62.7, num_updates=180, lr=5.4e-07, gnorm=21.698, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=45
2022-02-28 15:41:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-28 15:41:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:41:10 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 16.015 | ppl 66201.8 | wps 30824.3 | wpb 591.2 | bsz 29.9 | num_updates 184 | best_loss 16.015
2022-02-28 15:41:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 184 updates
2022-02-28 15:41:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 184 updates, score 16.015) (writing took 5.6414660650189035 seconds)
2022-02-28 15:41:16 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-28 15:41:16 | INFO | train | epoch 006 | loss 16.342 | ppl 83077.8 | wps 4514.3 | ups 3.62 | wpb 1246.2 | bsz 61.5 | num_updates 184 | lr 5.52e-07 | gnorm 21.84 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 51
2022-02-28 15:41:16 | INFO | fairseq.trainer | begin training epoch 7
2022-02-28 15:41:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:41:18 | INFO | train_inner | epoch 007:     16 / 31 loss=16.094, ppl=69966.1, wps=3243.4, ups=2.44, wpb=1328.3, bsz=61.6, num_updates=200, lr=6e-07, gnorm=21.287, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=53
2022-02-28 15:41:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:41:19 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 15.6 | ppl 49658.7 | wps 31340.2 | wpb 591.2 | bsz 29.9 | num_updates 215 | best_loss 15.6
2022-02-28 15:41:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 215 updates
2022-02-28 15:41:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 215 updates, score 15.6) (writing took 4.28927082096925 seconds)
2022-02-28 15:41:23 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-28 15:41:23 | INFO | train | epoch 007 | loss 15.998 | ppl 65435.6 | wps 5479.8 | ups 4.46 | wpb 1227.6 | bsz 61.6 | num_updates 215 | lr 6.45e-07 | gnorm 20.749 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 59
2022-02-28 15:41:23 | INFO | fairseq.trainer | begin training epoch 8
2022-02-28 15:41:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:41:24 | INFO | train_inner | epoch 008:      5 / 31 loss=15.897, ppl=61030, wps=3502.3, ups=3.23, wpb=1083.2, bsz=61.1, num_updates=220, lr=6.6e-07, gnorm=20.573, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=59
2022-02-28 15:41:25 | INFO | train_inner | epoch 008:     25 / 31 loss=15.657, ppl=51683.7, wps=20247.6, ups=14.96, wpb=1353.2, bsz=63.2, num_updates=240, lr=7.2e-07, gnorm=19.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=61
2022-02-28 15:41:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:41:26 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 15.2 | ppl 37630.2 | wps 31241.3 | wpb 591.2 | bsz 29.9 | num_updates 246 | best_loss 15.2
2022-02-28 15:41:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 246 updates
2022-02-28 15:41:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 246 updates, score 15.2) (writing took 4.264444908010773 seconds)
2022-02-28 15:41:30 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-28 15:41:31 | INFO | train | epoch 008 | loss 15.657 | ppl 51682.3 | wps 5523.4 | ups 4.5 | wpb 1227.6 | bsz 61.6 | num_updates 246 | lr 7.38e-07 | gnorm 20.119 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 66
2022-02-28 15:41:31 | INFO | fairseq.trainer | begin training epoch 9
2022-02-28 15:41:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:41:32 | INFO | train_inner | epoch 009:     14 / 31 loss=15.318, ppl=40853.8, wps=3696.7, ups=3.04, wpb=1215.7, bsz=59.8, num_updates=260, lr=7.8e-07, gnorm=20.633, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=67
2022-02-28 15:41:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:41:33 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 14.784 | ppl 28201.9 | wps 30403.7 | wpb 591.2 | bsz 29.9 | num_updates 277 | best_loss 14.784
2022-02-28 15:41:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 277 updates
2022-02-28 15:41:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:36 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 277 updates, score 14.784) (writing took 4.054626080964226 seconds)
2022-02-28 15:41:37 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-28 15:41:37 | INFO | train | epoch 009 | loss 15.208 | ppl 37842 | wps 5679.3 | ups 4.63 | wpb 1227.6 | bsz 61.6 | num_updates 277 | lr 8.31e-07 | gnorm 19.682 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 73
2022-02-28 15:41:37 | INFO | fairseq.trainer | begin training epoch 10
2022-02-28 15:41:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:41:38 | INFO | train_inner | epoch 010:      3 / 31 loss=15.104, ppl=35210.3, wps=3663.9, ups=3.35, wpb=1093.2, bsz=61.6, num_updates=280, lr=8.4e-07, gnorm=19.7, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=73
2022-02-28 15:41:39 | INFO | train_inner | epoch 010:     23 / 31 loss=14.765, ppl=27834.5, wps=19686.8, ups=14.64, wpb=1344.5, bsz=61.9, num_updates=300, lr=9e-07, gnorm=18.676, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=75
2022-02-28 15:41:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:41:40 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 14.464 | ppl 22595.8 | wps 30153 | wpb 591.2 | bsz 29.9 | num_updates 308 | best_loss 14.464
2022-02-28 15:41:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 308 updates
2022-02-28 15:41:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 308 updates, score 14.464) (writing took 3.9802623229916207 seconds)
2022-02-28 15:41:44 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-28 15:41:44 | INFO | train | epoch 010 | loss 14.759 | ppl 27720.4 | wps 5741.2 | ups 4.68 | wpb 1227.6 | bsz 61.6 | num_updates 308 | lr 9.24e-07 | gnorm 18.897 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 80
2022-02-28 15:41:44 | INFO | fairseq.trainer | begin training epoch 11
2022-02-28 15:41:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:41:45 | INFO | train_inner | epoch 011:     12 / 31 loss=14.6, ppl=24828.3, wps=4165.1, ups=3.39, wpb=1227, bsz=62.4, num_updates=320, lr=9.6e-07, gnorm=18.834, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=81
2022-02-28 15:41:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:41:47 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 14.073 | ppl 17235.3 | wps 31040.6 | wpb 591.2 | bsz 29.9 | num_updates 339 | best_loss 14.073
2022-02-28 15:41:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 339 updates
2022-02-28 15:41:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:41:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 339 updates, score 14.073) (writing took 12.858157415001187 seconds)
2022-02-28 15:41:59 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-28 15:41:59 | INFO | train | epoch 011 | loss 14.389 | ppl 21458.5 | wps 2454.2 | ups 2 | wpb 1227.6 | bsz 61.6 | num_updates 339 | lr 1.017e-06 | gnorm 18.183 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 95
2022-02-28 15:41:59 | INFO | fairseq.trainer | begin training epoch 12
2022-02-28 15:41:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:42:00 | INFO | train_inner | epoch 012:      1 / 31 loss=14.301, ppl=20191.7, wps=1599.6, ups=1.36, wpb=1179.8, bsz=60.3, num_updates=340, lr=1.02e-06, gnorm=17.967, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=95
2022-02-28 15:42:01 | INFO | train_inner | epoch 012:     21 / 31 loss=14.055, ppl=17025.1, wps=17639.7, ups=14.55, wpb=1212.4, bsz=64, num_updates=360, lr=1.08e-06, gnorm=17.003, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=97
2022-02-28 15:42:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:42:02 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 13.617 | ppl 12566.8 | wps 30869 | wpb 591.2 | bsz 29.9 | num_updates 370 | best_loss 13.617
2022-02-28 15:42:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 370 updates
2022-02-28 15:42:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:42:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:42:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 370 updates, score 13.617) (writing took 6.546425259031821 seconds)
2022-02-28 15:42:09 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-28 15:42:09 | INFO | train | epoch 012 | loss 14.003 | ppl 16418 | wps 4128.4 | ups 3.36 | wpb 1227.6 | bsz 61.6 | num_updates 370 | lr 1.11e-06 | gnorm 17.37 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 104
2022-02-28 15:42:09 | INFO | fairseq.trainer | begin training epoch 13
2022-02-28 15:42:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:42:09 | INFO | train_inner | epoch 013:     10 / 31 loss=13.75, ppl=13778.1, wps=3047.2, ups=2.37, wpb=1285.8, bsz=58.2, num_updates=380, lr=1.14e-06, gnorm=17.654, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=105
2022-02-28 15:42:11 | INFO | train_inner | epoch 013:     30 / 31 loss=13.514, ppl=11696.1, wps=18326.1, ups=15.07, wpb=1216.2, bsz=64, num_updates=400, lr=1.2e-06, gnorm=16.288, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=106
2022-02-28 15:42:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:42:11 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 13.2 | ppl 9412.47 | wps 30862.8 | wpb 591.2 | bsz 29.9 | num_updates 401 | best_loss 13.2
2022-02-28 15:42:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 401 updates
2022-02-28 15:42:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:42:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:42:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 401 updates, score 13.2) (writing took 4.04387338203378 seconds)
2022-02-28 15:42:15 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-28 15:42:15 | INFO | train | epoch 013 | loss 13.566 | ppl 12124.8 | wps 5727.1 | ups 4.67 | wpb 1227.6 | bsz 61.6 | num_updates 401 | lr 1.203e-06 | gnorm 16.666 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 111
2022-02-28 15:42:15 | INFO | fairseq.trainer | begin training epoch 14
2022-02-28 15:42:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:42:17 | INFO | train_inner | epoch 014:     19 / 31 loss=13.283, ppl=9968.58, wps=3948.9, ups=3.37, wpb=1171.7, bsz=60.3, num_updates=420, lr=1.26e-06, gnorm=16.419, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=112
2022-02-28 15:42:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:42:18 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 12.94 | ppl 7858.52 | wps 30436.1 | wpb 591.2 | bsz 29.9 | num_updates 432 | best_loss 12.94
2022-02-28 15:42:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 432 updates
2022-02-28 15:42:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:42:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:42:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 432 updates, score 12.94) (writing took 4.658682516019326 seconds)
2022-02-28 15:42:23 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-28 15:42:23 | INFO | train | epoch 014 | loss 13.295 | ppl 10051.3 | wps 5141 | ups 4.19 | wpb 1227.6 | bsz 61.6 | num_updates 432 | lr 1.296e-06 | gnorm 15.822 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 118
2022-02-28 15:42:23 | INFO | fairseq.trainer | begin training epoch 15
2022-02-28 15:42:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:42:23 | INFO | train_inner | epoch 015:      8 / 31 loss=13.17, ppl=9215.94, wps=3943.8, ups=3, wpb=1314.1, bsz=62.4, num_updates=440, lr=1.32e-06, gnorm=14.042, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=119
2022-02-28 15:42:25 | INFO | train_inner | epoch 015:     28 / 31 loss=12.932, ppl=7816.71, wps=17515.1, ups=14.58, wpb=1200.9, bsz=61.9, num_updates=460, lr=1.38e-06, gnorm=13.333, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=120
2022-02-28 15:42:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:42:25 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 12.746 | ppl 6868.48 | wps 30372.4 | wpb 591.2 | bsz 29.9 | num_updates 463 | best_loss 12.746
2022-02-28 15:42:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 463 updates
2022-02-28 15:42:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:42:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:42:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 463 updates, score 12.746) (writing took 4.756420888996217 seconds)
2022-02-28 15:42:30 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-28 15:42:30 | INFO | train | epoch 015 | loss 12.947 | ppl 7896.35 | wps 5094.2 | ups 4.15 | wpb 1227.6 | bsz 61.6 | num_updates 463 | lr 1.389e-06 | gnorm 13.448 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 126
2022-02-28 15:42:30 | INFO | fairseq.trainer | begin training epoch 16
2022-02-28 15:42:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:42:32 | INFO | train_inner | epoch 016:     17 / 31 loss=12.733, ppl=6806.07, wps=3668.4, ups=2.94, wpb=1248.2, bsz=61.6, num_updates=480, lr=1.44e-06, gnorm=13.042, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=127
2022-02-28 15:42:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:42:33 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 12.437 | ppl 5546.81 | wps 30470.3 | wpb 591.2 | bsz 29.9 | num_updates 494 | best_loss 12.437
2022-02-28 15:42:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 494 updates
2022-02-28 15:42:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:42:36 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:42:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 494 updates, score 12.437) (writing took 8.536385484971106 seconds)
2022-02-28 15:42:42 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-28 15:42:42 | INFO | train | epoch 016 | loss 12.642 | ppl 6390.9 | wps 3382.7 | ups 2.76 | wpb 1227.6 | bsz 61.6 | num_updates 494 | lr 1.482e-06 | gnorm 12.412 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 137
2022-02-28 15:42:42 | INFO | fairseq.trainer | begin training epoch 17
2022-02-28 15:42:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:42:42 | INFO | train_inner | epoch 017:      6 / 31 loss=12.471, ppl=5678.28, wps=2253.5, ups=1.91, wpb=1179.1, bsz=59.8, num_updates=500, lr=1.5e-06, gnorm=12.569, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=138
2022-02-28 15:42:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:42:42 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.396 | ppl 5388.95 | wps 30122.6 | wpb 591.2 | bsz 29.9 | num_updates 500 | best_loss 12.396
2022-02-28 15:42:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 500 updates
2022-02-28 15:42:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-28 15:42:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-28 15:42:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt (epoch 17 @ 500 updates, score 12.396) (writing took 16.77600294398144 seconds)
2022-02-28 15:43:01 | INFO | train_inner | epoch 017:     26 / 31 loss=12.461, ppl=5636.54, wps=1414.9, ups=1.07, wpb=1319, bsz=63.2, num_updates=520, lr=1.56e-06, gnorm=11.365, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=156
2022-02-28 15:43:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:43:01 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.156 | ppl 4562.25 | wps 30079.3 | wpb 591.2 | bsz 29.9 | num_updates 525 | best_loss 12.156
2022-02-28 15:43:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 525 updates
2022-02-28 15:43:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 525 updates, score 12.156) (writing took 5.464889559021685 seconds)
2022-02-28 15:43:07 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-28 15:43:07 | INFO | train | epoch 017 | loss 12.393 | ppl 5378.99 | wps 1496.7 | ups 1.22 | wpb 1227.6 | bsz 61.6 | num_updates 525 | lr 1.575e-06 | gnorm 12.106 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 163
2022-02-28 15:43:07 | INFO | fairseq.trainer | begin training epoch 18
2022-02-28 15:43:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:43:08 | INFO | train_inner | epoch 018:     15 / 31 loss=12.209, ppl=4735.05, wps=3505, ups=2.7, wpb=1297.2, bsz=61.1, num_updates=540, lr=1.62e-06, gnorm=11.763, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=164
2022-02-28 15:43:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:43:10 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 12.079 | ppl 4325.76 | wps 30332.9 | wpb 591.2 | bsz 29.9 | num_updates 556 | best_loss 12.079
2022-02-28 15:43:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 556 updates
2022-02-28 15:43:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 556 updates, score 12.079) (writing took 5.5656262740376405 seconds)
2022-02-28 15:43:15 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-28 15:43:15 | INFO | train | epoch 018 | loss 12.178 | ppl 4633.75 | wps 4639.8 | ups 3.78 | wpb 1227.6 | bsz 61.6 | num_updates 556 | lr 1.668e-06 | gnorm 11.289 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 171
2022-02-28 15:43:15 | INFO | fairseq.trainer | begin training epoch 19
2022-02-28 15:43:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:43:16 | INFO | train_inner | epoch 019:      4 / 31 loss=12.128, ppl=4475.38, wps=2744, ups=2.68, wpb=1023.3, bsz=61.6, num_updates=560, lr=1.68e-06, gnorm=11.567, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=171
2022-02-28 15:43:17 | INFO | train_inner | epoch 019:     24 / 31 loss=12.051, ppl=4242.98, wps=19122.8, ups=14.72, wpb=1298.8, bsz=63.2, num_updates=580, lr=1.74e-06, gnorm=10.492, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=173
2022-02-28 15:43:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:43:18 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.853 | ppl 3698.71 | wps 31084 | wpb 591.2 | bsz 29.9 | num_updates 587 | best_loss 11.853
2022-02-28 15:43:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 587 updates
2022-02-28 15:43:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 587 updates, score 11.853) (writing took 7.888122969015967 seconds)
2022-02-28 15:43:26 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-28 15:43:26 | INFO | train | epoch 019 | loss 12.034 | ppl 4194.02 | wps 3595.8 | ups 2.93 | wpb 1227.6 | bsz 61.6 | num_updates 587 | lr 1.761e-06 | gnorm 10.727 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 181
2022-02-28 15:43:26 | INFO | fairseq.trainer | begin training epoch 20
2022-02-28 15:43:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:43:27 | INFO | train_inner | epoch 020:     13 / 31 loss=11.956, ppl=3973.7, wps=2501.1, ups=2.03, wpb=1235, bsz=61.1, num_updates=600, lr=1.8e-06, gnorm=10.619, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=182
2022-02-28 15:43:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:43:28 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.572 | ppl 3045.53 | wps 31163.6 | wpb 591.2 | bsz 29.9 | num_updates 618 | best_loss 11.572
2022-02-28 15:43:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 618 updates
2022-02-28 15:43:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 618 updates, score 11.572) (writing took 6.557362794992514 seconds)
2022-02-28 15:43:35 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-28 15:43:35 | INFO | train | epoch 020 | loss 11.854 | ppl 3701.27 | wps 4142.9 | ups 3.37 | wpb 1227.6 | bsz 61.6 | num_updates 618 | lr 1.854e-06 | gnorm 10.794 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 191
2022-02-28 15:43:35 | INFO | fairseq.trainer | begin training epoch 21
2022-02-28 15:43:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:43:35 | INFO | train_inner | epoch 021:      2 / 31 loss=11.738, ppl=3416.46, wps=2809.7, ups=2.37, wpb=1183, bsz=60.3, num_updates=620, lr=1.86e-06, gnorm=11.033, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=191
2022-02-28 15:43:37 | INFO | train_inner | epoch 021:     22 / 31 loss=11.726, ppl=3386.53, wps=18174.6, ups=14.82, wpb=1226.5, bsz=62.7, num_updates=640, lr=1.92e-06, gnorm=9.959, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=192
2022-02-28 15:43:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:43:38 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.533 | ppl 2962.58 | wps 30367.8 | wpb 591.2 | bsz 29.9 | num_updates 649 | best_loss 11.533
2022-02-28 15:43:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 649 updates
2022-02-28 15:43:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 649 updates, score 11.533) (writing took 7.661085029016249 seconds)
2022-02-28 15:43:45 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-28 15:43:45 | INFO | train | epoch 021 | loss 11.678 | ppl 3277.52 | wps 3675 | ups 2.99 | wpb 1227.6 | bsz 61.6 | num_updates 649 | lr 1.947e-06 | gnorm 10.023 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 201
2022-02-28 15:43:45 | INFO | fairseq.trainer | begin training epoch 22
2022-02-28 15:43:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:43:46 | INFO | train_inner | epoch 022:     11 / 31 loss=11.569, ppl=3037.18, wps=2466.1, ups=2.06, wpb=1198, bsz=61.6, num_updates=660, lr=1.98e-06, gnorm=10.047, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=202
2022-02-28 15:43:48 | INFO | train_inner | epoch 022:     31 / 31 loss=11.568, ppl=3035.77, wps=17993.7, ups=14.32, wpb=1256.2, bsz=60.3, num_updates=680, lr=2.04e-06, gnorm=9.74, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=203
2022-02-28 15:43:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:43:48 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.366 | ppl 2639.72 | wps 28893.8 | wpb 591.2 | bsz 29.9 | num_updates 680 | best_loss 11.366
2022-02-28 15:43:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 680 updates
2022-02-28 15:43:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 680 updates, score 11.366) (writing took 4.554931662976742 seconds)
2022-02-28 15:43:53 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-28 15:43:53 | INFO | train | epoch 022 | loss 11.557 | ppl 3012.97 | wps 5153 | ups 4.2 | wpb 1227.6 | bsz 61.6 | num_updates 680 | lr 2.04e-06 | gnorm 9.852 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 208
2022-02-28 15:43:53 | INFO | fairseq.trainer | begin training epoch 23
2022-02-28 15:43:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:43:54 | INFO | train_inner | epoch 023:     20 / 31 loss=11.368, ppl=2643.67, wps=3849.8, ups=3.07, wpb=1255.1, bsz=61.9, num_updates=700, lr=2.1e-06, gnorm=9.283, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=210
2022-02-28 15:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:43:55 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.314 | ppl 2546.49 | wps 30742 | wpb 591.2 | bsz 29.9 | num_updates 711 | best_loss 11.314
2022-02-28 15:43:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 711 updates
2022-02-28 15:43:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:43:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 711 updates, score 11.314) (writing took 4.72766049898928 seconds)
2022-02-28 15:44:00 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-28 15:44:00 | INFO | train | epoch 023 | loss 11.38 | ppl 2665.89 | wps 5140.1 | ups 4.19 | wpb 1227.6 | bsz 61.6 | num_updates 711 | lr 2.133e-06 | gnorm 9.391 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 216
2022-02-28 15:44:00 | INFO | fairseq.trainer | begin training epoch 24
2022-02-28 15:44:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:44:01 | INFO | train_inner | epoch 024:      9 / 31 loss=11.42, ppl=2740.7, wps=3738.2, ups=2.99, wpb=1250.5, bsz=62.4, num_updates=720, lr=2.16e-06, gnorm=9.305, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=217
2022-02-28 15:44:02 | INFO | train_inner | epoch 024:     29 / 31 loss=11.343, ppl=2598.16, wps=17873, ups=14.56, wpb=1227.5, bsz=61.9, num_updates=740, lr=2.22e-06, gnorm=8.991, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=218
2022-02-28 15:44:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:44:03 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.101 | ppl 2196.9 | wps 30498.1 | wpb 591.2 | bsz 29.9 | num_updates 742 | best_loss 11.101
2022-02-28 15:44:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 742 updates
2022-02-28 15:44:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 742 updates, score 11.101) (writing took 7.5658476640237495 seconds)
2022-02-28 15:44:10 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-28 15:44:11 | INFO | train | epoch 024 | loss 11.348 | ppl 2606.95 | wps 3715.7 | ups 3.03 | wpb 1227.6 | bsz 61.6 | num_updates 742 | lr 2.226e-06 | gnorm 9.113 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 226
2022-02-28 15:44:11 | INFO | fairseq.trainer | begin training epoch 25
2022-02-28 15:44:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:44:12 | INFO | train_inner | epoch 025:     18 / 31 loss=11.181, ppl=2321.83, wps=2561.9, ups=2.08, wpb=1231.2, bsz=61.6, num_updates=760, lr=2.28e-06, gnorm=9.049, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=228
2022-02-28 15:44:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:44:13 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.038 | ppl 2102.85 | wps 29962.9 | wpb 591.2 | bsz 29.9 | num_updates 773 | best_loss 11.038
2022-02-28 15:44:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 773 updates
2022-02-28 15:44:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 773 updates, score 11.038) (writing took 4.685532264993526 seconds)
2022-02-28 15:44:18 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-28 15:44:18 | INFO | train | epoch 025 | loss 11.219 | ppl 2383.1 | wps 5156.6 | ups 4.2 | wpb 1227.6 | bsz 61.6 | num_updates 773 | lr 2.319e-06 | gnorm 8.764 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 234
2022-02-28 15:44:18 | INFO | fairseq.trainer | begin training epoch 26
2022-02-28 15:44:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:44:19 | INFO | train_inner | epoch 026:      7 / 31 loss=11.209, ppl=2368.01, wps=3417.1, ups=2.98, wpb=1146.8, bsz=59.8, num_updates=780, lr=2.34e-06, gnorm=9.105, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=234
2022-02-28 15:44:20 | INFO | train_inner | epoch 026:     27 / 31 loss=11.047, ppl=2116.21, wps=18855.9, ups=14.31, wpb=1317.7, bsz=63.2, num_updates=800, lr=2.4e-06, gnorm=9.509, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=236
2022-02-28 15:44:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:44:21 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 10.863 | ppl 1862.05 | wps 30928.7 | wpb 591.2 | bsz 29.9 | num_updates 804 | best_loss 10.863
2022-02-28 15:44:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 804 updates
2022-02-28 15:44:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 804 updates, score 10.863) (writing took 7.065111429023091 seconds)
2022-02-28 15:44:28 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-28 15:44:28 | INFO | train | epoch 026 | loss 11.112 | ppl 2213.52 | wps 3899.2 | ups 3.18 | wpb 1227.6 | bsz 61.6 | num_updates 804 | lr 2.412e-06 | gnorm 9.549 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 243
2022-02-28 15:44:28 | INFO | fairseq.trainer | begin training epoch 27
2022-02-28 15:44:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:44:29 | INFO | train_inner | epoch 027:     16 / 31 loss=11.031, ppl=2092.84, wps=2716.9, ups=2.23, wpb=1221, bsz=61.1, num_updates=820, lr=2.46e-06, gnorm=8.637, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=245
2022-02-28 15:44:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:44:30 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.767 | ppl 1742.94 | wps 29940.3 | wpb 591.2 | bsz 29.9 | num_updates 835 | best_loss 10.767
2022-02-28 15:44:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 835 updates
2022-02-28 15:44:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 835 updates, score 10.767) (writing took 7.338332708983216 seconds)
2022-02-28 15:44:38 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-28 15:44:38 | INFO | train | epoch 027 | loss 10.983 | ppl 2024.35 | wps 3799.9 | ups 3.1 | wpb 1227.6 | bsz 61.6 | num_updates 835 | lr 2.505e-06 | gnorm 8.711 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 254
2022-02-28 15:44:38 | INFO | fairseq.trainer | begin training epoch 28
2022-02-28 15:44:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:44:38 | INFO | train_inner | epoch 028:      5 / 31 loss=11.005, ppl=2055.19, wps=2430.7, ups=2.17, wpb=1122.6, bsz=60.8, num_updates=840, lr=2.52e-06, gnorm=8.849, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=254
2022-02-28 15:44:40 | INFO | train_inner | epoch 028:     25 / 31 loss=10.802, ppl=1785.47, wps=17638.8, ups=14.52, wpb=1214.5, bsz=62.7, num_updates=860, lr=2.58e-06, gnorm=8.421, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=255
2022-02-28 15:44:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:44:40 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.697 | ppl 1660.55 | wps 30247.8 | wpb 591.2 | bsz 29.9 | num_updates 866 | best_loss 10.697
2022-02-28 15:44:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 866 updates
2022-02-28 15:44:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 866 updates, score 10.697) (writing took 5.423616338986903 seconds)
2022-02-28 15:44:46 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-28 15:44:46 | INFO | train | epoch 028 | loss 10.87 | ppl 1871.78 | wps 4716.4 | ups 3.84 | wpb 1227.6 | bsz 61.6 | num_updates 866 | lr 2.598e-06 | gnorm 8.406 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 262
2022-02-28 15:44:46 | INFO | fairseq.trainer | begin training epoch 29
2022-02-28 15:44:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:44:47 | INFO | train_inner | epoch 029:     14 / 31 loss=10.911, ppl=1926.13, wps=3347.1, ups=2.72, wpb=1228.5, bsz=60.3, num_updates=880, lr=2.64e-06, gnorm=8.325, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=263
2022-02-28 15:44:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:44:49 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.624 | ppl 1578.09 | wps 30355.3 | wpb 591.2 | bsz 29.9 | num_updates 897 | best_loss 10.624
2022-02-28 15:44:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 897 updates
2022-02-28 15:44:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:44:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 897 updates, score 10.624) (writing took 4.5322125410311855 seconds)
2022-02-28 15:44:53 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-28 15:44:53 | INFO | train | epoch 029 | loss 10.818 | ppl 1805.74 | wps 5269.4 | ups 4.29 | wpb 1227.6 | bsz 61.6 | num_updates 897 | lr 2.691e-06 | gnorm 8.234 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 269
2022-02-28 15:44:53 | INFO | fairseq.trainer | begin training epoch 30
2022-02-28 15:44:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:44:53 | INFO | train_inner | epoch 030:      3 / 31 loss=10.754, ppl=1726.85, wps=4049.7, ups=3.11, wpb=1303.6, bsz=62.4, num_updates=900, lr=2.7e-06, gnorm=8.139, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=269
2022-02-28 15:44:55 | INFO | train_inner | epoch 030:     23 / 31 loss=10.694, ppl=1657.02, wps=16682.6, ups=14.25, wpb=1170.5, bsz=61.9, num_updates=920, lr=2.76e-06, gnorm=8.084, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=271
2022-02-28 15:44:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:44:56 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.651 | ppl 1608.01 | wps 29600.4 | wpb 591.2 | bsz 29.9 | num_updates 928 | best_loss 10.624
2022-02-28 15:44:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 928 updates
2022-02-28 15:44:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:44:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:44:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 30 @ 928 updates, score 10.651) (writing took 2.6859090580255724 seconds)
2022-02-28 15:44:59 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-28 15:44:59 | INFO | train | epoch 030 | loss 10.685 | ppl 1645.96 | wps 6983.8 | ups 5.69 | wpb 1227.6 | bsz 61.6 | num_updates 928 | lr 2.784e-06 | gnorm 8.031 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 274
2022-02-28 15:44:59 | INFO | fairseq.trainer | begin training epoch 31
2022-02-28 15:44:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:45:00 | INFO | train_inner | epoch 031:     12 / 31 loss=10.643, ppl=1598.79, wps=5468.2, ups=4.29, wpb=1273.7, bsz=62.4, num_updates=940, lr=2.82e-06, gnorm=8.514, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=275
2022-02-28 15:45:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:45:01 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.462 | ppl 1410.34 | wps 30630.1 | wpb 591.2 | bsz 29.9 | num_updates 959 | best_loss 10.462
2022-02-28 15:45:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 959 updates
2022-02-28 15:45:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:45:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:45:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 959 updates, score 10.462) (writing took 4.51778659701813 seconds)
2022-02-28 15:45:06 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-28 15:45:06 | INFO | train | epoch 031 | loss 10.627 | ppl 1580.89 | wps 5235.7 | ups 4.26 | wpb 1227.6 | bsz 61.6 | num_updates 959 | lr 2.877e-06 | gnorm 8.648 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 282
2022-02-28 15:45:06 | INFO | fairseq.trainer | begin training epoch 32
2022-02-28 15:45:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:45:06 | INFO | train_inner | epoch 032:      1 / 31 loss=10.626, ppl=1580.52, wps=3745.7, ups=3.06, wpb=1222.8, bsz=60.3, num_updates=960, lr=2.88e-06, gnorm=8.398, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=282
2022-02-28 15:45:07 | INFO | train_inner | epoch 032:     21 / 31 loss=10.584, ppl=1535.11, wps=17667.1, ups=14.32, wpb=1234, bsz=63.2, num_updates=980, lr=2.94e-06, gnorm=7.952, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=283
2022-02-28 15:45:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:45:09 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 10.477 | ppl 1425.67 | wps 30560 | wpb 591.2 | bsz 29.9 | num_updates 990 | best_loss 10.462
2022-02-28 15:45:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 990 updates
2022-02-28 15:45:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:45:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:45:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 32 @ 990 updates, score 10.477) (writing took 2.6738747560302727 seconds)
2022-02-28 15:45:11 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-28 15:45:11 | INFO | train | epoch 032 | loss 10.562 | ppl 1511.56 | wps 6986.2 | ups 5.69 | wpb 1227.6 | bsz 61.6 | num_updates 990 | lr 2.97e-06 | gnorm 8.069 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 287
2022-02-28 15:45:11 | INFO | fairseq.trainer | begin training epoch 33
2022-02-28 15:45:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:45:12 | INFO | train_inner | epoch 033:     10 / 31 loss=10.612, ppl=1565.02, wps=5409.4, ups=4.28, wpb=1262.8, bsz=59, num_updates=1000, lr=3e-06, gnorm=7.98, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=288
2022-02-28 15:45:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:45:13 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.451 | ppl 1399.52 | wps 28639.8 | wpb 591.2 | bsz 29.9 | num_updates 1000 | best_loss 10.451
2022-02-28 15:45:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1000 updates
2022-02-28 15:45:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-28 15:45:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-28 15:45:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt (epoch 33 @ 1000 updates, score 10.451) (writing took 11.683889959007502 seconds)
2022-02-28 15:45:26 | INFO | train_inner | epoch 033:     30 / 31 loss=10.373, ppl=1325.96, wps=1743.3, ups=1.45, wpb=1198.3, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=7.943, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=302
2022-02-28 15:45:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:45:26 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.384 | ppl 1336.47 | wps 30921.4 | wpb 591.2 | bsz 29.9 | num_updates 1021 | best_loss 10.384
2022-02-28 15:45:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1021 updates
2022-02-28 15:45:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:45:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:45:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1021 updates, score 10.384) (writing took 5.882244370994158 seconds)
2022-02-28 15:45:32 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-28 15:45:32 | INFO | train | epoch 033 | loss 10.482 | ppl 1430.49 | wps 1817.2 | ups 1.48 | wpb 1227.6 | bsz 61.6 | num_updates 1021 | lr 3.063e-06 | gnorm 7.891 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 308
2022-02-28 15:45:32 | INFO | fairseq.trainer | begin training epoch 34
2022-02-28 15:45:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:45:34 | INFO | train_inner | epoch 034:     19 / 31 loss=10.396, ppl=1347.73, wps=3087.8, ups=2.55, wpb=1211.8, bsz=62.4, num_updates=1040, lr=3.12e-06, gnorm=8.223, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=309
2022-02-28 15:45:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:45:35 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 10.372 | ppl 1324.8 | wps 30804.7 | wpb 591.2 | bsz 29.9 | num_updates 1052 | best_loss 10.372
2022-02-28 15:45:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1052 updates
2022-02-28 15:45:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:45:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:45:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1052 updates, score 10.372) (writing took 5.749921012029517 seconds)
2022-02-28 15:45:41 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-28 15:45:41 | INFO | train | epoch 034 | loss 10.425 | ppl 1374.69 | wps 4456.9 | ups 3.63 | wpb 1227.6 | bsz 61.6 | num_updates 1052 | lr 3.156e-06 | gnorm 8.085 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 316
2022-02-28 15:45:41 | INFO | fairseq.trainer | begin training epoch 35
2022-02-28 15:45:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:45:41 | INFO | train_inner | epoch 035:      8 / 31 loss=10.472, ppl=1420.68, wps=3162.8, ups=2.58, wpb=1227.2, bsz=60.3, num_updates=1060, lr=3.18e-06, gnorm=7.771, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=317
2022-02-28 15:45:43 | INFO | train_inner | epoch 035:     28 / 31 loss=10.306, ppl=1265.77, wps=19186.3, ups=14.8, wpb=1296, bsz=61.9, num_updates=1080, lr=3.24e-06, gnorm=7.906, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=319
2022-02-28 15:45:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:45:43 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 10.187 | ppl 1165.53 | wps 30844.2 | wpb 591.2 | bsz 29.9 | num_updates 1083 | best_loss 10.187
2022-02-28 15:45:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1083 updates
2022-02-28 15:45:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:45:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:45:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1083 updates, score 10.187) (writing took 5.144536070001777 seconds)
2022-02-28 15:45:49 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-28 15:45:49 | INFO | train | epoch 035 | loss 10.347 | ppl 1302.32 | wps 4826.1 | ups 3.93 | wpb 1227.6 | bsz 61.6 | num_updates 1083 | lr 3.249e-06 | gnorm 7.937 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 324
2022-02-28 15:45:49 | INFO | fairseq.trainer | begin training epoch 36
2022-02-28 15:45:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:45:50 | INFO | train_inner | epoch 036:     17 / 31 loss=10.332, ppl=1288.97, wps=3456.2, ups=2.73, wpb=1265.6, bsz=61.1, num_updates=1100, lr=3.3e-06, gnorm=8.024, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=326
2022-02-28 15:45:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:45:52 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 10.131 | ppl 1121.25 | wps 30272.5 | wpb 591.2 | bsz 29.9 | num_updates 1114 | best_loss 10.131
2022-02-28 15:45:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1114 updates
2022-02-28 15:45:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:45:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:45:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1114 updates, score 10.131) (writing took 4.177440300001763 seconds)
2022-02-28 15:45:56 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-28 15:45:56 | INFO | train | epoch 036 | loss 10.235 | ppl 1205.33 | wps 5458.3 | ups 4.45 | wpb 1227.6 | bsz 61.6 | num_updates 1114 | lr 3.342e-06 | gnorm 8.02 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 331
2022-02-28 15:45:56 | INFO | fairseq.trainer | begin training epoch 37
2022-02-28 15:45:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:45:56 | INFO | train_inner | epoch 037:      6 / 31 loss=10.098, ppl=1096.05, wps=3727.8, ups=3.29, wpb=1132.5, bsz=61.6, num_updates=1120, lr=3.36e-06, gnorm=7.895, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=332
2022-02-28 15:45:58 | INFO | train_inner | epoch 037:     26 / 31 loss=10.25, ppl=1217.99, wps=16882, ups=14, wpb=1205.9, bsz=61.9, num_updates=1140, lr=3.42e-06, gnorm=7.78, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=333
2022-02-28 15:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:45:58 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.967 | ppl 1001.08 | wps 30370.1 | wpb 591.2 | bsz 29.9 | num_updates 1145 | best_loss 9.967
2022-02-28 15:45:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1145 updates
2022-02-28 15:45:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1145 updates, score 9.967) (writing took 4.046869108977262 seconds)
2022-02-28 15:46:03 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-28 15:46:03 | INFO | train | epoch 037 | loss 10.195 | ppl 1172.51 | wps 5610.2 | ups 4.57 | wpb 1227.6 | bsz 61.6 | num_updates 1145 | lr 3.435e-06 | gnorm 7.599 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 338
2022-02-28 15:46:03 | INFO | fairseq.trainer | begin training epoch 38
2022-02-28 15:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:46:04 | INFO | train_inner | epoch 038:     15 / 31 loss=10.082, ppl=1084.22, wps=3912.2, ups=3.31, wpb=1181, bsz=62.4, num_updates=1160, lr=3.48e-06, gnorm=11.253, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=339
2022-02-28 15:46:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:46:05 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 10.042 | ppl 1054.52 | wps 29239.6 | wpb 591.2 | bsz 29.9 | num_updates 1176 | best_loss 9.967
2022-02-28 15:46:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1176 updates
2022-02-28 15:46:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:46:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:46:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 38 @ 1176 updates, score 10.042) (writing took 2.589252432982903 seconds)
2022-02-28 15:46:08 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-28 15:46:08 | INFO | train | epoch 038 | loss 10.11 | ppl 1105.35 | wps 7108.6 | ups 5.79 | wpb 1227.6 | bsz 61.6 | num_updates 1176 | lr 3.528e-06 | gnorm 9.99 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 344
2022-02-28 15:46:08 | INFO | fairseq.trainer | begin training epoch 39
2022-02-28 15:46:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:46:08 | INFO | train_inner | epoch 039:      4 / 31 loss=10.091, ppl=1090.86, wps=5655.1, ups=4.37, wpb=1293.3, bsz=60.3, num_updates=1180, lr=3.54e-06, gnorm=7.532, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=344
2022-02-28 15:46:10 | INFO | train_inner | epoch 039:     24 / 31 loss=10.017, ppl=1036.14, wps=18840, ups=14.75, wpb=1277.4, bsz=62.7, num_updates=1200, lr=3.6e-06, gnorm=7.848, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=345
2022-02-28 15:46:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:46:11 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.903 | ppl 957.56 | wps 29770.7 | wpb 591.2 | bsz 29.9 | num_updates 1207 | best_loss 9.903
2022-02-28 15:46:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1207 updates
2022-02-28 15:46:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1207 updates, score 9.903) (writing took 3.9215174229466356 seconds)
2022-02-28 15:46:15 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-28 15:46:15 | INFO | train | epoch 039 | loss 10.002 | ppl 1025.54 | wps 5657.6 | ups 4.61 | wpb 1227.6 | bsz 61.6 | num_updates 1207 | lr 3.621e-06 | gnorm 7.748 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 350
2022-02-28 15:46:15 | INFO | fairseq.trainer | begin training epoch 40
2022-02-28 15:46:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:46:16 | INFO | train_inner | epoch 040:     13 / 31 loss=9.882, ppl=943.46, wps=3598.4, ups=3.31, wpb=1086.4, bsz=60.8, num_updates=1220, lr=3.66e-06, gnorm=7.62, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=351
2022-02-28 15:46:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:46:17 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.73 | ppl 849.25 | wps 29572.4 | wpb 591.2 | bsz 29.9 | num_updates 1238 | best_loss 9.73
2022-02-28 15:46:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1238 updates
2022-02-28 15:46:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1238 updates, score 9.73) (writing took 4.561788356979378 seconds)
2022-02-28 15:46:22 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-28 15:46:22 | INFO | train | epoch 040 | loss 9.92 | ppl 968.95 | wps 5210.2 | ups 4.24 | wpb 1227.6 | bsz 61.6 | num_updates 1238 | lr 3.714e-06 | gnorm 7.799 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 358
2022-02-28 15:46:22 | INFO | fairseq.trainer | begin training epoch 41
2022-02-28 15:46:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:46:22 | INFO | train_inner | epoch 041:      2 / 31 loss=10.017, ppl=1036.14, wps=3918.4, ups=3.04, wpb=1288, bsz=61.1, num_updates=1240, lr=3.72e-06, gnorm=8.095, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=358
2022-02-28 15:46:24 | INFO | train_inner | epoch 041:     22 / 31 loss=9.892, ppl=950.09, wps=18812.3, ups=14.12, wpb=1331.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=7.618, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=359
2022-02-28 15:46:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:46:25 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.767 | ppl 871.34 | wps 31349.5 | wpb 591.2 | bsz 29.9 | num_updates 1269 | best_loss 9.73
2022-02-28 15:46:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1269 updates
2022-02-28 15:46:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:46:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:46:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 1269 updates, score 9.767) (writing took 2.6269663730054162 seconds)
2022-02-28 15:46:27 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-28 15:46:27 | INFO | train | epoch 041 | loss 9.924 | ppl 971.65 | wps 7019.3 | ups 5.72 | wpb 1227.6 | bsz 61.6 | num_updates 1269 | lr 3.807e-06 | gnorm 7.714 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 363
2022-02-28 15:46:27 | INFO | fairseq.trainer | begin training epoch 42
2022-02-28 15:46:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:46:28 | INFO | train_inner | epoch 042:     11 / 31 loss=9.928, ppl=974.32, wps=5104.1, ups=4.38, wpb=1164.5, bsz=61.6, num_updates=1280, lr=3.84e-06, gnorm=7.626, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=364
2022-02-28 15:46:30 | INFO | train_inner | epoch 042:     31 / 31 loss=9.721, ppl=844.08, wps=17321.9, ups=14.26, wpb=1214.5, bsz=60.3, num_updates=1300, lr=3.9e-06, gnorm=7.971, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=365
2022-02-28 15:46:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:46:30 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.679 | ppl 819.45 | wps 27517.3 | wpb 591.2 | bsz 29.9 | num_updates 1300 | best_loss 9.679
2022-02-28 15:46:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1300 updates
2022-02-28 15:46:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 1300 updates, score 9.679) (writing took 4.498764830001164 seconds)
2022-02-28 15:46:35 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-02-28 15:46:35 | INFO | train | epoch 042 | loss 9.791 | ppl 885.91 | wps 5248.9 | ups 4.28 | wpb 1227.6 | bsz 61.6 | num_updates 1300 | lr 3.9e-06 | gnorm 7.816 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 370
2022-02-28 15:46:35 | INFO | fairseq.trainer | begin training epoch 43
2022-02-28 15:46:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:46:36 | INFO | train_inner | epoch 043:     20 / 31 loss=9.799, ppl=890.7, wps=3894.9, ups=3.04, wpb=1280.2, bsz=61.9, num_updates=1320, lr=3.96e-06, gnorm=7.585, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=372
2022-02-28 15:46:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:46:37 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.651 | ppl 804.08 | wps 30854.8 | wpb 591.2 | bsz 29.9 | num_updates 1331 | best_loss 9.651
2022-02-28 15:46:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1331 updates
2022-02-28 15:46:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 43 @ 1331 updates, score 9.651) (writing took 3.889115678030066 seconds)
2022-02-28 15:46:41 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-02-28 15:46:41 | INFO | train | epoch 043 | loss 9.774 | ppl 875.48 | wps 5667.8 | ups 4.62 | wpb 1227.6 | bsz 61.6 | num_updates 1331 | lr 3.993e-06 | gnorm 7.642 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 377
2022-02-28 15:46:41 | INFO | fairseq.trainer | begin training epoch 44
2022-02-28 15:46:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:46:42 | INFO | train_inner | epoch 044:      9 / 31 loss=9.69, ppl=826.04, wps=4035.6, ups=3.41, wpb=1184.5, bsz=62.4, num_updates=1340, lr=4.02e-06, gnorm=7.943, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=378
2022-02-28 15:46:43 | INFO | train_inner | epoch 044:     29 / 31 loss=9.616, ppl=784.43, wps=17977.2, ups=14.72, wpb=1221.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=7.471, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=379
2022-02-28 15:46:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:46:44 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.623 | ppl 788.71 | wps 26762.1 | wpb 591.2 | bsz 29.9 | num_updates 1362 | best_loss 9.623
2022-02-28 15:46:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1362 updates
2022-02-28 15:46:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 1362 updates, score 9.623) (writing took 3.885641806991771 seconds)
2022-02-28 15:46:48 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-02-28 15:46:48 | INFO | train | epoch 044 | loss 9.664 | ppl 811.09 | wps 5727.7 | ups 4.67 | wpb 1227.6 | bsz 61.6 | num_updates 1362 | lr 4.086e-06 | gnorm 7.694 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 384
2022-02-28 15:46:48 | INFO | fairseq.trainer | begin training epoch 45
2022-02-28 15:46:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:46:49 | INFO | train_inner | epoch 045:     18 / 31 loss=9.639, ppl=797.04, wps=4048.6, ups=3.39, wpb=1195.7, bsz=60.3, num_updates=1380, lr=4.14e-06, gnorm=7.473, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=385
2022-02-28 15:46:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:46:51 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.52 | ppl 734.27 | wps 29655.3 | wpb 591.2 | bsz 29.9 | num_updates 1393 | best_loss 9.52
2022-02-28 15:46:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1393 updates
2022-02-28 15:46:51 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:46:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 1393 updates, score 9.52) (writing took 4.496253775025252 seconds)
2022-02-28 15:46:55 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-02-28 15:46:55 | INFO | train | epoch 045 | loss 9.565 | ppl 757.39 | wps 5237.7 | ups 4.27 | wpb 1227.6 | bsz 61.6 | num_updates 1393 | lr 4.179e-06 | gnorm 7.695 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 391
2022-02-28 15:46:55 | INFO | fairseq.trainer | begin training epoch 46
2022-02-28 15:46:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:46:56 | INFO | train_inner | epoch 046:      7 / 31 loss=9.537, ppl=742.99, wps=3866.4, ups=3.07, wpb=1258.5, bsz=61.6, num_updates=1400, lr=4.2e-06, gnorm=7.869, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=392
2022-02-28 15:46:57 | INFO | train_inner | epoch 046:     27 / 31 loss=9.513, ppl=730.82, wps=17177.4, ups=14.09, wpb=1219.4, bsz=61.9, num_updates=1420, lr=4.26e-06, gnorm=7.718, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=393
2022-02-28 15:46:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:46:58 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.589 | ppl 770.41 | wps 30290 | wpb 591.2 | bsz 29.9 | num_updates 1424 | best_loss 9.52
2022-02-28 15:46:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1424 updates
2022-02-28 15:46:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:47:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:47:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 46 @ 1424 updates, score 9.589) (writing took 2.6523342540021986 seconds)
2022-02-28 15:47:01 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-02-28 15:47:01 | INFO | train | epoch 046 | loss 9.525 | ppl 736.97 | wps 6979 | ups 5.68 | wpb 1227.6 | bsz 61.6 | num_updates 1424 | lr 4.272e-06 | gnorm 7.684 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 396
2022-02-28 15:47:01 | INFO | fairseq.trainer | begin training epoch 47
2022-02-28 15:47:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:47:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-28 15:47:02 | INFO | train_inner | epoch 047:     17 / 31 loss=9.486, ppl=717.24, wps=5078.5, ups=4.23, wpb=1199.8, bsz=61.6, num_updates=1440, lr=4.32e-06, gnorm=7.781, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=398
2022-02-28 15:47:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:47:04 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.348 | ppl 651.72 | wps 26506.8 | wpb 591.2 | bsz 29.9 | num_updates 1454 | best_loss 9.348
2022-02-28 15:47:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1454 updates
2022-02-28 15:47:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:47:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:47:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 1454 updates, score 9.348) (writing took 4.3999711709911935 seconds)
2022-02-28 15:47:08 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-02-28 15:47:08 | INFO | train | epoch 047 | loss 9.435 | ppl 692.18 | wps 5103.9 | ups 4.11 | wpb 1243 | bsz 61.5 | num_updates 1454 | lr 4.362e-06 | gnorm 7.837 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 404
2022-02-28 15:47:08 | INFO | fairseq.trainer | begin training epoch 48
2022-02-28 15:47:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:47:09 | INFO | train_inner | epoch 048:      6 / 31 loss=9.422, ppl=686.04, wps=3815.1, ups=3.05, wpb=1252, bsz=59.8, num_updates=1460, lr=4.38e-06, gnorm=7.91, clip=100, loss_scale=16, train_wall=2, gb_free=20.9, wall=404
2022-02-28 15:47:10 | INFO | train_inner | epoch 048:     26 / 31 loss=9.402, ppl=676.35, wps=18277.5, ups=13.84, wpb=1320.5, bsz=63.2, num_updates=1480, lr=4.44e-06, gnorm=7.251, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=406
2022-02-28 15:47:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:47:11 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.336 | ppl 646.16 | wps 28493.8 | wpb 591.2 | bsz 29.9 | num_updates 1485 | best_loss 9.336
2022-02-28 15:47:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1485 updates
2022-02-28 15:47:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:47:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:47:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 48 @ 1485 updates, score 9.336) (writing took 3.8500318760052323 seconds)
2022-02-28 15:47:15 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-02-28 15:47:15 | INFO | train | epoch 048 | loss 9.404 | ppl 677.65 | wps 5612.3 | ups 4.57 | wpb 1227.6 | bsz 61.6 | num_updates 1485 | lr 4.455e-06 | gnorm 7.551 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 411
2022-02-28 15:47:15 | INFO | fairseq.trainer | begin training epoch 49
2022-02-28 15:47:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:47:16 | INFO | train_inner | epoch 049:     15 / 31 loss=9.391, ppl=671.48, wps=4152.4, ups=3.36, wpb=1235.7, bsz=62.4, num_updates=1500, lr=4.5e-06, gnorm=7.535, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=412
2022-02-28 15:47:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:47:17 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.33 | ppl 643.54 | wps 27382.4 | wpb 591.2 | bsz 29.9 | num_updates 1500 | best_loss 9.33
2022-02-28 15:47:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1500 updates
2022-02-28 15:47:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-28 15:47:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-28 15:47:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt (epoch 49 @ 1500 updates, score 9.33) (writing took 7.890981596021447 seconds)
2022-02-28 15:47:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:47:26 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.342 | ppl 649.14 | wps 30418.4 | wpb 591.2 | bsz 29.9 | num_updates 1516 | best_loss 9.33
2022-02-28 15:47:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1516 updates
2022-02-28 15:47:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:47:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:47:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 49 @ 1516 updates, score 9.342) (writing took 2.9601469869958237 seconds)
2022-02-28 15:47:29 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-02-28 15:47:29 | INFO | train | epoch 049 | loss 9.318 | ppl 638.04 | wps 2636.2 | ups 2.15 | wpb 1227.6 | bsz 61.6 | num_updates 1516 | lr 4.548e-06 | gnorm 7.546 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 425
2022-02-28 15:47:29 | INFO | fairseq.trainer | begin training epoch 50
2022-02-28 15:47:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:47:30 | INFO | train_inner | epoch 050:      4 / 31 loss=9.18, ppl=580.06, wps=1653.8, ups=1.46, wpb=1129.8, bsz=60.3, num_updates=1520, lr=4.56e-06, gnorm=7.762, clip=100, loss_scale=16, train_wall=2, gb_free=20.9, wall=425
2022-02-28 15:47:31 | INFO | train_inner | epoch 050:     24 / 31 loss=9.29, ppl=625.94, wps=18939.5, ups=15.11, wpb=1253.5, bsz=61.9, num_updates=1540, lr=4.62e-06, gnorm=7.44, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=427
2022-02-28 15:47:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:47:32 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.291 | ppl 626.4 | wps 31219 | wpb 591.2 | bsz 29.9 | num_updates 1547 | best_loss 9.291
2022-02-28 15:47:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1547 updates
2022-02-28 15:47:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:47:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:47:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 1547 updates, score 9.291) (writing took 4.424903828999959 seconds)
2022-02-28 15:47:36 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-02-28 15:47:36 | INFO | train | epoch 050 | loss 9.264 | ppl 614.9 | wps 5325.4 | ups 4.34 | wpb 1227.6 | bsz 61.6 | num_updates 1547 | lr 4.641e-06 | gnorm 7.523 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 432
2022-02-28 15:47:36 | INFO | fairseq.trainer | begin training epoch 51
2022-02-28 15:47:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:47:37 | INFO | train_inner | epoch 051:     13 / 31 loss=9.332, ppl=644.33, wps=3938, ups=3.1, wpb=1270.6, bsz=60.3, num_updates=1560, lr=4.68e-06, gnorm=7.583, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=433
2022-02-28 15:47:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:47:39 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.327 | ppl 642.47 | wps 30425.7 | wpb 591.2 | bsz 29.9 | num_updates 1578 | best_loss 9.291
2022-02-28 15:47:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1578 updates
2022-02-28 15:47:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:47:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:47:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 51 @ 1578 updates, score 9.327) (writing took 2.6338691409910098 seconds)
2022-02-28 15:47:42 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-02-28 15:47:42 | INFO | train | epoch 051 | loss 9.231 | ppl 600.83 | wps 7136 | ups 5.81 | wpb 1227.6 | bsz 61.6 | num_updates 1578 | lr 4.734e-06 | gnorm 7.511 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 437
2022-02-28 15:47:42 | INFO | fairseq.trainer | begin training epoch 52
2022-02-28 15:47:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:47:42 | INFO | train_inner | epoch 052:      2 / 31 loss=9.185, ppl=581.93, wps=5166.2, ups=4.39, wpb=1178.2, bsz=62.4, num_updates=1580, lr=4.74e-06, gnorm=7.436, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=438
2022-02-28 15:47:43 | INFO | train_inner | epoch 052:     22 / 31 loss=9.097, ppl=547.43, wps=16667.2, ups=14.25, wpb=1169.9, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=7.324, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=439
2022-02-28 15:47:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:47:45 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.154 | ppl 569.56 | wps 30933.4 | wpb 591.2 | bsz 29.9 | num_updates 1609 | best_loss 9.154
2022-02-28 15:47:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1609 updates
2022-02-28 15:47:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:47:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:47:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 52 @ 1609 updates, score 9.154) (writing took 5.125499413989019 seconds)
2022-02-28 15:47:50 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-02-28 15:47:50 | INFO | train | epoch 052 | loss 9.17 | ppl 576.02 | wps 4851.4 | ups 3.95 | wpb 1227.6 | bsz 61.6 | num_updates 1609 | lr 4.827e-06 | gnorm 7.411 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 445
2022-02-28 15:47:50 | INFO | fairseq.trainer | begin training epoch 53
2022-02-28 15:47:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:47:51 | INFO | train_inner | epoch 053:     11 / 31 loss=9.197, ppl=586.83, wps=3549.6, ups=2.83, wpb=1252.5, bsz=59.5, num_updates=1620, lr=4.86e-06, gnorm=7.508, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=446
2022-02-28 15:47:52 | INFO | train_inner | epoch 053:     31 / 31 loss=9.035, ppl=524.4, wps=17814.1, ups=14.02, wpb=1270.5, bsz=62.4, num_updates=1640, lr=4.92e-06, gnorm=7.664, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=448
2022-02-28 15:47:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:47:52 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.182 | ppl 580.98 | wps 31437 | wpb 591.2 | bsz 29.9 | num_updates 1640 | best_loss 9.154
2022-02-28 15:47:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1640 updates
2022-02-28 15:47:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:47:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:47:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 53 @ 1640 updates, score 9.182) (writing took 2.9035681079840288 seconds)
2022-02-28 15:47:55 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-02-28 15:47:55 | INFO | train | epoch 053 | loss 9.069 | ppl 537.07 | wps 6752.3 | ups 5.5 | wpb 1227.6 | bsz 61.6 | num_updates 1640 | lr 4.92e-06 | gnorm 7.548 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 451
2022-02-28 15:47:55 | INFO | fairseq.trainer | begin training epoch 54
2022-02-28 15:47:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:47:57 | INFO | train_inner | epoch 054:     20 / 31 loss=9.035, ppl=524.53, wps=5392.8, ups=4.13, wpb=1307, bsz=61.9, num_updates=1660, lr=4.98e-06, gnorm=7.338, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=452
2022-02-28 15:47:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:47:58 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.084 | ppl 542.73 | wps 30951.1 | wpb 591.2 | bsz 29.9 | num_updates 1671 | best_loss 9.084
2022-02-28 15:47:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1671 updates
2022-02-28 15:47:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 54 @ 1671 updates, score 9.084) (writing took 7.190685631008819 seconds)
2022-02-28 15:48:05 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-02-28 15:48:05 | INFO | train | epoch 054 | loss 9.015 | ppl 517.18 | wps 3853.1 | ups 3.14 | wpb 1227.6 | bsz 61.6 | num_updates 1671 | lr 5.013e-06 | gnorm 7.577 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 461
2022-02-28 15:48:05 | INFO | fairseq.trainer | begin training epoch 55
2022-02-28 15:48:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:48:06 | INFO | train_inner | epoch 055:      9 / 31 loss=9.009, ppl=515.12, wps=2738, ups=2.19, wpb=1250.2, bsz=62.4, num_updates=1680, lr=5.04e-06, gnorm=7.562, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=462
2022-02-28 15:48:07 | INFO | train_inner | epoch 055:     29 / 31 loss=8.865, ppl=466.11, wps=16156.3, ups=13.61, wpb=1186.8, bsz=61.9, num_updates=1700, lr=5.1e-06, gnorm=7.607, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=463
2022-02-28 15:48:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:48:08 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 8.988 | ppl 507.61 | wps 30838.2 | wpb 591.2 | bsz 29.9 | num_updates 1702 | best_loss 8.988
2022-02-28 15:48:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1702 updates
2022-02-28 15:48:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 55 @ 1702 updates, score 8.988) (writing took 6.689789719006512 seconds)
2022-02-28 15:48:15 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-02-28 15:48:15 | INFO | train | epoch 055 | loss 8.923 | ppl 485.25 | wps 4007 | ups 3.26 | wpb 1227.6 | bsz 61.6 | num_updates 1702 | lr 5.106e-06 | gnorm 7.562 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 470
2022-02-28 15:48:15 | INFO | fairseq.trainer | begin training epoch 56
2022-02-28 15:48:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:48:16 | INFO | train_inner | epoch 056:     18 / 31 loss=8.836, ppl=457.02, wps=2804.4, ups=2.32, wpb=1211.3, bsz=61.1, num_updates=1720, lr=5.16e-06, gnorm=7.558, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=472
2022-02-28 15:48:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:48:17 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.927 | ppl 486.69 | wps 30069.1 | wpb 591.2 | bsz 29.9 | num_updates 1733 | best_loss 8.927
2022-02-28 15:48:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1733 updates
2022-02-28 15:48:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 1733 updates, score 8.927) (writing took 5.509440325025935 seconds)
2022-02-28 15:48:23 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-02-28 15:48:23 | INFO | train | epoch 056 | loss 8.898 | ppl 477 | wps 4620.3 | ups 3.76 | wpb 1227.6 | bsz 61.6 | num_updates 1733 | lr 5.199e-06 | gnorm 7.491 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 479
2022-02-28 15:48:23 | INFO | fairseq.trainer | begin training epoch 57
2022-02-28 15:48:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:48:24 | INFO | train_inner | epoch 057:      7 / 31 loss=8.993, ppl=509.59, wps=3034.5, ups=2.66, wpb=1139.6, bsz=60.3, num_updates=1740, lr=5.22e-06, gnorm=7.614, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=479
2022-02-28 15:48:25 | INFO | train_inner | epoch 057:     27 / 31 loss=8.725, ppl=423.24, wps=17892.5, ups=14.79, wpb=1209.8, bsz=63.2, num_updates=1760, lr=5.28e-06, gnorm=7.581, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=481
2022-02-28 15:48:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:48:26 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.945 | ppl 492.82 | wps 30745.6 | wpb 591.2 | bsz 29.9 | num_updates 1764 | best_loss 8.927
2022-02-28 15:48:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1764 updates
2022-02-28 15:48:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:48:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:48:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 57 @ 1764 updates, score 8.945) (writing took 2.6368097360245883 seconds)
2022-02-28 15:48:28 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-02-28 15:48:28 | INFO | train | epoch 057 | loss 8.832 | ppl 455.71 | wps 7103.3 | ups 5.79 | wpb 1227.6 | bsz 61.6 | num_updates 1764 | lr 5.292e-06 | gnorm 7.504 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 484
2022-02-28 15:48:28 | INFO | fairseq.trainer | begin training epoch 58
2022-02-28 15:48:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:48:30 | INFO | train_inner | epoch 058:     16 / 31 loss=8.845, ppl=459.99, wps=5654.8, ups=4.23, wpb=1336, bsz=61.1, num_updates=1780, lr=5.34e-06, gnorm=7.206, clip=100, loss_scale=16, train_wall=2, gb_free=20.9, wall=485
2022-02-28 15:48:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:48:31 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.88 | ppl 471.24 | wps 31713 | wpb 591.2 | bsz 29.9 | num_updates 1795 | best_loss 8.88
2022-02-28 15:48:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1795 updates
2022-02-28 15:48:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 58 @ 1795 updates, score 8.88) (writing took 5.66838290495798 seconds)
2022-02-28 15:48:37 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-02-28 15:48:37 | INFO | train | epoch 058 | loss 8.753 | ppl 431.53 | wps 4392 | ups 3.58 | wpb 1227.6 | bsz 61.6 | num_updates 1795 | lr 5.385e-06 | gnorm 7.457 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 493
2022-02-28 15:48:37 | INFO | fairseq.trainer | begin training epoch 59
2022-02-28 15:48:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:48:37 | INFO | train_inner | epoch 059:      5 / 31 loss=8.735, ppl=426.21, wps=3105.4, ups=2.56, wpb=1214.2, bsz=61.6, num_updates=1800, lr=5.4e-06, gnorm=7.674, clip=100, loss_scale=16, train_wall=2, gb_free=20.9, wall=493
2022-02-28 15:48:39 | INFO | train_inner | epoch 059:     25 / 31 loss=8.67, ppl=407.38, wps=17265.8, ups=14.5, wpb=1190.8, bsz=61.9, num_updates=1820, lr=5.46e-06, gnorm=7.338, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=495
2022-02-28 15:48:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:48:40 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.836 | ppl 456.89 | wps 31389.2 | wpb 591.2 | bsz 29.9 | num_updates 1826 | best_loss 8.836
2022-02-28 15:48:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1826 updates
2022-02-28 15:48:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 59 @ 1826 updates, score 8.836) (writing took 5.367686229990795 seconds)
2022-02-28 15:48:45 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-02-28 15:48:45 | INFO | train | epoch 059 | loss 8.72 | ppl 421.61 | wps 4734.1 | ups 3.86 | wpb 1227.6 | bsz 61.6 | num_updates 1826 | lr 5.478e-06 | gnorm 7.424 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 501
2022-02-28 15:48:46 | INFO | fairseq.trainer | begin training epoch 60
2022-02-28 15:48:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:48:47 | INFO | train_inner | epoch 060:     14 / 31 loss=8.722, ppl=422.28, wps=3135.4, ups=2.61, wpb=1202.4, bsz=60.3, num_updates=1840, lr=5.52e-06, gnorm=7.487, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=502
2022-02-28 15:48:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:48:48 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.742 | ppl 428.13 | wps 30622.5 | wpb 591.2 | bsz 29.9 | num_updates 1857 | best_loss 8.742
2022-02-28 15:48:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1857 updates
2022-02-28 15:48:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 60 @ 1857 updates, score 8.742) (writing took 5.413532631995622 seconds)
2022-02-28 15:48:54 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-02-28 15:48:54 | INFO | train | epoch 060 | loss 8.668 | ppl 406.73 | wps 4698.2 | ups 3.83 | wpb 1227.6 | bsz 61.6 | num_updates 1857 | lr 5.571e-06 | gnorm 7.408 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 509
2022-02-28 15:48:54 | INFO | fairseq.trainer | begin training epoch 61
2022-02-28 15:48:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:48:54 | INFO | train_inner | epoch 061:      3 / 31 loss=8.641, ppl=399.34, wps=3322.8, ups=2.72, wpb=1222.2, bsz=62.4, num_updates=1860, lr=5.58e-06, gnorm=7.427, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=510
2022-02-28 15:48:55 | INFO | train_inner | epoch 061:     23 / 31 loss=8.579, ppl=382.54, wps=17509.4, ups=14.34, wpb=1221.2, bsz=61.9, num_updates=1880, lr=5.64e-06, gnorm=7.391, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=511
2022-02-28 15:48:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:48:56 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.742 | ppl 428.06 | wps 30173.8 | wpb 591.2 | bsz 29.9 | num_updates 1888 | best_loss 8.742
2022-02-28 15:48:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1888 updates
2022-02-28 15:48:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:48:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 15:49:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 61 @ 1888 updates, score 8.742) (writing took 4.160879496019334 seconds)
2022-02-28 15:49:01 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-02-28 15:49:01 | INFO | train | epoch 061 | loss 8.59 | ppl 385.33 | wps 5478.1 | ups 4.46 | wpb 1227.6 | bsz 61.6 | num_updates 1888 | lr 5.664e-06 | gnorm 7.43 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 516
2022-02-28 15:49:01 | INFO | fairseq.trainer | begin training epoch 62
2022-02-28 15:49:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 15:49:01 | INFO | train_inner | epoch 062:     12 / 31 loss=8.619, ppl=393.16, wps=3995, ups=3.25, wpb=1231, bsz=61.6, num_updates=1900, lr=5.7e-06, gnorm=7.296, clip=100, loss_scale=16, train_wall=1, gb_free=20.9, wall=517
2022-02-28 15:49:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 15:49:03 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8.758 | ppl 432.83 | wps 30572.6 | wpb 591.2 | bsz 29.9 | num_updates 1919 | best_loss 8.742
2022-02-28 15:49:03 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 2 runs
2022-02-28 15:49:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1919 updates
2022-02-28 15:49:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:49:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 15:49:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 62 @ 1919 updates, score 8.758) (writing took 2.7673934870399535 seconds)
2022-02-28 15:49:06 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-02-28 15:49:06 | INFO | train | epoch 062 | loss 8.536 | ppl 371.25 | wps 6825.1 | ups 5.56 | wpb 1227.6 | bsz 61.6 | num_updates 1919 | lr 5.757e-06 | gnorm 7.492 | clip 100 | loss_scale 16 | train_wall 2 | gb_free 20.9 | wall 522
2022-02-28 15:49:06 | INFO | fairseq_cli.train | done training in 518.3 seconds
2022-02-28 16:12:13 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 2, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=2, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-28 16:12:13 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-28 16:12:15 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-28 16:12:15 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-28 16:12:15 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-28 16:12:15 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-28 16:12:15 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-28 16:12:15 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-28 16:12:16 | INFO | bartabst.data.data_utils | loaded 598 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-02-28 16:12:19 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-02-28 16:12:19 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-28 16:12:19 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-02-28 16:12:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 16:12:19 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-02-28 16:12:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-28 16:12:19 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-28 16:12:19 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-02-28 16:12:19 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-02-28 16:12:20 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-02-28 16:12:20 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-02-28 16:12:20 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-02-28 16:12:20 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-02-28 16:12:20 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-02-28 16:12:20 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-02-28 16:12:20 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-02-28 16:12:21 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-02-28 16:12:21 | INFO | fairseq.trainer | loading train data for epoch 1
no aos file, no transfer aos used
2022-02-28 16:12:22 | INFO | bartabst.data.data_utils | loaded 1,910 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-02-28 16:12:22 | INFO | fairseq.trainer | begin training epoch 1
2022-02-28 16:12:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:12:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-28 16:12:24 | INFO | train_inner | epoch 001:     21 / 31 loss=17.267, ppl=157724, wps=15863.6, ups=13.08, wpb=1226.7, bsz=63.2, num_updates=20, lr=6e-08, gnorm=23.218, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=5
2022-02-28 16:12:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:12:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.127 | ppl 143116 | wps 33219.6 | wpb 591.2 | bsz 29.9 | num_updates 30
2022-02-28 16:12:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 30 updates
2022-02-28 16:12:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:12:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:12:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 30 updates, score 17.127) (writing took 5.2132829740294255 seconds)
2022-02-28 16:12:31 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-28 16:12:31 | INFO | train | epoch 001 | loss 17.3 | ppl 161327 | wps 4475 | ups 3.64 | wpb 1238.7 | bsz 61.5 | num_updates 30 | lr 9e-08 | gnorm 23.531 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 12
2022-02-28 16:12:31 | INFO | fairseq.trainer | begin training epoch 2
2022-02-28 16:12:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:12:31 | INFO | train_inner | epoch 002:     10 / 31 loss=17.275, ppl=158579, wps=3537.6, ups=2.74, wpb=1290.9, bsz=59.8, num_updates=40, lr=1.2e-07, gnorm=23.583, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=12
2022-02-28 16:12:33 | INFO | train_inner | epoch 002:     30 / 31 loss=17.298, ppl=161138, wps=16569.6, ups=13.62, wpb=1216.2, bsz=63.2, num_updates=60, lr=1.8e-07, gnorm=23.346, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=14
2022-02-28 16:12:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:12:33 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 17.054 | ppl 136065 | wps 28944.6 | wpb 591.2 | bsz 29.9 | num_updates 61 | best_loss 17.054
2022-02-28 16:12:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 61 updates
2022-02-28 16:12:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:12:36 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:12:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 61 updates, score 17.054) (writing took 5.024858214019332 seconds)
2022-02-28 16:12:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-28 16:12:38 | INFO | train | epoch 002 | loss 17.254 | ppl 156312 | wps 4794.3 | ups 3.91 | wpb 1227.6 | bsz 61.6 | num_updates 61 | lr 1.83e-07 | gnorm 23.324 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 20
2022-02-28 16:12:39 | INFO | fairseq.trainer | begin training epoch 3
2022-02-28 16:12:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:12:40 | INFO | train_inner | epoch 003:     19 / 31 loss=17.196, ppl=150139, wps=3271, ups=2.79, wpb=1173.7, bsz=60.3, num_updates=80, lr=2.4e-07, gnorm=23.684, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=21
2022-02-28 16:12:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:12:41 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.898 | ppl 122164 | wps 29958 | wpb 591.2 | bsz 29.9 | num_updates 92 | best_loss 16.898
2022-02-28 16:12:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 92 updates
2022-02-28 16:12:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:12:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:12:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 92 updates, score 16.898) (writing took 5.308601292956155 seconds)
2022-02-28 16:12:47 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-28 16:12:47 | INFO | train | epoch 003 | loss 17.118 | ppl 142264 | wps 4725.9 | ups 3.85 | wpb 1227.6 | bsz 61.6 | num_updates 92 | lr 2.76e-07 | gnorm 23.04 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 28
2022-02-28 16:12:47 | INFO | fairseq.trainer | begin training epoch 4
2022-02-28 16:12:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:12:47 | INFO | train_inner | epoch 004:      8 / 31 loss=16.977, ppl=129002, wps=3321.5, ups=2.78, wpb=1196.8, bsz=62.4, num_updates=100, lr=3e-07, gnorm=22.425, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=28
2022-02-28 16:12:49 | INFO | train_inner | epoch 004:     28 / 31 loss=16.92, ppl=124037, wps=20257.1, ups=14.9, wpb=1359.5, bsz=61.9, num_updates=120, lr=3.6e-07, gnorm=22.225, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=30
2022-02-28 16:12:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:12:49 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 16.547 | ppl 95749.3 | wps 29508.6 | wpb 591.2 | bsz 29.9 | num_updates 123 | best_loss 16.547
2022-02-28 16:12:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 123 updates
2022-02-28 16:12:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:12:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:12:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 123 updates, score 16.547) (writing took 5.127088213979732 seconds)
2022-02-28 16:12:54 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-28 16:12:54 | INFO | train | epoch 004 | loss 16.923 | ppl 124269 | wps 4902.9 | ups 3.99 | wpb 1227.6 | bsz 61.6 | num_updates 123 | lr 3.69e-07 | gnorm 22.883 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 35
2022-02-28 16:12:54 | INFO | fairseq.trainer | begin training epoch 5
2022-02-28 16:12:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:12:56 | INFO | train_inner | epoch 005:     17 / 31 loss=16.68, ppl=105006, wps=3529.9, ups=2.84, wpb=1244, bsz=62.4, num_updates=140, lr=4.2e-07, gnorm=22.544, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=37
2022-02-28 16:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:12:57 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 16.256 | ppl 78268.7 | wps 30138.2 | wpb 591.2 | bsz 29.9 | num_updates 154 | best_loss 16.256
2022-02-28 16:12:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 154 updates
2022-02-28 16:12:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 154 updates, score 16.256) (writing took 5.680685202009045 seconds)
2022-02-28 16:13:03 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-28 16:13:03 | INFO | train | epoch 005 | loss 16.632 | ppl 101583 | wps 4545.7 | ups 3.7 | wpb 1227.6 | bsz 61.6 | num_updates 154 | lr 4.62e-07 | gnorm 22.153 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 44
2022-02-28 16:13:03 | INFO | fairseq.trainer | begin training epoch 6
2022-02-28 16:13:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:13:03 | INFO | train_inner | epoch 006:      6 / 31 loss=16.541, ppl=95381.6, wps=3140.9, ups=2.6, wpb=1207.8, bsz=59.5, num_updates=160, lr=4.8e-07, gnorm=22.416, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=44
2022-02-28 16:13:05 | INFO | train_inner | epoch 006:     26 / 31 loss=16.35, ppl=83504.3, wps=17556, ups=14.9, wpb=1178.5, bsz=62.7, num_updates=180, lr=5.4e-07, gnorm=21.698, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=46
2022-02-28 16:13:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-28 16:13:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:13:05 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 16.015 | ppl 66201.8 | wps 30871.1 | wpb 591.2 | bsz 29.9 | num_updates 184 | best_loss 16.015
2022-02-28 16:13:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 184 updates
2022-02-28 16:13:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 184 updates, score 16.015) (writing took 4.918924133002292 seconds)
2022-02-28 16:13:10 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-28 16:13:10 | INFO | train | epoch 006 | loss 16.342 | ppl 83077.8 | wps 4969.1 | ups 3.99 | wpb 1246.2 | bsz 61.5 | num_updates 184 | lr 5.52e-07 | gnorm 21.84 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 51
2022-02-28 16:13:10 | INFO | fairseq.trainer | begin training epoch 7
2022-02-28 16:13:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:13:12 | INFO | train_inner | epoch 007:     16 / 31 loss=16.094, ppl=69966.1, wps=3816.8, ups=2.87, wpb=1328.3, bsz=61.6, num_updates=200, lr=6e-07, gnorm=21.287, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=53
2022-02-28 16:13:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:13:13 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 15.6 | ppl 49658.7 | wps 31187.8 | wpb 591.2 | bsz 29.9 | num_updates 215 | best_loss 15.6
2022-02-28 16:13:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 215 updates
2022-02-28 16:13:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 215 updates, score 15.6) (writing took 5.4945066559594125 seconds)
2022-02-28 16:13:19 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-28 16:13:19 | INFO | train | epoch 007 | loss 15.998 | ppl 65435.6 | wps 4650.2 | ups 3.79 | wpb 1227.6 | bsz 61.6 | num_updates 215 | lr 6.45e-07 | gnorm 20.749 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 60
2022-02-28 16:13:19 | INFO | fairseq.trainer | begin training epoch 8
2022-02-28 16:13:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:13:19 | INFO | train_inner | epoch 008:      5 / 31 loss=15.897, ppl=61030, wps=2882.1, ups=2.66, wpb=1083.2, bsz=61.1, num_updates=220, lr=6.6e-07, gnorm=20.573, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=60
2022-02-28 16:13:20 | INFO | train_inner | epoch 008:     25 / 31 loss=15.657, ppl=51683.7, wps=20125.5, ups=14.87, wpb=1353.2, bsz=63.2, num_updates=240, lr=7.2e-07, gnorm=19.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=62
2022-02-28 16:13:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:13:21 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 15.2 | ppl 37630.2 | wps 30705.2 | wpb 591.2 | bsz 29.9 | num_updates 246 | best_loss 15.2
2022-02-28 16:13:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 246 updates
2022-02-28 16:13:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 246 updates, score 15.2) (writing took 5.304031953972299 seconds)
2022-02-28 16:13:27 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-28 16:13:27 | INFO | train | epoch 008 | loss 15.657 | ppl 51682.3 | wps 4779.3 | ups 3.89 | wpb 1227.6 | bsz 61.6 | num_updates 246 | lr 7.38e-07 | gnorm 20.119 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 68
2022-02-28 16:13:27 | INFO | fairseq.trainer | begin training epoch 9
2022-02-28 16:13:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:13:28 | INFO | train_inner | epoch 009:     14 / 31 loss=15.318, ppl=40853.8, wps=3290.6, ups=2.71, wpb=1215.7, bsz=59.8, num_updates=260, lr=7.8e-07, gnorm=20.633, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=69
2022-02-28 16:13:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:13:30 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 14.784 | ppl 28201.9 | wps 29767.7 | wpb 591.2 | bsz 29.9 | num_updates 277 | best_loss 14.784
2022-02-28 16:13:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 277 updates
2022-02-28 16:13:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 277 updates, score 14.784) (writing took 5.621086091035977 seconds)
2022-02-28 16:13:35 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-28 16:13:35 | INFO | train | epoch 009 | loss 15.208 | ppl 37842 | wps 4541.5 | ups 3.7 | wpb 1227.6 | bsz 61.6 | num_updates 277 | lr 8.31e-07 | gnorm 19.682 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 76
2022-02-28 16:13:35 | INFO | fairseq.trainer | begin training epoch 10
2022-02-28 16:13:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:13:35 | INFO | train_inner | epoch 010:      3 / 31 loss=15.111, ppl=35396.6, wps=2882.3, ups=2.64, wpb=1093.2, bsz=61.6, num_updates=280, lr=8.4e-07, gnorm=19.561, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=77
2022-02-28 16:13:37 | INFO | train_inner | epoch 010:     23 / 31 loss=14.767, ppl=27885.5, wps=19444.8, ups=14.46, wpb=1344.5, bsz=61.9, num_updates=300, lr=9e-07, gnorm=17.216, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=78
2022-02-28 16:13:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:13:38 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 14.438 | ppl 22195.1 | wps 31169.3 | wpb 591.2 | bsz 29.9 | num_updates 308 | best_loss 14.438
2022-02-28 16:13:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 308 updates
2022-02-28 16:13:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 308 updates, score 14.438) (writing took 6.714279774983879 seconds)
2022-02-28 16:13:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-28 16:13:45 | INFO | train | epoch 010 | loss 14.748 | ppl 27507.5 | wps 4035.2 | ups 3.29 | wpb 1227.6 | bsz 61.6 | num_updates 308 | lr 9.24e-07 | gnorm 17.533 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 86
2022-02-28 16:13:45 | INFO | fairseq.trainer | begin training epoch 11
2022-02-28 16:13:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:13:46 | INFO | train_inner | epoch 011:     12 / 31 loss=14.55, ppl=23989.2, wps=2820, ups=2.3, wpb=1227, bsz=62.4, num_updates=320, lr=9.6e-07, gnorm=17.459, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=87
2022-02-28 16:13:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:13:47 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 14.021 | ppl 16628.7 | wps 30164.7 | wpb 591.2 | bsz 29.9 | num_updates 339 | best_loss 14.021
2022-02-28 16:13:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 339 updates
2022-02-28 16:13:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 339 updates, score 14.021) (writing took 4.697298475948628 seconds)
2022-02-28 16:13:52 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-28 16:13:52 | INFO | train | epoch 011 | loss 14.325 | ppl 20528.7 | wps 5135.9 | ups 4.18 | wpb 1227.6 | bsz 61.6 | num_updates 339 | lr 1.017e-06 | gnorm 16.713 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 93
2022-02-28 16:13:52 | INFO | fairseq.trainer | begin training epoch 12
2022-02-28 16:13:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:13:52 | INFO | train_inner | epoch 012:      1 / 31 loss=14.189, ppl=18678.4, wps=3556.9, ups=3.01, wpb=1179.8, bsz=60.3, num_updates=340, lr=1.02e-06, gnorm=16.467, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=93
2022-02-28 16:13:54 | INFO | train_inner | epoch 012:     21 / 31 loss=13.987, ppl=16233.5, wps=16144.8, ups=13.32, wpb=1212.4, bsz=64, num_updates=360, lr=1.08e-06, gnorm=15.674, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=95
2022-02-28 16:13:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:13:55 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 13.544 | ppl 11946.6 | wps 31385 | wpb 591.2 | bsz 29.9 | num_updates 370 | best_loss 13.544
2022-02-28 16:13:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 370 updates
2022-02-28 16:13:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:13:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 370 updates, score 13.544) (writing took 5.824055522039998 seconds)
2022-02-28 16:14:01 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-28 16:14:01 | INFO | train | epoch 012 | loss 13.923 | ppl 15532.8 | wps 4373.2 | ups 3.56 | wpb 1227.6 | bsz 61.6 | num_updates 370 | lr 1.11e-06 | gnorm 15.821 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 102
2022-02-28 16:14:01 | INFO | fairseq.trainer | begin training epoch 13
2022-02-28 16:14:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:14:01 | INFO | train_inner | epoch 013:     10 / 31 loss=13.682, ppl=13142.1, wps=3291.5, ups=2.56, wpb=1285.8, bsz=58.2, num_updates=380, lr=1.14e-06, gnorm=15.643, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=103
2022-02-28 16:14:03 | INFO | train_inner | epoch 013:     30 / 31 loss=13.378, ppl=10642.9, wps=16847.9, ups=13.85, wpb=1216.2, bsz=64, num_updates=400, lr=1.2e-06, gnorm=14.797, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=104
2022-02-28 16:14:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:14:03 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 13.118 | ppl 8889.96 | wps 30816.7 | wpb 591.2 | bsz 29.9 | num_updates 401 | best_loss 13.118
2022-02-28 16:14:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 401 updates
2022-02-28 16:14:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 401 updates, score 13.118) (writing took 7.259571958973538 seconds)
2022-02-28 16:14:11 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-28 16:14:11 | INFO | train | epoch 013 | loss 13.449 | ppl 11185.6 | wps 3800.3 | ups 3.1 | wpb 1227.6 | bsz 61.6 | num_updates 401 | lr 1.203e-06 | gnorm 15.004 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 112
2022-02-28 16:14:11 | INFO | fairseq.trainer | begin training epoch 14
2022-02-28 16:14:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:14:12 | INFO | train_inner | epoch 014:     19 / 31 loss=13.181, ppl=9290.04, wps=2556.9, ups=2.18, wpb=1171.7, bsz=60.3, num_updates=420, lr=1.26e-06, gnorm=15.138, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=113
2022-02-28 16:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:14:13 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 12.85 | ppl 7380.64 | wps 30689.2 | wpb 591.2 | bsz 29.9 | num_updates 432 | best_loss 12.85
2022-02-28 16:14:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 432 updates
2022-02-28 16:14:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 432 updates, score 12.85) (writing took 6.66356206999626 seconds)
2022-02-28 16:14:20 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-28 16:14:20 | INFO | train | epoch 014 | loss 13.122 | ppl 8915.98 | wps 4060.6 | ups 3.31 | wpb 1227.6 | bsz 61.6 | num_updates 432 | lr 1.296e-06 | gnorm 14.476 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 121
2022-02-28 16:14:20 | INFO | fairseq.trainer | begin training epoch 15
2022-02-28 16:14:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:14:21 | INFO | train_inner | epoch 015:      8 / 31 loss=12.983, ppl=8094.65, wps=3035.5, ups=2.31, wpb=1314.1, bsz=62.4, num_updates=440, lr=1.32e-06, gnorm=13.071, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=122
2022-02-28 16:14:22 | INFO | train_inner | epoch 015:     28 / 31 loss=12.839, ppl=7327.74, wps=18407.8, ups=15.33, wpb=1200.9, bsz=61.9, num_updates=460, lr=1.38e-06, gnorm=13.141, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=123
2022-02-28 16:14:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:14:23 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 12.678 | ppl 6552.71 | wps 31298.5 | wpb 591.2 | bsz 29.9 | num_updates 463 | best_loss 12.678
2022-02-28 16:14:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 463 updates
2022-02-28 16:14:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 463 updates, score 12.678) (writing took 6.25906984496396 seconds)
2022-02-28 16:14:29 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-28 16:14:29 | INFO | train | epoch 015 | loss 12.854 | ppl 7404.58 | wps 4284.8 | ups 3.49 | wpb 1227.6 | bsz 61.6 | num_updates 463 | lr 1.389e-06 | gnorm 13.211 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 130
2022-02-28 16:14:29 | INFO | fairseq.trainer | begin training epoch 16
2022-02-28 16:14:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:14:30 | INFO | train_inner | epoch 016:     17 / 31 loss=12.654, ppl=6445.65, wps=3057.2, ups=2.45, wpb=1248.2, bsz=61.6, num_updates=480, lr=1.44e-06, gnorm=12.97, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=131
2022-02-28 16:14:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:14:32 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 12.382 | ppl 5337.52 | wps 30920.6 | wpb 591.2 | bsz 29.9 | num_updates 494 | best_loss 12.382
2022-02-28 16:14:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 494 updates
2022-02-28 16:14:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 494 updates, score 12.382) (writing took 5.172336058982182 seconds)
2022-02-28 16:14:37 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-28 16:14:37 | INFO | train | epoch 016 | loss 12.568 | ppl 6071.77 | wps 4869.7 | ups 3.97 | wpb 1227.6 | bsz 61.6 | num_updates 494 | lr 1.482e-06 | gnorm 12.295 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 138
2022-02-28 16:14:37 | INFO | fairseq.trainer | begin training epoch 17
2022-02-28 16:14:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:14:37 | INFO | train_inner | epoch 017:      6 / 31 loss=12.404, ppl=5417.92, wps=3320.3, ups=2.82, wpb=1179.1, bsz=59.8, num_updates=500, lr=1.5e-06, gnorm=12.802, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=138
2022-02-28 16:14:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:14:38 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.343 | ppl 5194.13 | wps 29935.7 | wpb 591.2 | bsz 29.9 | num_updates 500 | best_loss 12.343
2022-02-28 16:14:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 500 updates
2022-02-28 16:14:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-28 16:14:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-28 16:14:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt (epoch 17 @ 500 updates, score 12.343) (writing took 9.620665410999209 seconds)
2022-02-28 16:14:49 | INFO | train_inner | epoch 017:     26 / 31 loss=12.403, ppl=5414.69, wps=2261.9, ups=1.71, wpb=1319, bsz=63.2, num_updates=520, lr=1.56e-06, gnorm=11.284, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=150
2022-02-28 16:14:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:14:50 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.107 | ppl 4410.85 | wps 30792.9 | wpb 591.2 | bsz 29.9 | num_updates 525 | best_loss 12.107
2022-02-28 16:14:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 525 updates
2022-02-28 16:14:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:14:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 525 updates, score 12.107) (writing took 5.607036265020724 seconds)
2022-02-28 16:14:56 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-28 16:14:56 | INFO | train | epoch 017 | loss 12.334 | ppl 5163.78 | wps 2035.4 | ups 1.66 | wpb 1227.6 | bsz 61.6 | num_updates 525 | lr 1.575e-06 | gnorm 12.207 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 157
2022-02-28 16:14:56 | INFO | fairseq.trainer | begin training epoch 18
2022-02-28 16:14:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:14:57 | INFO | train_inner | epoch 018:     15 / 31 loss=12.154, ppl=4558.86, wps=3380.2, ups=2.61, wpb=1297.2, bsz=61.1, num_updates=540, lr=1.62e-06, gnorm=11.494, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=158
2022-02-28 16:14:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:14:58 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 12.041 | ppl 4213.69 | wps 30566.4 | wpb 591.2 | bsz 29.9 | num_updates 556 | best_loss 12.041
2022-02-28 16:14:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 556 updates
2022-02-28 16:14:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 556 updates, score 12.041) (writing took 6.463644459960051 seconds)
2022-02-28 16:15:05 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-28 16:15:05 | INFO | train | epoch 018 | loss 12.126 | ppl 4470.84 | wps 4143.8 | ups 3.38 | wpb 1227.6 | bsz 61.6 | num_updates 556 | lr 1.668e-06 | gnorm 11.134 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 166
2022-02-28 16:15:05 | INFO | fairseq.trainer | begin training epoch 19
2022-02-28 16:15:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:15:05 | INFO | train_inner | epoch 019:      4 / 31 loss=12.078, ppl=4324.56, wps=2424.3, ups=2.37, wpb=1023.3, bsz=61.6, num_updates=560, lr=1.68e-06, gnorm=11.391, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=166
2022-02-28 16:15:07 | INFO | train_inner | epoch 019:     24 / 31 loss=12.006, ppl=4113.69, wps=18546, ups=14.28, wpb=1298.8, bsz=63.2, num_updates=580, lr=1.74e-06, gnorm=10.387, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=168
2022-02-28 16:15:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:15:08 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.817 | ppl 3606.98 | wps 30634.8 | wpb 591.2 | bsz 29.9 | num_updates 587 | best_loss 11.817
2022-02-28 16:15:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 587 updates
2022-02-28 16:15:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 587 updates, score 11.817) (writing took 6.6797437649802305 seconds)
2022-02-28 16:15:14 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-28 16:15:14 | INFO | train | epoch 019 | loss 11.989 | ppl 4065.27 | wps 4019.5 | ups 3.27 | wpb 1227.6 | bsz 61.6 | num_updates 587 | lr 1.761e-06 | gnorm 10.621 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 175
2022-02-28 16:15:14 | INFO | fairseq.trainer | begin training epoch 20
2022-02-28 16:15:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:15:15 | INFO | train_inner | epoch 020:     13 / 31 loss=11.914, ppl=3858.43, wps=2834.2, ups=2.29, wpb=1235, bsz=61.1, num_updates=600, lr=1.8e-06, gnorm=10.549, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=176
2022-02-28 16:15:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:15:17 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.545 | ppl 2988.13 | wps 30110 | wpb 591.2 | bsz 29.9 | num_updates 618 | best_loss 11.545
2022-02-28 16:15:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 618 updates
2022-02-28 16:15:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 618 updates, score 11.545) (writing took 6.700442631961778 seconds)
2022-02-28 16:15:24 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-28 16:15:24 | INFO | train | epoch 020 | loss 11.815 | ppl 3603.1 | wps 4034.1 | ups 3.29 | wpb 1227.6 | bsz 61.6 | num_updates 618 | lr 1.854e-06 | gnorm 11.158 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 185
2022-02-28 16:15:24 | INFO | fairseq.trainer | begin training epoch 21
2022-02-28 16:15:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:15:24 | INFO | train_inner | epoch 021:      2 / 31 loss=11.702, ppl=3332.18, wps=2732.7, ups=2.31, wpb=1183, bsz=60.3, num_updates=620, lr=1.86e-06, gnorm=11.543, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=185
2022-02-28 16:15:25 | INFO | train_inner | epoch 021:     22 / 31 loss=11.691, ppl=3306.81, wps=17486.1, ups=14.26, wpb=1226.5, bsz=62.7, num_updates=640, lr=1.92e-06, gnorm=9.858, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=186
2022-02-28 16:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:15:27 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.508 | ppl 2912.61 | wps 29286.6 | wpb 591.2 | bsz 29.9 | num_updates 649 | best_loss 11.508
2022-02-28 16:15:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 649 updates
2022-02-28 16:15:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 649 updates, score 11.508) (writing took 4.602253392047714 seconds)
2022-02-28 16:15:31 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-28 16:15:31 | INFO | train | epoch 021 | loss 11.644 | ppl 3200.83 | wps 5106.9 | ups 4.16 | wpb 1227.6 | bsz 61.6 | num_updates 649 | lr 1.947e-06 | gnorm 9.871 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 192
2022-02-28 16:15:31 | INFO | fairseq.trainer | begin training epoch 22
2022-02-28 16:15:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:15:32 | INFO | train_inner | epoch 022:     11 / 31 loss=11.537, ppl=2971.62, wps=3557.9, ups=2.97, wpb=1198, bsz=61.6, num_updates=660, lr=1.98e-06, gnorm=9.992, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=193
2022-02-28 16:15:34 | INFO | train_inner | epoch 022:     31 / 31 loss=11.538, ppl=2974.03, wps=17871.6, ups=14.23, wpb=1256.2, bsz=60.3, num_updates=680, lr=2.04e-06, gnorm=9.724, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=195
2022-02-28 16:15:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:15:34 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.344 | ppl 2599.5 | wps 29756.8 | wpb 591.2 | bsz 29.9 | num_updates 680 | best_loss 11.344
2022-02-28 16:15:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 680 updates
2022-02-28 16:15:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 680 updates, score 11.344) (writing took 4.668741319968831 seconds)
2022-02-28 16:15:39 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-28 16:15:39 | INFO | train | epoch 022 | loss 11.527 | ppl 2951.19 | wps 5091.9 | ups 4.15 | wpb 1227.6 | bsz 61.6 | num_updates 680 | lr 2.04e-06 | gnorm 9.833 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 200
2022-02-28 16:15:39 | INFO | fairseq.trainer | begin training epoch 23
2022-02-28 16:15:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:15:40 | INFO | train_inner | epoch 023:     20 / 31 loss=11.339, ppl=2590.56, wps=3780.1, ups=3.01, wpb=1255.1, bsz=61.9, num_updates=700, lr=2.1e-06, gnorm=9.217, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=201
2022-02-28 16:15:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:15:41 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.297 | ppl 2515.29 | wps 28627.2 | wpb 591.2 | bsz 29.9 | num_updates 711 | best_loss 11.297
2022-02-28 16:15:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 711 updates
2022-02-28 16:15:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 711 updates, score 11.297) (writing took 7.054960417037364 seconds)
2022-02-28 16:15:49 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-28 16:15:49 | INFO | train | epoch 023 | loss 11.353 | ppl 2616.2 | wps 3851 | ups 3.14 | wpb 1227.6 | bsz 61.6 | num_updates 711 | lr 2.133e-06 | gnorm 9.329 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 210
2022-02-28 16:15:49 | INFO | fairseq.trainer | begin training epoch 24
2022-02-28 16:15:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:15:50 | INFO | train_inner | epoch 024:      9 / 31 loss=11.395, ppl=2693.73, wps=2601.5, ups=2.08, wpb=1250.5, bsz=62.4, num_updates=720, lr=2.16e-06, gnorm=9.237, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=211
2022-02-28 16:15:51 | INFO | train_inner | epoch 024:     29 / 31 loss=11.32, ppl=2557.36, wps=18401.4, ups=14.99, wpb=1227.5, bsz=61.9, num_updates=740, lr=2.22e-06, gnorm=8.948, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=212
2022-02-28 16:15:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:15:52 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.086 | ppl 2173.92 | wps 29726.4 | wpb 591.2 | bsz 29.9 | num_updates 742 | best_loss 11.086
2022-02-28 16:15:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 742 updates
2022-02-28 16:15:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:15:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 742 updates, score 11.086) (writing took 4.047360640019178 seconds)
2022-02-28 16:15:56 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-28 16:15:56 | INFO | train | epoch 024 | loss 11.324 | ppl 2564.11 | wps 5661.7 | ups 4.61 | wpb 1227.6 | bsz 61.6 | num_updates 742 | lr 2.226e-06 | gnorm 9.045 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 217
2022-02-28 16:15:56 | INFO | fairseq.trainer | begin training epoch 25
2022-02-28 16:15:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:15:57 | INFO | train_inner | epoch 025:     18 / 31 loss=11.159, ppl=2287.17, wps=4070.1, ups=3.31, wpb=1231.2, bsz=61.6, num_updates=760, lr=2.28e-06, gnorm=8.99, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=218
2022-02-28 16:15:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:15:59 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.026 | ppl 2084.92 | wps 29222.2 | wpb 591.2 | bsz 29.9 | num_updates 773 | best_loss 11.026
2022-02-28 16:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 773 updates
2022-02-28 16:15:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 773 updates, score 11.026) (writing took 4.622696053003892 seconds)
2022-02-28 16:16:03 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-28 16:16:03 | INFO | train | epoch 025 | loss 11.197 | ppl 2348.43 | wps 5129.3 | ups 4.18 | wpb 1227.6 | bsz 61.6 | num_updates 773 | lr 2.319e-06 | gnorm 8.724 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 224
2022-02-28 16:16:03 | INFO | fairseq.trainer | begin training epoch 26
2022-02-28 16:16:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:16:04 | INFO | train_inner | epoch 026:      7 / 31 loss=11.19, ppl=2336.16, wps=3481.7, ups=3.04, wpb=1146.8, bsz=59.8, num_updates=780, lr=2.34e-06, gnorm=9.033, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=225
2022-02-28 16:16:05 | INFO | train_inner | epoch 026:     27 / 31 loss=11.029, ppl=2089.19, wps=19596.1, ups=14.87, wpb=1317.7, bsz=63.2, num_updates=800, lr=2.4e-06, gnorm=9.143, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=226
2022-02-28 16:16:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:16:06 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 10.851 | ppl 1846.91 | wps 30289.5 | wpb 591.2 | bsz 29.9 | num_updates 804 | best_loss 10.851
2022-02-28 16:16:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 804 updates
2022-02-28 16:16:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 804 updates, score 10.851) (writing took 4.01614259701455 seconds)
2022-02-28 16:16:10 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-28 16:16:10 | INFO | train | epoch 026 | loss 11.093 | ppl 2185.07 | wps 5701.3 | ups 4.64 | wpb 1227.6 | bsz 61.6 | num_updates 804 | lr 2.412e-06 | gnorm 9.281 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 231
2022-02-28 16:16:10 | INFO | fairseq.trainer | begin training epoch 27
2022-02-28 16:16:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:16:11 | INFO | train_inner | epoch 027:     16 / 31 loss=11.013, ppl=2065.9, wps=4087.1, ups=3.35, wpb=1221, bsz=61.1, num_updates=820, lr=2.46e-06, gnorm=8.62, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=232
2022-02-28 16:16:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:16:13 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.76 | ppl 1733.8 | wps 30713.8 | wpb 591.2 | bsz 29.9 | num_updates 835 | best_loss 10.76
2022-02-28 16:16:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 835 updates
2022-02-28 16:16:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 835 updates, score 10.76) (writing took 5.745889152982272 seconds)
2022-02-28 16:16:18 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-28 16:16:18 | INFO | train | epoch 027 | loss 10.966 | ppl 1999.96 | wps 4501.1 | ups 3.67 | wpb 1227.6 | bsz 61.6 | num_updates 835 | lr 2.505e-06 | gnorm 8.672 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 239
2022-02-28 16:16:18 | INFO | fairseq.trainer | begin training epoch 28
2022-02-28 16:16:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:16:19 | INFO | train_inner | epoch 028:      5 / 31 loss=10.988, ppl=2031.63, wps=2918.1, ups=2.6, wpb=1122.6, bsz=60.8, num_updates=840, lr=2.52e-06, gnorm=8.795, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=240
2022-02-28 16:16:20 | INFO | train_inner | epoch 028:     25 / 31 loss=10.788, ppl=1767.82, wps=17279.7, ups=14.23, wpb=1214.5, bsz=62.7, num_updates=860, lr=2.58e-06, gnorm=10.262, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=241
2022-02-28 16:16:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:16:21 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.691 | ppl 1652.8 | wps 31135.3 | wpb 591.2 | bsz 29.9 | num_updates 866 | best_loss 10.691
2022-02-28 16:16:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 866 updates
2022-02-28 16:16:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 866 updates, score 10.691) (writing took 5.2426346659776755 seconds)
2022-02-28 16:16:26 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-28 16:16:27 | INFO | train | epoch 028 | loss 10.855 | ppl 1852.78 | wps 4777.6 | ups 3.89 | wpb 1227.6 | bsz 61.6 | num_updates 866 | lr 2.598e-06 | gnorm 9.585 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 247
2022-02-28 16:16:27 | INFO | fairseq.trainer | begin training epoch 29
2022-02-28 16:16:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:16:28 | INFO | train_inner | epoch 029:     14 / 31 loss=10.899, ppl=1909.23, wps=3169.2, ups=2.58, wpb=1228.5, bsz=60.3, num_updates=880, lr=2.64e-06, gnorm=8.296, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=249
2022-02-28 16:16:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:16:30 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.618 | ppl 1571.74 | wps 29743.3 | wpb 591.2 | bsz 29.9 | num_updates 897 | best_loss 10.618
2022-02-28 16:16:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 897 updates
2022-02-28 16:16:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 897 updates, score 10.618) (writing took 4.6541673459578305 seconds)
2022-02-28 16:16:34 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-28 16:16:34 | INFO | train | epoch 029 | loss 10.806 | ppl 1790.76 | wps 5174.1 | ups 4.21 | wpb 1227.6 | bsz 61.6 | num_updates 897 | lr 2.691e-06 | gnorm 8.207 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 255
2022-02-28 16:16:34 | INFO | fairseq.trainer | begin training epoch 30
2022-02-28 16:16:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:16:35 | INFO | train_inner | epoch 030:      3 / 31 loss=10.743, ppl=1713.4, wps=3958.2, ups=3.04, wpb=1303.6, bsz=62.4, num_updates=900, lr=2.7e-06, gnorm=8.117, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=256
2022-02-28 16:16:36 | INFO | train_inner | epoch 030:     23 / 31 loss=10.682, ppl=1643.05, wps=16160.4, ups=13.81, wpb=1170.5, bsz=61.9, num_updates=920, lr=2.76e-06, gnorm=8.06, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=257
2022-02-28 16:16:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:16:37 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.647 | ppl 1603.19 | wps 31005.3 | wpb 591.2 | bsz 29.9 | num_updates 928 | best_loss 10.618
2022-02-28 16:16:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 928 updates
2022-02-28 16:16:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:16:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 30 @ 928 updates, score 10.647) (writing took 2.685257510980591 seconds)
2022-02-28 16:16:40 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-28 16:16:40 | INFO | train | epoch 030 | loss 10.674 | ppl 1633.26 | wps 7008.2 | ups 5.71 | wpb 1227.6 | bsz 61.6 | num_updates 928 | lr 2.784e-06 | gnorm 8.007 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 261
2022-02-28 16:16:40 | INFO | fairseq.trainer | begin training epoch 31
2022-02-28 16:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:16:41 | INFO | train_inner | epoch 031:     12 / 31 loss=10.632, ppl=1587.05, wps=5530.9, ups=4.34, wpb=1273.7, bsz=62.4, num_updates=940, lr=2.82e-06, gnorm=8.049, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=262
2022-02-28 16:16:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:16:42 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.457 | ppl 1405.16 | wps 25771.6 | wpb 591.2 | bsz 29.9 | num_updates 959 | best_loss 10.457
2022-02-28 16:16:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 959 updates
2022-02-28 16:16:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:16:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 959 updates, score 10.457) (writing took 5.118365989997983 seconds)
2022-02-28 16:16:48 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-28 16:16:48 | INFO | train | epoch 031 | loss 10.615 | ppl 1568.13 | wps 4785.9 | ups 3.9 | wpb 1227.6 | bsz 61.6 | num_updates 959 | lr 2.877e-06 | gnorm 8.346 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 269
2022-02-28 16:16:48 | INFO | fairseq.trainer | begin training epoch 32
2022-02-28 16:16:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:16:48 | INFO | train_inner | epoch 032:      1 / 31 loss=10.614, ppl=1567.61, wps=3404.8, ups=2.78, wpb=1222.8, bsz=60.3, num_updates=960, lr=2.88e-06, gnorm=8.383, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=269
2022-02-28 16:16:49 | INFO | train_inner | epoch 032:     21 / 31 loss=10.572, ppl=1522.22, wps=18348.4, ups=14.87, wpb=1234, bsz=63.2, num_updates=980, lr=2.94e-06, gnorm=7.93, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=271
2022-02-28 16:16:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:16:51 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 10.473 | ppl 1421.34 | wps 31003 | wpb 591.2 | bsz 29.9 | num_updates 990 | best_loss 10.457
2022-02-28 16:16:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 990 updates
2022-02-28 16:16:51 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:16:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:16:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 32 @ 990 updates, score 10.473) (writing took 2.695299207000062 seconds)
2022-02-28 16:16:53 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-28 16:16:53 | INFO | train | epoch 032 | loss 10.551 | ppl 1499.93 | wps 6674.7 | ups 5.44 | wpb 1227.6 | bsz 61.6 | num_updates 990 | lr 2.97e-06 | gnorm 8.047 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 274
2022-02-28 16:16:53 | INFO | fairseq.trainer | begin training epoch 33
2022-02-28 16:16:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:16:54 | INFO | train_inner | epoch 033:     10 / 31 loss=10.603, ppl=1554.93, wps=5398.2, ups=4.27, wpb=1262.8, bsz=59, num_updates=1000, lr=3e-06, gnorm=7.961, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=275
2022-02-28 16:16:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:16:55 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.448 | ppl 1397.11 | wps 30431.7 | wpb 591.2 | bsz 29.9 | num_updates 1000 | best_loss 10.448
2022-02-28 16:16:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1000 updates
2022-02-28 16:16:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-28 16:16:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-28 16:17:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt (epoch 33 @ 1000 updates, score 10.448) (writing took 11.130801021994557 seconds)
2022-02-28 16:17:07 | INFO | train_inner | epoch 033:     30 / 31 loss=10.363, ppl=1317.27, wps=1830.3, ups=1.53, wpb=1198.3, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=7.931, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=288
2022-02-28 16:17:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:17:08 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.381 | ppl 1333.83 | wps 29251.1 | wpb 591.2 | bsz 29.9 | num_updates 1021 | best_loss 10.381
2022-02-28 16:17:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1021 updates
2022-02-28 16:17:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1021 updates, score 10.381) (writing took 4.691996862005908 seconds)
2022-02-28 16:17:12 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-28 16:17:12 | INFO | train | epoch 033 | loss 10.473 | ppl 1421.1 | wps 1990.8 | ups 1.62 | wpb 1227.6 | bsz 61.6 | num_updates 1021 | lr 3.063e-06 | gnorm 7.876 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 294
2022-02-28 16:17:12 | INFO | fairseq.trainer | begin training epoch 34
2022-02-28 16:17:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:17:14 | INFO | train_inner | epoch 034:     19 / 31 loss=10.387, ppl=1339.03, wps=3632.3, ups=3, wpb=1211.8, bsz=62.4, num_updates=1040, lr=3.12e-06, gnorm=8.232, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=295
2022-02-28 16:17:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:17:15 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 10.37 | ppl 1323.46 | wps 31467.3 | wpb 591.2 | bsz 29.9 | num_updates 1052 | best_loss 10.37
2022-02-28 16:17:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1052 updates
2022-02-28 16:17:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1052 updates, score 10.37) (writing took 6.694828373962082 seconds)
2022-02-28 16:17:22 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-28 16:17:22 | INFO | train | epoch 034 | loss 10.416 | ppl 1366.01 | wps 4052.6 | ups 3.3 | wpb 1227.6 | bsz 61.6 | num_updates 1052 | lr 3.156e-06 | gnorm 8.081 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 303
2022-02-28 16:17:22 | INFO | fairseq.trainer | begin training epoch 35
2022-02-28 16:17:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:17:23 | INFO | train_inner | epoch 035:      8 / 31 loss=10.464, ppl=1412.53, wps=2666.7, ups=2.17, wpb=1227.2, bsz=60.3, num_updates=1060, lr=3.18e-06, gnorm=7.755, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=304
2022-02-28 16:17:24 | INFO | train_inner | epoch 035:     28 / 31 loss=10.297, ppl=1258.35, wps=18810.5, ups=14.51, wpb=1296, bsz=61.9, num_updates=1080, lr=3.24e-06, gnorm=7.923, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=306
2022-02-28 16:17:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:17:25 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 10.186 | ppl 1164.52 | wps 30260 | wpb 591.2 | bsz 29.9 | num_updates 1083 | best_loss 10.186
2022-02-28 16:17:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1083 updates
2022-02-28 16:17:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1083 updates, score 10.186) (writing took 4.936358299979474 seconds)
2022-02-28 16:17:30 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-28 16:17:30 | INFO | train | epoch 035 | loss 10.339 | ppl 1294.92 | wps 4947.6 | ups 4.03 | wpb 1227.6 | bsz 61.6 | num_updates 1083 | lr 3.249e-06 | gnorm 7.947 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 311
2022-02-28 16:17:30 | INFO | fairseq.trainer | begin training epoch 36
2022-02-28 16:17:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:17:31 | INFO | train_inner | epoch 036:     17 / 31 loss=10.323, ppl=1281.35, wps=3636.2, ups=2.87, wpb=1265.6, bsz=61.1, num_updates=1100, lr=3.3e-06, gnorm=8.524, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=313
2022-02-28 16:17:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:17:33 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 10.132 | ppl 1121.9 | wps 29271.7 | wpb 591.2 | bsz 29.9 | num_updates 1114 | best_loss 10.132
2022-02-28 16:17:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1114 updates
2022-02-28 16:17:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1114 updates, score 10.132) (writing took 5.469858251046389 seconds)
2022-02-28 16:17:38 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-28 16:17:38 | INFO | train | epoch 036 | loss 10.227 | ppl 1198.54 | wps 4620.4 | ups 3.76 | wpb 1227.6 | bsz 61.6 | num_updates 1114 | lr 3.342e-06 | gnorm 8.36 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 319
2022-02-28 16:17:38 | INFO | fairseq.trainer | begin training epoch 37
2022-02-28 16:17:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:17:39 | INFO | train_inner | epoch 037:      6 / 31 loss=10.092, ppl=1091.07, wps=3012.7, ups=2.66, wpb=1132.5, bsz=61.6, num_updates=1120, lr=3.36e-06, gnorm=7.921, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=320
2022-02-28 16:17:40 | INFO | train_inner | epoch 037:     26 / 31 loss=10.244, ppl=1212.5, wps=17334.8, ups=14.38, wpb=1205.9, bsz=61.9, num_updates=1140, lr=3.42e-06, gnorm=7.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=321
2022-02-28 16:17:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:17:41 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.967 | ppl 1000.76 | wps 29976.7 | wpb 591.2 | bsz 29.9 | num_updates 1145 | best_loss 9.967
2022-02-28 16:17:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1145 updates
2022-02-28 16:17:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1145 updates, score 9.967) (writing took 5.190739027981181 seconds)
2022-02-28 16:17:46 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-28 16:17:46 | INFO | train | epoch 037 | loss 10.189 | ppl 1167.65 | wps 4819.8 | ups 3.93 | wpb 1227.6 | bsz 61.6 | num_updates 1145 | lr 3.435e-06 | gnorm 7.595 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 327
2022-02-28 16:17:46 | INFO | fairseq.trainer | begin training epoch 38
2022-02-28 16:17:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:17:47 | INFO | train_inner | epoch 038:     15 / 31 loss=10.075, ppl=1078.35, wps=3303.3, ups=2.8, wpb=1181, bsz=62.4, num_updates=1160, lr=3.48e-06, gnorm=7.75, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=329
2022-02-28 16:17:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:17:49 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 10.041 | ppl 1053.41 | wps 28585.1 | wpb 591.2 | bsz 29.9 | num_updates 1176 | best_loss 9.967
2022-02-28 16:17:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1176 updates
2022-02-28 16:17:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:17:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:17:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 38 @ 1176 updates, score 10.041) (writing took 2.6115396760287695 seconds)
2022-02-28 16:17:52 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-28 16:17:52 | INFO | train | epoch 038 | loss 10.103 | ppl 1099.7 | wps 7036.1 | ups 5.73 | wpb 1227.6 | bsz 61.6 | num_updates 1176 | lr 3.528e-06 | gnorm 7.73 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 333
2022-02-28 16:17:52 | INFO | fairseq.trainer | begin training epoch 39
2022-02-28 16:17:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:17:52 | INFO | train_inner | epoch 039:      4 / 31 loss=10.085, ppl=1086.02, wps=5605, ups=4.33, wpb=1293.3, bsz=60.3, num_updates=1180, lr=3.54e-06, gnorm=7.527, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=333
2022-02-28 16:17:54 | INFO | train_inner | epoch 039:     24 / 31 loss=10.01, ppl=1031.02, wps=18032.4, ups=14.12, wpb=1277.4, bsz=62.7, num_updates=1200, lr=3.6e-06, gnorm=7.835, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=335
2022-02-28 16:17:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:17:54 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.903 | ppl 957.5 | wps 30695.8 | wpb 591.2 | bsz 29.9 | num_updates 1207 | best_loss 9.903
2022-02-28 16:17:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1207 updates
2022-02-28 16:17:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:17:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1207 updates, score 9.903) (writing took 7.492296162003186 seconds)
2022-02-28 16:18:02 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-28 16:18:02 | INFO | train | epoch 039 | loss 9.996 | ppl 1020.98 | wps 3724.8 | ups 3.03 | wpb 1227.6 | bsz 61.6 | num_updates 1207 | lr 3.621e-06 | gnorm 7.737 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 343
2022-02-28 16:18:02 | INFO | fairseq.trainer | begin training epoch 40
2022-02-28 16:18:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:18:03 | INFO | train_inner | epoch 040:     13 / 31 loss=9.877, ppl=940.12, wps=2305.8, ups=2.12, wpb=1086.4, bsz=60.8, num_updates=1220, lr=3.66e-06, gnorm=7.614, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=344
2022-02-28 16:18:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:18:05 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.728 | ppl 848.31 | wps 30064 | wpb 591.2 | bsz 29.9 | num_updates 1238 | best_loss 9.728
2022-02-28 16:18:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1238 updates
2022-02-28 16:18:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1238 updates, score 9.728) (writing took 7.501923523959704 seconds)
2022-02-28 16:18:12 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-28 16:18:12 | INFO | train | epoch 040 | loss 9.915 | ppl 965.51 | wps 3730.1 | ups 3.04 | wpb 1227.6 | bsz 61.6 | num_updates 1238 | lr 3.714e-06 | gnorm 7.746 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 353
2022-02-28 16:18:12 | INFO | fairseq.trainer | begin training epoch 41
2022-02-28 16:18:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:18:12 | INFO | train_inner | epoch 041:      2 / 31 loss=10.012, ppl=1032.3, wps=2730.7, ups=2.12, wpb=1288, bsz=61.1, num_updates=1240, lr=3.72e-06, gnorm=8.023, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=354
2022-02-28 16:18:14 | INFO | train_inner | epoch 041:     22 / 31 loss=9.886, ppl=946.16, wps=18548.6, ups=13.93, wpb=1331.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=7.616, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=355
2022-02-28 16:18:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:18:15 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.767 | ppl 871.4 | wps 30085.9 | wpb 591.2 | bsz 29.9 | num_updates 1269 | best_loss 9.728
2022-02-28 16:18:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1269 updates
2022-02-28 16:18:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:18:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:18:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 1269 updates, score 9.767) (writing took 2.629315443977248 seconds)
2022-02-28 16:18:18 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-28 16:18:18 | INFO | train | epoch 041 | loss 9.918 | ppl 967.22 | wps 7031.7 | ups 5.73 | wpb 1227.6 | bsz 61.6 | num_updates 1269 | lr 3.807e-06 | gnorm 7.718 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 359
2022-02-28 16:18:18 | INFO | fairseq.trainer | begin training epoch 42
2022-02-28 16:18:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:18:18 | INFO | train_inner | epoch 042:     11 / 31 loss=9.921, ppl=969.71, wps=5041, ups=4.33, wpb=1164.5, bsz=61.6, num_updates=1280, lr=3.84e-06, gnorm=7.611, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=360
2022-02-28 16:18:20 | INFO | train_inner | epoch 042:     31 / 31 loss=9.716, ppl=841.31, wps=18151.9, ups=14.95, wpb=1214.5, bsz=60.3, num_updates=1300, lr=3.9e-06, gnorm=7.924, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=361
2022-02-28 16:18:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:18:20 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.678 | ppl 819.2 | wps 28923.6 | wpb 591.2 | bsz 29.9 | num_updates 1300 | best_loss 9.678
2022-02-28 16:18:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1300 updates
2022-02-28 16:18:20 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 1300 updates, score 9.678) (writing took 6.379865763999987 seconds)
2022-02-28 16:18:27 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-02-28 16:18:27 | INFO | train | epoch 042 | loss 9.786 | ppl 882.61 | wps 4189.6 | ups 3.41 | wpb 1227.6 | bsz 61.6 | num_updates 1300 | lr 3.9e-06 | gnorm 7.775 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 368
2022-02-28 16:18:27 | INFO | fairseq.trainer | begin training epoch 43
2022-02-28 16:18:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:18:28 | INFO | train_inner | epoch 043:     20 / 31 loss=9.795, ppl=888.08, wps=3056.9, ups=2.39, wpb=1280.2, bsz=61.9, num_updates=1320, lr=3.96e-06, gnorm=7.558, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=369
2022-02-28 16:18:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:18:30 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.652 | ppl 804.28 | wps 27518.8 | wpb 591.2 | bsz 29.9 | num_updates 1331 | best_loss 9.652
2022-02-28 16:18:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1331 updates
2022-02-28 16:18:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 43 @ 1331 updates, score 9.652) (writing took 5.784485079988372 seconds)
2022-02-28 16:18:35 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-02-28 16:18:35 | INFO | train | epoch 043 | loss 9.771 | ppl 873.56 | wps 4405.3 | ups 3.59 | wpb 1227.6 | bsz 61.6 | num_updates 1331 | lr 3.993e-06 | gnorm 7.659 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 376
2022-02-28 16:18:35 | INFO | fairseq.trainer | begin training epoch 44
2022-02-28 16:18:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:18:36 | INFO | train_inner | epoch 044:      9 / 31 loss=9.687, ppl=824.56, wps=3006.9, ups=2.54, wpb=1184.5, bsz=62.4, num_updates=1340, lr=4.02e-06, gnorm=8.028, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=377
2022-02-28 16:18:38 | INFO | train_inner | epoch 044:     29 / 31 loss=9.612, ppl=782.46, wps=16982.1, ups=13.9, wpb=1221.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=7.454, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=379
2022-02-28 16:18:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:18:38 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.624 | ppl 788.9 | wps 29139.9 | wpb 591.2 | bsz 29.9 | num_updates 1362 | best_loss 9.624
2022-02-28 16:18:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1362 updates
2022-02-28 16:18:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 1362 updates, score 9.624) (writing took 7.17763060197467 seconds)
2022-02-28 16:18:45 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-02-28 16:18:45 | INFO | train | epoch 044 | loss 9.66 | ppl 808.89 | wps 3809.1 | ups 3.1 | wpb 1227.6 | bsz 61.6 | num_updates 1362 | lr 4.086e-06 | gnorm 7.703 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 386
2022-02-28 16:18:45 | INFO | fairseq.trainer | begin training epoch 45
2022-02-28 16:18:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:18:47 | INFO | train_inner | epoch 045:     18 / 31 loss=9.634, ppl=794.66, wps=2609.2, ups=2.18, wpb=1195.7, bsz=60.3, num_updates=1380, lr=4.14e-06, gnorm=7.457, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=388
2022-02-28 16:18:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:18:48 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.52 | ppl 734.3 | wps 29463.3 | wpb 591.2 | bsz 29.9 | num_updates 1393 | best_loss 9.52
2022-02-28 16:18:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1393 updates
2022-02-28 16:18:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:18:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 1393 updates, score 9.52) (writing took 4.571562754979823 seconds)
2022-02-28 16:18:53 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-02-28 16:18:53 | INFO | train | epoch 045 | loss 9.561 | ppl 755.44 | wps 5219.9 | ups 4.25 | wpb 1227.6 | bsz 61.6 | num_updates 1393 | lr 4.179e-06 | gnorm 7.69 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 394
2022-02-28 16:18:53 | INFO | fairseq.trainer | begin training epoch 46
2022-02-28 16:18:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:18:53 | INFO | train_inner | epoch 046:      7 / 31 loss=9.535, ppl=741.64, wps=3855.1, ups=3.06, wpb=1258.5, bsz=61.6, num_updates=1400, lr=4.2e-06, gnorm=8.11, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=394
2022-02-28 16:18:55 | INFO | train_inner | epoch 046:     27 / 31 loss=9.51, ppl=729.21, wps=18052.6, ups=14.8, wpb=1219.4, bsz=61.9, num_updates=1420, lr=4.26e-06, gnorm=7.706, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=396
2022-02-28 16:18:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:18:55 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.591 | ppl 771.02 | wps 31228.9 | wpb 591.2 | bsz 29.9 | num_updates 1424 | best_loss 9.52
2022-02-28 16:18:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1424 updates
2022-02-28 16:18:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:18:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 46 @ 1424 updates, score 9.591) (writing took 2.709158333018422 seconds)
2022-02-28 16:18:58 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-02-28 16:18:58 | INFO | train | epoch 046 | loss 9.523 | ppl 735.7 | wps 7028.7 | ups 5.73 | wpb 1227.6 | bsz 61.6 | num_updates 1424 | lr 4.272e-06 | gnorm 7.805 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 399
2022-02-28 16:18:58 | INFO | fairseq.trainer | begin training epoch 47
2022-02-28 16:18:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:18:59 | INFO | train_inner | epoch 047:     16 / 31 loss=9.529, ppl=738.55, wps=5172.4, ups=4.29, wpb=1206, bsz=61.6, num_updates=1440, lr=4.32e-06, gnorm=7.821, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=400
2022-02-28 16:19:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:19:01 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.38 | ppl 666.3 | wps 29915.5 | wpb 591.2 | bsz 29.9 | num_updates 1455 | best_loss 9.38
2022-02-28 16:19:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1455 updates
2022-02-28 16:19:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:19:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:19:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 1455 updates, score 9.38) (writing took 7.41029815695947 seconds)
2022-02-28 16:19:08 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-02-28 16:19:09 | INFO | train | epoch 047 | loss 9.443 | ppl 695.87 | wps 3757.1 | ups 3.06 | wpb 1227.6 | bsz 61.6 | num_updates 1455 | lr 4.365e-06 | gnorm 7.908 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 409
2022-02-28 16:19:09 | INFO | fairseq.trainer | begin training epoch 48
2022-02-28 16:19:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:19:09 | INFO | train_inner | epoch 048:      5 / 31 loss=9.411, ppl=680.69, wps=2529.9, ups=2.03, wpb=1244.2, bsz=59.8, num_updates=1460, lr=4.38e-06, gnorm=7.895, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=410
2022-02-28 16:19:10 | INFO | train_inner | epoch 048:     25 / 31 loss=9.321, ppl=639.61, wps=18584.2, ups=14.46, wpb=1285.3, bsz=63.2, num_updates=1480, lr=4.44e-06, gnorm=7.138, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=412
2022-02-28 16:19:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:19:11 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.397 | ppl 674.27 | wps 30075.2 | wpb 591.2 | bsz 29.9 | num_updates 1486 | best_loss 9.38
2022-02-28 16:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1486 updates
2022-02-28 16:19:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:19:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:19:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 48 @ 1486 updates, score 9.397) (writing took 3.0686923299799673 seconds)
2022-02-28 16:19:14 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-02-28 16:19:14 | INFO | train | epoch 048 | loss 9.37 | ppl 661.52 | wps 6527.1 | ups 5.32 | wpb 1227.6 | bsz 61.6 | num_updates 1486 | lr 4.458e-06 | gnorm 7.353 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 416
2022-02-28 16:19:14 | INFO | fairseq.trainer | begin training epoch 49
2022-02-28 16:19:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:19:16 | INFO | train_inner | epoch 049:     14 / 31 loss=9.437, ppl=693.28, wps=4975.4, ups=3.93, wpb=1265.4, bsz=62.4, num_updates=1500, lr=4.5e-06, gnorm=7.318, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=417
2022-02-28 16:19:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:19:16 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.329 | ppl 643.22 | wps 29323 | wpb 591.2 | bsz 29.9 | num_updates 1500 | best_loss 9.329
2022-02-28 16:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1500 updates
2022-02-28 16:19:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-28 16:19:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-28 16:19:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt (epoch 49 @ 1500 updates, score 9.329) (writing took 9.093151148001198 seconds)
2022-02-28 16:19:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:19:27 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.28 | ppl 621.66 | wps 29686 | wpb 591.2 | bsz 29.9 | num_updates 1517 | best_loss 9.28
2022-02-28 16:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1517 updates
2022-02-28 16:19:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:19:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 1517 updates, score 9.28) (writing took 6.11443173000589 seconds)
2022-02-28 16:19:33 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-02-28 16:19:33 | INFO | train | epoch 049 | loss 9.284 | ppl 623.57 | wps 2052.4 | ups 1.67 | wpb 1227.6 | bsz 61.6 | num_updates 1517 | lr 4.551e-06 | gnorm 7.73 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 434
2022-02-28 16:19:33 | INFO | fairseq.trainer | begin training epoch 50
2022-02-28 16:19:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:19:33 | INFO | train_inner | epoch 050:      3 / 31 loss=9.082, ppl=541.94, wps=1238.6, ups=1.13, wpb=1100.2, bsz=60.3, num_updates=1520, lr=4.56e-06, gnorm=8.202, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=434
2022-02-28 16:19:35 | INFO | train_inner | epoch 050:     23 / 31 loss=9.277, ppl=620.56, wps=18130.1, ups=14.46, wpb=1254.2, bsz=61.9, num_updates=1540, lr=4.62e-06, gnorm=7.504, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=436
2022-02-28 16:19:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:19:36 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.271 | ppl 617.75 | wps 30206.9 | wpb 591.2 | bsz 29.9 | num_updates 1548 | best_loss 9.271
2022-02-28 16:19:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1548 updates
2022-02-28 16:19:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:19:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:19:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 1548 updates, score 9.271) (writing took 6.1341107719927095 seconds)
2022-02-28 16:19:42 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-02-28 16:19:42 | INFO | train | epoch 050 | loss 9.25 | ppl 608.87 | wps 4308.9 | ups 3.51 | wpb 1227.6 | bsz 61.6 | num_updates 1548 | lr 4.644e-06 | gnorm 7.543 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 443
2022-02-28 16:19:42 | INFO | fairseq.trainer | begin training epoch 51
2022-02-28 16:19:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:19:43 | INFO | train_inner | epoch 051:     12 / 31 loss=9.361, ppl=657.67, wps=3211.2, ups=2.47, wpb=1299.3, bsz=60.3, num_updates=1560, lr=4.68e-06, gnorm=7.962, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=444
2022-02-28 16:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:19:45 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.257 | ppl 611.94 | wps 30141.1 | wpb 591.2 | bsz 29.9 | num_updates 1579 | best_loss 9.257
2022-02-28 16:19:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1579 updates
2022-02-28 16:19:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:19:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:19:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 51 @ 1579 updates, score 9.257) (writing took 6.987075001990888 seconds)
2022-02-28 16:19:52 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-02-28 16:19:53 | INFO | train | epoch 051 | loss 9.225 | ppl 598.25 | wps 3911 | ups 3.19 | wpb 1227.6 | bsz 61.6 | num_updates 1579 | lr 4.737e-06 | gnorm 7.8 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 453
2022-02-28 16:19:53 | INFO | fairseq.trainer | begin training epoch 52
2022-02-28 16:19:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:19:53 | INFO | train_inner | epoch 052:      1 / 31 loss=9.133, ppl=561.34, wps=2341.1, ups=2.02, wpb=1157.5, bsz=62.4, num_updates=1580, lr=4.74e-06, gnorm=7.452, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=454
2022-02-28 16:19:54 | INFO | train_inner | epoch 052:     21 / 31 loss=9.101, ppl=549.15, wps=17108.8, ups=14.5, wpb=1179.5, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=7.438, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=455
2022-02-28 16:19:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:19:55 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.17 | ppl 576.18 | wps 28481.6 | wpb 591.2 | bsz 29.9 | num_updates 1610 | best_loss 9.17
2022-02-28 16:19:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1610 updates
2022-02-28 16:19:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:19:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:20:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 52 @ 1610 updates, score 9.17) (writing took 5.796033441030886 seconds)
2022-02-28 16:20:01 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-02-28 16:20:02 | INFO | train | epoch 052 | loss 9.172 | ppl 577.02 | wps 4444.3 | ups 3.62 | wpb 1227.6 | bsz 61.6 | num_updates 1610 | lr 4.83e-06 | gnorm 7.504 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 462
2022-02-28 16:20:02 | INFO | fairseq.trainer | begin training epoch 53
2022-02-28 16:20:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:20:02 | INFO | train_inner | epoch 053:     10 / 31 loss=9.248, ppl=607.92, wps=3046.7, ups=2.4, wpb=1267.7, bsz=59.5, num_updates=1620, lr=4.86e-06, gnorm=7.46, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=464
2022-02-28 16:20:04 | INFO | train_inner | epoch 053:     30 / 31 loss=9.024, ppl=520.43, wps=18023.9, ups=13.99, wpb=1288.1, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=7.523, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=465
2022-02-28 16:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:20:04 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.057 | ppl 532.74 | wps 29566.4 | wpb 591.2 | bsz 29.9 | num_updates 1641 | best_loss 9.057
2022-02-28 16:20:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1641 updates
2022-02-28 16:20:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:20:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:20:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 1641 updates, score 9.057) (writing took 4.24379170202883 seconds)
2022-02-28 16:20:09 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-02-28 16:20:09 | INFO | train | epoch 053 | loss 9.081 | ppl 541.46 | wps 5441.8 | ups 4.43 | wpb 1227.6 | bsz 61.6 | num_updates 1641 | lr 4.923e-06 | gnorm 7.519 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 470
2022-02-28 16:20:09 | INFO | fairseq.trainer | begin training epoch 54
2022-02-28 16:20:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:20:10 | INFO | train_inner | epoch 054:     19 / 31 loss=9.064, ppl=535.23, wps=4109.2, ups=3.21, wpb=1280.2, bsz=60.3, num_updates=1660, lr=4.98e-06, gnorm=7.437, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=471
2022-02-28 16:20:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:20:11 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.02 | ppl 519.27 | wps 30729.5 | wpb 591.2 | bsz 29.9 | num_updates 1672 | best_loss 9.02
2022-02-28 16:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1672 updates
2022-02-28 16:20:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:20:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:20:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 54 @ 1672 updates, score 9.02) (writing took 5.414706750016194 seconds)
2022-02-28 16:20:17 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-02-28 16:20:17 | INFO | train | epoch 054 | loss 9.038 | ppl 525.64 | wps 4673.1 | ups 3.81 | wpb 1227.6 | bsz 61.6 | num_updates 1672 | lr 5.016e-06 | gnorm 7.589 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 478
2022-02-28 16:20:17 | INFO | fairseq.trainer | begin training epoch 55
2022-02-28 16:20:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:20:18 | INFO | train_inner | epoch 055:      8 / 31 loss=9.001, ppl=512.28, wps=3074.3, ups=2.52, wpb=1220, bsz=62.4, num_updates=1680, lr=5.04e-06, gnorm=7.672, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=479
2022-02-28 16:20:20 | INFO | train_inner | epoch 055:     28 / 31 loss=8.935, ppl=489.36, wps=15553, ups=12.75, wpb=1219.8, bsz=61.9, num_updates=1700, lr=5.1e-06, gnorm=7.593, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=481
2022-02-28 16:20:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:20:20 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.04 | ppl 526.48 | wps 31379.7 | wpb 591.2 | bsz 29.9 | num_updates 1703 | best_loss 9.02
2022-02-28 16:20:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1703 updates
2022-02-28 16:20:20 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:20:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:20:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 55 @ 1703 updates, score 9.04) (writing took 14.599523153039627 seconds)
2022-02-28 16:20:35 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-02-28 16:20:35 | INFO | train | epoch 055 | loss 8.944 | ppl 492.38 | wps 2165.9 | ups 1.76 | wpb 1227.6 | bsz 61.6 | num_updates 1703 | lr 5.109e-06 | gnorm 7.62 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 496
2022-02-28 16:20:35 | INFO | fairseq.trainer | begin training epoch 56
2022-02-28 16:20:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:20:36 | INFO | train_inner | epoch 056:     17 / 31 loss=8.831, ppl=455.37, wps=1403, ups=1.2, wpb=1164.4, bsz=61.1, num_updates=1720, lr=5.16e-06, gnorm=7.686, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=497
2022-02-28 16:20:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:20:38 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.985 | ppl 506.57 | wps 31678.5 | wpb 591.2 | bsz 29.9 | num_updates 1734 | best_loss 8.985
2022-02-28 16:20:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1734 updates
2022-02-28 16:20:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:20:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:20:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 1734 updates, score 8.985) (writing took 4.675053537997883 seconds)
2022-02-28 16:20:42 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-02-28 16:20:42 | INFO | train | epoch 056 | loss 8.9 | ppl 477.76 | wps 5131.8 | ups 4.18 | wpb 1227.6 | bsz 61.6 | num_updates 1734 | lr 5.202e-06 | gnorm 7.488 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 503
2022-02-28 16:20:42 | INFO | fairseq.trainer | begin training epoch 57
2022-02-28 16:20:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:20:43 | INFO | train_inner | epoch 057:      6 / 31 loss=8.975, ppl=503.13, wps=3539.8, ups=2.98, wpb=1186.1, bsz=60.3, num_updates=1740, lr=5.22e-06, gnorm=7.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=504
2022-02-28 16:20:44 | INFO | train_inner | epoch 057:     26 / 31 loss=8.748, ppl=429.85, wps=17220.9, ups=14.28, wpb=1205.9, bsz=63.2, num_updates=1760, lr=5.28e-06, gnorm=7.639, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=505
2022-02-28 16:20:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:20:45 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.917 | ppl 483.34 | wps 32407.2 | wpb 591.2 | bsz 29.9 | num_updates 1765 | best_loss 8.917
2022-02-28 16:20:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1765 updates
2022-02-28 16:20:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:20:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-28 16:21:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 57 @ 1765 updates, score 8.917) (writing took 14.989166194980498 seconds)
2022-02-28 16:21:00 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-02-28 16:21:00 | INFO | train | epoch 057 | loss 8.831 | ppl 455.29 | wps 2147.6 | ups 1.75 | wpb 1227.6 | bsz 61.6 | num_updates 1765 | lr 5.295e-06 | gnorm 7.701 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 521
2022-02-28 16:21:00 | INFO | fairseq.trainer | begin training epoch 58
2022-02-28 16:21:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:21:01 | INFO | train_inner | epoch 058:     15 / 31 loss=8.879, ppl=470.76, wps=1551, ups=1.18, wpb=1316, bsz=61.1, num_updates=1780, lr=5.34e-06, gnorm=7.282, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=522
2022-02-28 16:21:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:21:03 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.929 | ppl 487.52 | wps 29589.1 | wpb 591.2 | bsz 29.9 | num_updates 1796 | best_loss 8.917
2022-02-28 16:21:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1796 updates
2022-02-28 16:21:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:21:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:21:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 58 @ 1796 updates, score 8.929) (writing took 3.171486701001413 seconds)
2022-02-28 16:21:06 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-02-28 16:21:06 | INFO | train | epoch 058 | loss 8.785 | ppl 441.11 | wps 6494.6 | ups 5.29 | wpb 1227.6 | bsz 61.6 | num_updates 1796 | lr 5.388e-06 | gnorm 8.058 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 527
2022-02-28 16:21:06 | INFO | fairseq.trainer | begin training epoch 59
2022-02-28 16:21:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-28 16:21:06 | INFO | train_inner | epoch 059:      4 / 31 loss=8.758, ppl=432.86, wps=4829.7, ups=3.95, wpb=1223.8, bsz=61.6, num_updates=1800, lr=5.4e-06, gnorm=8.515, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=527
2022-02-28 16:21:08 | INFO | train_inner | epoch 059:     24 / 31 loss=8.653, ppl=402.6, wps=16632.2, ups=13.8, wpb=1205.6, bsz=61.9, num_updates=1820, lr=5.46e-06, gnorm=7.411, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=529
2022-02-28 16:21:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-28 16:21:09 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.931 | ppl 488.15 | wps 29334.1 | wpb 591.2 | bsz 29.9 | num_updates 1827 | best_loss 8.917
2022-02-28 16:21:09 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 2 runs
2022-02-28 16:21:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1827 updates
2022-02-28 16:21:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:21:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-28 16:21:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 59 @ 1827 updates, score 8.931) (writing took 20.49211299896706 seconds)
2022-02-28 16:21:29 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-02-28 16:21:29 | INFO | train | epoch 059 | loss 8.739 | ppl 427.18 | wps 1631.5 | ups 1.33 | wpb 1227.6 | bsz 61.6 | num_updates 1827 | lr 5.481e-06 | gnorm 7.532 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 550
2022-02-28 16:21:29 | INFO | fairseq_cli.train | done training in 547.0 seconds
