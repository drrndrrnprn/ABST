2022-01-12 23:51:38 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=50, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 50, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-12 23:51:38 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-01-12 23:51:41 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-01-12 23:51:41 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-01-12 23:51:41 | INFO | fairseq_cli.train | model: BARTMLModel
2022-01-12 23:51:41 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-01-12 23:51:41 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-01-12 23:51:41 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-12 23:51:41 | INFO | bartabst.data.data_utils | loaded 908 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-01-12 23:51:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-01-12 23:51:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-12 23:51:45 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-01-12 23:51:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-12 23:51:45 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-01-12 23:51:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-12 23:51:45 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-12 23:51:45 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-01-12 23:51:45 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-01-12 23:51:46 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-01-12 23:51:47 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-01-12 23:51:47 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-01-12 23:51:47 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-12 23:51:49 | INFO | bartabst.data.data_utils | loaded 3,174 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-01-12 23:51:49 | INFO | fairseq.trainer | begin training epoch 1
2022-01-12 23:51:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:51:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-01-12 23:51:51 | INFO | train_inner | epoch 001:     21 / 50 loss=17.159, ppl=146394, wps=16341.4, ups=14.05, wpb=1196, bsz=62.7, num_updates=20, lr=6e-08, gnorm=23.615, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=6
2022-01-12 23:51:53 | INFO | train_inner | epoch 001:     41 / 50 loss=17.296, ppl=160960, wps=14414.7, ups=11.41, wpb=1263.2, bsz=64, num_updates=40, lr=1.2e-07, gnorm=23.468, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=8
2022-01-12 23:51:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:51:54 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.132 | ppl 143673 | wps 27473.4 | wpb 556.6 | bsz 30.3 | num_updates 49
2022-01-12 23:51:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 49 updates
2022-01-12 23:51:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:51:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 49 updates, score 17.132) (writing took 5.029121475992724 seconds)
2022-01-12 23:51:59 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-12 23:51:59 | INFO | train | epoch 001 | loss 17.245 | ppl 155295 | wps 6090.9 | ups 5.05 | wpb 1219.7 | bsz 63.5 | num_updates 49 | lr 1.47e-07 | gnorm 23.417 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 14
2022-01-12 23:51:59 | INFO | fairseq.trainer | begin training epoch 2
2022-01-12 23:51:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:00 | INFO | train_inner | epoch 002:     11 / 50 loss=17.221, ppl=152767, wps=3093.8, ups=2.74, wpb=1130.5, bsz=62.7, num_updates=60, lr=1.8e-07, gnorm=23.114, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=15
2022-01-12 23:52:02 | INFO | train_inner | epoch 002:     31 / 50 loss=17.199, ppl=150443, wps=18866, ups=14.44, wpb=1306.8, bsz=64, num_updates=80, lr=2.4e-07, gnorm=22.864, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=17
2022-01-12 23:52:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 16.922 | ppl 124169 | wps 29092.5 | wpb 556.6 | bsz 30.3 | num_updates 99 | best_loss 16.922
2022-01-12 23:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 99 updates
2022-01-12 23:52:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 99 updates, score 16.922) (writing took 4.468111583031714 seconds)
2022-01-12 23:52:08 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-12 23:52:08 | INFO | train | epoch 002 | loss 17.126 | ppl 143014 | wps 7062 | ups 5.73 | wpb 1232.9 | bsz 63.5 | num_updates 99 | lr 2.97e-07 | gnorm 22.899 | clip 100 | loss_scale 64 | train_wall 3 | gb_free 20.9 | wall 23
2022-01-12 23:52:08 | INFO | fairseq.trainer | begin training epoch 3
2022-01-12 23:52:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:08 | INFO | train_inner | epoch 003:      1 / 50 loss=17.043, ppl=135068, wps=3738.2, ups=3.03, wpb=1233.1, bsz=64, num_updates=100, lr=3e-07, gnorm=22.704, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=23
2022-01-12 23:52:10 | INFO | train_inner | epoch 003:     21 / 50 loss=16.922, ppl=124168, wps=16858.9, ups=13.79, wpb=1222.8, bsz=62.7, num_updates=120, lr=3.6e-07, gnorm=22.369, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=25
2022-01-12 23:52:11 | INFO | train_inner | epoch 003:     41 / 50 loss=16.713, ppl=107421, wps=18444.4, ups=13.69, wpb=1347.1, bsz=64, num_updates=140, lr=4.2e-07, gnorm=21.783, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=26
2022-01-12 23:52:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:13 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.449 | ppl 89457.4 | wps 28067.7 | wpb 556.6 | bsz 30.3 | num_updates 149 | best_loss 16.449
2022-01-12 23:52:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 149 updates
2022-01-12 23:52:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 149 updates, score 16.449) (writing took 4.802918447880074 seconds)
2022-01-12 23:52:17 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-12 23:52:17 | INFO | train | epoch 003 | loss 16.775 | ppl 112183 | wps 6585.6 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 149 | lr 4.47e-07 | gnorm 22.104 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 32
2022-01-12 23:52:17 | INFO | fairseq.trainer | begin training epoch 4
2022-01-12 23:52:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:18 | INFO | train_inner | epoch 004:     11 / 50 loss=16.552, ppl=96075.3, wps=3100, ups=2.79, wpb=1109.7, bsz=62.7, num_updates=160, lr=4.8e-07, gnorm=22.008, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=33
2022-01-12 23:52:20 | INFO | train_inner | epoch 004:     31 / 50 loss=16.283, ppl=79750.9, wps=17829.4, ups=14.14, wpb=1261, bsz=64, num_updates=180, lr=5.4e-07, gnorm=21.031, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=35
2022-01-12 23:52:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:22 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 15.807 | ppl 57342.1 | wps 26205.5 | wpb 556.6 | bsz 30.3 | num_updates 199 | best_loss 15.807
2022-01-12 23:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 199 updates
2022-01-12 23:52:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 199 updates, score 15.807) (writing took 4.1711140119004995 seconds)
2022-01-12 23:52:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-12 23:52:26 | INFO | train | epoch 004 | loss 16.271 | ppl 79071.1 | wps 7202.4 | ups 5.84 | wpb 1232.9 | bsz 63.5 | num_updates 199 | lr 5.97e-07 | gnorm 21.383 | clip 100 | loss_scale 64 | train_wall 3 | gb_free 20.9 | wall 41
2022-01-12 23:52:26 | INFO | fairseq.trainer | begin training epoch 5
2022-01-12 23:52:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:26 | INFO | train_inner | epoch 005:      1 / 50 loss=16.097, ppl=70117.1, wps=3768.2, ups=3.11, wpb=1210.8, bsz=64, num_updates=200, lr=6e-07, gnorm=21.412, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=41
2022-01-12 23:52:28 | INFO | train_inner | epoch 005:     21 / 50 loss=15.771, ppl=55917.4, wps=16429.9, ups=13.29, wpb=1235.9, bsz=64, num_updates=220, lr=6.6e-07, gnorm=20.211, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=43
2022-01-12 23:52:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-01-12 23:52:29 | INFO | train_inner | epoch 005:     42 / 50 loss=15.559, ppl=48277.4, wps=18448.4, ups=14.02, wpb=1316.2, bsz=64, num_updates=240, lr=7.2e-07, gnorm=19.435, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=44
2022-01-12 23:52:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:30 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 15.243 | ppl 38786.9 | wps 27354.8 | wpb 556.6 | bsz 30.3 | num_updates 248 | best_loss 15.243
2022-01-12 23:52:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 248 updates
2022-01-12 23:52:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 248 updates, score 15.243) (writing took 4.165005217073485 seconds)
2022-01-12 23:52:34 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-12 23:52:35 | INFO | train | epoch 005 | loss 15.636 | ppl 50916.4 | wps 7098.7 | ups 5.71 | wpb 1243 | bsz 63.5 | num_updates 248 | lr 7.44e-07 | gnorm 20.049 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 50
2022-01-12 23:52:35 | INFO | fairseq.trainer | begin training epoch 6
2022-01-12 23:52:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:35 | INFO | train_inner | epoch 006:     12 / 50 loss=15.337, ppl=41381, wps=3760.2, ups=3.09, wpb=1217.2, bsz=62.7, num_updates=260, lr=7.8e-07, gnorm=19.805, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=51
2022-01-12 23:52:37 | INFO | train_inner | epoch 006:     32 / 50 loss=14.997, ppl=32702.2, wps=18633.8, ups=14.21, wpb=1311.7, bsz=62.7, num_updates=280, lr=8.4e-07, gnorm=19.077, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=52
2022-01-12 23:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:39 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 14.581 | ppl 24503.1 | wps 27435.2 | wpb 556.6 | bsz 30.3 | num_updates 298 | best_loss 14.581
2022-01-12 23:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 298 updates
2022-01-12 23:52:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 298 updates, score 14.581) (writing took 4.1451350569259375 seconds)
2022-01-12 23:52:43 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-12 23:52:43 | INFO | train | epoch 006 | loss 15.033 | ppl 33520 | wps 7267.8 | ups 5.9 | wpb 1232.9 | bsz 63.5 | num_updates 298 | lr 8.94e-07 | gnorm 18.987 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 58
2022-01-12 23:52:43 | INFO | fairseq.trainer | begin training epoch 7
2022-01-12 23:52:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:43 | INFO | train_inner | epoch 007:      2 / 50 loss=14.896, ppl=30486.8, wps=3582.8, ups=3.15, wpb=1136.9, bsz=64, num_updates=300, lr=9e-07, gnorm=18.768, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=58
2022-01-12 23:52:45 | INFO | train_inner | epoch 007:     22 / 50 loss=14.558, ppl=24115.6, wps=18272.6, ups=13.89, wpb=1315.3, bsz=64, num_updates=320, lr=9.6e-07, gnorm=17.902, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=60
2022-01-12 23:52:46 | INFO | train_inner | epoch 007:     42 / 50 loss=14.268, ppl=19731.4, wps=15200.6, ups=14.45, wpb=1052.3, bsz=62.7, num_updates=340, lr=1.02e-06, gnorm=19.044, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=61
2022-01-12 23:52:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:48 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 13.939 | ppl 15700.3 | wps 26097.4 | wpb 556.6 | bsz 30.3 | num_updates 348 | best_loss 13.939
2022-01-12 23:52:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 348 updates
2022-01-12 23:52:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 348 updates, score 13.939) (writing took 4.287074175197631 seconds)
2022-01-12 23:52:52 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-12 23:52:52 | INFO | train | epoch 007 | loss 14.362 | ppl 21061.2 | wps 6958.9 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 348 | lr 1.044e-06 | gnorm 18.136 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 67
2022-01-12 23:52:52 | INFO | fairseq.trainer | begin training epoch 8
2022-01-12 23:52:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:52:53 | INFO | train_inner | epoch 008:     12 / 50 loss=13.982, ppl=16177, wps=4133.3, ups=2.95, wpb=1399.5, bsz=64, num_updates=360, lr=1.08e-06, gnorm=18.713, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=68
2022-01-12 23:52:54 | INFO | train_inner | epoch 008:     32 / 50 loss=13.673, ppl=13065.5, wps=17796, ups=14.28, wpb=1246.2, bsz=64, num_updates=380, lr=1.14e-06, gnorm=16.927, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=69
2022-01-12 23:52:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:52:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 13.325 | ppl 10265.3 | wps 26425.9 | wpb 556.6 | bsz 30.3 | num_updates 398 | best_loss 13.325
2022-01-12 23:52:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 398 updates
2022-01-12 23:52:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:52:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 398 updates, score 13.325) (writing took 4.099818815011531 seconds)
2022-01-12 23:53:00 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-12 23:53:00 | INFO | train | epoch 008 | loss 13.671 | ppl 13041.4 | wps 7217.3 | ups 5.85 | wpb 1232.9 | bsz 63.5 | num_updates 398 | lr 1.194e-06 | gnorm 17.703 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 76
2022-01-12 23:53:00 | INFO | fairseq.trainer | begin training epoch 9
2022-01-12 23:53:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:01 | INFO | train_inner | epoch 009:      2 / 50 loss=13.445, ppl=11152.4, wps=3693.6, ups=3.12, wpb=1185.2, bsz=62.7, num_updates=400, lr=1.2e-06, gnorm=16.722, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=76
2022-01-12 23:53:02 | INFO | train_inner | epoch 009:     22 / 50 loss=13.342, ppl=10380.7, wps=16812.5, ups=13.58, wpb=1238, bsz=62.7, num_updates=420, lr=1.26e-06, gnorm=17.348, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=77
2022-01-12 23:53:04 | INFO | train_inner | epoch 009:     42 / 50 loss=13.019, ppl=8299.91, wps=16697.7, ups=13.69, wpb=1220, bsz=64, num_updates=440, lr=1.32e-06, gnorm=14.824, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=79
2022-01-12 23:53:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 12.838 | ppl 7324.38 | wps 26997 | wpb 556.6 | bsz 30.3 | num_updates 448 | best_loss 12.838
2022-01-12 23:53:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 448 updates
2022-01-12 23:53:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 448 updates, score 12.838) (writing took 4.447679174831137 seconds)
2022-01-12 23:53:09 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-12 23:53:09 | INFO | train | epoch 009 | loss 13.134 | ppl 8992.19 | wps 6858.8 | ups 5.56 | wpb 1232.9 | bsz 63.5 | num_updates 448 | lr 1.344e-06 | gnorm 15.786 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 85
2022-01-12 23:53:09 | INFO | fairseq.trainer | begin training epoch 10
2022-01-12 23:53:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:10 | INFO | train_inner | epoch 010:     12 / 50 loss=12.786, ppl=7061.32, wps=3611, ups=2.97, wpb=1217.8, bsz=62.7, num_updates=460, lr=1.38e-06, gnorm=14.703, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=85
2022-01-12 23:53:12 | INFO | train_inner | epoch 010:     32 / 50 loss=12.692, ppl=6618.46, wps=17043.9, ups=13.82, wpb=1232.8, bsz=64, num_updates=480, lr=1.44e-06, gnorm=14.378, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=87
2022-01-12 23:53:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:14 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 12.463 | ppl 5644.55 | wps 27276.5 | wpb 556.6 | bsz 30.3 | num_updates 498 | best_loss 12.463
2022-01-12 23:53:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 498 updates
2022-01-12 23:53:14 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 498 updates, score 12.463) (writing took 4.289617713075131 seconds)
2022-01-12 23:53:18 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-12 23:53:18 | INFO | train | epoch 010 | loss 12.642 | ppl 6391.42 | wps 6987.8 | ups 5.67 | wpb 1232.9 | bsz 63.5 | num_updates 498 | lr 1.494e-06 | gnorm 14.015 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 93
2022-01-12 23:53:18 | INFO | fairseq.trainer | begin training epoch 11
2022-01-12 23:53:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:19 | INFO | train_inner | epoch 011:      2 / 50 loss=12.533, ppl=5928.39, wps=3577.6, ups=2.99, wpb=1197, bsz=64, num_updates=500, lr=1.5e-06, gnorm=13.101, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=94
2022-01-12 23:53:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:19 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.415 | ppl 5460.19 | wps 26942 | wpb 556.6 | bsz 30.3 | num_updates 500 | best_loss 12.415
2022-01-12 23:53:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 500 updates
2022-01-12 23:53:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt
2022-01-12 23:53:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt
2022-01-12 23:53:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt (epoch 11 @ 500 updates, score 12.415) (writing took 7.498780622147024 seconds)
2022-01-12 23:53:28 | INFO | train_inner | epoch 011:     22 / 50 loss=12.392, ppl=5374.44, wps=2657, ups=2.03, wpb=1310, bsz=64, num_updates=520, lr=1.56e-06, gnorm=12.485, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=104
2022-01-12 23:53:30 | INFO | train_inner | epoch 011:     42 / 50 loss=12.215, ppl=4755.5, wps=15773.4, ups=13.26, wpb=1189.1, bsz=64, num_updates=540, lr=1.62e-06, gnorm=13.026, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=105
2022-01-12 23:53:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:31 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.077 | ppl 4321.61 | wps 30497.9 | wpb 556.6 | bsz 30.3 | num_updates 548 | best_loss 12.077
2022-01-12 23:53:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 548 updates
2022-01-12 23:53:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 548 updates, score 12.077) (writing took 4.4391100709326565 seconds)
2022-01-12 23:53:36 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-12 23:53:36 | INFO | train | epoch 011 | loss 12.282 | ppl 4980.5 | wps 3551.4 | ups 2.88 | wpb 1232.9 | bsz 63.5 | num_updates 548 | lr 1.644e-06 | gnorm 12.803 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 111
2022-01-12 23:53:36 | INFO | fairseq.trainer | begin training epoch 12
2022-01-12 23:53:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:37 | INFO | train_inner | epoch 012:     12 / 50 loss=12.166, ppl=4596.88, wps=3448.7, ups=2.84, wpb=1214.3, bsz=61.4, num_updates=560, lr=1.68e-06, gnorm=12.663, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=112
2022-01-12 23:53:38 | INFO | train_inner | epoch 012:     32 / 50 loss=12.011, ppl=4127.61, wps=16454.9, ups=13.47, wpb=1221.2, bsz=64, num_updates=580, lr=1.74e-06, gnorm=12.837, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=114
2022-01-12 23:53:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:40 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.796 | ppl 3555.78 | wps 28198.2 | wpb 556.6 | bsz 30.3 | num_updates 598 | best_loss 11.796
2022-01-12 23:53:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 598 updates
2022-01-12 23:53:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 598 updates, score 11.796) (writing took 4.454071433050558 seconds)
2022-01-12 23:53:45 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-12 23:53:45 | INFO | train | epoch 012 | loss 12.036 | ppl 4199.48 | wps 6988 | ups 5.67 | wpb 1232.9 | bsz 63.5 | num_updates 598 | lr 1.794e-06 | gnorm 12.537 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 120
2022-01-12 23:53:45 | INFO | fairseq.trainer | begin training epoch 13
2022-01-12 23:53:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:45 | INFO | train_inner | epoch 013:      2 / 50 loss=11.921, ppl=3878.97, wps=3726.4, ups=3.02, wpb=1232.8, bsz=64, num_updates=600, lr=1.8e-06, gnorm=12.194, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=120
2022-01-12 23:53:46 | INFO | train_inner | epoch 013:     22 / 50 loss=11.721, ppl=3376.6, wps=16894.1, ups=13.92, wpb=1213.5, bsz=62.7, num_updates=620, lr=1.86e-06, gnorm=11.924, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=122
2022-01-12 23:53:48 | INFO | train_inner | epoch 013:     42 / 50 loss=11.651, ppl=3215.5, wps=17250.3, ups=13.97, wpb=1234.5, bsz=64, num_updates=640, lr=1.92e-06, gnorm=11.082, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=123
2022-01-12 23:53:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:49 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.555 | ppl 3009.11 | wps 26811.2 | wpb 556.6 | bsz 30.3 | num_updates 648 | best_loss 11.555
2022-01-12 23:53:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 648 updates
2022-01-12 23:53:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:53:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 648 updates, score 11.555) (writing took 4.713082507951185 seconds)
2022-01-12 23:53:54 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-12 23:53:54 | INFO | train | epoch 013 | loss 11.741 | ppl 3423.89 | wps 6679 | ups 5.42 | wpb 1232.9 | bsz 63.5 | num_updates 648 | lr 1.944e-06 | gnorm 11.452 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 129
2022-01-12 23:53:54 | INFO | fairseq.trainer | begin training epoch 14
2022-01-12 23:53:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:53:55 | INFO | train_inner | epoch 014:     12 / 50 loss=11.813, ppl=3596.98, wps=3568.5, ups=2.83, wpb=1259.2, bsz=64, num_updates=660, lr=1.98e-06, gnorm=11.379, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=130
2022-01-12 23:53:56 | INFO | train_inner | epoch 014:     32 / 50 loss=11.503, ppl=2902.06, wps=18879.4, ups=14.21, wpb=1329, bsz=62.7, num_updates=680, lr=2.04e-06, gnorm=10.932, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=132
2022-01-12 23:53:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:53:58 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.387 | ppl 2678 | wps 25868 | wpb 556.6 | bsz 30.3 | num_updates 698 | best_loss 11.387
2022-01-12 23:53:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 698 updates
2022-01-12 23:53:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 698 updates, score 11.387) (writing took 4.417004777817056 seconds)
2022-01-12 23:54:03 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-12 23:54:03 | INFO | train | epoch 014 | loss 11.52 | ppl 2937.44 | wps 6941.2 | ups 5.63 | wpb 1232.9 | bsz 63.5 | num_updates 698 | lr 2.094e-06 | gnorm 11.088 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 138
2022-01-12 23:54:03 | INFO | fairseq.trainer | begin training epoch 15
2022-01-12 23:54:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:03 | INFO | train_inner | epoch 015:      2 / 50 loss=11.432, ppl=2762.96, wps=3347.2, ups=2.95, wpb=1136.4, bsz=64, num_updates=700, lr=2.1e-06, gnorm=10.866, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=138
2022-01-12 23:54:05 | INFO | train_inner | epoch 015:     22 / 50 loss=11.372, ppl=2650.52, wps=17312.9, ups=14.56, wpb=1188.8, bsz=64, num_updates=720, lr=2.16e-06, gnorm=11.018, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=140
2022-01-12 23:54:06 | INFO | train_inner | epoch 015:     42 / 50 loss=11.221, ppl=2387.45, wps=19193.5, ups=14.02, wpb=1368.7, bsz=64, num_updates=740, lr=2.22e-06, gnorm=10.342, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=141
2022-01-12 23:54:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:07 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.136 | ppl 2250.2 | wps 29516.2 | wpb 556.6 | bsz 30.3 | num_updates 748 | best_loss 11.136
2022-01-12 23:54:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 748 updates
2022-01-12 23:54:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 748 updates, score 11.136) (writing took 4.124612756073475 seconds)
2022-01-12 23:54:11 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-12 23:54:11 | INFO | train | epoch 015 | loss 11.292 | ppl 2507.03 | wps 7200.7 | ups 5.84 | wpb 1232.9 | bsz 63.5 | num_updates 748 | lr 2.244e-06 | gnorm 10.679 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 147
2022-01-12 23:54:12 | INFO | fairseq.trainer | begin training epoch 16
2022-01-12 23:54:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:12 | INFO | train_inner | epoch 016:     12 / 50 loss=11.361, ppl=2629.88, wps=3515.4, ups=3.09, wpb=1138.8, bsz=62.7, num_updates=760, lr=2.28e-06, gnorm=10.219, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=148
2022-01-12 23:54:14 | INFO | train_inner | epoch 016:     32 / 50 loss=11.075, ppl=2156.92, wps=15435.3, ups=13.67, wpb=1129.2, bsz=64, num_updates=780, lr=2.34e-06, gnorm=11.173, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=149
2022-01-12 23:54:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:16 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.99 | ppl 2033.53 | wps 26551.2 | wpb 556.6 | bsz 30.3 | num_updates 798 | best_loss 10.99
2022-01-12 23:54:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 798 updates
2022-01-12 23:54:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 798 updates, score 10.99) (writing took 4.637598752975464 seconds)
2022-01-12 23:54:21 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-12 23:54:21 | INFO | train | epoch 016 | loss 11.192 | ppl 2340.24 | wps 6766.1 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 798 | lr 2.394e-06 | gnorm 10.345 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 156
2022-01-12 23:54:21 | INFO | fairseq.trainer | begin training epoch 17
2022-01-12 23:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:21 | INFO | train_inner | epoch 017:      2 / 50 loss=11.151, ppl=2273.34, wps=3784.2, ups=2.88, wpb=1312.2, bsz=62.7, num_updates=800, lr=2.4e-06, gnorm=9.816, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=156
2022-01-12 23:54:22 | INFO | train_inner | epoch 017:     22 / 50 loss=10.972, ppl=2008.1, wps=15993, ups=13.78, wpb=1160.7, bsz=62.7, num_updates=820, lr=2.46e-06, gnorm=10.458, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=157
2022-01-12 23:54:24 | INFO | train_inner | epoch 017:     42 / 50 loss=11.179, ppl=2318.11, wps=19648.3, ups=13.37, wpb=1469.3, bsz=64, num_updates=840, lr=2.52e-06, gnorm=9.358, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=159
2022-01-12 23:54:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:25 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.807 | ppl 1791.87 | wps 25449.6 | wpb 556.6 | bsz 30.3 | num_updates 848 | best_loss 10.807
2022-01-12 23:54:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 848 updates
2022-01-12 23:54:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 848 updates, score 10.807) (writing took 4.269093121169135 seconds)
2022-01-12 23:54:29 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-12 23:54:29 | INFO | train | epoch 017 | loss 11.03 | ppl 2090.71 | wps 6977.1 | ups 5.66 | wpb 1232.9 | bsz 63.5 | num_updates 848 | lr 2.544e-06 | gnorm 9.918 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 165
2022-01-12 23:54:29 | INFO | fairseq.trainer | begin training epoch 18
2022-01-12 23:54:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:30 | INFO | train_inner | epoch 018:     12 / 50 loss=10.843, ppl=1836.91, wps=3308.3, ups=3.04, wpb=1087.5, bsz=64, num_updates=860, lr=2.58e-06, gnorm=9.365, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=166
2022-01-12 23:54:32 | INFO | train_inner | epoch 018:     32 / 50 loss=10.828, ppl=1818.15, wps=17520.8, ups=13.82, wpb=1268, bsz=62.7, num_updates=880, lr=2.64e-06, gnorm=10.213, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=167
2022-01-12 23:54:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:34 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.597 | ppl 1548.95 | wps 26539 | wpb 556.6 | bsz 30.3 | num_updates 898 | best_loss 10.597
2022-01-12 23:54:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 898 updates
2022-01-12 23:54:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 898 updates, score 10.597) (writing took 4.445276870159432 seconds)
2022-01-12 23:54:39 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-12 23:54:39 | INFO | train | epoch 018 | loss 10.846 | ppl 1840.12 | wps 6763.6 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 898 | lr 2.694e-06 | gnorm 9.592 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 174
2022-01-12 23:54:39 | INFO | fairseq.trainer | begin training epoch 19
2022-01-12 23:54:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:39 | INFO | train_inner | epoch 019:      2 / 50 loss=10.821, ppl=1808.99, wps=3363.1, ups=2.89, wpb=1164.9, bsz=64, num_updates=900, lr=2.7e-06, gnorm=9.536, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=174
2022-01-12 23:54:40 | INFO | train_inner | epoch 019:     22 / 50 loss=10.713, ppl=1678.08, wps=16347.1, ups=13.48, wpb=1212.3, bsz=62.7, num_updates=920, lr=2.76e-06, gnorm=9.34, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=175
2022-01-12 23:54:42 | INFO | train_inner | epoch 019:     42 / 50 loss=10.76, ppl=1734.71, wps=15970.7, ups=12.28, wpb=1300.9, bsz=64, num_updates=940, lr=2.82e-06, gnorm=9.489, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=177
2022-01-12 23:54:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:43 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 10.489 | ppl 1437.09 | wps 26452.6 | wpb 556.6 | bsz 30.3 | num_updates 948 | best_loss 10.489
2022-01-12 23:54:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 948 updates
2022-01-12 23:54:43 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 948 updates, score 10.489) (writing took 4.3387409390416 seconds)
2022-01-12 23:54:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-12 23:54:48 | INFO | train | epoch 019 | loss 10.694 | ppl 1657.03 | wps 6778.2 | ups 5.5 | wpb 1232.9 | bsz 63.5 | num_updates 948 | lr 2.844e-06 | gnorm 9.546 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 183
2022-01-12 23:54:48 | INFO | fairseq.trainer | begin training epoch 20
2022-01-12 23:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:49 | INFO | train_inner | epoch 020:     12 / 50 loss=10.489, ppl=1437.05, wps=3413.7, ups=2.98, wpb=1145.7, bsz=64, num_updates=960, lr=2.88e-06, gnorm=10.143, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=184
2022-01-12 23:54:50 | INFO | train_inner | epoch 020:     32 / 50 loss=10.624, ppl=1578.44, wps=17115, ups=13.8, wpb=1240, bsz=62.7, num_updates=980, lr=2.94e-06, gnorm=9.196, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=185
2022-01-12 23:54:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:52 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 10.395 | ppl 1346.34 | wps 27010.2 | wpb 556.6 | bsz 30.3 | num_updates 998 | best_loss 10.395
2022-01-12 23:54:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 998 updates
2022-01-12 23:54:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:54:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 998 updates, score 10.395) (writing took 4.420613712863997 seconds)
2022-01-12 23:54:57 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-12 23:54:57 | INFO | train | epoch 020 | loss 10.579 | ppl 1529.35 | wps 6888.6 | ups 5.59 | wpb 1232.9 | bsz 63.5 | num_updates 998 | lr 2.994e-06 | gnorm 9.368 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 192
2022-01-12 23:54:57 | INFO | fairseq.trainer | begin training epoch 21
2022-01-12 23:54:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:54:57 | INFO | train_inner | epoch 021:      2 / 50 loss=10.563, ppl=1513.22, wps=3660.2, ups=2.87, wpb=1273.9, bsz=64, num_updates=1000, lr=3e-06, gnorm=9.044, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=192
2022-01-12 23:54:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:54:58 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.327 | ppl 1284.77 | wps 26793.8 | wpb 556.6 | bsz 30.3 | num_updates 1000 | best_loss 10.327
2022-01-12 23:54:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1000 updates
2022-01-12 23:54:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt
2022-01-12 23:55:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt
2022-01-12 23:55:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt (epoch 21 @ 1000 updates, score 10.327) (writing took 6.069689427968115 seconds)
2022-01-12 23:55:06 | INFO | train_inner | epoch 021:     22 / 50 loss=10.557, ppl=1506.07, wps=3021.3, ups=2.32, wpb=1300.8, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=9.176, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=201
2022-01-12 23:55:07 | INFO | train_inner | epoch 021:     42 / 50 loss=10.38, ppl=1332.46, wps=16585.7, ups=13.6, wpb=1219.5, bsz=64, num_updates=1040, lr=3.12e-06, gnorm=8.935, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=202
2022-01-12 23:55:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:08 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.192 | ppl 1169.41 | wps 27626.5 | wpb 556.6 | bsz 30.3 | num_updates 1048 | best_loss 10.192
2022-01-12 23:55:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1048 updates
2022-01-12 23:55:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 1048 updates, score 10.192) (writing took 4.609479784034193 seconds)
2022-01-12 23:55:13 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-12 23:55:15 | INFO | train | epoch 021 | loss 10.483 | ppl 1431.35 | wps 3800.6 | ups 3.08 | wpb 1232.9 | bsz 63.5 | num_updates 1048 | lr 3.144e-06 | gnorm 9.147 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 208
2022-01-12 23:55:15 | INFO | fairseq.trainer | begin training epoch 22
2022-01-12 23:55:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:16 | INFO | train_inner | epoch 022:     12 / 50 loss=10.365, ppl=1318.47, wps=2851.5, ups=2.39, wpb=1195.2, bsz=62.7, num_updates=1060, lr=3.18e-06, gnorm=9.221, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=211
2022-01-12 23:55:17 | INFO | train_inner | epoch 022:     32 / 50 loss=10.429, ppl=1378.86, wps=18621.7, ups=14.71, wpb=1265.5, bsz=64, num_updates=1080, lr=3.24e-06, gnorm=8.68, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=212
2022-01-12 23:55:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:19 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 10.095 | ppl 1093.8 | wps 26662.4 | wpb 556.6 | bsz 30.3 | num_updates 1098 | best_loss 10.095
2022-01-12 23:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1098 updates
2022-01-12 23:55:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 1098 updates, score 10.095) (writing took 4.358964095823467 seconds)
2022-01-12 23:55:23 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-12 23:55:23 | INFO | train | epoch 022 | loss 10.341 | ppl 1297.09 | wps 7068.2 | ups 5.73 | wpb 1232.9 | bsz 63.5 | num_updates 1098 | lr 3.294e-06 | gnorm 9.153 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 218
2022-01-12 23:55:23 | INFO | fairseq.trainer | begin training epoch 23
2022-01-12 23:55:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:24 | INFO | train_inner | epoch 023:      2 / 50 loss=10.307, ppl=1266.61, wps=3506.3, ups=3.01, wpb=1164.5, bsz=62.7, num_updates=1100, lr=3.3e-06, gnorm=9.695, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=219
2022-01-12 23:55:25 | INFO | train_inner | epoch 023:     22 / 50 loss=10.193, ppl=1170.63, wps=16977, ups=13.89, wpb=1222.5, bsz=64, num_updates=1120, lr=3.36e-06, gnorm=8.713, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=220
2022-01-12 23:55:26 | INFO | train_inner | epoch 023:     42 / 50 loss=10.203, ppl=1178.85, wps=20223.5, ups=14.75, wpb=1370.7, bsz=64, num_updates=1140, lr=3.42e-06, gnorm=8.855, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=221
2022-01-12 23:55:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:28 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 10.011 | ppl 1031.65 | wps 26137.9 | wpb 556.6 | bsz 30.3 | num_updates 1148 | best_loss 10.011
2022-01-12 23:55:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1148 updates
2022-01-12 23:55:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 1148 updates, score 10.011) (writing took 4.447407083818689 seconds)
2022-01-12 23:55:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-12 23:55:32 | INFO | train | epoch 023 | loss 10.189 | ppl 1167.45 | wps 7030 | ups 5.7 | wpb 1232.9 | bsz 63.5 | num_updates 1148 | lr 3.444e-06 | gnorm 9.034 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 227
2022-01-12 23:55:32 | INFO | fairseq.trainer | begin training epoch 24
2022-01-12 23:55:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:33 | INFO | train_inner | epoch 024:     12 / 50 loss=10.111, ppl=1105.92, wps=2916.6, ups=2.93, wpb=994, bsz=62.7, num_updates=1160, lr=3.48e-06, gnorm=9.566, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=228
2022-01-12 23:55:35 | INFO | train_inner | epoch 024:     32 / 50 loss=10.131, ppl=1121.41, wps=17811.8, ups=12.5, wpb=1425.5, bsz=62.7, num_updates=1180, lr=3.54e-06, gnorm=8.593, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=230
2022-01-12 23:55:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:37 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.868 | ppl 934.23 | wps 26896.4 | wpb 556.6 | bsz 30.3 | num_updates 1198 | best_loss 9.868
2022-01-12 23:55:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1198 updates
2022-01-12 23:55:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:40 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 1198 updates, score 9.868) (writing took 4.69009534898214 seconds)
2022-01-12 23:55:42 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-12 23:55:42 | INFO | train | epoch 024 | loss 10.057 | ppl 1065.31 | wps 6488.8 | ups 5.26 | wpb 1232.9 | bsz 63.5 | num_updates 1198 | lr 3.594e-06 | gnorm 8.873 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 237
2022-01-12 23:55:42 | INFO | fairseq.trainer | begin training epoch 25
2022-01-12 23:55:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:42 | INFO | train_inner | epoch 025:      2 / 50 loss=9.954, ppl=991.84, wps=3271.9, ups=2.81, wpb=1164.8, bsz=64, num_updates=1200, lr=3.6e-06, gnorm=8.989, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=237
2022-01-12 23:55:43 | INFO | train_inner | epoch 025:     22 / 50 loss=9.978, ppl=1008.7, wps=18366.2, ups=13.79, wpb=1331.4, bsz=62.7, num_updates=1220, lr=3.66e-06, gnorm=9.158, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=238
2022-01-12 23:55:45 | INFO | train_inner | epoch 025:     42 / 50 loss=9.923, ppl=970.62, wps=16170.2, ups=13.64, wpb=1185.5, bsz=64, num_updates=1240, lr=3.72e-06, gnorm=8.794, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=240
2022-01-12 23:55:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:46 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.696 | ppl 829.18 | wps 26867.5 | wpb 556.6 | bsz 30.3 | num_updates 1248 | best_loss 9.696
2022-01-12 23:55:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1248 updates
2022-01-12 23:55:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 1248 updates, score 9.696) (writing took 4.413138160016388 seconds)
2022-01-12 23:55:51 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-12 23:55:51 | INFO | train | epoch 025 | loss 9.919 | ppl 968.37 | wps 6820.8 | ups 5.53 | wpb 1232.9 | bsz 63.5 | num_updates 1248 | lr 3.744e-06 | gnorm 8.967 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 246
2022-01-12 23:55:51 | INFO | fairseq.trainer | begin training epoch 26
2022-01-12 23:55:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:55:52 | INFO | train_inner | epoch 026:     12 / 50 loss=9.809, ppl=896.91, wps=3597, ups=2.92, wpb=1231.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=9.137, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=247
2022-01-12 23:55:53 | INFO | train_inner | epoch 026:     32 / 50 loss=9.827, ppl=908.3, wps=14846.7, ups=12.64, wpb=1174.3, bsz=64, num_updates=1280, lr=3.84e-06, gnorm=8.768, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=248
2022-01-12 23:55:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:55:55 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.581 | ppl 765.72 | wps 26388.5 | wpb 556.6 | bsz 30.3 | num_updates 1298 | best_loss 9.581
2022-01-12 23:55:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1298 updates
2022-01-12 23:55:55 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:55:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 1298 updates, score 9.581) (writing took 4.977432888932526 seconds)
2022-01-12 23:56:00 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-12 23:56:00 | INFO | train | epoch 026 | loss 9.811 | ppl 898.38 | wps 6337.7 | ups 5.14 | wpb 1232.9 | bsz 63.5 | num_updates 1298 | lr 3.894e-06 | gnorm 8.908 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 256
2022-01-12 23:56:00 | INFO | fairseq.trainer | begin training epoch 27
2022-01-12 23:56:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:01 | INFO | train_inner | epoch 027:      2 / 50 loss=9.732, ppl=850.61, wps=3375.5, ups=2.69, wpb=1252.9, bsz=64, num_updates=1300, lr=3.9e-06, gnorm=8.941, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=256
2022-01-12 23:56:02 | INFO | train_inner | epoch 027:     22 / 50 loss=9.839, ppl=916, wps=16485.7, ups=13.25, wpb=1244.4, bsz=64, num_updates=1320, lr=3.96e-06, gnorm=8.61, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=257
2022-01-12 23:56:04 | INFO | train_inner | epoch 027:     42 / 50 loss=9.762, ppl=868.56, wps=16674.8, ups=13.12, wpb=1271.2, bsz=62.7, num_updates=1340, lr=4.02e-06, gnorm=9.589, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=259
2022-01-12 23:56:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:05 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.511 | ppl 729.67 | wps 24503.1 | wpb 556.6 | bsz 30.3 | num_updates 1348 | best_loss 9.511
2022-01-12 23:56:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1348 updates
2022-01-12 23:56:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 1348 updates, score 9.511) (writing took 4.767022804124281 seconds)
2022-01-12 23:56:10 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-12 23:56:10 | INFO | train | epoch 027 | loss 9.714 | ppl 840.1 | wps 6437.5 | ups 5.22 | wpb 1232.9 | bsz 63.5 | num_updates 1348 | lr 4.044e-06 | gnorm 9.17 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 265
2022-01-12 23:56:10 | INFO | fairseq.trainer | begin training epoch 28
2022-01-12 23:56:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:11 | INFO | train_inner | epoch 028:     12 / 50 loss=9.468, ppl=707.99, wps=3060.6, ups=2.75, wpb=1113.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=9.25, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=266
2022-01-12 23:56:13 | INFO | train_inner | epoch 028:     32 / 50 loss=9.61, ppl=781.23, wps=13882.5, ups=11.36, wpb=1222.3, bsz=64, num_updates=1380, lr=4.14e-06, gnorm=8.775, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=268
2022-01-12 23:56:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:15 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.386 | ppl 668.91 | wps 25395.5 | wpb 556.6 | bsz 30.3 | num_updates 1398 | best_loss 9.386
2022-01-12 23:56:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1398 updates
2022-01-12 23:56:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:18 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 1398 updates, score 9.386) (writing took 4.761819014092907 seconds)
2022-01-12 23:56:20 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-12 23:56:20 | INFO | train | epoch 028 | loss 9.54 | ppl 744.64 | wps 6158.4 | ups 5 | wpb 1232.9 | bsz 63.5 | num_updates 1398 | lr 4.194e-06 | gnorm 9.601 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 275
2022-01-12 23:56:20 | INFO | fairseq.trainer | begin training epoch 29
2022-01-12 23:56:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:20 | INFO | train_inner | epoch 029:      2 / 50 loss=9.477, ppl=712.75, wps=3633.5, ups=2.66, wpb=1367.7, bsz=64, num_updates=1400, lr=4.2e-06, gnorm=10.426, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=275
2022-01-12 23:56:22 | INFO | train_inner | epoch 029:     22 / 50 loss=9.416, ppl=682.92, wps=14915.4, ups=12.79, wpb=1166, bsz=64, num_updates=1420, lr=4.26e-06, gnorm=8.797, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=277
2022-01-12 23:56:23 | INFO | train_inner | epoch 029:     42 / 50 loss=9.494, ppl=720.93, wps=17046.6, ups=13.03, wpb=1308.7, bsz=62.7, num_updates=1440, lr=4.32e-06, gnorm=9.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=279
2022-01-12 23:56:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:25 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.331 | ppl 643.99 | wps 25605.2 | wpb 556.6 | bsz 30.3 | num_updates 1448 | best_loss 9.331
2022-01-12 23:56:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1448 updates
2022-01-12 23:56:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 1448 updates, score 9.331) (writing took 4.545991942053661 seconds)
2022-01-12 23:56:29 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-12 23:56:29 | INFO | train | epoch 029 | loss 9.453 | ppl 700.95 | wps 6588.5 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 1448 | lr 4.344e-06 | gnorm 9.121 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 284
2022-01-12 23:56:29 | INFO | fairseq.trainer | begin training epoch 30
2022-01-12 23:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:30 | INFO | train_inner | epoch 030:     12 / 50 loss=9.476, ppl=712.32, wps=3644.8, ups=2.87, wpb=1269.3, bsz=64, num_updates=1460, lr=4.38e-06, gnorm=9.229, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=285
2022-01-12 23:56:32 | INFO | train_inner | epoch 030:     32 / 50 loss=9.288, ppl=625.25, wps=15289.5, ups=13.05, wpb=1171.5, bsz=64, num_updates=1480, lr=4.44e-06, gnorm=9.017, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=287
2022-01-12 23:56:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:34 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.253 | ppl 610.25 | wps 28299.3 | wpb 556.6 | bsz 30.3 | num_updates 1498 | best_loss 9.253
2022-01-12 23:56:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1498 updates
2022-01-12 23:56:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 30 @ 1498 updates, score 9.253) (writing took 4.596516771009192 seconds)
2022-01-12 23:56:39 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-12 23:56:39 | INFO | train | epoch 030 | loss 9.33 | ppl 643.78 | wps 6721.4 | ups 5.45 | wpb 1232.9 | bsz 63.5 | num_updates 1498 | lr 4.494e-06 | gnorm 8.929 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 294
2022-01-12 23:56:39 | INFO | fairseq.trainer | begin training epoch 31
2022-01-12 23:56:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:56:39 | INFO | train_inner | epoch 031:      2 / 50 loss=9.202, ppl=588.98, wps=3378.8, ups=2.84, wpb=1189.5, bsz=62.7, num_updates=1500, lr=4.5e-06, gnorm=9.095, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=294
2022-01-12 23:56:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:40 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.232 | ppl 601.18 | wps 23535 | wpb 556.6 | bsz 30.3 | num_updates 1500 | best_loss 9.232
2022-01-12 23:56:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1500 updates
2022-01-12 23:56:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt
2022-01-12 23:56:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt
2022-01-12 23:56:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt (epoch 31 @ 1500 updates, score 9.232) (writing took 9.790959219215438 seconds)
2022-01-12 23:56:51 | INFO | train_inner | epoch 031:     22 / 50 loss=9.321, ppl=639.69, wps=2117.9, ups=1.6, wpb=1321.7, bsz=62.7, num_updates=1520, lr=4.56e-06, gnorm=9.707, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=307
2022-01-12 23:56:53 | INFO | train_inner | epoch 031:     42 / 50 loss=9.174, ppl=577.5, wps=14784.7, ups=12.44, wpb=1188.6, bsz=64, num_updates=1540, lr=4.62e-06, gnorm=9.061, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=308
2022-01-12 23:56:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:56:54 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.117 | ppl 555.3 | wps 27764.4 | wpb 556.6 | bsz 30.3 | num_updates 1548 | best_loss 9.117
2022-01-12 23:56:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1548 updates
2022-01-12 23:56:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:56:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 1548 updates, score 9.117) (writing took 4.195408324943855 seconds)
2022-01-12 23:56:59 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-12 23:56:59 | INFO | train | epoch 031 | loss 9.198 | ppl 587.19 | wps 3091.7 | ups 2.51 | wpb 1232.9 | bsz 63.5 | num_updates 1548 | lr 4.644e-06 | gnorm 9.24 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 314
2022-01-12 23:56:59 | INFO | fairseq.trainer | begin training epoch 32
2022-01-12 23:56:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:00 | INFO | train_inner | epoch 032:     12 / 50 loss=9.169, ppl=575.58, wps=3594.8, ups=3.06, wpb=1174, bsz=64, num_updates=1560, lr=4.68e-06, gnorm=8.772, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=315
2022-01-12 23:57:01 | INFO | train_inner | epoch 032:     32 / 50 loss=9.072, ppl=538.37, wps=18087.3, ups=14.18, wpb=1275.7, bsz=64, num_updates=1580, lr=4.74e-06, gnorm=8.731, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=316
2022-01-12 23:57:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:03 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.027 | ppl 521.78 | wps 28324.7 | wpb 556.6 | bsz 30.3 | num_updates 1598 | best_loss 9.027
2022-01-12 23:57:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1598 updates
2022-01-12 23:57:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 32 @ 1598 updates, score 9.027) (writing took 4.552312697982416 seconds)
2022-01-12 23:57:07 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-12 23:57:07 | INFO | train | epoch 032 | loss 9.134 | ppl 562.01 | wps 6984.5 | ups 5.67 | wpb 1232.9 | bsz 63.5 | num_updates 1598 | lr 4.794e-06 | gnorm 8.798 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 323
2022-01-12 23:57:07 | INFO | fairseq.trainer | begin training epoch 33
2022-01-12 23:57:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:08 | INFO | train_inner | epoch 033:      2 / 50 loss=9.083, ppl=542.28, wps=3466.3, ups=2.97, wpb=1168.8, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=8.898, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=323
2022-01-12 23:57:09 | INFO | train_inner | epoch 033:     22 / 50 loss=9.16, ppl=572.2, wps=17745, ups=14.51, wpb=1223.3, bsz=62.7, num_updates=1620, lr=4.86e-06, gnorm=8.639, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=324
2022-01-12 23:57:11 | INFO | train_inner | epoch 033:     42 / 50 loss=8.97, ppl=501.37, wps=18239.8, ups=14.09, wpb=1294.6, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=8.891, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=326
2022-01-12 23:57:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:12 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.96 | ppl 498.03 | wps 27182 | wpb 556.6 | bsz 30.3 | num_updates 1648 | best_loss 8.96
2022-01-12 23:57:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1648 updates
2022-01-12 23:57:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1648 updates, score 8.96) (writing took 4.40379764395766 seconds)
2022-01-12 23:57:16 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-12 23:57:16 | INFO | train | epoch 033 | loss 9.046 | ppl 528.63 | wps 6999.9 | ups 5.68 | wpb 1232.9 | bsz 63.5 | num_updates 1648 | lr 4.944e-06 | gnorm 8.853 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 331
2022-01-12 23:57:16 | INFO | fairseq.trainer | begin training epoch 34
2022-01-12 23:57:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:17 | INFO | train_inner | epoch 034:     12 / 50 loss=8.992, ppl=509.02, wps=3243.3, ups=2.95, wpb=1098.5, bsz=64, num_updates=1660, lr=4.98e-06, gnorm=9.15, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=332
2022-01-12 23:57:19 | INFO | train_inner | epoch 034:     32 / 50 loss=8.89, ppl=474.37, wps=16938.6, ups=14.18, wpb=1194.3, bsz=64, num_updates=1680, lr=5.04e-06, gnorm=8.716, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=334
2022-01-12 23:57:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:21 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.847 | ppl 460.56 | wps 27702.4 | wpb 556.6 | bsz 30.3 | num_updates 1698 | best_loss 8.847
2022-01-12 23:57:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1698 updates
2022-01-12 23:57:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1698 updates, score 8.847) (writing took 4.3369038458913565 seconds)
2022-01-12 23:57:25 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-12 23:57:25 | INFO | train | epoch 034 | loss 8.964 | ppl 499.34 | wps 7080.6 | ups 5.74 | wpb 1232.9 | bsz 63.5 | num_updates 1698 | lr 5.094e-06 | gnorm 8.822 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 340
2022-01-12 23:57:25 | INFO | fairseq.trainer | begin training epoch 35
2022-01-12 23:57:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:25 | INFO | train_inner | epoch 035:      2 / 50 loss=9.058, ppl=532.94, wps=4522.8, ups=3.06, wpb=1478.3, bsz=62.7, num_updates=1700, lr=5.1e-06, gnorm=8.623, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=340
2022-01-12 23:57:27 | INFO | train_inner | epoch 035:     22 / 50 loss=8.871, ppl=468.19, wps=16268.2, ups=14.23, wpb=1143.2, bsz=64, num_updates=1720, lr=5.16e-06, gnorm=8.842, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=342
2022-01-12 23:57:28 | INFO | train_inner | epoch 035:     42 / 50 loss=8.779, ppl=439.19, wps=18255.3, ups=13.68, wpb=1334.7, bsz=62.7, num_updates=1740, lr=5.22e-06, gnorm=8.948, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=343
2022-01-12 23:57:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:29 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.728 | ppl 424.15 | wps 28176.1 | wpb 556.6 | bsz 30.3 | num_updates 1748 | best_loss 8.728
2022-01-12 23:57:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1748 updates
2022-01-12 23:57:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1748 updates, score 8.728) (writing took 4.49892555992119 seconds)
2022-01-12 23:57:34 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-12 23:57:34 | INFO | train | epoch 035 | loss 8.816 | ppl 450.69 | wps 6926.2 | ups 5.62 | wpb 1232.9 | bsz 63.5 | num_updates 1748 | lr 5.244e-06 | gnorm 8.869 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 349
2022-01-12 23:57:34 | INFO | fairseq.trainer | begin training epoch 36
2022-01-12 23:57:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:35 | INFO | train_inner | epoch 036:     12 / 50 loss=8.727, ppl=423.83, wps=3625.6, ups=2.92, wpb=1242.2, bsz=64, num_updates=1760, lr=5.28e-06, gnorm=8.865, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=350
2022-01-12 23:57:36 | INFO | train_inner | epoch 036:     32 / 50 loss=8.629, ppl=395.97, wps=18633.1, ups=14.4, wpb=1293.7, bsz=62.7, num_updates=1780, lr=5.34e-06, gnorm=8.793, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=352
2022-01-12 23:57:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:38 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.65 | ppl 401.69 | wps 27451.5 | wpb 556.6 | bsz 30.3 | num_updates 1798 | best_loss 8.65
2022-01-12 23:57:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1798 updates
2022-01-12 23:57:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1798 updates, score 8.65) (writing took 5.092594149056822 seconds)
2022-01-12 23:57:43 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-12 23:57:43 | INFO | train | epoch 036 | loss 8.673 | ppl 408.06 | wps 6446.3 | ups 5.23 | wpb 1232.9 | bsz 63.5 | num_updates 1798 | lr 5.394e-06 | gnorm 8.941 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 359
2022-01-12 23:57:44 | INFO | fairseq.trainer | begin training epoch 37
2022-01-12 23:57:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:44 | INFO | train_inner | epoch 037:      2 / 50 loss=8.652, ppl=402.3, wps=3091.1, ups=2.72, wpb=1136.7, bsz=64, num_updates=1800, lr=5.4e-06, gnorm=9.128, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=359
2022-01-12 23:57:45 | INFO | train_inner | epoch 037:     22 / 50 loss=8.584, ppl=383.85, wps=16061.9, ups=13.49, wpb=1191, bsz=62.7, num_updates=1820, lr=5.46e-06, gnorm=9.037, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=360
2022-01-12 23:57:47 | INFO | train_inner | epoch 037:     42 / 50 loss=8.691, ppl=413.41, wps=15540.4, ups=12.57, wpb=1236.2, bsz=64, num_updates=1840, lr=5.52e-06, gnorm=8.902, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=362
2022-01-12 23:57:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:48 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.519 | ppl 366.92 | wps 27076.3 | wpb 556.6 | bsz 30.3 | num_updates 1848 | best_loss 8.519
2022-01-12 23:57:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1848 updates
2022-01-12 23:57:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:57:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1848 updates, score 8.519) (writing took 4.238679522881284 seconds)
2022-01-12 23:57:52 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-12 23:57:53 | INFO | train | epoch 037 | loss 8.641 | ppl 399.21 | wps 6837.3 | ups 5.55 | wpb 1232.9 | bsz 63.5 | num_updates 1848 | lr 5.544e-06 | gnorm 9.062 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 368
2022-01-12 23:57:53 | INFO | fairseq.trainer | begin training epoch 38
2022-01-12 23:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:57:53 | INFO | train_inner | epoch 038:     12 / 50 loss=8.59, ppl=385.25, wps=3697.6, ups=3, wpb=1231.5, bsz=64, num_updates=1860, lr=5.58e-06, gnorm=9.033, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=369
2022-01-12 23:57:55 | INFO | train_inner | epoch 038:     32 / 50 loss=8.532, ppl=370.25, wps=15065.4, ups=13.6, wpb=1108.1, bsz=64, num_updates=1880, lr=5.64e-06, gnorm=10.302, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=370
2022-01-12 23:57:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:57:57 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.463 | ppl 352.84 | wps 27608.5 | wpb 556.6 | bsz 30.3 | num_updates 1898 | best_loss 8.463
2022-01-12 23:57:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1898 updates
2022-01-12 23:57:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 38 @ 1898 updates, score 8.463) (writing took 4.939729632111266 seconds)
2022-01-12 23:58:02 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-12 23:58:02 | INFO | train | epoch 038 | loss 8.496 | ppl 360.92 | wps 6408.3 | ups 5.2 | wpb 1232.9 | bsz 63.5 | num_updates 1898 | lr 5.694e-06 | gnorm 9.393 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 377
2022-01-12 23:58:02 | INFO | fairseq.trainer | begin training epoch 39
2022-01-12 23:58:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:02 | INFO | train_inner | epoch 039:      2 / 50 loss=8.431, ppl=345.12, wps=3629.9, ups=2.67, wpb=1357.8, bsz=62.7, num_updates=1900, lr=5.7e-06, gnorm=8.871, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=378
2022-01-12 23:58:04 | INFO | train_inner | epoch 039:     22 / 50 loss=8.31, ppl=317.3, wps=18391, ups=13.19, wpb=1394.3, bsz=62.7, num_updates=1920, lr=5.76e-06, gnorm=8.856, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=379
2022-01-12 23:58:06 | INFO | train_inner | epoch 039:     42 / 50 loss=8.6, ppl=388.07, wps=14391.7, ups=13.04, wpb=1103.3, bsz=64, num_updates=1940, lr=5.82e-06, gnorm=8.831, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=381
2022-01-12 23:58:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:07 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.338 | ppl 323.55 | wps 26376.4 | wpb 556.6 | bsz 30.3 | num_updates 1948 | best_loss 8.338
2022-01-12 23:58:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1948 updates
2022-01-12 23:58:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1948 updates, score 8.338) (writing took 4.365342371165752 seconds)
2022-01-12 23:58:11 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-12 23:58:11 | INFO | train | epoch 039 | loss 8.412 | ppl 340.57 | wps 6727.9 | ups 5.46 | wpb 1232.9 | bsz 63.5 | num_updates 1948 | lr 5.844e-06 | gnorm 8.833 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 386
2022-01-12 23:58:11 | INFO | fairseq.trainer | begin training epoch 40
2022-01-12 23:58:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:12 | INFO | train_inner | epoch 040:     12 / 50 loss=8.234, ppl=301.06, wps=3408.9, ups=2.94, wpb=1158.2, bsz=62.7, num_updates=1960, lr=5.88e-06, gnorm=8.929, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=387
2022-01-12 23:58:14 | INFO | train_inner | epoch 040:     32 / 50 loss=8.445, ppl=348.56, wps=16243, ups=13.6, wpb=1193.9, bsz=64, num_updates=1980, lr=5.94e-06, gnorm=8.674, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=389
2022-01-12 23:58:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:16 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.301 | ppl 315.3 | wps 26733.9 | wpb 556.6 | bsz 30.3 | num_updates 1998 | best_loss 8.301
2022-01-12 23:58:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1998 updates
2022-01-12 23:58:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1998 updates, score 8.301) (writing took 4.93078504386358 seconds)
2022-01-12 23:58:21 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-12 23:58:21 | INFO | train | epoch 040 | loss 8.351 | ppl 326.42 | wps 6578.5 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 1998 | lr 5.994e-06 | gnorm 8.829 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 396
2022-01-12 23:58:21 | INFO | fairseq.trainer | begin training epoch 41
2022-01-12 23:58:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:21 | INFO | train_inner | epoch 041:      2 / 50 loss=8.301, ppl=315.32, wps=3697.6, ups=2.79, wpb=1325.2, bsz=64, num_updates=2000, lr=6e-06, gnorm=8.798, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=396
2022-01-12 23:58:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:22 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.232 | ppl 300.58 | wps 25796.3 | wpb 556.6 | bsz 30.3 | num_updates 2000 | best_loss 8.232
2022-01-12 23:58:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2000 updates
2022-01-12 23:58:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt
2022-01-12 23:58:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt
2022-01-12 23:58:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt (epoch 41 @ 2000 updates, score 8.232) (writing took 9.888902221107855 seconds)
2022-01-12 23:58:33 | INFO | train_inner | epoch 041:     22 / 50 loss=8.306, ppl=316.57, wps=2124.6, ups=1.62, wpb=1308.8, bsz=64, num_updates=2020, lr=6.06e-06, gnorm=8.913, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=408
2022-01-12 23:58:35 | INFO | train_inner | epoch 041:     42 / 50 loss=8.255, ppl=305.54, wps=15575.6, ups=13.04, wpb=1194.7, bsz=62.7, num_updates=2040, lr=6.12e-06, gnorm=8.872, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=410
2022-01-12 23:58:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:36 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.254 | ppl 305.35 | wps 26718.4 | wpb 556.6 | bsz 30.3 | num_updates 2048 | best_loss 8.232
2022-01-12 23:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2048 updates
2022-01-12 23:58:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 2048 updates, score 8.254) (writing took 2.7794300818350166 seconds)
2022-01-12 23:58:39 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-12 23:58:39 | INFO | train | epoch 041 | loss 8.275 | ppl 309.75 | wps 3353.6 | ups 2.72 | wpb 1232.9 | bsz 63.5 | num_updates 2048 | lr 6.144e-06 | gnorm 8.832 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 414
2022-01-12 23:58:39 | INFO | fairseq.trainer | begin training epoch 42
2022-01-12 23:58:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:40 | INFO | train_inner | epoch 042:     12 / 50 loss=8.202, ppl=294.42, wps=4650.6, ups=3.82, wpb=1217.2, bsz=64, num_updates=2060, lr=6.18e-06, gnorm=8.577, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=415
2022-01-12 23:58:41 | INFO | train_inner | epoch 042:     32 / 50 loss=8.245, ppl=303.39, wps=17383.4, ups=13.94, wpb=1247.2, bsz=64, num_updates=2080, lr=6.24e-06, gnorm=8.729, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=417
2022-01-12 23:58:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:44 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.09 | ppl 272.57 | wps 25664.9 | wpb 556.6 | bsz 30.3 | num_updates 2098 | best_loss 8.09
2022-01-12 23:58:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2098 updates
2022-01-12 23:58:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:58:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 2098 updates, score 8.09) (writing took 4.342212746152654 seconds)
2022-01-12 23:58:48 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-12 23:58:48 | INFO | train | epoch 042 | loss 8.209 | ppl 295.91 | wps 6966.1 | ups 5.65 | wpb 1232.9 | bsz 63.5 | num_updates 2098 | lr 6.294e-06 | gnorm 8.768 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 423
2022-01-12 23:58:48 | INFO | fairseq.trainer | begin training epoch 43
2022-01-12 23:58:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:48 | INFO | train_inner | epoch 043:      2 / 50 loss=8.117, ppl=277.53, wps=3522.9, ups=2.98, wpb=1182.5, bsz=62.7, num_updates=2100, lr=6.3e-06, gnorm=8.905, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=423
2022-01-12 23:58:50 | INFO | train_inner | epoch 043:     22 / 50 loss=7.966, ppl=250.03, wps=17217.8, ups=14.07, wpb=1223.8, bsz=62.7, num_updates=2120, lr=6.36e-06, gnorm=8.955, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=425
2022-01-12 23:58:51 | INFO | train_inner | epoch 043:     42 / 50 loss=8.312, ppl=317.78, wps=16118.4, ups=12.53, wpb=1286.2, bsz=64, num_updates=2140, lr=6.42e-06, gnorm=8.445, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=426
2022-01-12 23:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:58:53 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.113 | ppl 276.8 | wps 25381.2 | wpb 556.6 | bsz 30.3 | num_updates 2148 | best_loss 8.09
2022-01-12 23:58:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2148 updates
2022-01-12 23:58:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-12 23:58:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 43 @ 2148 updates, score 8.113) (writing took 2.8221724259201437 seconds)
2022-01-12 23:58:55 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-12 23:58:55 | INFO | train | epoch 043 | loss 8.081 | ppl 270.87 | wps 8190.8 | ups 6.64 | wpb 1232.9 | bsz 63.5 | num_updates 2148 | lr 6.444e-06 | gnorm 8.733 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 431
2022-01-12 23:58:56 | INFO | fairseq.trainer | begin training epoch 44
2022-01-12 23:58:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:58:56 | INFO | train_inner | epoch 044:     12 / 50 loss=7.9, ppl=238.79, wps=4185.8, ups=3.86, wpb=1083.7, bsz=62.7, num_updates=2160, lr=6.48e-06, gnorm=8.953, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=432
2022-01-12 23:58:58 | INFO | train_inner | epoch 044:     32 / 50 loss=7.977, ppl=251.93, wps=16719.6, ups=13.21, wpb=1265.7, bsz=64, num_updates=2180, lr=6.54e-06, gnorm=8.731, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=433
2022-01-12 23:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:00 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.037 | ppl 262.58 | wps 26798.8 | wpb 556.6 | bsz 30.3 | num_updates 2198 | best_loss 8.037
2022-01-12 23:59:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2198 updates
2022-01-12 23:59:00 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 2198 updates, score 8.037) (writing took 5.0613508629612625 seconds)
2022-01-12 23:59:05 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-12 23:59:05 | INFO | train | epoch 044 | loss 7.974 | ppl 251.45 | wps 6452.4 | ups 5.23 | wpb 1232.9 | bsz 63.5 | num_updates 2198 | lr 6.594e-06 | gnorm 8.718 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 440
2022-01-12 23:59:05 | INFO | fairseq.trainer | begin training epoch 45
2022-01-12 23:59:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:05 | INFO | train_inner | epoch 045:      2 / 50 loss=7.975, ppl=251.59, wps=3463.6, ups=2.71, wpb=1277.9, bsz=64, num_updates=2200, lr=6.6e-06, gnorm=8.586, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=440
2022-01-12 23:59:07 | INFO | train_inner | epoch 045:     22 / 50 loss=7.894, ppl=237.79, wps=17351.7, ups=14.22, wpb=1220.5, bsz=62.7, num_updates=2220, lr=6.66e-06, gnorm=8.658, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=442
2022-01-12 23:59:08 | INFO | train_inner | epoch 045:     42 / 50 loss=7.856, ppl=231.67, wps=16552.2, ups=13.52, wpb=1224.2, bsz=64, num_updates=2240, lr=6.72e-06, gnorm=8.725, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=443
2022-01-12 23:59:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:10 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.94 | ppl 245.65 | wps 28042.6 | wpb 556.6 | bsz 30.3 | num_updates 2248 | best_loss 7.94
2022-01-12 23:59:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2248 updates
2022-01-12 23:59:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 2248 updates, score 7.94) (writing took 4.605860479874536 seconds)
2022-01-12 23:59:14 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-12 23:59:14 | INFO | train | epoch 045 | loss 7.891 | ppl 237.32 | wps 6729.4 | ups 5.46 | wpb 1232.9 | bsz 63.5 | num_updates 2248 | lr 6.744e-06 | gnorm 8.697 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 449
2022-01-12 23:59:14 | INFO | fairseq.trainer | begin training epoch 46
2022-01-12 23:59:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:15 | INFO | train_inner | epoch 046:     12 / 50 loss=7.994, ppl=254.98, wps=3714.2, ups=2.87, wpb=1296.3, bsz=62.7, num_updates=2260, lr=6.78e-06, gnorm=8.626, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=450
2022-01-12 23:59:17 | INFO | train_inner | epoch 046:     32 / 50 loss=7.749, ppl=215.1, wps=15671.5, ups=13.17, wpb=1189.8, bsz=64, num_updates=2280, lr=6.84e-06, gnorm=8.917, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=452
2022-01-12 23:59:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:19 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.904 | ppl 239.51 | wps 26257.2 | wpb 556.6 | bsz 30.3 | num_updates 2298 | best_loss 7.904
2022-01-12 23:59:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2298 updates
2022-01-12 23:59:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:22 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 46 @ 2298 updates, score 7.904) (writing took 4.576131146866828 seconds)
2022-01-12 23:59:23 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-12 23:59:23 | INFO | train | epoch 046 | loss 7.837 | ppl 228.63 | wps 6706.1 | ups 5.44 | wpb 1232.9 | bsz 63.5 | num_updates 2298 | lr 6.894e-06 | gnorm 8.75 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 459
2022-01-12 23:59:23 | INFO | fairseq.trainer | begin training epoch 47
2022-01-12 23:59:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:24 | INFO | train_inner | epoch 047:      2 / 50 loss=7.835, ppl=228.34, wps=3669.3, ups=2.88, wpb=1275.9, bsz=64, num_updates=2300, lr=6.9e-06, gnorm=9.032, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=459
2022-01-12 23:59:25 | INFO | train_inner | epoch 047:     22 / 50 loss=7.848, ppl=230.47, wps=17697.2, ups=14.21, wpb=1245.6, bsz=64, num_updates=2320, lr=6.96e-06, gnorm=8.976, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=460
2022-01-12 23:59:27 | INFO | train_inner | epoch 047:     42 / 50 loss=7.73, ppl=212.26, wps=17404.8, ups=13.66, wpb=1274.3, bsz=62.7, num_updates=2340, lr=7.02e-06, gnorm=8.723, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=462
2022-01-12 23:59:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:28 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.786 | ppl 220.69 | wps 24253.9 | wpb 556.6 | bsz 30.3 | num_updates 2348 | best_loss 7.786
2022-01-12 23:59:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 2348 updates
2022-01-12 23:59:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 2348 updates, score 7.786) (writing took 4.3219620999880135 seconds)
2022-01-12 23:59:32 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-12 23:59:32 | INFO | train | epoch 047 | loss 7.767 | ppl 217.86 | wps 6944.2 | ups 5.63 | wpb 1232.9 | bsz 63.5 | num_updates 2348 | lr 7.044e-06 | gnorm 8.984 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 467
2022-01-12 23:59:32 | INFO | fairseq.trainer | begin training epoch 48
2022-01-12 23:59:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:33 | INFO | train_inner | epoch 048:     12 / 50 loss=7.671, ppl=203.85, wps=3673.4, ups=2.96, wpb=1239.3, bsz=62.7, num_updates=2360, lr=7.08e-06, gnorm=8.791, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=468
2022-01-12 23:59:35 | INFO | train_inner | epoch 048:     32 / 50 loss=7.678, ppl=204.72, wps=16620.8, ups=14.11, wpb=1177.8, bsz=64, num_updates=2380, lr=7.14e-06, gnorm=8.765, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=470
2022-01-12 23:59:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:37 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.707 | ppl 208.95 | wps 25456.3 | wpb 556.6 | bsz 30.3 | num_updates 2398 | best_loss 7.707
2022-01-12 23:59:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 2398 updates
2022-01-12 23:59:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 48 @ 2398 updates, score 7.707) (writing took 4.2475571180693805 seconds)
2022-01-12 23:59:41 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-12 23:59:41 | INFO | train | epoch 048 | loss 7.747 | ppl 214.77 | wps 7075 | ups 5.74 | wpb 1232.9 | bsz 63.5 | num_updates 2398 | lr 7.194e-06 | gnorm 8.696 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 476
2022-01-12 23:59:41 | INFO | fairseq.trainer | begin training epoch 49
2022-01-12 23:59:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:41 | INFO | train_inner | epoch 049:      2 / 50 loss=7.856, ppl=231.67, wps=3639.7, ups=3.05, wpb=1192.6, bsz=64, num_updates=2400, lr=7.2e-06, gnorm=8.574, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=476
2022-01-12 23:59:43 | INFO | train_inner | epoch 049:     22 / 50 loss=7.437, ppl=173.24, wps=18505.6, ups=13.87, wpb=1334.5, bsz=64, num_updates=2420, lr=7.26e-06, gnorm=8.444, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=478
2022-01-12 23:59:44 | INFO | train_inner | epoch 049:     42 / 50 loss=7.571, ppl=190.18, wps=16310, ups=14.33, wpb=1138.3, bsz=62.7, num_updates=2440, lr=7.32e-06, gnorm=9.063, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=479
2022-01-12 23:59:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:45 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.68 | ppl 205.05 | wps 23843.5 | wpb 556.6 | bsz 30.3 | num_updates 2448 | best_loss 7.68
2022-01-12 23:59:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 2448 updates
2022-01-12 23:59:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 2448 updates, score 7.68) (writing took 4.621699879877269 seconds)
2022-01-12 23:59:50 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-12 23:59:50 | INFO | train | epoch 049 | loss 7.559 | ppl 188.55 | wps 6798.7 | ups 5.51 | wpb 1232.9 | bsz 63.5 | num_updates 2448 | lr 7.344e-06 | gnorm 8.73 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 485
2022-01-12 23:59:50 | INFO | fairseq.trainer | begin training epoch 50
2022-01-12 23:59:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-12 23:59:53 | INFO | train_inner | epoch 050:     12 / 50 loss=7.856, ppl=231.71, wps=3024.6, ups=2.37, wpb=1276.5, bsz=62.7, num_updates=2460, lr=7.38e-06, gnorm=8.075, clip=100, loss_scale=32, train_wall=3, gb_free=20.9, wall=488
2022-01-12 23:59:54 | INFO | train_inner | epoch 050:     32 / 50 loss=7.992, ppl=254.5, wps=17201.1, ups=13.51, wpb=1273.2, bsz=64, num_updates=2480, lr=7.44e-06, gnorm=8.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=489
2022-01-12 23:59:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-12 23:59:56 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.607 | ppl 194.93 | wps 26515.6 | wpb 556.6 | bsz 30.3 | num_updates 2498 | best_loss 7.607
2022-01-12 23:59:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2498 updates
2022-01-12 23:59:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-12 23:59:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 2498 updates, score 7.607) (writing took 4.406413435935974 seconds)
2022-01-13 00:00:01 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-13 00:00:01 | INFO | train | epoch 050 | loss 7.954 | ppl 247.89 | wps 5911.6 | ups 4.79 | wpb 1232.9 | bsz 63.5 | num_updates 2498 | lr 7.494e-06 | gnorm 7.901 | clip 100 | loss_scale 32 | train_wall 5 | gb_free 20.9 | wall 496
2022-01-13 00:00:01 | INFO | fairseq.trainer | begin training epoch 51
2022-01-13 00:00:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:01 | INFO | train_inner | epoch 051:      2 / 50 loss=7.965, ppl=249.94, wps=3527.3, ups=2.97, wpb=1188, bsz=64, num_updates=2500, lr=7.5e-06, gnorm=7.812, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=496
2022-01-13 00:00:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:02 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.556 | ppl 188.12 | wps 23617.3 | wpb 556.6 | bsz 30.3 | num_updates 2500 | best_loss 7.556
2022-01-13 00:00:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2500 updates
2022-01-13 00:00:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt
2022-01-13 00:00:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt
2022-01-13 00:00:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt (epoch 51 @ 2500 updates, score 7.556) (writing took 9.116204982856289 seconds)
2022-01-13 00:00:13 | INFO | train_inner | epoch 051:     22 / 50 loss=7.947, ppl=246.79, wps=2113.4, ups=1.7, wpb=1242.9, bsz=64, num_updates=2520, lr=7.56e-06, gnorm=7.705, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=508
2022-01-13 00:00:14 | INFO | train_inner | epoch 051:     42 / 50 loss=7.815, ppl=225.15, wps=16295.3, ups=13.01, wpb=1252.8, bsz=62.7, num_updates=2540, lr=7.62e-06, gnorm=7.548, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=509
2022-01-13 00:00:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:16 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.582 | ppl 191.56 | wps 24534.5 | wpb 556.6 | bsz 30.3 | num_updates 2548 | best_loss 7.556
2022-01-13 00:00:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2548 updates
2022-01-13 00:00:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 51 @ 2548 updates, score 7.582) (writing took 3.3303923478815705 seconds)
2022-01-13 00:00:19 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-13 00:00:19 | INFO | train | epoch 051 | loss 7.866 | ppl 233.29 | wps 3360.1 | ups 2.73 | wpb 1232.9 | bsz 63.5 | num_updates 2548 | lr 7.644e-06 | gnorm 7.713 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 514
2022-01-13 00:00:19 | INFO | fairseq.trainer | begin training epoch 52
2022-01-13 00:00:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:20 | INFO | train_inner | epoch 052:     12 / 50 loss=7.753, ppl=215.67, wps=4079.6, ups=3.47, wpb=1176.3, bsz=62.7, num_updates=2560, lr=7.68e-06, gnorm=8.2, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=515
2022-01-13 00:00:21 | INFO | train_inner | epoch 052:     32 / 50 loss=7.812, ppl=224.76, wps=15926.7, ups=13.37, wpb=1191, bsz=64, num_updates=2580, lr=7.74e-06, gnorm=7.874, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=517
2022-01-13 00:00:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:24 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.559 | ppl 188.59 | wps 25404.3 | wpb 556.6 | bsz 30.3 | num_updates 2598 | best_loss 7.556
2022-01-13 00:00:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 2598 updates
2022-01-13 00:00:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 52 @ 2598 updates, score 7.559) (writing took 3.254428870975971 seconds)
2022-01-13 00:00:27 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-13 00:00:27 | INFO | train | epoch 052 | loss 7.796 | ppl 222.19 | wps 7595.6 | ups 6.16 | wpb 1232.9 | bsz 63.5 | num_updates 2598 | lr 7.794e-06 | gnorm 7.966 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 522
2022-01-13 00:00:27 | INFO | fairseq.trainer | begin training epoch 53
2022-01-13 00:00:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:27 | INFO | train_inner | epoch 053:      2 / 50 loss=7.772, ppl=218.64, wps=4332.3, ups=3.41, wpb=1271.5, bsz=64, num_updates=2600, lr=7.8e-06, gnorm=7.908, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=522
2022-01-13 00:00:29 | INFO | train_inner | epoch 053:     22 / 50 loss=7.807, ppl=223.88, wps=17655, ups=13.44, wpb=1313.8, bsz=64, num_updates=2620, lr=7.86e-06, gnorm=7.316, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=524
2022-01-13 00:00:30 | INFO | train_inner | epoch 053:     42 / 50 loss=7.632, ppl=198.38, wps=13717.1, ups=11.88, wpb=1154.8, bsz=62.7, num_updates=2640, lr=7.92e-06, gnorm=8.024, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=526
2022-01-13 00:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:32 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.412 | ppl 170.27 | wps 27303.1 | wpb 556.6 | bsz 30.3 | num_updates 2648 | best_loss 7.412
2022-01-13 00:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 2648 updates
2022-01-13 00:00:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 2648 updates, score 7.412) (writing took 5.089505098061636 seconds)
2022-01-13 00:00:37 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-13 00:00:37 | INFO | train | epoch 053 | loss 7.707 | ppl 209.01 | wps 6233.1 | ups 5.06 | wpb 1232.9 | bsz 63.5 | num_updates 2648 | lr 7.944e-06 | gnorm 7.65 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 532
2022-01-13 00:00:37 | INFO | fairseq.trainer | begin training epoch 54
2022-01-13 00:00:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:38 | INFO | train_inner | epoch 054:     12 / 50 loss=7.668, ppl=203.31, wps=3287.3, ups=2.65, wpb=1242.8, bsz=64, num_updates=2660, lr=7.98e-06, gnorm=7.521, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=533
2022-01-13 00:00:39 | INFO | train_inner | epoch 054:     32 / 50 loss=7.665, ppl=202.93, wps=17643.7, ups=14.19, wpb=1243.4, bsz=64, num_updates=2680, lr=8.04e-06, gnorm=7.633, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=535
2022-01-13 00:00:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:41 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.433 | ppl 172.86 | wps 26600.4 | wpb 556.6 | bsz 30.3 | num_updates 2698 | best_loss 7.412
2022-01-13 00:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 2698 updates
2022-01-13 00:00:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:00:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 54 @ 2698 updates, score 7.433) (writing took 3.989198715193197 seconds)
2022-01-13 00:00:45 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-13 00:00:45 | INFO | train | epoch 054 | loss 7.627 | ppl 197.69 | wps 7212.2 | ups 5.85 | wpb 1232.9 | bsz 63.5 | num_updates 2698 | lr 8.094e-06 | gnorm 7.588 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 541
2022-01-13 00:00:45 | INFO | fairseq.trainer | begin training epoch 55
2022-01-13 00:00:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:46 | INFO | train_inner | epoch 055:      2 / 50 loss=7.49, ppl=179.72, wps=3767.4, ups=3.19, wpb=1182.4, bsz=62.7, num_updates=2700, lr=8.1e-06, gnorm=7.608, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=541
2022-01-13 00:00:47 | INFO | train_inner | epoch 055:     22 / 50 loss=7.332, ppl=161.16, wps=14790.9, ups=13.87, wpb=1066.3, bsz=62.7, num_updates=2720, lr=8.16e-06, gnorm=9.287, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=542
2022-01-13 00:00:49 | INFO | train_inner | epoch 055:     42 / 50 loss=7.628, ppl=197.85, wps=17417.2, ups=13.48, wpb=1291.8, bsz=64, num_updates=2740, lr=8.22e-06, gnorm=7.261, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=544
2022-01-13 00:00:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:50 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.352 | ppl 163.41 | wps 26194.6 | wpb 556.6 | bsz 30.3 | num_updates 2748 | best_loss 7.352
2022-01-13 00:00:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 2748 updates
2022-01-13 00:00:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:00:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 55 @ 2748 updates, score 7.352) (writing took 4.29561490402557 seconds)
2022-01-13 00:00:54 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-13 00:00:54 | INFO | train | epoch 055 | loss 7.528 | ppl 184.52 | wps 6895.5 | ups 5.59 | wpb 1232.9 | bsz 63.5 | num_updates 2748 | lr 8.244e-06 | gnorm 8.049 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 550
2022-01-13 00:00:54 | INFO | fairseq.trainer | begin training epoch 56
2022-01-13 00:00:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:00:55 | INFO | train_inner | epoch 056:     12 / 50 loss=7.617, ppl=196.36, wps=3931.7, ups=2.95, wpb=1332, bsz=64, num_updates=2760, lr=8.28e-06, gnorm=7.379, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=551
2022-01-13 00:00:57 | INFO | train_inner | epoch 056:     32 / 50 loss=7.455, ppl=175.47, wps=17931.7, ups=13.83, wpb=1297, bsz=62.7, num_updates=2780, lr=8.34e-06, gnorm=7.609, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=552
2022-01-13 00:00:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:00:59 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.268 | ppl 154.13 | wps 26234.9 | wpb 556.6 | bsz 30.3 | num_updates 2798 | best_loss 7.268
2022-01-13 00:00:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 2798 updates
2022-01-13 00:00:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 2798 updates, score 7.268) (writing took 4.465936854016036 seconds)
2022-01-13 00:01:03 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-13 00:01:03 | INFO | train | epoch 056 | loss 7.522 | ppl 183.82 | wps 6816.9 | ups 5.53 | wpb 1232.9 | bsz 63.5 | num_updates 2798 | lr 8.394e-06 | gnorm 7.608 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 559
2022-01-13 00:01:03 | INFO | fairseq.trainer | begin training epoch 57
2022-01-13 00:01:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:04 | INFO | train_inner | epoch 057:      2 / 50 loss=7.603, ppl=194.4, wps=3479.8, ups=2.9, wpb=1200.4, bsz=64, num_updates=2800, lr=8.4e-06, gnorm=7.566, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=559
2022-01-13 00:01:05 | INFO | train_inner | epoch 057:     22 / 50 loss=7.343, ppl=162.36, wps=18166.1, ups=13.92, wpb=1304.7, bsz=64, num_updates=2820, lr=8.46e-06, gnorm=7.346, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=560
2022-01-13 00:01:07 | INFO | train_inner | epoch 057:     42 / 50 loss=7.482, ppl=178.81, wps=14792.5, ups=12.62, wpb=1172.3, bsz=62.7, num_updates=2840, lr=8.52e-06, gnorm=7.784, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=562
2022-01-13 00:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:08 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.213 | ppl 148.41 | wps 26417.6 | wpb 556.6 | bsz 30.3 | num_updates 2848 | best_loss 7.213
2022-01-13 00:01:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 2848 updates
2022-01-13 00:01:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:11 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 57 @ 2848 updates, score 7.213) (writing took 4.1772787710651755 seconds)
2022-01-13 00:01:12 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-13 00:01:12 | INFO | train | epoch 057 | loss 7.442 | ppl 173.9 | wps 6952 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 2848 | lr 8.544e-06 | gnorm 7.599 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 567
2022-01-13 00:01:12 | INFO | fairseq.trainer | begin training epoch 58
2022-01-13 00:01:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:13 | INFO | train_inner | epoch 058:     12 / 50 loss=7.471, ppl=177.43, wps=3762.3, ups=3.07, wpb=1224.3, bsz=64, num_updates=2860, lr=8.58e-06, gnorm=7.622, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=568
2022-01-13 00:01:15 | INFO | train_inner | epoch 058:     32 / 50 loss=7.347, ppl=162.82, wps=13996.8, ups=11.97, wpb=1169.8, bsz=62.7, num_updates=2880, lr=8.64e-06, gnorm=7.758, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=570
2022-01-13 00:01:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:17 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.06 | ppl 133.46 | wps 25253.9 | wpb 556.6 | bsz 30.3 | num_updates 2898 | best_loss 7.06
2022-01-13 00:01:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 2898 updates
2022-01-13 00:01:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 58 @ 2898 updates, score 7.06) (writing took 4.362164995865896 seconds)
2022-01-13 00:01:21 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-13 00:01:22 | INFO | train | epoch 058 | loss 7.386 | ppl 167.28 | wps 6767.3 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 2898 | lr 8.694e-06 | gnorm 7.642 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 577
2022-01-13 00:01:22 | INFO | fairseq.trainer | begin training epoch 59
2022-01-13 00:01:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:22 | INFO | train_inner | epoch 059:      2 / 50 loss=7.393, ppl=168.03, wps=3606, ups=2.84, wpb=1271, bsz=62.7, num_updates=2900, lr=8.7e-06, gnorm=7.68, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=577
2022-01-13 00:01:24 | INFO | train_inner | epoch 059:     22 / 50 loss=7.228, ppl=149.9, wps=15681.9, ups=12.92, wpb=1213.8, bsz=64, num_updates=2920, lr=8.76e-06, gnorm=7.561, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=579
2022-01-13 00:01:25 | INFO | train_inner | epoch 059:     42 / 50 loss=7.287, ppl=156.23, wps=16704.3, ups=13.86, wpb=1205.3, bsz=64, num_updates=2940, lr=8.82e-06, gnorm=7.534, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=580
2022-01-13 00:01:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:26 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.04 | ppl 131.59 | wps 26928.6 | wpb 556.6 | bsz 30.3 | num_updates 2948 | best_loss 7.04
2022-01-13 00:01:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 2948 updates
2022-01-13 00:01:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 59 @ 2948 updates, score 7.04) (writing took 4.292766103055328 seconds)
2022-01-13 00:01:31 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-13 00:01:31 | INFO | train | epoch 059 | loss 7.284 | ppl 155.88 | wps 6850.3 | ups 5.56 | wpb 1232.9 | bsz 63.5 | num_updates 2948 | lr 8.844e-06 | gnorm 7.553 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 586
2022-01-13 00:01:31 | INFO | fairseq.trainer | begin training epoch 60
2022-01-13 00:01:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:32 | INFO | train_inner | epoch 060:     12 / 50 loss=7.297, ppl=157.28, wps=4063.7, ups=2.97, wpb=1369, bsz=64, num_updates=2960, lr=8.88e-06, gnorm=7.687, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=587
2022-01-13 00:01:33 | INFO | train_inner | epoch 060:     32 / 50 loss=7.322, ppl=160, wps=16409.5, ups=12.81, wpb=1281.2, bsz=62.7, num_updates=2980, lr=8.94e-06, gnorm=7.751, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=588
2022-01-13 00:01:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:35 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.034 | ppl 131.08 | wps 28028.6 | wpb 556.6 | bsz 30.3 | num_updates 2998 | best_loss 7.034
2022-01-13 00:01:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 2998 updates
2022-01-13 00:01:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:01:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 60 @ 2998 updates, score 7.034) (writing took 5.088655893923715 seconds)
2022-01-13 00:01:40 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-13 00:01:40 | INFO | train | epoch 060 | loss 7.224 | ppl 149.47 | wps 6355.5 | ups 5.15 | wpb 1232.9 | bsz 63.5 | num_updates 2998 | lr 8.994e-06 | gnorm 7.765 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 596
2022-01-13 00:01:40 | INFO | fairseq.trainer | begin training epoch 61
2022-01-13 00:01:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:41 | INFO | train_inner | epoch 061:      2 / 50 loss=7.147, ppl=141.71, wps=2947.8, ups=2.71, wpb=1087.5, bsz=64, num_updates=3000, lr=9e-06, gnorm=7.685, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=596
2022-01-13 00:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:42 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.08 | ppl 135.29 | wps 28111.9 | wpb 556.6 | bsz 30.3 | num_updates 3000 | best_loss 7.034
2022-01-13 00:01:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3000 updates
2022-01-13 00:01:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt
2022-01-13 00:01:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt
2022-01-13 00:01:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt (epoch 61 @ 3000 updates, score 7.08) (writing took 5.089682440040633 seconds)
2022-01-13 00:01:48 | INFO | train_inner | epoch 061:     22 / 50 loss=7.18, ppl=145, wps=3129.5, ups=2.65, wpb=1181.2, bsz=62.7, num_updates=3020, lr=9.06e-06, gnorm=7.701, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=603
2022-01-13 00:01:50 | INFO | train_inner | epoch 061:     42 / 50 loss=7.218, ppl=148.87, wps=16939.1, ups=13.17, wpb=1286.7, bsz=64, num_updates=3040, lr=9.12e-06, gnorm=7.684, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=605
2022-01-13 00:01:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:51 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.068 | ppl 134.22 | wps 25034.2 | wpb 556.6 | bsz 30.3 | num_updates 3048 | best_loss 7.034
2022-01-13 00:01:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3048 updates
2022-01-13 00:01:51 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:01:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:01:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 61 @ 3048 updates, score 7.068) (writing took 3.1684718979522586 seconds)
2022-01-13 00:01:54 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-13 00:01:54 | INFO | train | epoch 061 | loss 7.189 | ppl 145.9 | wps 4447.5 | ups 3.61 | wpb 1232.9 | bsz 63.5 | num_updates 3048 | lr 9.144e-06 | gnorm 7.641 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 609
2022-01-13 00:01:54 | INFO | fairseq.trainer | begin training epoch 62
2022-01-13 00:01:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:01:55 | INFO | train_inner | epoch 062:     12 / 50 loss=7.247, ppl=151.87, wps=4768.9, ups=3.63, wpb=1314.8, bsz=64, num_updates=3060, lr=9.18e-06, gnorm=7.253, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=610
2022-01-13 00:01:57 | INFO | train_inner | epoch 062:     32 / 50 loss=6.903, ppl=119.71, wps=14414.4, ups=13.22, wpb=1090.7, bsz=64, num_updates=3080, lr=9.24e-06, gnorm=7.925, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=612
2022-01-13 00:01:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:01:59 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.997 | ppl 127.75 | wps 25642.7 | wpb 556.6 | bsz 30.3 | num_updates 3098 | best_loss 6.997
2022-01-13 00:01:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 3098 updates
2022-01-13 00:01:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:02 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 62 @ 3098 updates, score 6.997) (writing took 4.368456893134862 seconds)
2022-01-13 00:02:03 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-13 00:02:03 | INFO | train | epoch 062 | loss 7.123 | ppl 139.36 | wps 6768.3 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 3098 | lr 9.294e-06 | gnorm 7.606 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 619
2022-01-13 00:02:03 | INFO | fairseq.trainer | begin training epoch 63
2022-01-13 00:02:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:04 | INFO | train_inner | epoch 063:      2 / 50 loss=7.231, ppl=150.2, wps=3953.2, ups=2.91, wpb=1357.5, bsz=62.7, num_updates=3100, lr=9.3e-06, gnorm=7.648, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=619
2022-01-13 00:02:05 | INFO | train_inner | epoch 063:     22 / 50 loss=7.031, ppl=130.81, wps=17231.3, ups=13.6, wpb=1267, bsz=64, num_updates=3120, lr=9.36e-06, gnorm=7.415, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=620
2022-01-13 00:02:07 | INFO | train_inner | epoch 063:     42 / 50 loss=7.028, ppl=130.51, wps=16932.6, ups=14.56, wpb=1162.7, bsz=62.7, num_updates=3140, lr=9.42e-06, gnorm=7.81, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=622
2022-01-13 00:02:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:08 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.847 | ppl 115.13 | wps 26909.2 | wpb 556.6 | bsz 30.3 | num_updates 3148 | best_loss 6.847
2022-01-13 00:02:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 3148 updates
2022-01-13 00:02:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 63 @ 3148 updates, score 6.847) (writing took 5.7117693750187755 seconds)
2022-01-13 00:02:14 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-13 00:02:14 | INFO | train | epoch 063 | loss 7.043 | ppl 131.85 | wps 6090.9 | ups 4.94 | wpb 1232.9 | bsz 63.5 | num_updates 3148 | lr 9.444e-06 | gnorm 7.687 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 629
2022-01-13 00:02:14 | INFO | fairseq.trainer | begin training epoch 64
2022-01-13 00:02:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:15 | INFO | train_inner | epoch 064:     12 / 50 loss=6.993, ppl=127.4, wps=2939.8, ups=2.48, wpb=1186.6, bsz=64, num_updates=3160, lr=9.48e-06, gnorm=7.72, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=630
2022-01-13 00:02:16 | INFO | train_inner | epoch 064:     32 / 50 loss=7.146, ppl=141.66, wps=16954.1, ups=13.54, wpb=1252.2, bsz=64, num_updates=3180, lr=9.54e-06, gnorm=7.821, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=631
2022-01-13 00:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:18 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.863 | ppl 116.38 | wps 26723.1 | wpb 556.6 | bsz 30.3 | num_updates 3198 | best_loss 6.847
2022-01-13 00:02:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 3198 updates
2022-01-13 00:02:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 64 @ 3198 updates, score 6.863) (writing took 2.9973171290475875 seconds)
2022-01-13 00:02:21 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-13 00:02:21 | INFO | train | epoch 064 | loss 7.015 | ppl 129.3 | wps 8209.7 | ups 6.66 | wpb 1232.9 | bsz 63.5 | num_updates 3198 | lr 9.594e-06 | gnorm 7.69 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 636
2022-01-13 00:02:21 | INFO | fairseq.trainer | begin training epoch 65
2022-01-13 00:02:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:21 | INFO | train_inner | epoch 065:      2 / 50 loss=6.916, ppl=120.75, wps=4731, ups=3.83, wpb=1234.3, bsz=62.7, num_updates=3200, lr=9.6e-06, gnorm=7.598, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=636
2022-01-13 00:02:23 | INFO | train_inner | epoch 065:     22 / 50 loss=6.815, ppl=112.58, wps=17373.8, ups=13.56, wpb=1281.6, bsz=64, num_updates=3220, lr=9.66e-06, gnorm=7.468, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=638
2022-01-13 00:02:24 | INFO | train_inner | epoch 065:     42 / 50 loss=6.897, ppl=119.19, wps=17407.9, ups=13.92, wpb=1250.8, bsz=64, num_updates=3240, lr=9.72e-06, gnorm=7.956, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=639
2022-01-13 00:02:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:26 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.754 | ppl 107.96 | wps 27435.4 | wpb 556.6 | bsz 30.3 | num_updates 3248 | best_loss 6.754
2022-01-13 00:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 3248 updates
2022-01-13 00:02:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 65 @ 3248 updates, score 6.754) (writing took 5.165929597103968 seconds)
2022-01-13 00:02:31 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-01-13 00:02:31 | INFO | train | epoch 065 | loss 6.907 | ppl 119.99 | wps 6332.7 | ups 5.14 | wpb 1232.9 | bsz 63.5 | num_updates 3248 | lr 9.744e-06 | gnorm 7.732 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 646
2022-01-13 00:02:31 | INFO | fairseq.trainer | begin training epoch 66
2022-01-13 00:02:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:32 | INFO | train_inner | epoch 066:     12 / 50 loss=6.969, ppl=125.26, wps=3262.8, ups=2.63, wpb=1240.5, bsz=62.7, num_updates=3260, lr=9.78e-06, gnorm=7.703, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=647
2022-01-13 00:02:33 | INFO | train_inner | epoch 066:     32 / 50 loss=6.83, ppl=113.75, wps=17104.4, ups=13.29, wpb=1286.8, bsz=64, num_updates=3280, lr=9.84e-06, gnorm=7.987, clip=100, loss_scale=32, train_wall=1, gb_free=20.8, wall=648
2022-01-13 00:02:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:36 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.703 | ppl 104.17 | wps 28140.3 | wpb 556.6 | bsz 30.3 | num_updates 3298 | best_loss 6.703
2022-01-13 00:02:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 3298 updates
2022-01-13 00:02:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 66 @ 3298 updates, score 6.703) (writing took 5.57637187698856 seconds)
2022-01-13 00:02:41 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-01-13 00:02:41 | INFO | train | epoch 066 | loss 6.911 | ppl 120.32 | wps 5936 | ups 4.81 | wpb 1232.9 | bsz 63.5 | num_updates 3298 | lr 9.894e-06 | gnorm 7.846 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 656
2022-01-13 00:02:41 | INFO | fairseq.trainer | begin training epoch 67
2022-01-13 00:02:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:41 | INFO | train_inner | epoch 067:      2 / 50 loss=7.013, ppl=129.17, wps=2863.2, ups=2.46, wpb=1165.1, bsz=62.7, num_updates=3300, lr=9.9e-06, gnorm=7.776, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=657
2022-01-13 00:02:43 | INFO | train_inner | epoch 067:     22 / 50 loss=6.888, ppl=118.45, wps=16588.8, ups=13.29, wpb=1248.2, bsz=64, num_updates=3320, lr=9.96e-06, gnorm=7.519, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=658
2022-01-13 00:02:45 | INFO | train_inner | epoch 067:     42 / 50 loss=6.84, ppl=114.56, wps=15103.3, ups=13.02, wpb=1160.2, bsz=62.7, num_updates=3340, lr=1.002e-05, gnorm=7.814, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=660
2022-01-13 00:02:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:46 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.727 | ppl 105.91 | wps 27156.8 | wpb 556.6 | bsz 30.3 | num_updates 3348 | best_loss 6.703
2022-01-13 00:02:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 3348 updates
2022-01-13 00:02:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:02:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 67 @ 3348 updates, score 6.727) (writing took 2.760889601893723 seconds)
2022-01-13 00:02:49 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-01-13 00:02:49 | INFO | train | epoch 067 | loss 6.829 | ppl 113.68 | wps 8100.9 | ups 6.57 | wpb 1232.9 | bsz 63.5 | num_updates 3348 | lr 1.0044e-05 | gnorm 7.7 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 664
2022-01-13 00:02:49 | INFO | fairseq.trainer | begin training epoch 68
2022-01-13 00:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:50 | INFO | train_inner | epoch 068:     12 / 50 loss=6.693, ppl=103.5, wps=5267.1, ups=3.77, wpb=1396.4, bsz=64, num_updates=3360, lr=1.008e-05, gnorm=7.661, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=665
2022-01-13 00:02:51 | INFO | train_inner | epoch 068:     32 / 50 loss=6.787, ppl=110.46, wps=16931.4, ups=13.57, wpb=1247.9, bsz=62.7, num_updates=3380, lr=1.014e-05, gnorm=7.696, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=666
2022-01-13 00:02:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:02:53 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.69 | ppl 103.24 | wps 25091.9 | wpb 556.6 | bsz 30.3 | num_updates 3398 | best_loss 6.69
2022-01-13 00:02:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 3398 updates
2022-01-13 00:02:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:02:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 68 @ 3398 updates, score 6.69) (writing took 3.931047295918688 seconds)
2022-01-13 00:02:57 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-01-13 00:02:57 | INFO | train | epoch 068 | loss 6.759 | ppl 108.32 | wps 7220.8 | ups 5.86 | wpb 1232.9 | bsz 63.5 | num_updates 3398 | lr 1.0194e-05 | gnorm 7.655 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 672
2022-01-13 00:02:57 | INFO | fairseq.trainer | begin training epoch 69
2022-01-13 00:02:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:02:58 | INFO | train_inner | epoch 069:      2 / 50 loss=6.71, ppl=104.71, wps=3431.3, ups=3.19, wpb=1076.8, bsz=64, num_updates=3400, lr=1.02e-05, gnorm=7.74, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=673
2022-01-13 00:02:59 | INFO | train_inner | epoch 069:     22 / 50 loss=6.817, ppl=112.74, wps=18982.9, ups=13.78, wpb=1377.8, bsz=64, num_updates=3420, lr=1.026e-05, gnorm=7.292, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=674
2022-01-13 00:03:01 | INFO | train_inner | epoch 069:     42 / 50 loss=6.604, ppl=97.29, wps=15050.8, ups=12.86, wpb=1170.5, bsz=62.7, num_updates=3440, lr=1.032e-05, gnorm=7.848, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=676
2022-01-13 00:03:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:02 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.554 | ppl 93.93 | wps 26835.2 | wpb 556.6 | bsz 30.3 | num_updates 3448 | best_loss 6.554
2022-01-13 00:03:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 3448 updates
2022-01-13 00:03:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:03:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 00:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 69 @ 3448 updates, score 6.554) (writing took 4.591499911854044 seconds)
2022-01-13 00:03:06 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-01-13 00:03:06 | INFO | train | epoch 069 | loss 6.691 | ppl 103.3 | wps 6762.5 | ups 5.49 | wpb 1232.9 | bsz 63.5 | num_updates 3448 | lr 1.0344e-05 | gnorm 7.666 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 682
2022-01-13 00:03:07 | INFO | fairseq.trainer | begin training epoch 70
2022-01-13 00:03:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:03:07 | INFO | train_inner | epoch 070:     12 / 50 loss=6.726, ppl=105.89, wps=3573.3, ups=2.93, wpb=1221.6, bsz=62.7, num_updates=3460, lr=1.038e-05, gnorm=7.935, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=683
2022-01-13 00:03:09 | INFO | train_inner | epoch 070:     32 / 50 loss=6.528, ppl=92.26, wps=16406.3, ups=13.63, wpb=1203.3, bsz=64, num_updates=3480, lr=1.044e-05, gnorm=7.645, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=684
2022-01-13 00:03:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:11 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.655 | ppl 100.75 | wps 24804 | wpb 556.6 | bsz 30.3 | num_updates 3498 | best_loss 6.554
2022-01-13 00:03:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 3498 updates
2022-01-13 00:03:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 70 @ 3498 updates, score 6.655) (writing took 2.7075513841118664 seconds)
2022-01-13 00:03:14 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-01-13 00:03:14 | INFO | train | epoch 070 | loss 6.615 | ppl 98.03 | wps 8547.8 | ups 6.93 | wpb 1232.9 | bsz 63.5 | num_updates 3498 | lr 1.0494e-05 | gnorm 7.604 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 689
2022-01-13 00:03:14 | INFO | fairseq.trainer | begin training epoch 71
2022-01-13 00:03:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 00:03:14 | INFO | train_inner | epoch 071:      2 / 50 loss=6.691, ppl=103.3, wps=4979.5, ups=3.99, wpb=1248.8, bsz=64, num_updates=3500, lr=1.05e-05, gnorm=7.38, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=689
2022-01-13 00:03:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:15 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.669 | ppl 101.79 | wps 26710.3 | wpb 556.6 | bsz 30.3 | num_updates 3500 | best_loss 6.554
2022-01-13 00:03:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3500 updates
2022-01-13 00:03:15 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt
2022-01-13 00:03:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt
2022-01-13 00:03:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt (epoch 71 @ 3500 updates, score 6.669) (writing took 3.828785134013742 seconds)
2022-01-13 00:03:20 | INFO | train_inner | epoch 071:     22 / 50 loss=6.569, ppl=94.96, wps=3699.3, ups=3.16, wpb=1169.3, bsz=64, num_updates=3520, lr=1.056e-05, gnorm=7.909, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=695
2022-01-13 00:03:22 | INFO | train_inner | epoch 071:     42 / 50 loss=6.61, ppl=97.65, wps=14561.6, ups=12.03, wpb=1210.1, bsz=64, num_updates=3540, lr=1.062e-05, gnorm=7.789, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=697
2022-01-13 00:03:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 00:03:23 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.584 | ppl 95.94 | wps 24738 | wpb 556.6 | bsz 30.3 | num_updates 3548 | best_loss 6.554
2022-01-13 00:03:23 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-01-13 00:03:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3548 updates
2022-01-13 00:03:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 00:03:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 71 @ 3548 updates, score 6.584) (writing took 2.709606857970357 seconds)
2022-01-13 00:03:26 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-01-13 00:03:26 | INFO | train | epoch 071 | loss 6.637 | ppl 99.51 | wps 5003.6 | ups 4.06 | wpb 1232.9 | bsz 63.5 | num_updates 3548 | lr 1.0644e-05 | gnorm 7.79 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 701
2022-01-13 00:03:26 | INFO | fairseq_cli.train | done training in 696.7 seconds
2022-01-13 16:04:14 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-13 16:04:14 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-01-13 16:04:16 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-01-13 16:04:16 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-01-13 16:04:16 | INFO | fairseq_cli.train | model: BARTMLModel
2022-01-13 16:04:16 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-01-13 16:04:16 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-01-13 16:04:16 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-13 16:04:17 | INFO | bartabst.data.data_utils | loaded 908 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-01-13 16:04:20 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-01-13 16:04:20 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-13 16:04:20 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-01-13 16:04:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-13 16:04:20 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-01-13 16:04:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-13 16:04:20 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-13 16:04:20 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-01-13 16:04:20 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-01-13 16:04:22 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-01-13 16:04:22 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-01-13 16:04:22 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-01-13 16:04:22 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-13 16:04:25 | INFO | bartabst.data.data_utils | loaded 3,174 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-01-13 16:04:25 | INFO | fairseq.trainer | begin training epoch 1
2022-01-13 16:04:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:04:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-01-13 16:04:26 | INFO | train_inner | epoch 001:     21 / 50 loss=17.159, ppl=146394, wps=15982, ups=13.74, wpb=1196, bsz=62.7, num_updates=20, lr=6e-08, gnorm=23.615, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=6
2022-01-13 16:04:28 | INFO | train_inner | epoch 001:     41 / 50 loss=17.296, ppl=160960, wps=16788.3, ups=13.29, wpb=1263.2, bsz=64, num_updates=40, lr=1.2e-07, gnorm=23.468, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=8
2022-01-13 16:04:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:04:29 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.132 | ppl 143673 | wps 25542.1 | wpb 556.6 | bsz 30.3 | num_updates 49
2022-01-13 16:04:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 49 updates
2022-01-13 16:04:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 49 updates, score 17.132) (writing took 5.906699588987976 seconds)
2022-01-13 16:04:35 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-13 16:04:35 | INFO | train | epoch 001 | loss 17.245 | ppl 155295 | wps 5639.8 | ups 4.67 | wpb 1219.7 | bsz 63.5 | num_updates 49 | lr 1.47e-07 | gnorm 23.417 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 15
2022-01-13 16:04:35 | INFO | fairseq.trainer | begin training epoch 2
2022-01-13 16:04:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:04:36 | INFO | train_inner | epoch 002:     11 / 50 loss=17.221, ppl=152767, wps=2724.9, ups=2.41, wpb=1130.5, bsz=62.7, num_updates=60, lr=1.8e-07, gnorm=23.114, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=16
2022-01-13 16:04:38 | INFO | train_inner | epoch 002:     31 / 50 loss=17.199, ppl=150443, wps=18228.5, ups=13.95, wpb=1306.8, bsz=64, num_updates=80, lr=2.4e-07, gnorm=22.864, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=18
2022-01-13 16:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:04:40 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 16.922 | ppl 124169 | wps 26985.5 | wpb 556.6 | bsz 30.3 | num_updates 99 | best_loss 16.922
2022-01-13 16:04:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 99 updates
2022-01-13 16:04:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 99 updates, score 16.922) (writing took 4.311357039026916 seconds)
2022-01-13 16:04:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-13 16:04:44 | INFO | train | epoch 002 | loss 17.126 | ppl 143014 | wps 7050.7 | ups 5.72 | wpb 1232.9 | bsz 63.5 | num_updates 99 | lr 2.97e-07 | gnorm 22.899 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 24
2022-01-13 16:04:44 | INFO | fairseq.trainer | begin training epoch 3
2022-01-13 16:04:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:04:44 | INFO | train_inner | epoch 003:      1 / 50 loss=17.043, ppl=135068, wps=3761.6, ups=3.05, wpb=1233.1, bsz=64, num_updates=100, lr=3e-07, gnorm=22.704, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=24
2022-01-13 16:04:46 | INFO | train_inner | epoch 003:     21 / 50 loss=16.922, ppl=124168, wps=16640.3, ups=13.61, wpb=1222.8, bsz=62.7, num_updates=120, lr=3.6e-07, gnorm=22.369, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=26
2022-01-13 16:04:47 | INFO | train_inner | epoch 003:     41 / 50 loss=16.713, ppl=107421, wps=19230.2, ups=14.28, wpb=1347.1, bsz=64, num_updates=140, lr=4.2e-07, gnorm=21.783, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=27
2022-01-13 16:04:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:04:49 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.449 | ppl 89457.4 | wps 27430.9 | wpb 556.6 | bsz 30.3 | num_updates 149 | best_loss 16.449
2022-01-13 16:04:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 149 updates
2022-01-13 16:04:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:04:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 149 updates, score 16.449) (writing took 4.084286751924083 seconds)
2022-01-13 16:04:53 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-13 16:04:53 | INFO | train | epoch 003 | loss 16.775 | ppl 112183 | wps 7112.2 | ups 5.77 | wpb 1232.9 | bsz 63.5 | num_updates 149 | lr 4.47e-07 | gnorm 22.104 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 33
2022-01-13 16:04:53 | INFO | fairseq.trainer | begin training epoch 4
2022-01-13 16:04:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:04:54 | INFO | train_inner | epoch 004:     11 / 50 loss=16.552, ppl=96075.3, wps=3424.6, ups=3.09, wpb=1109.7, bsz=62.7, num_updates=160, lr=4.8e-07, gnorm=22.008, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=33
2022-01-13 16:04:55 | INFO | train_inner | epoch 004:     31 / 50 loss=16.283, ppl=79750.9, wps=16219, ups=12.86, wpb=1261, bsz=64, num_updates=180, lr=5.4e-07, gnorm=21.031, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=35
2022-01-13 16:04:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:04:58 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 15.807 | ppl 57342.1 | wps 27509.2 | wpb 556.6 | bsz 30.3 | num_updates 199 | best_loss 15.807
2022-01-13 16:04:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 199 updates
2022-01-13 16:04:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 199 updates, score 15.807) (writing took 4.109877816168591 seconds)
2022-01-13 16:05:02 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-13 16:05:02 | INFO | train | epoch 004 | loss 16.271 | ppl 79071.1 | wps 6888.1 | ups 5.59 | wpb 1232.9 | bsz 63.5 | num_updates 199 | lr 5.97e-07 | gnorm 21.383 | clip 100 | loss_scale 64 | train_wall 4 | gb_free 20.9 | wall 42
2022-01-13 16:05:02 | INFO | fairseq.trainer | begin training epoch 5
2022-01-13 16:05:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:02 | INFO | train_inner | epoch 005:      1 / 50 loss=16.097, ppl=70117.1, wps=3609.4, ups=2.98, wpb=1210.8, bsz=64, num_updates=200, lr=6e-07, gnorm=21.412, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=42
2022-01-13 16:05:03 | INFO | train_inner | epoch 005:     21 / 50 loss=15.771, ppl=55917.4, wps=17116.2, ups=13.85, wpb=1235.9, bsz=64, num_updates=220, lr=6.6e-07, gnorm=20.211, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=43
2022-01-13 16:05:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-01-13 16:05:05 | INFO | train_inner | epoch 005:     42 / 50 loss=15.559, ppl=48277.4, wps=18462.5, ups=14.03, wpb=1316.2, bsz=64, num_updates=240, lr=7.2e-07, gnorm=19.435, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=45
2022-01-13 16:05:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:06 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 15.243 | ppl 38786.9 | wps 29118.5 | wpb 556.6 | bsz 30.3 | num_updates 248 | best_loss 15.243
2022-01-13 16:05:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 248 updates
2022-01-13 16:05:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 248 updates, score 15.243) (writing took 3.950551080983132 seconds)
2022-01-13 16:05:10 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-13 16:05:10 | INFO | train | epoch 005 | loss 15.636 | ppl 50916.4 | wps 7237.5 | ups 5.82 | wpb 1243 | bsz 63.5 | num_updates 248 | lr 7.44e-07 | gnorm 20.049 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 50
2022-01-13 16:05:10 | INFO | fairseq.trainer | begin training epoch 6
2022-01-13 16:05:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:11 | INFO | train_inner | epoch 006:     12 / 50 loss=15.337, ppl=41381, wps=3835.3, ups=3.15, wpb=1217.2, bsz=62.7, num_updates=260, lr=7.8e-07, gnorm=19.805, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=51
2022-01-13 16:05:12 | INFO | train_inner | epoch 006:     32 / 50 loss=14.997, ppl=32702.2, wps=18977.4, ups=14.47, wpb=1311.7, bsz=62.7, num_updates=280, lr=8.4e-07, gnorm=19.077, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=52
2022-01-13 16:05:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:14 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 14.581 | ppl 24503.1 | wps 28916.4 | wpb 556.6 | bsz 30.3 | num_updates 298 | best_loss 14.581
2022-01-13 16:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 298 updates
2022-01-13 16:05:14 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:17 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 298 updates, score 14.581) (writing took 4.286012321943417 seconds)
2022-01-13 16:05:19 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-13 16:05:19 | INFO | train | epoch 006 | loss 15.033 | ppl 33520 | wps 7152.8 | ups 5.8 | wpb 1232.9 | bsz 63.5 | num_updates 298 | lr 8.94e-07 | gnorm 18.987 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 59
2022-01-13 16:05:19 | INFO | fairseq.trainer | begin training epoch 7
2022-01-13 16:05:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:19 | INFO | train_inner | epoch 007:      2 / 50 loss=14.896, ppl=30486.8, wps=3494.6, ups=3.07, wpb=1136.9, bsz=64, num_updates=300, lr=9e-07, gnorm=18.768, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=59
2022-01-13 16:05:20 | INFO | train_inner | epoch 007:     22 / 50 loss=14.558, ppl=24115.6, wps=18699.9, ups=14.22, wpb=1315.3, bsz=64, num_updates=320, lr=9.6e-07, gnorm=17.902, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=60
2022-01-13 16:05:22 | INFO | train_inner | epoch 007:     42 / 50 loss=14.268, ppl=19731.4, wps=15578.2, ups=14.8, wpb=1052.3, bsz=62.7, num_updates=340, lr=1.02e-06, gnorm=19.044, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=62
2022-01-13 16:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:23 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 13.939 | ppl 15700.3 | wps 29150.7 | wpb 556.6 | bsz 30.3 | num_updates 348 | best_loss 13.939
2022-01-13 16:05:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 348 updates
2022-01-13 16:05:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 348 updates, score 13.939) (writing took 3.853371197823435 seconds)
2022-01-13 16:05:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-13 16:05:27 | INFO | train | epoch 007 | loss 14.362 | ppl 21061.2 | wps 7559 | ups 6.13 | wpb 1232.9 | bsz 63.5 | num_updates 348 | lr 1.044e-06 | gnorm 18.136 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 67
2022-01-13 16:05:27 | INFO | fairseq.trainer | begin training epoch 8
2022-01-13 16:05:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:28 | INFO | train_inner | epoch 008:     12 / 50 loss=13.982, ppl=16177, wps=4613.2, ups=3.3, wpb=1399.5, bsz=64, num_updates=360, lr=1.08e-06, gnorm=18.713, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=68
2022-01-13 16:05:29 | INFO | train_inner | epoch 008:     32 / 50 loss=13.673, ppl=13065.5, wps=18309.4, ups=14.69, wpb=1246.2, bsz=64, num_updates=380, lr=1.14e-06, gnorm=16.927, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=69
2022-01-13 16:05:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:31 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 13.325 | ppl 10265.3 | wps 30046.1 | wpb 556.6 | bsz 30.3 | num_updates 398 | best_loss 13.325
2022-01-13 16:05:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 398 updates
2022-01-13 16:05:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 398 updates, score 13.325) (writing took 4.423732958966866 seconds)
2022-01-13 16:05:36 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-13 16:05:36 | INFO | train | epoch 008 | loss 13.671 | ppl 13041.4 | wps 7070.2 | ups 5.73 | wpb 1232.9 | bsz 63.5 | num_updates 398 | lr 1.194e-06 | gnorm 17.703 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 76
2022-01-13 16:05:36 | INFO | fairseq.trainer | begin training epoch 9
2022-01-13 16:05:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:36 | INFO | train_inner | epoch 009:      2 / 50 loss=13.445, ppl=11152.4, wps=3388.7, ups=2.86, wpb=1185.2, bsz=62.7, num_updates=400, lr=1.2e-06, gnorm=16.722, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=76
2022-01-13 16:05:38 | INFO | train_inner | epoch 009:     22 / 50 loss=13.342, ppl=10380.7, wps=17530, ups=14.16, wpb=1238, bsz=62.7, num_updates=420, lr=1.26e-06, gnorm=17.348, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=77
2022-01-13 16:05:39 | INFO | train_inner | epoch 009:     42 / 50 loss=13.019, ppl=8299.91, wps=17288.2, ups=14.17, wpb=1220, bsz=64, num_updates=440, lr=1.32e-06, gnorm=14.824, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=79
2022-01-13 16:05:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:40 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 12.838 | ppl 7324.38 | wps 27468.9 | wpb 556.6 | bsz 30.3 | num_updates 448 | best_loss 12.838
2022-01-13 16:05:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 448 updates
2022-01-13 16:05:40 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 448 updates, score 12.838) (writing took 3.9607309179846197 seconds)
2022-01-13 16:05:44 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-13 16:05:44 | INFO | train | epoch 009 | loss 13.134 | ppl 8992.19 | wps 7398.4 | ups 6 | wpb 1232.9 | bsz 63.5 | num_updates 448 | lr 1.344e-06 | gnorm 15.786 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 84
2022-01-13 16:05:44 | INFO | fairseq.trainer | begin training epoch 10
2022-01-13 16:05:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:45 | INFO | train_inner | epoch 010:     12 / 50 loss=12.802, ppl=7139, wps=3932.3, ups=3.23, wpb=1217.8, bsz=62.7, num_updates=460, lr=1.38e-06, gnorm=13.664, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=85
2022-01-13 16:05:47 | INFO | train_inner | epoch 010:     32 / 50 loss=12.768, ppl=6972.94, wps=17250.1, ups=13.99, wpb=1232.8, bsz=64, num_updates=480, lr=1.44e-06, gnorm=12.989, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=87
2022-01-13 16:05:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:49 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 12.445 | ppl 5577.56 | wps 28296.4 | wpb 556.6 | bsz 30.3 | num_updates 498 | best_loss 12.445
2022-01-13 16:05:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 498 updates
2022-01-13 16:05:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:05:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 498 updates, score 12.445) (writing took 5.133760615019128 seconds)
2022-01-13 16:05:54 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-13 16:05:55 | INFO | train | epoch 010 | loss 12.653 | ppl 6442.06 | wps 6492.7 | ups 5.27 | wpb 1232.9 | bsz 63.5 | num_updates 498 | lr 1.494e-06 | gnorm 12.537 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 94
2022-01-13 16:05:55 | INFO | fairseq.trainer | begin training epoch 11
2022-01-13 16:05:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:05:55 | INFO | train_inner | epoch 011:      2 / 50 loss=12.5, ppl=5793.55, wps=2798.6, ups=2.34, wpb=1197, bsz=64, num_updates=500, lr=1.5e-06, gnorm=11.699, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=95
2022-01-13 16:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:05:56 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.396 | ppl 5389.64 | wps 27319.1 | wpb 556.6 | bsz 30.3 | num_updates 500 | best_loss 12.396
2022-01-13 16:05:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 500 updates
2022-01-13 16:05:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt
2022-01-13 16:05:58 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt
2022-01-13 16:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_11_500.pt (epoch 11 @ 500 updates, score 12.396) (writing took 6.879543789895251 seconds)
2022-01-13 16:06:05 | INFO | train_inner | epoch 011:     22 / 50 loss=12.339, ppl=5181.57, wps=2810.1, ups=2.15, wpb=1310, bsz=64, num_updates=520, lr=1.56e-06, gnorm=10.909, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=104
2022-01-13 16:06:06 | INFO | train_inner | epoch 011:     42 / 50 loss=12.22, ppl=4771.57, wps=15190.4, ups=12.77, wpb=1189.1, bsz=64, num_updates=540, lr=1.62e-06, gnorm=11.548, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=106
2022-01-13 16:06:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:07 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.046 | ppl 4227.29 | wps 26647.7 | wpb 556.6 | bsz 30.3 | num_updates 548 | best_loss 12.046
2022-01-13 16:06:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 548 updates
2022-01-13 16:06:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 548 updates, score 12.046) (writing took 4.2199249600525945 seconds)
2022-01-13 16:06:12 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-13 16:06:12 | INFO | train | epoch 011 | loss 12.283 | ppl 4984.74 | wps 3663.8 | ups 2.97 | wpb 1232.9 | bsz 63.5 | num_updates 548 | lr 1.644e-06 | gnorm 11.357 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 112
2022-01-13 16:06:12 | INFO | fairseq.trainer | begin training epoch 12
2022-01-13 16:06:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:13 | INFO | train_inner | epoch 012:     12 / 50 loss=12.14, ppl=4512.23, wps=3669.4, ups=3.02, wpb=1214.3, bsz=61.4, num_updates=560, lr=1.68e-06, gnorm=11.904, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=113
2022-01-13 16:06:14 | INFO | train_inner | epoch 012:     32 / 50 loss=11.944, ppl=3939.08, wps=16126.2, ups=13.21, wpb=1221.2, bsz=64, num_updates=580, lr=1.74e-06, gnorm=11.565, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=114
2022-01-13 16:06:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:16 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.751 | ppl 3447.89 | wps 28540 | wpb 556.6 | bsz 30.3 | num_updates 598 | best_loss 11.751
2022-01-13 16:06:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 598 updates
2022-01-13 16:06:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 598 updates, score 11.751) (writing took 4.101295632077381 seconds)
2022-01-13 16:06:20 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-13 16:06:20 | INFO | train | epoch 012 | loss 11.971 | ppl 4014.07 | wps 7091.9 | ups 5.75 | wpb 1232.9 | bsz 63.5 | num_updates 598 | lr 1.794e-06 | gnorm 12.637 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 120
2022-01-13 16:06:20 | INFO | fairseq.trainer | begin training epoch 13
2022-01-13 16:06:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:21 | INFO | train_inner | epoch 013:      2 / 50 loss=11.855, ppl=3705.11, wps=3836, ups=3.11, wpb=1232.8, bsz=64, num_updates=600, lr=1.8e-06, gnorm=13.871, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=121
2022-01-13 16:06:22 | INFO | train_inner | epoch 013:     22 / 50 loss=11.757, ppl=3459.88, wps=16800.1, ups=13.84, wpb=1213.5, bsz=62.7, num_updates=620, lr=1.86e-06, gnorm=10.483, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=122
2022-01-13 16:06:24 | INFO | train_inner | epoch 013:     42 / 50 loss=11.588, ppl=3078.23, wps=14068.7, ups=11.4, wpb=1234.5, bsz=64, num_updates=640, lr=1.92e-06, gnorm=9.733, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=124
2022-01-13 16:06:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:25 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.506 | ppl 2908.49 | wps 28868.6 | wpb 556.6 | bsz 30.3 | num_updates 648 | best_loss 11.506
2022-01-13 16:06:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 648 updates
2022-01-13 16:06:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 648 updates, score 11.506) (writing took 5.000021601095796 seconds)
2022-01-13 16:06:30 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-13 16:06:30 | INFO | train | epoch 013 | loss 11.719 | ppl 3371.76 | wps 6296 | ups 5.11 | wpb 1232.9 | bsz 63.5 | num_updates 648 | lr 1.944e-06 | gnorm 10.011 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 130
2022-01-13 16:06:30 | INFO | fairseq.trainer | begin training epoch 14
2022-01-13 16:06:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:31 | INFO | train_inner | epoch 014:     12 / 50 loss=11.784, ppl=3526.14, wps=3444.5, ups=2.74, wpb=1259.2, bsz=64, num_updates=660, lr=1.98e-06, gnorm=11.754, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=131
2022-01-13 16:06:33 | INFO | train_inner | epoch 014:     32 / 50 loss=11.538, ppl=2972.65, wps=18085.2, ups=13.61, wpb=1329, bsz=62.7, num_updates=680, lr=2.04e-06, gnorm=9.592, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=133
2022-01-13 16:06:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:35 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.335 | ppl 2583.71 | wps 26225.2 | wpb 556.6 | bsz 30.3 | num_updates 698 | best_loss 11.335
2022-01-13 16:06:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 698 updates
2022-01-13 16:06:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 698 updates, score 11.335) (writing took 4.299873220035806 seconds)
2022-01-13 16:06:39 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-13 16:06:39 | INFO | train | epoch 014 | loss 11.504 | ppl 2903.88 | wps 6949.9 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 698 | lr 2.094e-06 | gnorm 10.531 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 139
2022-01-13 16:06:39 | INFO | fairseq.trainer | begin training epoch 15
2022-01-13 16:06:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:39 | INFO | train_inner | epoch 015:      2 / 50 loss=11.364, ppl=2635.67, wps=3404.2, ups=3, wpb=1136.4, bsz=64, num_updates=700, lr=2.1e-06, gnorm=9.71, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=139
2022-01-13 16:06:41 | INFO | train_inner | epoch 015:     22 / 50 loss=11.366, ppl=2639.55, wps=17012.7, ups=14.31, wpb=1188.8, bsz=64, num_updates=720, lr=2.16e-06, gnorm=9.584, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=141
2022-01-13 16:06:42 | INFO | train_inner | epoch 015:     42 / 50 loss=11.254, ppl=2442.64, wps=18913, ups=13.82, wpb=1368.7, bsz=64, num_updates=740, lr=2.22e-06, gnorm=8.667, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=142
2022-01-13 16:06:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:44 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.099 | ppl 2193.77 | wps 26439.4 | wpb 556.6 | bsz 30.3 | num_updates 748 | best_loss 11.099
2022-01-13 16:06:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 748 updates
2022-01-13 16:06:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 748 updates, score 11.099) (writing took 4.298957463121042 seconds)
2022-01-13 16:06:48 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-13 16:06:48 | INFO | train | epoch 015 | loss 11.293 | ppl 2508.52 | wps 7034.6 | ups 5.71 | wpb 1232.9 | bsz 63.5 | num_updates 748 | lr 2.244e-06 | gnorm 9.173 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 148
2022-01-13 16:06:48 | INFO | fairseq.trainer | begin training epoch 16
2022-01-13 16:06:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:49 | INFO | train_inner | epoch 016:     12 / 50 loss=11.251, ppl=2436.61, wps=3436.8, ups=3.02, wpb=1138.8, bsz=62.7, num_updates=760, lr=2.28e-06, gnorm=8.97, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=149
2022-01-13 16:06:50 | INFO | train_inner | epoch 016:     32 / 50 loss=11.045, ppl=2112.64, wps=16052.3, ups=14.21, wpb=1129.2, bsz=64, num_updates=780, lr=2.34e-06, gnorm=9.986, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=150
2022-01-13 16:06:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:06:52 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.946 | ppl 1973.05 | wps 26753.4 | wpb 556.6 | bsz 30.3 | num_updates 798 | best_loss 10.946
2022-01-13 16:06:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 798 updates
2022-01-13 16:06:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 798 updates, score 10.946) (writing took 4.205898888874799 seconds)
2022-01-13 16:06:56 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-13 16:06:56 | INFO | train | epoch 016 | loss 11.149 | ppl 2271.5 | wps 7206.1 | ups 5.84 | wpb 1232.9 | bsz 63.5 | num_updates 798 | lr 2.394e-06 | gnorm 9.108 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 156
2022-01-13 16:06:56 | INFO | fairseq.trainer | begin training epoch 17
2022-01-13 16:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:06:57 | INFO | train_inner | epoch 017:      2 / 50 loss=11.146, ppl=2265.88, wps=4084.1, ups=3.11, wpb=1312.2, bsz=62.7, num_updates=800, lr=2.4e-06, gnorm=8.511, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=157
2022-01-13 16:06:58 | INFO | train_inner | epoch 017:     22 / 50 loss=10.91, ppl=1923.48, wps=16704.3, ups=14.39, wpb=1160.7, bsz=62.7, num_updates=820, lr=2.46e-06, gnorm=8.976, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=158
2022-01-13 16:06:59 | INFO | train_inner | epoch 017:     42 / 50 loss=11.13, ppl=2240.58, wps=20189.9, ups=13.74, wpb=1469.3, bsz=64, num_updates=840, lr=2.52e-06, gnorm=8.087, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=159
2022-01-13 16:07:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:01 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.77 | ppl 1745.71 | wps 26281.8 | wpb 556.6 | bsz 30.3 | num_updates 848 | best_loss 10.77
2022-01-13 16:07:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 848 updates
2022-01-13 16:07:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 848 updates, score 10.77) (writing took 4.110174420988187 seconds)
2022-01-13 16:07:05 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-13 16:07:05 | INFO | train | epoch 017 | loss 10.988 | ppl 2031.55 | wps 7250.3 | ups 5.88 | wpb 1232.9 | bsz 63.5 | num_updates 848 | lr 2.544e-06 | gnorm 8.635 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 165
2022-01-13 16:07:05 | INFO | fairseq.trainer | begin training epoch 18
2022-01-13 16:07:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:07:06 | INFO | train_inner | epoch 018:     12 / 50 loss=10.811, ppl=1796.58, wps=3415.6, ups=3.14, wpb=1087.5, bsz=64, num_updates=860, lr=2.58e-06, gnorm=8.306, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=166
2022-01-13 16:07:07 | INFO | train_inner | epoch 018:     32 / 50 loss=10.884, ppl=1889.38, wps=18001.4, ups=14.2, wpb=1268, bsz=62.7, num_updates=880, lr=2.64e-06, gnorm=9.078, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=167
2022-01-13 16:07:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:09 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.576 | ppl 1526.65 | wps 26991.8 | wpb 556.6 | bsz 30.3 | num_updates 898 | best_loss 10.576
2022-01-13 16:07:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 898 updates
2022-01-13 16:07:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 898 updates, score 10.576) (writing took 4.190769501030445 seconds)
2022-01-13 16:07:14 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-13 16:07:14 | INFO | train | epoch 018 | loss 10.831 | ppl 1821.56 | wps 7172.2 | ups 5.82 | wpb 1232.9 | bsz 63.5 | num_updates 898 | lr 2.694e-06 | gnorm 8.465 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 173
2022-01-13 16:07:14 | INFO | fairseq.trainer | begin training epoch 19
2022-01-13 16:07:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:07:14 | INFO | train_inner | epoch 019:      2 / 50 loss=10.767, ppl=1742.59, wps=3609.8, ups=3.1, wpb=1164.9, bsz=64, num_updates=900, lr=2.7e-06, gnorm=8.402, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=174
2022-01-13 16:07:15 | INFO | train_inner | epoch 019:     22 / 50 loss=10.706, ppl=1670.31, wps=17261.7, ups=14.24, wpb=1212.3, bsz=62.7, num_updates=920, lr=2.76e-06, gnorm=8.029, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=175
2022-01-13 16:07:17 | INFO | train_inner | epoch 019:     42 / 50 loss=10.779, ppl=1757.66, wps=18074.1, ups=13.89, wpb=1300.9, bsz=64, num_updates=940, lr=2.82e-06, gnorm=8.294, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=176
2022-01-13 16:07:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:18 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 10.468 | ppl 1416.62 | wps 26580.8 | wpb 556.6 | bsz 30.3 | num_updates 948 | best_loss 10.468
2022-01-13 16:07:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 948 updates
2022-01-13 16:07:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 948 updates, score 10.468) (writing took 4.056778114056215 seconds)
2022-01-13 16:07:22 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-13 16:07:22 | INFO | train | epoch 019 | loss 10.697 | ppl 1659.98 | wps 7313.8 | ups 5.93 | wpb 1232.9 | bsz 63.5 | num_updates 948 | lr 2.844e-06 | gnorm 8.25 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 182
2022-01-13 16:07:22 | INFO | fairseq.trainer | begin training epoch 20
2022-01-13 16:07:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:07:23 | INFO | train_inner | epoch 020:     12 / 50 loss=10.408, ppl=1358.86, wps=3650.6, ups=3.19, wpb=1145.7, bsz=64, num_updates=960, lr=2.88e-06, gnorm=8.498, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=183
2022-01-13 16:07:24 | INFO | train_inner | epoch 020:     32 / 50 loss=10.645, ppl=1601.75, wps=17553.3, ups=14.16, wpb=1240, bsz=62.7, num_updates=980, lr=2.94e-06, gnorm=7.943, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=184
2022-01-13 16:07:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:26 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 10.37 | ppl 1323 | wps 25591.8 | wpb 556.6 | bsz 30.3 | num_updates 998 | best_loss 10.37
2022-01-13 16:07:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 998 updates
2022-01-13 16:07:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 998 updates, score 10.37) (writing took 3.997481770813465 seconds)
2022-01-13 16:07:30 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-13 16:07:30 | INFO | train | epoch 020 | loss 10.568 | ppl 1517.83 | wps 7364.6 | ups 5.97 | wpb 1232.9 | bsz 63.5 | num_updates 998 | lr 2.994e-06 | gnorm 7.986 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 190
2022-01-13 16:07:30 | INFO | fairseq.trainer | begin training epoch 21
2022-01-13 16:07:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:07:31 | INFO | train_inner | epoch 021:      2 / 50 loss=10.55, ppl=1499.7, wps=4030.3, ups=3.16, wpb=1273.9, bsz=64, num_updates=1000, lr=3e-06, gnorm=7.78, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=191
2022-01-13 16:07:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:31 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.298 | ppl 1259.29 | wps 27011.2 | wpb 556.6 | bsz 30.3 | num_updates 1000 | best_loss 10.298
2022-01-13 16:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1000 updates
2022-01-13 16:07:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt
2022-01-13 16:07:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt
2022-01-13 16:07:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_21_1000.pt (epoch 21 @ 1000 updates, score 10.298) (writing took 9.162504152860492 seconds)
2022-01-13 16:07:42 | INFO | train_inner | epoch 021:     22 / 50 loss=10.495, ppl=1443.49, wps=2214.3, ups=1.7, wpb=1300.8, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=7.772, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=202
2022-01-13 16:07:44 | INFO | train_inner | epoch 021:     42 / 50 loss=10.42, ppl=1370.16, wps=15273.6, ups=12.53, wpb=1219.5, bsz=64, num_updates=1040, lr=3.12e-06, gnorm=7.891, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=204
2022-01-13 16:07:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:45 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.162 | ppl 1145.6 | wps 27897.3 | wpb 556.6 | bsz 30.3 | num_updates 1048 | best_loss 10.162
2022-01-13 16:07:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1048 updates
2022-01-13 16:07:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 1048 updates, score 10.162) (writing took 4.235683298902586 seconds)
2022-01-13 16:07:50 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-13 16:07:50 | INFO | train | epoch 021 | loss 10.453 | ppl 1401.98 | wps 3207.5 | ups 2.6 | wpb 1232.9 | bsz 63.5 | num_updates 1048 | lr 3.144e-06 | gnorm 7.943 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 210
2022-01-13 16:07:50 | INFO | fairseq.trainer | begin training epoch 22
2022-01-13 16:07:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:07:51 | INFO | train_inner | epoch 022:     12 / 50 loss=10.37, ppl=1322.98, wps=3596.5, ups=3.01, wpb=1195.2, bsz=62.7, num_updates=1060, lr=3.18e-06, gnorm=8.135, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=211
2022-01-13 16:07:52 | INFO | train_inner | epoch 022:     32 / 50 loss=10.341, ppl=1297.05, wps=18189.6, ups=14.37, wpb=1265.5, bsz=64, num_updates=1080, lr=3.24e-06, gnorm=7.394, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=212
2022-01-13 16:07:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:07:54 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 10.077 | ppl 1080.39 | wps 27714.2 | wpb 556.6 | bsz 30.3 | num_updates 1098 | best_loss 10.077
2022-01-13 16:07:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1098 updates
2022-01-13 16:07:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 1098 updates, score 10.077) (writing took 5.392136902781203 seconds)
2022-01-13 16:07:59 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-13 16:07:59 | INFO | train | epoch 022 | loss 10.317 | ppl 1275.64 | wps 6308.4 | ups 5.12 | wpb 1232.9 | bsz 63.5 | num_updates 1098 | lr 3.294e-06 | gnorm 7.911 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 219
2022-01-13 16:07:59 | INFO | fairseq.trainer | begin training epoch 23
2022-01-13 16:07:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:00 | INFO | train_inner | epoch 023:      2 / 50 loss=10.312, ppl=1270.95, wps=3065.8, ups=2.63, wpb=1164.5, bsz=62.7, num_updates=1100, lr=3.3e-06, gnorm=8.39, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=220
2022-01-13 16:08:01 | INFO | train_inner | epoch 023:     22 / 50 loss=10.177, ppl=1157.55, wps=16238.3, ups=13.28, wpb=1222.5, bsz=64, num_updates=1120, lr=3.36e-06, gnorm=7.806, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=221
2022-01-13 16:08:03 | INFO | train_inner | epoch 023:     42 / 50 loss=10.255, ppl=1221.88, wps=19375.1, ups=14.14, wpb=1370.7, bsz=64, num_updates=1140, lr=3.42e-06, gnorm=7.533, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=222
2022-01-13 16:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:04 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 10.007 | ppl 1028.67 | wps 27370.5 | wpb 556.6 | bsz 30.3 | num_updates 1148 | best_loss 10.007
2022-01-13 16:08:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1148 updates
2022-01-13 16:08:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 1148 updates, score 10.007) (writing took 4.165350906085223 seconds)
2022-01-13 16:08:08 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-13 16:08:08 | INFO | train | epoch 023 | loss 10.205 | ppl 1180.3 | wps 7068.2 | ups 5.73 | wpb 1232.9 | bsz 63.5 | num_updates 1148 | lr 3.444e-06 | gnorm 7.872 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 228
2022-01-13 16:08:08 | INFO | fairseq.trainer | begin training epoch 24
2022-01-13 16:08:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:09 | INFO | train_inner | epoch 024:     12 / 50 loss=10.067, ppl=1072.99, wps=3057.7, ups=3.08, wpb=994, bsz=62.7, num_updates=1160, lr=3.48e-06, gnorm=8.31, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=229
2022-01-13 16:08:11 | INFO | train_inner | epoch 024:     32 / 50 loss=10.122, ppl=1114.56, wps=19354.5, ups=13.58, wpb=1425.5, bsz=62.7, num_updates=1180, lr=3.54e-06, gnorm=7.221, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=230
2022-01-13 16:08:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:13 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.844 | ppl 918.93 | wps 27018 | wpb 556.6 | bsz 30.3 | num_updates 1198 | best_loss 9.844
2022-01-13 16:08:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1198 updates
2022-01-13 16:08:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:15 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 1198 updates, score 9.844) (writing took 4.539449128089473 seconds)
2022-01-13 16:08:17 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-13 16:08:18 | INFO | train | epoch 024 | loss 10.053 | ppl 1062.49 | wps 6883 | ups 5.58 | wpb 1232.9 | bsz 63.5 | num_updates 1198 | lr 3.594e-06 | gnorm 7.597 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 237
2022-01-13 16:08:18 | INFO | fairseq.trainer | begin training epoch 25
2022-01-13 16:08:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:18 | INFO | train_inner | epoch 025:      2 / 50 loss=9.972, ppl=1004.14, wps=3160.9, ups=2.71, wpb=1164.8, bsz=64, num_updates=1200, lr=3.6e-06, gnorm=7.785, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=238
2022-01-13 16:08:19 | INFO | train_inner | epoch 025:     22 / 50 loss=10.077, ppl=1079.87, wps=18231.5, ups=13.69, wpb=1331.4, bsz=62.7, num_updates=1220, lr=3.66e-06, gnorm=7.686, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=239
2022-01-13 16:08:21 | INFO | train_inner | epoch 025:     42 / 50 loss=9.954, ppl=991.78, wps=15680.6, ups=13.23, wpb=1185.5, bsz=64, num_updates=1240, lr=3.72e-06, gnorm=7.555, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=241
2022-01-13 16:08:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:22 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.688 | ppl 824.62 | wps 27998.8 | wpb 556.6 | bsz 30.3 | num_updates 1248 | best_loss 9.688
2022-01-13 16:08:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1248 updates
2022-01-13 16:08:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 1248 updates, score 9.688) (writing took 4.02902780007571 seconds)
2022-01-13 16:08:26 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-13 16:08:26 | INFO | train | epoch 025 | loss 9.965 | ppl 999.15 | wps 7214.8 | ups 5.85 | wpb 1232.9 | bsz 63.5 | num_updates 1248 | lr 3.744e-06 | gnorm 7.66 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 246
2022-01-13 16:08:26 | INFO | fairseq.trainer | begin training epoch 26
2022-01-13 16:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:27 | INFO | train_inner | epoch 026:     12 / 50 loss=9.851, ppl=923.41, wps=3918.3, ups=3.18, wpb=1231.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=7.752, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=247
2022-01-13 16:08:29 | INFO | train_inner | epoch 026:     32 / 50 loss=9.812, ppl=899.17, wps=17144.9, ups=14.6, wpb=1174.3, bsz=64, num_updates=1280, lr=3.84e-06, gnorm=7.481, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=248
2022-01-13 16:08:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:31 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.575 | ppl 762.84 | wps 26759.6 | wpb 556.6 | bsz 30.3 | num_updates 1298 | best_loss 9.575
2022-01-13 16:08:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1298 updates
2022-01-13 16:08:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 1298 updates, score 9.575) (writing took 4.224266140954569 seconds)
2022-01-13 16:08:35 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-13 16:08:35 | INFO | train | epoch 026 | loss 9.85 | ppl 922.87 | wps 7164 | ups 5.81 | wpb 1232.9 | bsz 63.5 | num_updates 1298 | lr 3.894e-06 | gnorm 7.53 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 255
2022-01-13 16:08:35 | INFO | fairseq.trainer | begin training epoch 27
2022-01-13 16:08:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:35 | INFO | train_inner | epoch 027:      2 / 50 loss=9.789, ppl=884.85, wps=3855.1, ups=3.08, wpb=1252.9, bsz=64, num_updates=1300, lr=3.9e-06, gnorm=7.594, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=255
2022-01-13 16:08:37 | INFO | train_inner | epoch 027:     22 / 50 loss=9.88, ppl=942.3, wps=16714.2, ups=13.43, wpb=1244.4, bsz=64, num_updates=1320, lr=3.96e-06, gnorm=7.444, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=256
2022-01-13 16:08:38 | INFO | train_inner | epoch 027:     42 / 50 loss=9.866, ppl=933.26, wps=18015.5, ups=14.17, wpb=1271.2, bsz=62.7, num_updates=1340, lr=4.02e-06, gnorm=8.862, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=258
2022-01-13 16:08:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:39 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.503 | ppl 725.35 | wps 27187.1 | wpb 556.6 | bsz 30.3 | num_updates 1348 | best_loss 9.503
2022-01-13 16:08:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1348 updates
2022-01-13 16:08:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 1348 updates, score 9.503) (writing took 4.56431334791705 seconds)
2022-01-13 16:08:44 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-13 16:08:44 | INFO | train | epoch 027 | loss 9.78 | ppl 878.96 | wps 6836.1 | ups 5.54 | wpb 1232.9 | bsz 63.5 | num_updates 1348 | lr 4.044e-06 | gnorm 8.138 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 264
2022-01-13 16:08:44 | INFO | fairseq.trainer | begin training epoch 28
2022-01-13 16:08:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:45 | INFO | train_inner | epoch 028:     12 / 50 loss=9.496, ppl=722.02, wps=3279.4, ups=2.94, wpb=1113.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=8.056, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=265
2022-01-13 16:08:46 | INFO | train_inner | epoch 028:     32 / 50 loss=9.724, ppl=845.63, wps=16865.1, ups=13.8, wpb=1222.3, bsz=64, num_updates=1380, lr=4.14e-06, gnorm=7.729, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=266
2022-01-13 16:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:48 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.378 | ppl 665.44 | wps 26741.6 | wpb 556.6 | bsz 30.3 | num_updates 1398 | best_loss 9.378
2022-01-13 16:08:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1398 updates
2022-01-13 16:08:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 1398 updates, score 9.378) (writing took 4.184823209187016 seconds)
2022-01-13 16:08:52 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-13 16:08:52 | INFO | train | epoch 028 | loss 9.662 | ppl 810.1 | wps 7169.3 | ups 5.82 | wpb 1232.9 | bsz 63.5 | num_updates 1398 | lr 4.194e-06 | gnorm 7.946 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 272
2022-01-13 16:08:52 | INFO | fairseq.trainer | begin training epoch 29
2022-01-13 16:08:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:08:53 | INFO | train_inner | epoch 029:      2 / 50 loss=9.669, ppl=813.92, wps=4210.8, ups=3.08, wpb=1367.7, bsz=64, num_updates=1400, lr=4.2e-06, gnorm=7.827, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=273
2022-01-13 16:08:54 | INFO | train_inner | epoch 029:     22 / 50 loss=9.513, ppl=730.82, wps=16695.9, ups=14.32, wpb=1166, bsz=64, num_updates=1420, lr=4.26e-06, gnorm=7.468, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=274
2022-01-13 16:08:56 | INFO | train_inner | epoch 029:     42 / 50 loss=9.602, ppl=777.39, wps=17190.2, ups=13.14, wpb=1308.7, bsz=62.7, num_updates=1440, lr=4.32e-06, gnorm=7.71, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=276
2022-01-13 16:08:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:08:57 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.324 | ppl 640.96 | wps 25006 | wpb 556.6 | bsz 30.3 | num_updates 1448 | best_loss 9.324
2022-01-13 16:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1448 updates
2022-01-13 16:08:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 1448 updates, score 9.324) (writing took 4.171758497133851 seconds)
2022-01-13 16:09:01 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-13 16:09:01 | INFO | train | epoch 029 | loss 9.536 | ppl 742.55 | wps 7074.6 | ups 5.74 | wpb 1232.9 | bsz 63.5 | num_updates 1448 | lr 4.344e-06 | gnorm 7.551 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 281
2022-01-13 16:09:01 | INFO | fairseq.trainer | begin training epoch 30
2022-01-13 16:09:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:02 | INFO | train_inner | epoch 030:     12 / 50 loss=9.507, ppl=727.58, wps=3910.9, ups=3.08, wpb=1269.3, bsz=64, num_updates=1460, lr=4.38e-06, gnorm=7.389, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=282
2022-01-13 16:09:04 | INFO | train_inner | epoch 030:     32 / 50 loss=9.384, ppl=668.08, wps=15949.3, ups=13.61, wpb=1171.5, bsz=64, num_updates=1480, lr=4.44e-06, gnorm=7.701, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=284
2022-01-13 16:09:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:06 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.248 | ppl 608.12 | wps 28662.7 | wpb 556.6 | bsz 30.3 | num_updates 1498 | best_loss 9.248
2022-01-13 16:09:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1498 updates
2022-01-13 16:09:06 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 30 @ 1498 updates, score 9.248) (writing took 4.14149253978394 seconds)
2022-01-13 16:09:10 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-13 16:09:10 | INFO | train | epoch 030 | loss 9.409 | ppl 679.85 | wps 7225.8 | ups 5.86 | wpb 1232.9 | bsz 63.5 | num_updates 1498 | lr 4.494e-06 | gnorm 7.598 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 290
2022-01-13 16:09:10 | INFO | fairseq.trainer | begin training epoch 31
2022-01-13 16:09:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:10 | INFO | train_inner | epoch 031:      2 / 50 loss=9.278, ppl=620.82, wps=3751.6, ups=3.15, wpb=1189.5, bsz=62.7, num_updates=1500, lr=4.5e-06, gnorm=7.758, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=290
2022-01-13 16:09:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:11 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.218 | ppl 595.34 | wps 27220.4 | wpb 556.6 | bsz 30.3 | num_updates 1500 | best_loss 9.218
2022-01-13 16:09:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1500 updates
2022-01-13 16:09:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt
2022-01-13 16:09:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt
2022-01-13 16:09:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_31_1500.pt (epoch 31 @ 1500 updates, score 9.218) (writing took 10.45356583292596 seconds)
2022-01-13 16:09:23 | INFO | train_inner | epoch 031:     22 / 50 loss=9.421, ppl=685.44, wps=2025, ups=1.53, wpb=1321.7, bsz=62.7, num_updates=1520, lr=4.56e-06, gnorm=7.377, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=303
2022-01-13 16:09:25 | INFO | train_inner | epoch 031:     42 / 50 loss=9.218, ppl=595.48, wps=14570.1, ups=12.26, wpb=1188.6, bsz=64, num_updates=1540, lr=4.62e-06, gnorm=7.754, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=305
2022-01-13 16:09:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:26 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.111 | ppl 552.86 | wps 27716.4 | wpb 556.6 | bsz 30.3 | num_updates 1548 | best_loss 9.111
2022-01-13 16:09:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1548 updates
2022-01-13 16:09:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 1548 updates, score 9.111) (writing took 4.13538451702334 seconds)
2022-01-13 16:09:30 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-13 16:09:30 | INFO | train | epoch 031 | loss 9.286 | ppl 624.08 | wps 3013.4 | ups 2.44 | wpb 1232.9 | bsz 63.5 | num_updates 1548 | lr 4.644e-06 | gnorm 7.536 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 310
2022-01-13 16:09:30 | INFO | fairseq.trainer | begin training epoch 32
2022-01-13 16:09:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:31 | INFO | train_inner | epoch 032:     12 / 50 loss=9.229, ppl=599.99, wps=3606.9, ups=3.07, wpb=1174, bsz=64, num_updates=1560, lr=4.68e-06, gnorm=7.511, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=311
2022-01-13 16:09:33 | INFO | train_inner | epoch 032:     32 / 50 loss=9.248, ppl=608.19, wps=18191.9, ups=14.26, wpb=1275.7, bsz=64, num_updates=1580, lr=4.74e-06, gnorm=7.446, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=312
2022-01-13 16:09:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:35 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.037 | ppl 525.23 | wps 26910.1 | wpb 556.6 | bsz 30.3 | num_updates 1598 | best_loss 9.037
2022-01-13 16:09:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1598 updates
2022-01-13 16:09:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 32 @ 1598 updates, score 9.037) (writing took 4.456599232042208 seconds)
2022-01-13 16:09:39 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-13 16:09:39 | INFO | train | epoch 032 | loss 9.223 | ppl 597.56 | wps 7006.5 | ups 5.68 | wpb 1232.9 | bsz 63.5 | num_updates 1598 | lr 4.794e-06 | gnorm 7.531 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 319
2022-01-13 16:09:39 | INFO | fairseq.trainer | begin training epoch 33
2022-01-13 16:09:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:39 | INFO | train_inner | epoch 033:      2 / 50 loss=9.115, ppl=554.63, wps=3508.5, ups=3, wpb=1168.8, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=7.641, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=319
2022-01-13 16:09:41 | INFO | train_inner | epoch 033:     22 / 50 loss=9.143, ppl=565.48, wps=17039.7, ups=13.93, wpb=1223.3, bsz=62.7, num_updates=1620, lr=4.86e-06, gnorm=7.254, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=321
2022-01-13 16:09:42 | INFO | train_inner | epoch 033:     42 / 50 loss=9.159, ppl=571.73, wps=17201.7, ups=13.29, wpb=1294.6, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=7.575, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=322
2022-01-13 16:09:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:44 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.956 | ppl 496.46 | wps 25895.8 | wpb 556.6 | bsz 30.3 | num_updates 1648 | best_loss 8.956
2022-01-13 16:09:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1648 updates
2022-01-13 16:09:44 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1648 updates, score 8.956) (writing took 4.015369780128822 seconds)
2022-01-13 16:09:48 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-13 16:09:48 | INFO | train | epoch 033 | loss 9.118 | ppl 555.59 | wps 7152.4 | ups 5.8 | wpb 1232.9 | bsz 63.5 | num_updates 1648 | lr 4.944e-06 | gnorm 7.498 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 327
2022-01-13 16:09:48 | INFO | fairseq.trainer | begin training epoch 34
2022-01-13 16:09:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:49 | INFO | train_inner | epoch 034:     12 / 50 loss=8.979, ppl=504.59, wps=3454.6, ups=3.14, wpb=1098.5, bsz=64, num_updates=1660, lr=4.98e-06, gnorm=7.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=328
2022-01-13 16:09:50 | INFO | train_inner | epoch 034:     32 / 50 loss=8.947, ppl=493.64, wps=16548.8, ups=13.86, wpb=1194.3, bsz=64, num_updates=1680, lr=5.04e-06, gnorm=7.478, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=330
2022-01-13 16:09:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:09:52 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.854 | ppl 462.74 | wps 26760.1 | wpb 556.6 | bsz 30.3 | num_updates 1698 | best_loss 8.854
2022-01-13 16:09:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1698 updates
2022-01-13 16:09:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:55 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1698 updates, score 8.854) (writing took 4.170731611782685 seconds)
2022-01-13 16:09:56 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-13 16:09:56 | INFO | train | epoch 034 | loss 9.039 | ppl 526.11 | wps 7200.2 | ups 5.84 | wpb 1232.9 | bsz 63.5 | num_updates 1698 | lr 5.094e-06 | gnorm 7.489 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 336
2022-01-13 16:09:56 | INFO | fairseq.trainer | begin training epoch 35
2022-01-13 16:09:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:09:56 | INFO | train_inner | epoch 035:      2 / 50 loss=9.221, ppl=596.9, wps=4602.8, ups=3.11, wpb=1478.3, bsz=62.7, num_updates=1700, lr=5.1e-06, gnorm=7.15, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=336
2022-01-13 16:09:58 | INFO | train_inner | epoch 035:     22 / 50 loss=8.91, ppl=481.01, wps=15877.5, ups=13.89, wpb=1143.2, bsz=64, num_updates=1720, lr=5.16e-06, gnorm=7.452, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=338
2022-01-13 16:09:59 | INFO | train_inner | epoch 035:     42 / 50 loss=8.951, ppl=494.93, wps=19175, ups=14.37, wpb=1334.7, bsz=62.7, num_updates=1740, lr=5.22e-06, gnorm=7.48, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=339
2022-01-13 16:10:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:01 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.746 | ppl 429.3 | wps 26160.1 | wpb 556.6 | bsz 30.3 | num_updates 1748 | best_loss 8.746
2022-01-13 16:10:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1748 updates
2022-01-13 16:10:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1748 updates, score 8.746) (writing took 4.151188254123554 seconds)
2022-01-13 16:10:05 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-13 16:10:05 | INFO | train | epoch 035 | loss 8.912 | ppl 481.56 | wps 7217.2 | ups 5.85 | wpb 1232.9 | bsz 63.5 | num_updates 1748 | lr 5.244e-06 | gnorm 7.451 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 345
2022-01-13 16:10:05 | INFO | fairseq.trainer | begin training epoch 36
2022-01-13 16:10:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:06 | INFO | train_inner | epoch 036:     12 / 50 loss=8.877, ppl=470.26, wps=3778.6, ups=3.04, wpb=1242.2, bsz=64, num_updates=1760, lr=5.28e-06, gnorm=7.571, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=346
2022-01-13 16:10:07 | INFO | train_inner | epoch 036:     32 / 50 loss=8.837, ppl=457.17, wps=18349.9, ups=14.18, wpb=1293.7, bsz=62.7, num_updates=1780, lr=5.34e-06, gnorm=7.468, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=347
2022-01-13 16:10:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:09 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.661 | ppl 404.91 | wps 25730 | wpb 556.6 | bsz 30.3 | num_updates 1798 | best_loss 8.661
2022-01-13 16:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1798 updates
2022-01-13 16:10:09 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1798 updates, score 8.661) (writing took 4.309992542024702 seconds)
2022-01-13 16:10:14 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-13 16:10:14 | INFO | train | epoch 036 | loss 8.866 | ppl 466.5 | wps 6961.3 | ups 5.65 | wpb 1232.9 | bsz 63.5 | num_updates 1798 | lr 5.394e-06 | gnorm 7.548 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 354
2022-01-13 16:10:14 | INFO | fairseq.trainer | begin training epoch 37
2022-01-13 16:10:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:14 | INFO | train_inner | epoch 037:      2 / 50 loss=8.84, ppl=458.23, wps=3427.3, ups=3.02, wpb=1136.7, bsz=64, num_updates=1800, lr=5.4e-06, gnorm=7.544, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=354
2022-01-13 16:10:15 | INFO | train_inner | epoch 037:     22 / 50 loss=8.691, ppl=413.28, wps=16389.6, ups=13.76, wpb=1191, bsz=62.7, num_updates=1820, lr=5.46e-06, gnorm=7.491, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=355
2022-01-13 16:10:17 | INFO | train_inner | epoch 037:     42 / 50 loss=8.782, ppl=440.22, wps=14674.6, ups=11.87, wpb=1236.2, bsz=64, num_updates=1840, lr=5.52e-06, gnorm=7.484, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=357
2022-01-13 16:10:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:18 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.541 | ppl 372.56 | wps 31255.1 | wpb 556.6 | bsz 30.3 | num_updates 1848 | best_loss 8.541
2022-01-13 16:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1848 updates
2022-01-13 16:10:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1848 updates, score 8.541) (writing took 4.009414136176929 seconds)
2022-01-13 16:10:22 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-13 16:10:22 | INFO | train | epoch 037 | loss 8.762 | ppl 434.03 | wps 7128.4 | ups 5.78 | wpb 1232.9 | bsz 63.5 | num_updates 1848 | lr 5.544e-06 | gnorm 7.596 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 362
2022-01-13 16:10:22 | INFO | fairseq.trainer | begin training epoch 38
2022-01-13 16:10:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:23 | INFO | train_inner | epoch 038:     12 / 50 loss=8.765, ppl=434.97, wps=3952.6, ups=3.21, wpb=1231.5, bsz=64, num_updates=1860, lr=5.58e-06, gnorm=7.787, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=363
2022-01-13 16:10:25 | INFO | train_inner | epoch 038:     32 / 50 loss=8.652, ppl=402.27, wps=15816.4, ups=14.27, wpb=1108.1, bsz=64, num_updates=1880, lr=5.64e-06, gnorm=7.524, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=365
2022-01-13 16:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:27 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.489 | ppl 359.29 | wps 26429.1 | wpb 556.6 | bsz 30.3 | num_updates 1898 | best_loss 8.489
2022-01-13 16:10:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1898 updates
2022-01-13 16:10:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 38 @ 1898 updates, score 8.489) (writing took 4.010337166022509 seconds)
2022-01-13 16:10:31 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-13 16:10:31 | INFO | train | epoch 038 | loss 8.72 | ppl 421.81 | wps 7324.5 | ups 5.94 | wpb 1232.9 | bsz 63.5 | num_updates 1898 | lr 5.694e-06 | gnorm 7.447 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 371
2022-01-13 16:10:31 | INFO | fairseq.trainer | begin training epoch 39
2022-01-13 16:10:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:31 | INFO | train_inner | epoch 039:      2 / 50 loss=8.76, ppl=433.48, wps=4321.9, ups=3.18, wpb=1357.8, bsz=62.7, num_updates=1900, lr=5.7e-06, gnorm=7.385, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=371
2022-01-13 16:10:32 | INFO | train_inner | epoch 039:     22 / 50 loss=8.66, ppl=404.52, wps=17956.5, ups=12.88, wpb=1394.3, bsz=62.7, num_updates=1920, lr=5.76e-06, gnorm=7.404, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=372
2022-01-13 16:10:34 | INFO | train_inner | epoch 039:     42 / 50 loss=8.679, ppl=409.79, wps=13311.2, ups=12.06, wpb=1103.3, bsz=64, num_updates=1940, lr=5.82e-06, gnorm=7.47, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=374
2022-01-13 16:10:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:36 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.371 | ppl 330.97 | wps 26976.6 | wpb 556.6 | bsz 30.3 | num_updates 1948 | best_loss 8.371
2022-01-13 16:10:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1948 updates
2022-01-13 16:10:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1948 updates, score 8.371) (writing took 4.564471570076421 seconds)
2022-01-13 16:10:40 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-13 16:10:40 | INFO | train | epoch 039 | loss 8.623 | ppl 394.21 | wps 6542 | ups 5.31 | wpb 1232.9 | bsz 63.5 | num_updates 1948 | lr 5.844e-06 | gnorm 7.454 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 380
2022-01-13 16:10:40 | INFO | fairseq.trainer | begin training epoch 40
2022-01-13 16:10:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:41 | INFO | train_inner | epoch 040:     12 / 50 loss=8.468, ppl=354.2, wps=3326, ups=2.87, wpb=1158.2, bsz=62.7, num_updates=1960, lr=5.88e-06, gnorm=7.667, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=381
2022-01-13 16:10:43 | INFO | train_inner | epoch 040:     32 / 50 loss=8.6, ppl=388.06, wps=16215.4, ups=13.58, wpb=1193.9, bsz=64, num_updates=1980, lr=5.94e-06, gnorm=7.286, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=383
2022-01-13 16:10:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:45 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.359 | ppl 328.42 | wps 25789.3 | wpb 556.6 | bsz 30.3 | num_updates 1998 | best_loss 8.359
2022-01-13 16:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1998 updates
2022-01-13 16:10:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1998 updates, score 8.359) (writing took 4.422414128202945 seconds)
2022-01-13 16:10:49 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-13 16:10:49 | INFO | train | epoch 040 | loss 8.554 | ppl 375.94 | wps 6908.7 | ups 5.6 | wpb 1232.9 | bsz 63.5 | num_updates 1998 | lr 5.994e-06 | gnorm 7.422 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 389
2022-01-13 16:10:49 | INFO | fairseq.trainer | begin training epoch 41
2022-01-13 16:10:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:10:49 | INFO | train_inner | epoch 041:      2 / 50 loss=8.543, ppl=373.01, wps=3903, ups=2.95, wpb=1325.2, bsz=64, num_updates=2000, lr=6e-06, gnorm=7.283, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=389
2022-01-13 16:10:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:10:50 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.283 | ppl 311.4 | wps 27538.8 | wpb 556.6 | bsz 30.3 | num_updates 2000 | best_loss 8.283
2022-01-13 16:10:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2000 updates
2022-01-13 16:10:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt
2022-01-13 16:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt
2022-01-13 16:11:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_41_2000.pt (epoch 41 @ 2000 updates, score 8.283) (writing took 9.639275103108957 seconds)
2022-01-13 16:11:01 | INFO | train_inner | epoch 041:     22 / 50 loss=8.504, ppl=363.03, wps=2179.8, ups=1.67, wpb=1308.8, bsz=64, num_updates=2020, lr=6.06e-06, gnorm=7.23, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=401
2022-01-13 16:11:03 | INFO | train_inner | epoch 041:     42 / 50 loss=8.451, ppl=349.86, wps=15604.2, ups=13.06, wpb=1194.7, bsz=62.7, num_updates=2040, lr=6.12e-06, gnorm=7.374, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=403
2022-01-13 16:11:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:04 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.289 | ppl 312.71 | wps 26983.3 | wpb 556.6 | bsz 30.3 | num_updates 2048 | best_loss 8.283
2022-01-13 16:11:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2048 updates
2022-01-13 16:11:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:11:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:11:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 2048 updates, score 8.289) (writing took 2.782855710014701 seconds)
2022-01-13 16:11:07 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-13 16:11:07 | INFO | train | epoch 041 | loss 8.453 | ppl 350.35 | wps 3412.4 | ups 2.77 | wpb 1232.9 | bsz 63.5 | num_updates 2048 | lr 6.144e-06 | gnorm 7.295 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 407
2022-01-13 16:11:07 | INFO | fairseq.trainer | begin training epoch 42
2022-01-13 16:11:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:08 | INFO | train_inner | epoch 042:     12 / 50 loss=8.347, ppl=325.52, wps=4706.6, ups=3.87, wpb=1217.2, bsz=64, num_updates=2060, lr=6.18e-06, gnorm=7.363, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=408
2022-01-13 16:11:10 | INFO | train_inner | epoch 042:     32 / 50 loss=8.433, ppl=345.67, wps=16015.5, ups=12.84, wpb=1247.2, bsz=64, num_updates=2080, lr=6.24e-06, gnorm=7.501, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=410
2022-01-13 16:11:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:12 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.137 | ppl 281.45 | wps 30256.7 | wpb 556.6 | bsz 30.3 | num_updates 2098 | best_loss 8.137
2022-01-13 16:11:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2098 updates
2022-01-13 16:11:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 2098 updates, score 8.137) (writing took 4.345485378988087 seconds)
2022-01-13 16:11:16 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-13 16:11:16 | INFO | train | epoch 042 | loss 8.402 | ppl 338.38 | wps 6854.6 | ups 5.56 | wpb 1232.9 | bsz 63.5 | num_updates 2098 | lr 6.294e-06 | gnorm 7.467 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 416
2022-01-13 16:11:16 | INFO | fairseq.trainer | begin training epoch 43
2022-01-13 16:11:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:16 | INFO | train_inner | epoch 043:      2 / 50 loss=8.318, ppl=319.2, wps=3537.6, ups=2.99, wpb=1182.5, bsz=62.7, num_updates=2100, lr=6.3e-06, gnorm=7.432, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=416
2022-01-13 16:11:18 | INFO | train_inner | epoch 043:     22 / 50 loss=8.177, ppl=289.45, wps=16532.8, ups=13.51, wpb=1223.8, bsz=62.7, num_updates=2120, lr=6.36e-06, gnorm=7.531, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=418
2022-01-13 16:11:19 | INFO | train_inner | epoch 043:     42 / 50 loss=8.548, ppl=374.37, wps=16729.1, ups=13.01, wpb=1286.2, bsz=64, num_updates=2140, lr=6.42e-06, gnorm=7.07, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=419
2022-01-13 16:11:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:21 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.136 | ppl 281.38 | wps 27232.9 | wpb 556.6 | bsz 30.3 | num_updates 2148 | best_loss 8.136
2022-01-13 16:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2148 updates
2022-01-13 16:11:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 43 @ 2148 updates, score 8.136) (writing took 4.420745020965114 seconds)
2022-01-13 16:11:25 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-13 16:11:25 | INFO | train | epoch 043 | loss 8.313 | ppl 317.92 | wps 6704.3 | ups 5.44 | wpb 1232.9 | bsz 63.5 | num_updates 2148 | lr 6.444e-06 | gnorm 7.325 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 425
2022-01-13 16:11:25 | INFO | fairseq.trainer | begin training epoch 44
2022-01-13 16:11:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:26 | INFO | train_inner | epoch 044:     12 / 50 loss=8.121, ppl=278.34, wps=3163.3, ups=2.92, wpb=1083.7, bsz=62.7, num_updates=2160, lr=6.48e-06, gnorm=7.573, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=426
2022-01-13 16:11:28 | INFO | train_inner | epoch 044:     32 / 50 loss=8.235, ppl=301.22, wps=16333.8, ups=12.91, wpb=1265.7, bsz=64, num_updates=2180, lr=6.54e-06, gnorm=7.183, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=428
2022-01-13 16:11:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:30 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.064 | ppl 267.6 | wps 28635.2 | wpb 556.6 | bsz 30.3 | num_updates 2198 | best_loss 8.064
2022-01-13 16:11:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2198 updates
2022-01-13 16:11:30 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 2198 updates, score 8.064) (writing took 4.677682719193399 seconds)
2022-01-13 16:11:35 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-13 16:11:35 | INFO | train | epoch 044 | loss 8.246 | ppl 303.65 | wps 6578.2 | ups 5.34 | wpb 1232.9 | bsz 63.5 | num_updates 2198 | lr 6.594e-06 | gnorm 7.281 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 435
2022-01-13 16:11:35 | INFO | fairseq.trainer | begin training epoch 45
2022-01-13 16:11:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:35 | INFO | train_inner | epoch 045:      2 / 50 loss=8.289, ppl=312.74, wps=3579.7, ups=2.8, wpb=1277.9, bsz=64, num_updates=2200, lr=6.6e-06, gnorm=7.232, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=435
2022-01-13 16:11:36 | INFO | train_inner | epoch 045:     22 / 50 loss=8.118, ppl=277.89, wps=16832.3, ups=13.79, wpb=1220.5, bsz=62.7, num_updates=2220, lr=6.66e-06, gnorm=7.418, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=436
2022-01-13 16:11:38 | INFO | train_inner | epoch 045:     42 / 50 loss=8.123, ppl=278.72, wps=16909.4, ups=13.81, wpb=1224.2, bsz=64, num_updates=2240, lr=6.72e-06, gnorm=7.486, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=438
2022-01-13 16:11:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:39 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.984 | ppl 253.12 | wps 26321.5 | wpb 556.6 | bsz 30.3 | num_updates 2248 | best_loss 7.984
2022-01-13 16:11:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2248 updates
2022-01-13 16:11:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 2248 updates, score 7.984) (writing took 4.313323424896225 seconds)
2022-01-13 16:11:44 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-13 16:11:44 | INFO | train | epoch 045 | loss 8.153 | ppl 284.65 | wps 7001.8 | ups 5.68 | wpb 1232.9 | bsz 63.5 | num_updates 2248 | lr 6.744e-06 | gnorm 7.412 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 443
2022-01-13 16:11:44 | INFO | fairseq.trainer | begin training epoch 46
2022-01-13 16:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:44 | INFO | train_inner | epoch 046:     12 / 50 loss=8.342, ppl=324.52, wps=3920.8, ups=3.02, wpb=1296.3, bsz=62.7, num_updates=2260, lr=6.78e-06, gnorm=7.174, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=444
2022-01-13 16:11:46 | INFO | train_inner | epoch 046:     32 / 50 loss=7.947, ppl=246.77, wps=16123.6, ups=13.55, wpb=1189.8, bsz=64, num_updates=2280, lr=6.84e-06, gnorm=7.652, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=446
2022-01-13 16:11:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:48 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.968 | ppl 250.4 | wps 27339.4 | wpb 556.6 | bsz 30.3 | num_updates 2298 | best_loss 7.968
2022-01-13 16:11:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2298 updates
2022-01-13 16:11:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:11:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 46 @ 2298 updates, score 7.968) (writing took 4.097854980966076 seconds)
2022-01-13 16:11:52 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-13 16:11:53 | INFO | train | epoch 046 | loss 8.086 | ppl 271.68 | wps 7038.4 | ups 5.71 | wpb 1232.9 | bsz 63.5 | num_updates 2298 | lr 6.894e-06 | gnorm 7.444 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 452
2022-01-13 16:11:54 | INFO | fairseq.trainer | begin training epoch 47
2022-01-13 16:11:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:11:54 | INFO | train_inner | epoch 047:      2 / 50 loss=8.117, ppl=277.68, wps=3279.6, ups=2.57, wpb=1275.9, bsz=64, num_updates=2300, lr=6.9e-06, gnorm=7.698, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=454
2022-01-13 16:11:55 | INFO | train_inner | epoch 047:     22 / 50 loss=8.119, ppl=278.04, wps=17783.4, ups=14.28, wpb=1245.6, bsz=64, num_updates=2320, lr=6.96e-06, gnorm=7.318, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=455
2022-01-13 16:11:57 | INFO | train_inner | epoch 047:     42 / 50 loss=7.939, ppl=245.34, wps=17622, ups=13.83, wpb=1274.3, bsz=62.7, num_updates=2340, lr=7.02e-06, gnorm=7.391, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=457
2022-01-13 16:11:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:11:58 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.843 | ppl 229.65 | wps 26060.8 | wpb 556.6 | bsz 30.3 | num_updates 2348 | best_loss 7.843
2022-01-13 16:11:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 2348 updates
2022-01-13 16:11:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 2348 updates, score 7.843) (writing took 4.281112919095904 seconds)
2022-01-13 16:12:02 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-13 16:12:02 | INFO | train | epoch 047 | loss 8.031 | ppl 261.61 | wps 7133.3 | ups 5.79 | wpb 1232.9 | bsz 63.5 | num_updates 2348 | lr 7.044e-06 | gnorm 7.469 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 462
2022-01-13 16:12:02 | INFO | fairseq.trainer | begin training epoch 48
2022-01-13 16:12:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:03 | INFO | train_inner | epoch 048:     12 / 50 loss=7.919, ppl=241.94, wps=3815.9, ups=3.08, wpb=1239.3, bsz=62.7, num_updates=2360, lr=7.08e-06, gnorm=9.644, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=463
2022-01-13 16:12:05 | INFO | train_inner | epoch 048:     32 / 50 loss=7.847, ppl=230.21, wps=16414.4, ups=13.94, wpb=1177.8, bsz=64, num_updates=2380, lr=7.14e-06, gnorm=8.052, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=464
2022-01-13 16:12:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:07 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.761 | ppl 216.96 | wps 26199.4 | wpb 556.6 | bsz 30.3 | num_updates 2398 | best_loss 7.761
2022-01-13 16:12:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 2398 updates
2022-01-13 16:12:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 48 @ 2398 updates, score 7.761) (writing took 5.027700481005013 seconds)
2022-01-13 16:12:12 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-13 16:12:12 | INFO | train | epoch 048 | loss 7.932 | ppl 244.17 | wps 6489.4 | ups 5.26 | wpb 1232.9 | bsz 63.5 | num_updates 2398 | lr 7.194e-06 | gnorm 8.748 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 472
2022-01-13 16:12:12 | INFO | fairseq.trainer | begin training epoch 49
2022-01-13 16:12:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:12 | INFO | train_inner | epoch 049:      2 / 50 loss=7.993, ppl=254.71, wps=3073.5, ups=2.58, wpb=1192.6, bsz=64, num_updates=2400, lr=7.2e-06, gnorm=7.797, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=472
2022-01-13 16:12:14 | INFO | train_inner | epoch 049:     22 / 50 loss=7.819, ppl=225.86, wps=18038.5, ups=13.52, wpb=1334.5, bsz=64, num_updates=2420, lr=7.26e-06, gnorm=7.082, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=474
2022-01-13 16:12:15 | INFO | train_inner | epoch 049:     42 / 50 loss=7.893, ppl=237.66, wps=15250.2, ups=13.4, wpb=1138.3, bsz=62.7, num_updates=2440, lr=7.32e-06, gnorm=10.43, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=475
2022-01-13 16:12:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:17 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.75 | ppl 215.26 | wps 27477.2 | wpb 556.6 | bsz 30.3 | num_updates 2448 | best_loss 7.75
2022-01-13 16:12:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 2448 updates
2022-01-13 16:12:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 2448 updates, score 7.75) (writing took 6.951026380993426 seconds)
2022-01-13 16:12:24 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-13 16:12:24 | INFO | train | epoch 049 | loss 7.901 | ppl 239.06 | wps 5250.4 | ups 4.26 | wpb 1232.9 | bsz 63.5 | num_updates 2448 | lr 7.344e-06 | gnorm 8.476 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 483
2022-01-13 16:12:24 | INFO | fairseq.trainer | begin training epoch 50
2022-01-13 16:12:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:25 | INFO | train_inner | epoch 050:     12 / 50 loss=7.935, ppl=244.74, wps=2738.1, ups=2.14, wpb=1276.5, bsz=62.7, num_updates=2460, lr=7.38e-06, gnorm=7.32, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=485
2022-01-13 16:12:26 | INFO | train_inner | epoch 050:     32 / 50 loss=7.874, ppl=234.54, wps=16877.8, ups=13.26, wpb=1273.2, bsz=64, num_updates=2480, lr=7.44e-06, gnorm=7.515, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=486
2022-01-13 16:12:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:28 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.689 | ppl 206.29 | wps 27047.5 | wpb 556.6 | bsz 30.3 | num_updates 2498 | best_loss 7.689
2022-01-13 16:12:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2498 updates
2022-01-13 16:12:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:31 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 2498 updates, score 7.689) (writing took 4.139003769960254 seconds)
2022-01-13 16:12:32 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-13 16:12:32 | INFO | train | epoch 050 | loss 7.832 | ppl 227.86 | wps 6958.1 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 2498 | lr 7.494e-06 | gnorm 7.37 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 492
2022-01-13 16:12:32 | INFO | fairseq.trainer | begin training epoch 51
2022-01-13 16:12:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:33 | INFO | train_inner | epoch 051:      2 / 50 loss=7.842, ppl=229.4, wps=3624.7, ups=3.05, wpb=1188, bsz=64, num_updates=2500, lr=7.5e-06, gnorm=7.233, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=493
2022-01-13 16:12:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:33 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.631 | ppl 198.27 | wps 27071.8 | wpb 556.6 | bsz 30.3 | num_updates 2500 | best_loss 7.631
2022-01-13 16:12:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2500 updates
2022-01-13 16:12:33 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt
2022-01-13 16:12:36 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt
2022-01-13 16:12:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_51_2500.pt (epoch 51 @ 2500 updates, score 7.631) (writing took 8.39964364701882 seconds)
2022-01-13 16:12:44 | INFO | train_inner | epoch 051:     22 / 50 loss=7.839, ppl=229, wps=2271.9, ups=1.83, wpb=1242.9, bsz=64, num_updates=2520, lr=7.56e-06, gnorm=7.368, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=504
2022-01-13 16:12:45 | INFO | train_inner | epoch 051:     42 / 50 loss=7.727, ppl=211.93, wps=13503.6, ups=10.78, wpb=1252.8, bsz=62.7, num_updates=2540, lr=7.62e-06, gnorm=7.282, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=505
2022-01-13 16:12:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:47 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.639 | ppl 199.31 | wps 27398.3 | wpb 556.6 | bsz 30.3 | num_updates 2548 | best_loss 7.631
2022-01-13 16:12:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2548 updates
2022-01-13 16:12:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:12:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:12:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 51 @ 2548 updates, score 7.639) (writing took 2.8562494858633727 seconds)
2022-01-13 16:12:50 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-13 16:12:50 | INFO | train | epoch 051 | loss 7.765 | ppl 217.47 | wps 3560.3 | ups 2.89 | wpb 1232.9 | bsz 63.5 | num_updates 2548 | lr 7.644e-06 | gnorm 7.384 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 510
2022-01-13 16:12:50 | INFO | fairseq.trainer | begin training epoch 52
2022-01-13 16:12:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:51 | INFO | train_inner | epoch 052:     12 / 50 loss=7.654, ppl=201.47, wps=4490.8, ups=3.82, wpb=1176.3, bsz=62.7, num_updates=2560, lr=7.68e-06, gnorm=7.704, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=511
2022-01-13 16:12:52 | INFO | train_inner | epoch 052:     32 / 50 loss=7.736, ppl=213.22, wps=16047.8, ups=13.47, wpb=1191, bsz=64, num_updates=2580, lr=7.74e-06, gnorm=7.258, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=512
2022-01-13 16:12:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:12:54 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.614 | ppl 195.94 | wps 25819.1 | wpb 556.6 | bsz 30.3 | num_updates 2598 | best_loss 7.614
2022-01-13 16:12:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 2598 updates
2022-01-13 16:12:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:12:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 52 @ 2598 updates, score 7.614) (writing took 4.360743889817968 seconds)
2022-01-13 16:12:59 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-13 16:12:59 | INFO | train | epoch 052 | loss 7.712 | ppl 209.65 | wps 6971.8 | ups 5.65 | wpb 1232.9 | bsz 63.5 | num_updates 2598 | lr 7.794e-06 | gnorm 7.467 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 518
2022-01-13 16:12:59 | INFO | fairseq.trainer | begin training epoch 53
2022-01-13 16:12:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:12:59 | INFO | train_inner | epoch 053:      2 / 50 loss=7.686, ppl=205.88, wps=3851.5, ups=3.03, wpb=1271.5, bsz=64, num_updates=2600, lr=7.8e-06, gnorm=7.638, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=519
2022-01-13 16:13:00 | INFO | train_inner | epoch 053:     22 / 50 loss=7.714, ppl=210.03, wps=17902.4, ups=13.63, wpb=1313.8, bsz=64, num_updates=2620, lr=7.86e-06, gnorm=7.042, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=520
2022-01-13 16:13:02 | INFO | train_inner | epoch 053:     42 / 50 loss=7.551, ppl=187.55, wps=15547.8, ups=13.46, wpb=1154.8, bsz=62.7, num_updates=2640, lr=7.92e-06, gnorm=7.727, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=522
2022-01-13 16:13:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:03 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.473 | ppl 177.71 | wps 27355.1 | wpb 556.6 | bsz 30.3 | num_updates 2648 | best_loss 7.473
2022-01-13 16:13:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 2648 updates
2022-01-13 16:13:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:05 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 2648 updates, score 7.473) (writing took 3.8562265541404486 seconds)
2022-01-13 16:13:07 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-13 16:13:07 | INFO | train | epoch 053 | loss 7.625 | ppl 197.36 | wps 7384.9 | ups 5.99 | wpb 1232.9 | bsz 63.5 | num_updates 2648 | lr 7.944e-06 | gnorm 7.394 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 527
2022-01-13 16:13:07 | INFO | fairseq.trainer | begin training epoch 54
2022-01-13 16:13:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:08 | INFO | train_inner | epoch 054:     12 / 50 loss=7.592, ppl=192.88, wps=3972.9, ups=3.2, wpb=1242.8, bsz=64, num_updates=2660, lr=7.98e-06, gnorm=7.365, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=528
2022-01-13 16:13:09 | INFO | train_inner | epoch 054:     32 / 50 loss=7.59, ppl=192.72, wps=17855.5, ups=14.36, wpb=1243.4, bsz=64, num_updates=2680, lr=8.04e-06, gnorm=7.408, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=529
2022-01-13 16:13:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:11 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.484 | ppl 179 | wps 28413.3 | wpb 556.6 | bsz 30.3 | num_updates 2698 | best_loss 7.473
2022-01-13 16:13:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 2698 updates
2022-01-13 16:13:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:13:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:13:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 54 @ 2698 updates, score 7.484) (writing took 2.5603727689012885 seconds)
2022-01-13 16:13:14 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-13 16:13:14 | INFO | train | epoch 054 | loss 7.553 | ppl 187.85 | wps 8895 | ups 7.21 | wpb 1232.9 | bsz 63.5 | num_updates 2698 | lr 8.094e-06 | gnorm 7.379 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 534
2022-01-13 16:13:14 | INFO | fairseq.trainer | begin training epoch 55
2022-01-13 16:13:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:14 | INFO | train_inner | epoch 055:      2 / 50 loss=7.427, ppl=172.14, wps=4951.5, ups=4.19, wpb=1182.4, bsz=62.7, num_updates=2700, lr=8.1e-06, gnorm=7.373, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=534
2022-01-13 16:13:16 | INFO | train_inner | epoch 055:     22 / 50 loss=7.272, ppl=154.52, wps=14923.3, ups=13.99, wpb=1066.3, bsz=62.7, num_updates=2720, lr=8.16e-06, gnorm=7.8, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=536
2022-01-13 16:13:17 | INFO | train_inner | epoch 055:     42 / 50 loss=7.566, ppl=189.53, wps=17768.6, ups=13.75, wpb=1291.8, bsz=64, num_updates=2740, lr=8.22e-06, gnorm=7.07, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=537
2022-01-13 16:13:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:18 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.407 | ppl 169.77 | wps 26186.7 | wpb 556.6 | bsz 30.3 | num_updates 2748 | best_loss 7.407
2022-01-13 16:13:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 2748 updates
2022-01-13 16:13:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 55 @ 2748 updates, score 7.407) (writing took 4.511528666131198 seconds)
2022-01-13 16:13:23 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-13 16:13:23 | INFO | train | epoch 055 | loss 7.466 | ppl 176.84 | wps 6886.5 | ups 5.59 | wpb 1232.9 | bsz 63.5 | num_updates 2748 | lr 8.244e-06 | gnorm 7.343 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 543
2022-01-13 16:13:23 | INFO | fairseq.trainer | begin training epoch 56
2022-01-13 16:13:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:24 | INFO | train_inner | epoch 056:     12 / 50 loss=7.544, ppl=186.62, wps=3937.4, ups=2.96, wpb=1332, bsz=64, num_updates=2760, lr=8.28e-06, gnorm=7.184, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=544
2022-01-13 16:13:25 | INFO | train_inner | epoch 056:     32 / 50 loss=7.389, ppl=167.56, wps=17904.2, ups=13.8, wpb=1297, bsz=62.7, num_updates=2780, lr=8.34e-06, gnorm=7.493, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=545
2022-01-13 16:13:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:27 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.34 | ppl 162.03 | wps 26327.5 | wpb 556.6 | bsz 30.3 | num_updates 2798 | best_loss 7.34
2022-01-13 16:13:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 2798 updates
2022-01-13 16:13:27 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 2798 updates, score 7.34) (writing took 4.353646798990667 seconds)
2022-01-13 16:13:32 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-13 16:13:32 | INFO | train | epoch 056 | loss 7.451 | ppl 174.93 | wps 6955.9 | ups 5.64 | wpb 1232.9 | bsz 63.5 | num_updates 2798 | lr 8.394e-06 | gnorm 7.433 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 552
2022-01-13 16:13:32 | INFO | fairseq.trainer | begin training epoch 57
2022-01-13 16:13:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:32 | INFO | train_inner | epoch 057:      2 / 50 loss=7.527, ppl=184.47, wps=3423.8, ups=2.85, wpb=1200.4, bsz=64, num_updates=2800, lr=8.4e-06, gnorm=7.354, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=552
2022-01-13 16:13:34 | INFO | train_inner | epoch 057:     22 / 50 loss=7.296, ppl=157.17, wps=18587, ups=14.25, wpb=1304.7, bsz=64, num_updates=2820, lr=8.46e-06, gnorm=7.394, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=554
2022-01-13 16:13:35 | INFO | train_inner | epoch 057:     42 / 50 loss=7.427, ppl=172.06, wps=16458.7, ups=14.04, wpb=1172.3, bsz=62.7, num_updates=2840, lr=8.52e-06, gnorm=7.617, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=555
2022-01-13 16:13:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:37 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.28 | ppl 155.41 | wps 27213.2 | wpb 556.6 | bsz 30.3 | num_updates 2848 | best_loss 7.28
2022-01-13 16:13:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 2848 updates
2022-01-13 16:13:37 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:39 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 57 @ 2848 updates, score 7.28) (writing took 4.000386480009183 seconds)
2022-01-13 16:13:41 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-13 16:13:41 | INFO | train | epoch 057 | loss 7.388 | ppl 167.51 | wps 7241.4 | ups 5.87 | wpb 1232.9 | bsz 63.5 | num_updates 2848 | lr 8.544e-06 | gnorm 7.517 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 561
2022-01-13 16:13:41 | INFO | fairseq.trainer | begin training epoch 58
2022-01-13 16:13:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:42 | INFO | train_inner | epoch 058:     12 / 50 loss=7.412, ppl=170.29, wps=3865, ups=3.16, wpb=1224.3, bsz=64, num_updates=2860, lr=8.58e-06, gnorm=7.526, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=562
2022-01-13 16:13:43 | INFO | train_inner | epoch 058:     32 / 50 loss=7.298, ppl=157.41, wps=16586.6, ups=14.18, wpb=1169.8, bsz=62.7, num_updates=2880, lr=8.64e-06, gnorm=7.549, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=563
2022-01-13 16:13:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:45 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.129 | ppl 140.02 | wps 24879.7 | wpb 556.6 | bsz 30.3 | num_updates 2898 | best_loss 7.129
2022-01-13 16:13:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 2898 updates
2022-01-13 16:13:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:47 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 58 @ 2898 updates, score 7.129) (writing took 3.902147063985467 seconds)
2022-01-13 16:13:49 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-13 16:13:49 | INFO | train | epoch 058 | loss 7.33 | ppl 160.9 | wps 7347.2 | ups 5.96 | wpb 1232.9 | bsz 63.5 | num_updates 2898 | lr 8.694e-06 | gnorm 7.467 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 569
2022-01-13 16:13:49 | INFO | fairseq.trainer | begin training epoch 59
2022-01-13 16:13:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:49 | INFO | train_inner | epoch 059:      2 / 50 loss=7.333, ppl=161.19, wps=4078.2, ups=3.21, wpb=1271, bsz=62.7, num_updates=2900, lr=8.7e-06, gnorm=7.482, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=569
2022-01-13 16:13:51 | INFO | train_inner | epoch 059:     22 / 50 loss=7.177, ppl=144.74, wps=17024.7, ups=14.03, wpb=1213.8, bsz=64, num_updates=2920, lr=8.76e-06, gnorm=7.498, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=571
2022-01-13 16:13:52 | INFO | train_inner | epoch 059:     42 / 50 loss=7.246, ppl=151.84, wps=16724.5, ups=13.88, wpb=1205.3, bsz=64, num_updates=2940, lr=8.82e-06, gnorm=7.388, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=572
2022-01-13 16:13:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:13:53 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.105 | ppl 137.7 | wps 25309.5 | wpb 556.6 | bsz 30.3 | num_updates 2948 | best_loss 7.105
2022-01-13 16:13:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 2948 updates
2022-01-13 16:13:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:13:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 59 @ 2948 updates, score 7.105) (writing took 3.966971711954102 seconds)
2022-01-13 16:13:57 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-13 16:13:57 | INFO | train | epoch 059 | loss 7.241 | ppl 151.23 | wps 7308.2 | ups 5.93 | wpb 1232.9 | bsz 63.5 | num_updates 2948 | lr 8.844e-06 | gnorm 7.427 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 577
2022-01-13 16:13:57 | INFO | fairseq.trainer | begin training epoch 60
2022-01-13 16:13:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:13:58 | INFO | train_inner | epoch 060:     12 / 50 loss=7.267, ppl=154, wps=4317.4, ups=3.15, wpb=1369, bsz=64, num_updates=2960, lr=8.88e-06, gnorm=7.296, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=578
2022-01-13 16:14:00 | INFO | train_inner | epoch 060:     32 / 50 loss=7.274, ppl=154.81, wps=17903, ups=13.97, wpb=1281.2, bsz=62.7, num_updates=2980, lr=8.94e-06, gnorm=7.473, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=580
2022-01-13 16:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:02 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.103 | ppl 137.51 | wps 25294.3 | wpb 556.6 | bsz 30.3 | num_updates 2998 | best_loss 7.103
2022-01-13 16:14:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 2998 updates
2022-01-13 16:14:02 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 60 @ 2998 updates, score 7.103) (writing took 4.383154954062775 seconds)
2022-01-13 16:14:06 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-13 16:14:06 | INFO | train | epoch 060 | loss 7.186 | ppl 145.57 | wps 6911.9 | ups 5.61 | wpb 1232.9 | bsz 63.5 | num_updates 2998 | lr 8.994e-06 | gnorm 7.489 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 586
2022-01-13 16:14:06 | INFO | fairseq.trainer | begin training epoch 61
2022-01-13 16:14:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:07 | INFO | train_inner | epoch 061:      2 / 50 loss=7.114, ppl=138.56, wps=3206.2, ups=2.95, wpb=1087.5, bsz=64, num_updates=3000, lr=9e-06, gnorm=7.546, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=587
2022-01-13 16:14:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:07 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.155 | ppl 142.48 | wps 28446.7 | wpb 556.6 | bsz 30.3 | num_updates 3000 | best_loss 7.103
2022-01-13 16:14:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3000 updates
2022-01-13 16:14:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt
2022-01-13 16:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt
2022-01-13 16:14:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_61_3000.pt (epoch 61 @ 3000 updates, score 7.155) (writing took 4.007071870146319 seconds)
2022-01-13 16:14:13 | INFO | train_inner | epoch 061:     22 / 50 loss=7.134, ppl=140.5, wps=3663, ups=3.1, wpb=1181.2, bsz=62.7, num_updates=3020, lr=9.06e-06, gnorm=7.758, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=593
2022-01-13 16:14:15 | INFO | train_inner | epoch 061:     42 / 50 loss=7.174, ppl=144.38, wps=15567.3, ups=12.1, wpb=1286.7, bsz=64, num_updates=3040, lr=9.12e-06, gnorm=7.543, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=595
2022-01-13 16:14:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:16 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.13 | ppl 140.04 | wps 26763.2 | wpb 556.6 | bsz 30.3 | num_updates 3048 | best_loss 7.103
2022-01-13 16:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3048 updates
2022-01-13 16:14:16 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:14:19 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:14:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 61 @ 3048 updates, score 7.13) (writing took 2.425368594005704 seconds)
2022-01-13 16:14:19 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-13 16:14:19 | INFO | train | epoch 061 | loss 7.147 | ppl 141.77 | wps 5032.4 | ups 4.08 | wpb 1232.9 | bsz 63.5 | num_updates 3048 | lr 9.144e-06 | gnorm 7.58 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 599
2022-01-13 16:14:19 | INFO | fairseq.trainer | begin training epoch 62
2022-01-13 16:14:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:20 | INFO | train_inner | epoch 062:     12 / 50 loss=7.211, ppl=148.12, wps=5454.1, ups=4.15, wpb=1314.8, bsz=64, num_updates=3060, lr=9.18e-06, gnorm=7.136, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=600
2022-01-13 16:14:21 | INFO | train_inner | epoch 062:     32 / 50 loss=6.871, ppl=117.06, wps=15097, ups=13.84, wpb=1090.7, bsz=64, num_updates=3080, lr=9.24e-06, gnorm=7.801, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=601
2022-01-13 16:14:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:23 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 7.051 | ppl 132.58 | wps 27969 | wpb 556.6 | bsz 30.3 | num_updates 3098 | best_loss 7.051
2022-01-13 16:14:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 3098 updates
2022-01-13 16:14:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 62 @ 3098 updates, score 7.051) (writing took 3.8703499040566385 seconds)
2022-01-13 16:14:27 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-13 16:14:27 | INFO | train | epoch 062 | loss 7.091 | ppl 136.32 | wps 7467 | ups 6.06 | wpb 1232.9 | bsz 63.5 | num_updates 3098 | lr 9.294e-06 | gnorm 7.458 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 607
2022-01-13 16:14:27 | INFO | fairseq.trainer | begin training epoch 63
2022-01-13 16:14:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:27 | INFO | train_inner | epoch 063:      2 / 50 loss=7.208, ppl=147.8, wps=4457.8, ups=3.28, wpb=1357.5, bsz=62.7, num_updates=3100, lr=9.3e-06, gnorm=7.551, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=607
2022-01-13 16:14:29 | INFO | train_inner | epoch 063:     22 / 50 loss=7.018, ppl=129.61, wps=17364, ups=13.71, wpb=1267, bsz=64, num_updates=3120, lr=9.36e-06, gnorm=7.317, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=609
2022-01-13 16:14:30 | INFO | train_inner | epoch 063:     42 / 50 loss=7, ppl=127.96, wps=16197.2, ups=13.93, wpb=1162.7, bsz=62.7, num_updates=3140, lr=9.42e-06, gnorm=7.997, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=610
2022-01-13 16:14:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:31 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.924 | ppl 121.46 | wps 26643.7 | wpb 556.6 | bsz 30.3 | num_updates 3148 | best_loss 6.924
2022-01-13 16:14:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 3148 updates
2022-01-13 16:14:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:34 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 63 @ 3148 updates, score 6.924) (writing took 5.0752614790108055 seconds)
2022-01-13 16:14:37 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-13 16:14:37 | INFO | train | epoch 063 | loss 7.02 | ppl 129.79 | wps 6412.6 | ups 5.2 | wpb 1232.9 | bsz 63.5 | num_updates 3148 | lr 9.444e-06 | gnorm 7.741 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 616
2022-01-13 16:14:37 | INFO | fairseq.trainer | begin training epoch 64
2022-01-13 16:14:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:38 | INFO | train_inner | epoch 064:     12 / 50 loss=6.956, ppl=124.17, wps=3181.1, ups=2.68, wpb=1186.6, bsz=64, num_updates=3160, lr=9.48e-06, gnorm=7.598, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=617
2022-01-13 16:14:39 | INFO | train_inner | epoch 064:     32 / 50 loss=7.119, ppl=138.97, wps=17852.6, ups=14.26, wpb=1252.2, bsz=64, num_updates=3180, lr=9.54e-06, gnorm=7.707, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=619
2022-01-13 16:14:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:41 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.933 | ppl 122.18 | wps 27222.8 | wpb 556.6 | bsz 30.3 | num_updates 3198 | best_loss 6.924
2022-01-13 16:14:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 3198 updates
2022-01-13 16:14:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:14:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:14:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 64 @ 3198 updates, score 6.933) (writing took 2.620714506134391 seconds)
2022-01-13 16:14:44 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-13 16:14:44 | INFO | train | epoch 064 | loss 6.989 | ppl 127 | wps 8742.7 | ups 7.09 | wpb 1232.9 | bsz 63.5 | num_updates 3198 | lr 9.594e-06 | gnorm 7.571 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 623
2022-01-13 16:14:44 | INFO | fairseq.trainer | begin training epoch 65
2022-01-13 16:14:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:44 | INFO | train_inner | epoch 065:      2 / 50 loss=6.893, ppl=118.87, wps=5045.2, ups=4.09, wpb=1234.3, bsz=62.7, num_updates=3200, lr=9.6e-06, gnorm=7.485, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=624
2022-01-13 16:14:45 | INFO | train_inner | epoch 065:     22 / 50 loss=6.801, ppl=111.49, wps=17900.8, ups=13.97, wpb=1281.6, bsz=64, num_updates=3220, lr=9.66e-06, gnorm=7.353, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=625
2022-01-13 16:14:47 | INFO | train_inner | epoch 065:     42 / 50 loss=6.883, ppl=118.04, wps=16901.7, ups=13.51, wpb=1250.8, bsz=64, num_updates=3240, lr=9.72e-06, gnorm=7.533, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=627
2022-01-13 16:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:48 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.826 | ppl 113.47 | wps 26266.4 | wpb 556.6 | bsz 30.3 | num_updates 3248 | best_loss 6.826
2022-01-13 16:14:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 3248 updates
2022-01-13 16:14:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 65 @ 3248 updates, score 6.826) (writing took 4.1442303659860045 seconds)
2022-01-13 16:14:52 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-01-13 16:14:52 | INFO | train | epoch 065 | loss 6.89 | ppl 118.57 | wps 7175.4 | ups 5.82 | wpb 1232.9 | bsz 63.5 | num_updates 3248 | lr 9.744e-06 | gnorm 7.496 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 632
2022-01-13 16:14:52 | INFO | fairseq.trainer | begin training epoch 66
2022-01-13 16:14:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:14:53 | INFO | train_inner | epoch 066:     12 / 50 loss=6.948, ppl=123.49, wps=3881, ups=3.13, wpb=1240.5, bsz=62.7, num_updates=3260, lr=9.78e-06, gnorm=7.594, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=633
2022-01-13 16:14:55 | INFO | train_inner | epoch 066:     32 / 50 loss=6.802, ppl=111.62, wps=18003.3, ups=13.99, wpb=1286.8, bsz=64, num_updates=3280, lr=9.84e-06, gnorm=7.914, clip=100, loss_scale=32, train_wall=1, gb_free=20.8, wall=634
2022-01-13 16:14:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:14:57 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.769 | ppl 109.03 | wps 25176.8 | wpb 556.6 | bsz 30.3 | num_updates 3298 | best_loss 6.769
2022-01-13 16:14:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 3298 updates
2022-01-13 16:14:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:15:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 66 @ 3298 updates, score 6.769) (writing took 3.8774605200160295 seconds)
2022-01-13 16:15:00 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-01-13 16:15:00 | INFO | train | epoch 066 | loss 6.885 | ppl 118.16 | wps 7420.2 | ups 6.02 | wpb 1232.9 | bsz 63.5 | num_updates 3298 | lr 9.894e-06 | gnorm 7.755 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 640
2022-01-13 16:15:01 | INFO | fairseq.trainer | begin training epoch 67
2022-01-13 16:15:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:15:01 | INFO | train_inner | epoch 067:      2 / 50 loss=6.981, ppl=126.33, wps=3786.6, ups=3.25, wpb=1165.1, bsz=62.7, num_updates=3300, lr=9.9e-06, gnorm=7.689, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=641
2022-01-13 16:15:02 | INFO | train_inner | epoch 067:     22 / 50 loss=6.867, ppl=116.77, wps=17056.3, ups=13.66, wpb=1248.2, bsz=64, num_updates=3320, lr=9.96e-06, gnorm=7.413, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=642
2022-01-13 16:15:04 | INFO | train_inner | epoch 067:     42 / 50 loss=6.827, ppl=113.54, wps=16466.4, ups=14.19, wpb=1160.2, bsz=62.7, num_updates=3340, lr=1.002e-05, gnorm=7.797, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=644
2022-01-13 16:15:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:05 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.801 | ppl 111.5 | wps 26137.7 | wpb 556.6 | bsz 30.3 | num_updates 3348 | best_loss 6.769
2022-01-13 16:15:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 3348 updates
2022-01-13 16:15:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 67 @ 3348 updates, score 6.801) (writing took 2.5771543020382524 seconds)
2022-01-13 16:15:07 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-01-13 16:15:07 | INFO | train | epoch 067 | loss 6.809 | ppl 112.15 | wps 8808.9 | ups 7.15 | wpb 1232.9 | bsz 63.5 | num_updates 3348 | lr 1.0044e-05 | gnorm 7.633 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 647
2022-01-13 16:15:08 | INFO | fairseq.trainer | begin training epoch 68
2022-01-13 16:15:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:15:08 | INFO | train_inner | epoch 068:     12 / 50 loss=6.672, ppl=101.97, wps=5768, ups=4.13, wpb=1396.4, bsz=64, num_updates=3360, lr=1.008e-05, gnorm=7.473, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=648
2022-01-13 16:15:10 | INFO | train_inner | epoch 068:     32 / 50 loss=6.765, ppl=108.76, wps=16973.8, ups=13.6, wpb=1247.9, bsz=62.7, num_updates=3380, lr=1.014e-05, gnorm=7.725, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=650
2022-01-13 16:15:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:12 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.755 | ppl 108 | wps 26198.4 | wpb 556.6 | bsz 30.3 | num_updates 3398 | best_loss 6.755
2022-01-13 16:15:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 3398 updates
2022-01-13 16:15:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:15:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:15:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 68 @ 3398 updates, score 6.755) (writing took 3.872889523860067 seconds)
2022-01-13 16:15:16 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-01-13 16:15:16 | INFO | train | epoch 068 | loss 6.74 | ppl 106.92 | wps 7393.7 | ups 6 | wpb 1232.9 | bsz 63.5 | num_updates 3398 | lr 1.0194e-05 | gnorm 7.578 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 656
2022-01-13 16:15:16 | INFO | fairseq.trainer | begin training epoch 69
2022-01-13 16:15:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:15:16 | INFO | train_inner | epoch 069:      2 / 50 loss=6.698, ppl=103.81, wps=3486.5, ups=3.24, wpb=1076.8, bsz=64, num_updates=3400, lr=1.02e-05, gnorm=7.644, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=656
2022-01-13 16:15:18 | INFO | train_inner | epoch 069:     22 / 50 loss=6.801, ppl=111.49, wps=17688.4, ups=12.84, wpb=1377.8, bsz=64, num_updates=3420, lr=1.026e-05, gnorm=7.17, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=658
2022-01-13 16:15:19 | INFO | train_inner | epoch 069:     42 / 50 loss=6.594, ppl=96.63, wps=16251.7, ups=13.88, wpb=1170.5, bsz=62.7, num_updates=3440, lr=1.032e-05, gnorm=7.768, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=659
2022-01-13 16:15:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:21 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.625 | ppl 98.73 | wps 26391.5 | wpb 556.6 | bsz 30.3 | num_updates 3448 | best_loss 6.625
2022-01-13 16:15:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 3448 updates
2022-01-13 16:15:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:15:23 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-01-13 16:15:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 69 @ 3448 updates, score 6.625) (writing took 3.9896547191310674 seconds)
2022-01-13 16:15:25 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-01-13 16:15:25 | INFO | train | epoch 069 | loss 6.675 | ppl 102.22 | wps 7095.6 | ups 5.76 | wpb 1232.9 | bsz 63.5 | num_updates 3448 | lr 1.0344e-05 | gnorm 7.563 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 664
2022-01-13 16:15:25 | INFO | fairseq.trainer | begin training epoch 70
2022-01-13 16:15:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:15:26 | INFO | train_inner | epoch 070:     12 / 50 loss=6.707, ppl=104.47, wps=3812.4, ups=3.12, wpb=1221.6, bsz=62.7, num_updates=3460, lr=1.038e-05, gnorm=7.845, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=665
2022-01-13 16:15:27 | INFO | train_inner | epoch 070:     32 / 50 loss=6.524, ppl=92.03, wps=16901.9, ups=14.05, wpb=1203.3, bsz=64, num_updates=3480, lr=1.044e-05, gnorm=7.546, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=667
2022-01-13 16:15:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:29 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.717 | ppl 105.22 | wps 27867.3 | wpb 556.6 | bsz 30.3 | num_updates 3498 | best_loss 6.625
2022-01-13 16:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 3498 updates
2022-01-13 16:15:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 70 @ 3498 updates, score 6.717) (writing took 2.5344681250862777 seconds)
2022-01-13 16:15:32 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-01-13 16:15:32 | INFO | train | epoch 070 | loss 6.609 | ppl 97.6 | wps 8852.7 | ups 7.18 | wpb 1232.9 | bsz 63.5 | num_updates 3498 | lr 1.0494e-05 | gnorm 7.512 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 671
2022-01-13 16:15:32 | INFO | fairseq.trainer | begin training epoch 71
2022-01-13 16:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-13 16:15:32 | INFO | train_inner | epoch 071:      2 / 50 loss=6.685, ppl=102.91, wps=5199.8, ups=4.16, wpb=1248.8, bsz=64, num_updates=3500, lr=1.05e-05, gnorm=7.301, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=672
2022-01-13 16:15:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:32 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.722 | ppl 105.6 | wps 27954.9 | wpb 556.6 | bsz 30.3 | num_updates 3500 | best_loss 6.625
2022-01-13 16:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3500 updates
2022-01-13 16:15:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt
2022-01-13 16:15:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt
2022-01-13 16:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_71_3500.pt (epoch 71 @ 3500 updates, score 6.722) (writing took 4.007647620048374 seconds)
2022-01-13 16:15:38 | INFO | train_inner | epoch 071:     22 / 50 loss=6.547, ppl=93.53, wps=3558, ups=3.04, wpb=1169.3, bsz=64, num_updates=3520, lr=1.056e-05, gnorm=7.828, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=678
2022-01-13 16:15:40 | INFO | train_inner | epoch 071:     42 / 50 loss=6.582, ppl=95.81, wps=16583.3, ups=13.7, wpb=1210.1, bsz=64, num_updates=3540, lr=1.062e-05, gnorm=7.678, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=680
2022-01-13 16:15:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-13 16:15:41 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.665 | ppl 101.48 | wps 27130 | wpb 556.6 | bsz 30.3 | num_updates 3548 | best_loss 6.625
2022-01-13 16:15:41 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-01-13 16:15:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3548 updates
2022-01-13 16:15:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:44 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-01-13 16:15:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 71 @ 3548 updates, score 6.665) (writing took 2.46215064288117 seconds)
2022-01-13 16:15:44 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-01-13 16:15:44 | INFO | train | epoch 071 | loss 6.616 | ppl 98.1 | wps 5065 | ups 4.11 | wpb 1232.9 | bsz 63.5 | num_updates 3548 | lr 1.0644e-05 | gnorm 7.696 | clip 100 | loss_scale 32 | train_wall 4 | gb_free 20.9 | wall 684
2022-01-13 16:15:44 | INFO | fairseq_cli.train | done training in 679.1 seconds
2022-02-25 16:31:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-25 16:31:30 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-25 16:31:32 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-25 16:31:32 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-25 16:31:32 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-25 16:31:32 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-25 16:31:32 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-25 16:31:32 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-25 16:32:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-25 16:32:28 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-25 16:32:31 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-25 16:32:31 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-25 16:32:31 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-25 16:32:31 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-25 16:32:31 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-25 16:32:31 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-26 17:18:50 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-26 17:18:50 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-26 17:18:52 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-26 17:18:52 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-26 17:18:52 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-26 17:18:52 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-26 17:18:52 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-26 17:18:52 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-26 17:19:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-26 17:19:30 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-26 17:19:33 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-26 17:19:33 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-26 17:19:33 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-26 17:19:33 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-26 17:19:33 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-26 17:19:33 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
mmmm
2022-02-26 19:08:08 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/home/drrndrrnprn/nlp/ABST/bartabst/', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'bartabst/checkpoints/bart.mlm/dev', 'restore_file': 'bartabst/checkpoints/bart.base/model.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_abst', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_abst', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', data_buffer_size=10, dataset_impl=None, dataset_implem='raw', ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layers=6, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0.0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gpt2_encoder_json='dummy', gpt2_vocab_bpe='dummy', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', mask_multiple_length=1, mask_prob=0.0, mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=1, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.0, relu_dropout=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='bartabst/checkpoints/bart.base/model.pt', sample_break_mode='none', save_dir='bartabst/checkpoints/bart.mlm/dev', save_interval=1, save_interval_updates=500, scoring='bleu', seed=222, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='bart_e_mlm', tensorboard_logdir='/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/tensorboard', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=1024, total_num_update='40000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_freq=[2], use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/home/drrndrrnprn/nlp/ABST/bartabst/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_epoch=10, warmup_updates=10000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'bart_e_mlm', 'data': '/home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw', 'sample_break_mode': 'none', 'tokens_per_sample': 1024, 'mask_prob': 0.0, 'leave_unmasked_prob': 0.0, 'random_token_prob': 0.0, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'warmup_epoch': 10, 'shorten_method': 'none', 'shorten_data_split_list': '', 'dataset_implem': 'raw', 'gpt2_encoder_json': 'dummy', 'gpt2_vocab_bpe': 'dummy', 'seed': 222}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 10000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-26 19:08:08 | INFO | bartabst.tasks.bart_e_mlm | dictionary: 51200 types
2022-02-26 19:08:11 | INFO | fairseq_cli.train | BARTMLModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51205, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51205, bias=False)
  )
  (classification_heads): ModuleDict()
  (lm_head): BARTEncoderLMHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2022-02-26 19:08:11 | INFO | fairseq_cli.train | task: BARTEncoderMLMTask
2022-02-26 19:08:11 | INFO | fairseq_cli.train | model: BARTMLModel
2022-02-26 19:08:11 | INFO | fairseq_cli.train | criterion: MaskedLmLoss
2022-02-26 19:08:11 | INFO | fairseq_cli.train | num. shared model params: 140,785,669 (num. trained: 140,785,669)
2022-02-26 19:08:11 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
no aos file, no transfer aos used
2022-02-26 19:08:11 | INFO | bartabst.data.data_utils | loaded 598 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/valid
2022-02-26 19:08:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-02-26 19:08:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-26 19:08:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- lm_head.weight
2022-02-26 19:08:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-26 19:08:16 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 24.000 GB ; name = NVIDIA GeForce RTX 3090                 
2022-02-26 19:08:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-26 19:08:16 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-26 19:08:16 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = 32
2022-02-26 19:08:16 | INFO | fairseq.trainer | Preparing to load checkpoint bartabst/checkpoints/bart.base/model.pt
2022-02-26 19:08:19 | INFO | bartabst.models.model | Adding extra mask tokens embeddings not found in pretrained model for continued pretraining of BARTMLModel with extra mask tokens.
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.weight
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.bias
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.dense.weight
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.dense.bias
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.weight
2022-02-26 19:08:19 | INFO | bartabst.models.model | Overwriting lm_head.layer_norm.bias
2022-02-26 19:08:20 | INFO | fairseq.trainer | Loaded checkpoint bartabst/checkpoints/bart.base/model.pt (epoch 14 @ 0 updates)
2022-02-26 19:08:20 | INFO | fairseq.trainer | loading train data for epoch 1
no aos file, no transfer aos used
2022-02-26 19:08:21 | INFO | bartabst.data.data_utils | loaded 1,910 examples from: /home/drrndrrnprn/nlp/ABST/datasets/semeval-pengb/analyzed/data-raw/train
2022-02-26 19:08:21 | INFO | fairseq.trainer | begin training epoch 1
2022-02-26 19:08:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:08:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-26 19:08:23 | INFO | train_inner | epoch 001:     21 / 31 loss=17.267, ppl=157724, wps=16734.4, ups=13.81, wpb=1226.7, bsz=63.2, num_updates=20, lr=6e-08, gnorm=23.218, clip=100, loss_scale=64, train_wall=2, gb_free=20.9, wall=7
2022-02-26 19:08:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:08:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.127 | ppl 143116 | wps 27660.5 | wpb 591.2 | bsz 29.9 | num_updates 30
2022-02-26 19:08:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 30 updates
2022-02-26 19:08:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 1 @ 30 updates, score 17.127) (writing took 11.127412697998807 seconds)
2022-02-26 19:08:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-26 19:08:36 | INFO | train | epoch 001 | loss 17.3 | ppl 161327 | wps 2577.3 | ups 2.1 | wpb 1238.7 | bsz 61.5 | num_updates 30 | lr 9e-08 | gnorm 23.531 | clip 100 | loss_scale 64 | train_wall 3 | gb_free 20.9 | wall 20
2022-02-26 19:08:36 | INFO | fairseq.trainer | begin training epoch 2
2022-02-26 19:08:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:08:36 | INFO | train_inner | epoch 002:     10 / 31 loss=17.275, ppl=158579, wps=1949.4, ups=1.51, wpb=1290.9, bsz=59.8, num_updates=40, lr=1.2e-07, gnorm=23.583, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=20
2022-02-26 19:08:38 | INFO | train_inner | epoch 002:     30 / 31 loss=17.298, ppl=161138, wps=18669.7, ups=15.35, wpb=1216.2, bsz=63.2, num_updates=60, lr=1.8e-07, gnorm=23.346, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=22
2022-02-26 19:08:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:08:38 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 17.054 | ppl 136065 | wps 31990.7 | wpb 591.2 | bsz 29.9 | num_updates 61 | best_loss 17.054
2022-02-26 19:08:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 61 updates
2022-02-26 19:08:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 2 @ 61 updates, score 17.054) (writing took 6.530774657992879 seconds)
2022-02-26 19:08:45 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-26 19:08:45 | INFO | train | epoch 002 | loss 17.254 | ppl 156312 | wps 4169.8 | ups 3.4 | wpb 1227.6 | bsz 61.6 | num_updates 61 | lr 1.83e-07 | gnorm 23.324 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 29
2022-02-26 19:08:45 | INFO | fairseq.trainer | begin training epoch 3
2022-02-26 19:08:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:08:46 | INFO | train_inner | epoch 003:     19 / 31 loss=17.196, ppl=150139, wps=2790.8, ups=2.38, wpb=1173.7, bsz=60.3, num_updates=80, lr=2.4e-07, gnorm=23.684, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=30
2022-02-26 19:08:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:08:47 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.898 | ppl 122164 | wps 30591.3 | wpb 591.2 | bsz 29.9 | num_updates 92 | best_loss 16.898
2022-02-26 19:08:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 92 updates
2022-02-26 19:08:47 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:08:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 3 @ 92 updates, score 16.898) (writing took 6.711596481996821 seconds)
2022-02-26 19:08:54 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-26 19:08:54 | INFO | train | epoch 003 | loss 17.118 | ppl 142264 | wps 4057.4 | ups 3.3 | wpb 1227.6 | bsz 61.6 | num_updates 92 | lr 2.76e-07 | gnorm 23.04 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 38
2022-02-26 19:08:54 | INFO | fairseq.trainer | begin training epoch 4
2022-02-26 19:08:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:08:55 | INFO | train_inner | epoch 004:      8 / 31 loss=16.977, ppl=129002, wps=2733.6, ups=2.28, wpb=1196.8, bsz=62.4, num_updates=100, lr=3e-07, gnorm=22.425, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=39
2022-02-26 19:08:56 | INFO | train_inner | epoch 004:     28 / 31 loss=16.92, ppl=124037, wps=20762.6, ups=15.27, wpb=1359.5, bsz=61.9, num_updates=120, lr=3.6e-07, gnorm=22.225, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=40
2022-02-26 19:08:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:08:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 16.547 | ppl 95749.3 | wps 31889.3 | wpb 591.2 | bsz 29.9 | num_updates 123 | best_loss 16.547
2022-02-26 19:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 123 updates
2022-02-26 19:08:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:00 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 4 @ 123 updates, score 16.547) (writing took 5.007286517997272 seconds)
2022-02-26 19:09:02 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-26 19:09:02 | INFO | train | epoch 004 | loss 16.923 | ppl 124269 | wps 4930.9 | ups 4.02 | wpb 1227.6 | bsz 61.6 | num_updates 123 | lr 3.69e-07 | gnorm 22.883 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 46
2022-02-26 19:09:02 | INFO | fairseq.trainer | begin training epoch 5
2022-02-26 19:09:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:03 | INFO | train_inner | epoch 005:     17 / 31 loss=16.68, ppl=105006, wps=3596.8, ups=2.89, wpb=1244, bsz=62.4, num_updates=140, lr=4.2e-07, gnorm=22.544, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=47
2022-02-26 19:09:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:04 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 16.256 | ppl 78268.7 | wps 31397.1 | wpb 591.2 | bsz 29.9 | num_updates 154 | best_loss 16.256
2022-02-26 19:09:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 154 updates
2022-02-26 19:09:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 5 @ 154 updates, score 16.256) (writing took 4.709121777996188 seconds)
2022-02-26 19:09:09 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-26 19:09:09 | INFO | train | epoch 005 | loss 16.632 | ppl 101583 | wps 5241.2 | ups 4.27 | wpb 1227.6 | bsz 61.6 | num_updates 154 | lr 4.62e-07 | gnorm 22.153 | clip 100 | loss_scale 64 | train_wall 2 | gb_free 20.9 | wall 53
2022-02-26 19:09:09 | INFO | fairseq.trainer | begin training epoch 6
2022-02-26 19:09:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:10 | INFO | train_inner | epoch 006:      6 / 31 loss=16.541, ppl=95381.6, wps=3624.2, ups=3, wpb=1207.8, bsz=59.5, num_updates=160, lr=4.8e-07, gnorm=22.416, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=54
2022-02-26 19:09:11 | INFO | train_inner | epoch 006:     26 / 31 loss=16.35, ppl=83504.3, wps=17172.5, ups=14.57, wpb=1178.5, bsz=62.7, num_updates=180, lr=5.4e-07, gnorm=21.698, clip=100, loss_scale=64, train_wall=1, gb_free=20.9, wall=55
2022-02-26 19:09:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-26 19:09:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:12 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 16.015 | ppl 66201.8 | wps 31495.4 | wpb 591.2 | bsz 29.9 | num_updates 184 | best_loss 16.015
2022-02-26 19:09:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 184 updates
2022-02-26 19:09:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 6 @ 184 updates, score 16.015) (writing took 3.9126116810075473 seconds)
2022-02-26 19:09:16 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-26 19:09:16 | INFO | train | epoch 006 | loss 16.342 | ppl 83077.8 | wps 5605.1 | ups 4.5 | wpb 1246.2 | bsz 61.5 | num_updates 184 | lr 5.52e-07 | gnorm 21.84 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 60
2022-02-26 19:09:16 | INFO | fairseq.trainer | begin training epoch 7
2022-02-26 19:09:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:17 | INFO | train_inner | epoch 007:     16 / 31 loss=16.094, ppl=69966.1, wps=4486.3, ups=3.38, wpb=1328.3, bsz=61.6, num_updates=200, lr=6e-07, gnorm=21.287, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=61
2022-02-26 19:09:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:18 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 15.6 | ppl 49658.7 | wps 31329.4 | wpb 591.2 | bsz 29.9 | num_updates 215 | best_loss 15.6
2022-02-26 19:09:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 215 updates
2022-02-26 19:09:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 7 @ 215 updates, score 15.6) (writing took 3.949698372001876 seconds)
2022-02-26 19:09:22 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-26 19:09:22 | INFO | train | epoch 007 | loss 15.998 | ppl 65435.6 | wps 5724.5 | ups 4.66 | wpb 1227.6 | bsz 61.6 | num_updates 215 | lr 6.45e-07 | gnorm 20.749 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 67
2022-02-26 19:09:22 | INFO | fairseq.trainer | begin training epoch 8
2022-02-26 19:09:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:23 | INFO | train_inner | epoch 008:      5 / 31 loss=15.897, ppl=61030, wps=3694, ups=3.41, wpb=1083.2, bsz=61.1, num_updates=220, lr=6.6e-07, gnorm=20.573, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=67
2022-02-26 19:09:24 | INFO | train_inner | epoch 008:     25 / 31 loss=15.657, ppl=51683.7, wps=20923.1, ups=15.46, wpb=1353.2, bsz=63.2, num_updates=240, lr=7.2e-07, gnorm=19.099, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=68
2022-02-26 19:09:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:25 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 15.2 | ppl 37630.2 | wps 31294.9 | wpb 591.2 | bsz 29.9 | num_updates 246 | best_loss 15.2
2022-02-26 19:09:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 246 updates
2022-02-26 19:09:25 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:28 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 8 @ 246 updates, score 15.2) (writing took 4.546259619994089 seconds)
2022-02-26 19:09:30 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-26 19:09:30 | INFO | train | epoch 008 | loss 15.657 | ppl 51682.3 | wps 5323.9 | ups 4.34 | wpb 1227.6 | bsz 61.6 | num_updates 246 | lr 7.38e-07 | gnorm 20.119 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 74
2022-02-26 19:09:30 | INFO | fairseq.trainer | begin training epoch 9
2022-02-26 19:09:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:31 | INFO | train_inner | epoch 009:     14 / 31 loss=15.318, ppl=40853.8, wps=3772.8, ups=3.1, wpb=1215.7, bsz=59.8, num_updates=260, lr=7.8e-07, gnorm=20.633, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=75
2022-02-26 19:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:32 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 14.784 | ppl 28201.9 | wps 30723.6 | wpb 591.2 | bsz 29.9 | num_updates 277 | best_loss 14.784
2022-02-26 19:09:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 277 updates
2022-02-26 19:09:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 9 @ 277 updates, score 14.784) (writing took 3.9165174679947086 seconds)
2022-02-26 19:09:36 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-26 19:09:36 | INFO | train | epoch 009 | loss 15.208 | ppl 37842 | wps 5834.8 | ups 4.75 | wpb 1227.6 | bsz 61.6 | num_updates 277 | lr 8.31e-07 | gnorm 19.682 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 80
2022-02-26 19:09:36 | INFO | fairseq.trainer | begin training epoch 10
2022-02-26 19:09:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:36 | INFO | train_inner | epoch 010:      3 / 31 loss=15.111, ppl=35396.6, wps=3779.2, ups=3.46, wpb=1093.2, bsz=61.6, num_updates=280, lr=8.4e-07, gnorm=19.561, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=81
2022-02-26 19:09:38 | INFO | train_inner | epoch 010:     23 / 31 loss=14.767, ppl=27885.5, wps=19380, ups=14.41, wpb=1344.5, bsz=61.9, num_updates=300, lr=9e-07, gnorm=17.216, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=82
2022-02-26 19:09:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:39 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 14.438 | ppl 22195.1 | wps 32152 | wpb 591.2 | bsz 29.9 | num_updates 308 | best_loss 14.438
2022-02-26 19:09:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 308 updates
2022-02-26 19:09:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 10 @ 308 updates, score 14.438) (writing took 3.778908355991007 seconds)
2022-02-26 19:09:43 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-26 19:09:43 | INFO | train | epoch 010 | loss 14.748 | ppl 27507.5 | wps 5887.4 | ups 4.8 | wpb 1227.6 | bsz 61.6 | num_updates 308 | lr 9.24e-07 | gnorm 17.533 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 87
2022-02-26 19:09:43 | INFO | fairseq.trainer | begin training epoch 11
2022-02-26 19:09:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:43 | INFO | train_inner | epoch 011:     12 / 31 loss=14.55, ppl=23989.2, wps=4322.3, ups=3.52, wpb=1227, bsz=62.4, num_updates=320, lr=9.6e-07, gnorm=17.459, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=88
2022-02-26 19:09:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:45 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 14.021 | ppl 16628.7 | wps 32804.8 | wpb 591.2 | bsz 29.9 | num_updates 339 | best_loss 14.021
2022-02-26 19:09:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 339 updates
2022-02-26 19:09:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 11 @ 339 updates, score 14.021) (writing took 3.782653207992553 seconds)
2022-02-26 19:09:49 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-26 19:09:49 | INFO | train | epoch 011 | loss 14.325 | ppl 20528.7 | wps 5956.9 | ups 4.85 | wpb 1227.6 | bsz 61.6 | num_updates 339 | lr 1.017e-06 | gnorm 16.713 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 93
2022-02-26 19:09:49 | INFO | fairseq.trainer | begin training epoch 12
2022-02-26 19:09:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:49 | INFO | train_inner | epoch 012:      1 / 31 loss=14.189, ppl=18678.4, wps=4173.9, ups=3.54, wpb=1179.8, bsz=60.3, num_updates=340, lr=1.02e-06, gnorm=16.467, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=93
2022-02-26 19:09:50 | INFO | train_inner | epoch 012:     21 / 31 loss=13.987, ppl=16233.5, wps=18195.8, ups=15.01, wpb=1212.4, bsz=64, num_updates=360, lr=1.08e-06, gnorm=15.674, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=95
2022-02-26 19:09:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:52 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 13.544 | ppl 11946.6 | wps 30924.4 | wpb 591.2 | bsz 29.9 | num_updates 370 | best_loss 13.544
2022-02-26 19:09:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 370 updates
2022-02-26 19:09:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 12 @ 370 updates, score 13.544) (writing took 3.8822677620046306 seconds)
2022-02-26 19:09:56 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-26 19:09:56 | INFO | train | epoch 012 | loss 13.923 | ppl 15532.8 | wps 5823.2 | ups 4.74 | wpb 1227.6 | bsz 61.6 | num_updates 370 | lr 1.11e-06 | gnorm 15.821 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 100
2022-02-26 19:09:56 | INFO | fairseq.trainer | begin training epoch 13
2022-02-26 19:09:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:09:56 | INFO | train_inner | epoch 013:     10 / 31 loss=13.682, ppl=13142.1, wps=4378.4, ups=3.41, wpb=1285.8, bsz=58.2, num_updates=380, lr=1.14e-06, gnorm=15.643, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=100
2022-02-26 19:09:58 | INFO | train_inner | epoch 013:     30 / 31 loss=13.378, ppl=10642.9, wps=17980.5, ups=14.78, wpb=1216.2, bsz=64, num_updates=400, lr=1.2e-06, gnorm=14.797, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=102
2022-02-26 19:09:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:09:58 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 13.118 | ppl 8889.96 | wps 26814.4 | wpb 591.2 | bsz 29.9 | num_updates 401 | best_loss 13.118
2022-02-26 19:09:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 401 updates
2022-02-26 19:09:58 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 13 @ 401 updates, score 13.118) (writing took 3.7940042299887864 seconds)
2022-02-26 19:10:02 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-26 19:10:02 | INFO | train | epoch 013 | loss 13.449 | ppl 11185.6 | wps 5868.9 | ups 4.78 | wpb 1227.6 | bsz 61.6 | num_updates 401 | lr 1.203e-06 | gnorm 15.004 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 106
2022-02-26 19:10:02 | INFO | fairseq.trainer | begin training epoch 14
2022-02-26 19:10:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:03 | INFO | train_inner | epoch 014:     19 / 31 loss=13.181, ppl=9290.04, wps=4056.5, ups=3.46, wpb=1171.7, bsz=60.3, num_updates=420, lr=1.26e-06, gnorm=15.138, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=108
2022-02-26 19:10:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:05 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 12.85 | ppl 7380.64 | wps 32035 | wpb 591.2 | bsz 29.9 | num_updates 432 | best_loss 12.85
2022-02-26 19:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 432 updates
2022-02-26 19:10:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:07 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 14 @ 432 updates, score 12.85) (writing took 3.7495829359977506 seconds)
2022-02-26 19:10:09 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-26 19:10:09 | INFO | train | epoch 014 | loss 13.122 | ppl 8915.98 | wps 5886.3 | ups 4.79 | wpb 1227.6 | bsz 61.6 | num_updates 432 | lr 1.296e-06 | gnorm 14.476 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 113
2022-02-26 19:10:09 | INFO | fairseq.trainer | begin training epoch 15
2022-02-26 19:10:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:09 | INFO | train_inner | epoch 015:      8 / 31 loss=12.983, ppl=8094.65, wps=4602, ups=3.5, wpb=1314.1, bsz=62.4, num_updates=440, lr=1.32e-06, gnorm=13.071, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=113
2022-02-26 19:10:11 | INFO | train_inner | epoch 015:     28 / 31 loss=12.839, ppl=7327.74, wps=18360.9, ups=15.29, wpb=1200.9, bsz=61.9, num_updates=460, lr=1.38e-06, gnorm=13.141, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=115
2022-02-26 19:10:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:11 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 12.678 | ppl 6552.71 | wps 32283.3 | wpb 591.2 | bsz 29.9 | num_updates 463 | best_loss 12.678
2022-02-26 19:10:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 463 updates
2022-02-26 19:10:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 15 @ 463 updates, score 12.678) (writing took 3.7875990480097244 seconds)
2022-02-26 19:10:15 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-26 19:10:15 | INFO | train | epoch 015 | loss 12.854 | ppl 7404.58 | wps 5959.7 | ups 4.85 | wpb 1227.6 | bsz 61.6 | num_updates 463 | lr 1.389e-06 | gnorm 13.211 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 119
2022-02-26 19:10:15 | INFO | fairseq.trainer | begin training epoch 16
2022-02-26 19:10:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:16 | INFO | train_inner | epoch 016:     17 / 31 loss=12.654, ppl=6445.65, wps=4413, ups=3.54, wpb=1248.2, bsz=61.6, num_updates=480, lr=1.44e-06, gnorm=12.97, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=120
2022-02-26 19:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:18 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 12.382 | ppl 5337.52 | wps 31585 | wpb 591.2 | bsz 29.9 | num_updates 494 | best_loss 12.382
2022-02-26 19:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 494 updates
2022-02-26 19:10:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 16 @ 494 updates, score 12.382) (writing took 4.13467990400386 seconds)
2022-02-26 19:10:22 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-26 19:10:22 | INFO | train | epoch 016 | loss 12.568 | ppl 6071.77 | wps 5613.4 | ups 4.57 | wpb 1227.6 | bsz 61.6 | num_updates 494 | lr 1.482e-06 | gnorm 12.295 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 126
2022-02-26 19:10:22 | INFO | fairseq.trainer | begin training epoch 17
2022-02-26 19:10:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:22 | INFO | train_inner | epoch 017:      6 / 31 loss=12.404, ppl=5417.92, wps=3887.8, ups=3.3, wpb=1179.1, bsz=59.8, num_updates=500, lr=1.5e-06, gnorm=12.802, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=126
2022-02-26 19:10:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:23 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.343 | ppl 5194.13 | wps 30918.9 | wpb 591.2 | bsz 29.9 | num_updates 500 | best_loss 12.343
2022-02-26 19:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 500 updates
2022-02-26 19:10:23 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-26 19:10:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt
2022-02-26 19:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_17_500.pt (epoch 17 @ 500 updates, score 12.343) (writing took 9.700520924990997 seconds)
2022-02-26 19:10:34 | INFO | train_inner | epoch 017:     26 / 31 loss=12.403, ppl=5414.69, wps=2238.4, ups=1.7, wpb=1319, bsz=63.2, num_updates=520, lr=1.56e-06, gnorm=11.284, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=138
2022-02-26 19:10:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:35 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.107 | ppl 4410.85 | wps 30057.2 | wpb 591.2 | bsz 29.9 | num_updates 525 | best_loss 12.107
2022-02-26 19:10:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 525 updates
2022-02-26 19:10:35 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 17 @ 525 updates, score 12.107) (writing took 4.579516243000398 seconds)
2022-02-26 19:10:39 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-26 19:10:39 | INFO | train | epoch 017 | loss 12.334 | ppl 5163.78 | wps 2146.3 | ups 1.75 | wpb 1227.6 | bsz 61.6 | num_updates 525 | lr 1.575e-06 | gnorm 12.207 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 144
2022-02-26 19:10:39 | INFO | fairseq.trainer | begin training epoch 18
2022-02-26 19:10:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:41 | INFO | train_inner | epoch 018:     15 / 31 loss=12.154, ppl=4558.86, wps=3985.1, ups=3.07, wpb=1297.2, bsz=61.1, num_updates=540, lr=1.62e-06, gnorm=11.494, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=145
2022-02-26 19:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:42 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 12.041 | ppl 4213.69 | wps 30888.2 | wpb 591.2 | bsz 29.9 | num_updates 556 | best_loss 12.041
2022-02-26 19:10:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 556 updates
2022-02-26 19:10:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:46 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 18 @ 556 updates, score 12.041) (writing took 5.406727541994769 seconds)
2022-02-26 19:10:48 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-26 19:10:48 | INFO | train | epoch 018 | loss 12.126 | ppl 4470.84 | wps 4715.9 | ups 3.84 | wpb 1227.6 | bsz 61.6 | num_updates 556 | lr 1.668e-06 | gnorm 11.134 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 152
2022-02-26 19:10:48 | INFO | fairseq.trainer | begin training epoch 19
2022-02-26 19:10:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:48 | INFO | train_inner | epoch 019:      4 / 31 loss=12.078, ppl=4324.56, wps=2770.6, ups=2.71, wpb=1023.3, bsz=61.6, num_updates=560, lr=1.68e-06, gnorm=11.391, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=152
2022-02-26 19:10:49 | INFO | train_inner | epoch 019:     24 / 31 loss=12.006, ppl=4113.69, wps=19849.2, ups=15.28, wpb=1298.8, bsz=63.2, num_updates=580, lr=1.74e-06, gnorm=10.387, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=153
2022-02-26 19:10:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:50 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.817 | ppl 3606.98 | wps 32030.5 | wpb 591.2 | bsz 29.9 | num_updates 587 | best_loss 11.817
2022-02-26 19:10:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 587 updates
2022-02-26 19:10:50 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 19 @ 587 updates, score 11.817) (writing took 3.7990156619925983 seconds)
2022-02-26 19:10:54 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-26 19:10:54 | INFO | train | epoch 019 | loss 11.989 | ppl 4065.27 | wps 5941.9 | ups 4.84 | wpb 1227.6 | bsz 61.6 | num_updates 587 | lr 1.761e-06 | gnorm 10.621 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 158
2022-02-26 19:10:54 | INFO | fairseq.trainer | begin training epoch 20
2022-02-26 19:10:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:10:55 | INFO | train_inner | epoch 020:     13 / 31 loss=11.914, ppl=3858.43, wps=4315.6, ups=3.49, wpb=1235, bsz=61.1, num_updates=600, lr=1.8e-06, gnorm=10.549, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=159
2022-02-26 19:10:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:10:57 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.545 | ppl 2988.13 | wps 31427.2 | wpb 591.2 | bsz 29.9 | num_updates 618 | best_loss 11.545
2022-02-26 19:10:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 618 updates
2022-02-26 19:10:57 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:10:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 20 @ 618 updates, score 11.545) (writing took 3.8354490509955212 seconds)
2022-02-26 19:11:01 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-26 19:11:01 | INFO | train | epoch 020 | loss 11.815 | ppl 3603.1 | wps 5735.7 | ups 4.67 | wpb 1227.6 | bsz 61.6 | num_updates 618 | lr 1.854e-06 | gnorm 11.158 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 165
2022-02-26 19:11:01 | INFO | fairseq.trainer | begin training epoch 21
2022-02-26 19:11:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:01 | INFO | train_inner | epoch 021:      2 / 31 loss=11.702, ppl=3332.18, wps=4048.3, ups=3.42, wpb=1183, bsz=60.3, num_updates=620, lr=1.86e-06, gnorm=11.543, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=165
2022-02-26 19:11:02 | INFO | train_inner | epoch 021:     22 / 31 loss=11.691, ppl=3306.81, wps=17977.6, ups=14.66, wpb=1226.5, bsz=62.7, num_updates=640, lr=1.92e-06, gnorm=9.858, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=166
2022-02-26 19:11:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:03 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.508 | ppl 2912.61 | wps 32105.3 | wpb 591.2 | bsz 29.9 | num_updates 649 | best_loss 11.508
2022-02-26 19:11:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 649 updates
2022-02-26 19:11:03 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 21 @ 649 updates, score 11.508) (writing took 4.798500946999411 seconds)
2022-02-26 19:11:08 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-26 19:11:08 | INFO | train | epoch 021 | loss 11.644 | ppl 3200.83 | wps 5121 | ups 4.17 | wpb 1227.6 | bsz 61.6 | num_updates 649 | lr 1.947e-06 | gnorm 9.871 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 172
2022-02-26 19:11:08 | INFO | fairseq.trainer | begin training epoch 22
2022-02-26 19:11:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:09 | INFO | train_inner | epoch 022:     11 / 31 loss=11.537, ppl=2971.62, wps=3589.4, ups=3, wpb=1198, bsz=61.6, num_updates=660, lr=1.98e-06, gnorm=9.992, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=173
2022-02-26 19:11:10 | INFO | train_inner | epoch 022:     31 / 31 loss=11.538, ppl=2974.03, wps=18382.4, ups=14.63, wpb=1256.2, bsz=60.3, num_updates=680, lr=2.04e-06, gnorm=9.724, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=174
2022-02-26 19:11:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:11 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.344 | ppl 2599.5 | wps 29212 | wpb 591.2 | bsz 29.9 | num_updates 680 | best_loss 11.344
2022-02-26 19:11:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 680 updates
2022-02-26 19:11:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 22 @ 680 updates, score 11.344) (writing took 3.9118930459953845 seconds)
2022-02-26 19:11:15 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-26 19:11:15 | INFO | train | epoch 022 | loss 11.527 | ppl 2951.19 | wps 5766.3 | ups 4.7 | wpb 1227.6 | bsz 61.6 | num_updates 680 | lr 2.04e-06 | gnorm 9.833 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 179
2022-02-26 19:11:15 | INFO | fairseq.trainer | begin training epoch 23
2022-02-26 19:11:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:16 | INFO | train_inner | epoch 023:     20 / 31 loss=11.339, ppl=2590.56, wps=4263.1, ups=3.4, wpb=1255.1, bsz=61.9, num_updates=700, lr=2.1e-06, gnorm=9.217, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=180
2022-02-26 19:11:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:17 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.297 | ppl 2515.29 | wps 31052 | wpb 591.2 | bsz 29.9 | num_updates 711 | best_loss 11.297
2022-02-26 19:11:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 711 updates
2022-02-26 19:11:17 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 23 @ 711 updates, score 11.297) (writing took 3.854022731000441 seconds)
2022-02-26 19:11:21 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-26 19:11:21 | INFO | train | epoch 023 | loss 11.353 | ppl 2616.2 | wps 5885.9 | ups 4.79 | wpb 1227.6 | bsz 61.6 | num_updates 711 | lr 2.133e-06 | gnorm 9.329 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 185
2022-02-26 19:11:21 | INFO | fairseq.trainer | begin training epoch 24
2022-02-26 19:11:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:22 | INFO | train_inner | epoch 024:      9 / 31 loss=11.395, ppl=2693.73, wps=4374.9, ups=3.5, wpb=1250.5, bsz=62.4, num_updates=720, lr=2.16e-06, gnorm=9.237, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=186
2022-02-26 19:11:23 | INFO | train_inner | epoch 024:     29 / 31 loss=11.32, ppl=2557.36, wps=18468.3, ups=15.05, wpb=1227.5, bsz=61.9, num_updates=740, lr=2.22e-06, gnorm=8.948, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=187
2022-02-26 19:11:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:24 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.086 | ppl 2173.92 | wps 30058.4 | wpb 591.2 | bsz 29.9 | num_updates 742 | best_loss 11.086
2022-02-26 19:11:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 742 updates
2022-02-26 19:11:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:26 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 24 @ 742 updates, score 11.086) (writing took 3.908917416993063 seconds)
2022-02-26 19:11:28 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-26 19:11:28 | INFO | train | epoch 024 | loss 11.324 | ppl 2564.11 | wps 5822.6 | ups 4.74 | wpb 1227.6 | bsz 61.6 | num_updates 742 | lr 2.226e-06 | gnorm 9.045 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 192
2022-02-26 19:11:28 | INFO | fairseq.trainer | begin training epoch 25
2022-02-26 19:11:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:29 | INFO | train_inner | epoch 025:     18 / 31 loss=11.159, ppl=2287.17, wps=3909.8, ups=3.18, wpb=1231.2, bsz=61.6, num_updates=760, lr=2.28e-06, gnorm=8.99, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=194
2022-02-26 19:11:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:31 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.026 | ppl 2084.92 | wps 26860 | wpb 591.2 | bsz 29.9 | num_updates 773 | best_loss 11.026
2022-02-26 19:11:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 773 updates
2022-02-26 19:11:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 25 @ 773 updates, score 11.026) (writing took 4.987794471002417 seconds)
2022-02-26 19:11:36 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-26 19:11:36 | INFO | train | epoch 025 | loss 11.197 | ppl 2348.43 | wps 4510.6 | ups 3.67 | wpb 1227.6 | bsz 61.6 | num_updates 773 | lr 2.319e-06 | gnorm 8.724 | clip 100 | loss_scale 32 | train_wall 3 | gb_free 20.9 | wall 200
2022-02-26 19:11:36 | INFO | fairseq.trainer | begin training epoch 26
2022-02-26 19:11:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:37 | INFO | train_inner | epoch 026:      7 / 31 loss=11.19, ppl=2336.16, wps=3176.8, ups=2.77, wpb=1146.8, bsz=59.8, num_updates=780, lr=2.34e-06, gnorm=9.033, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=201
2022-02-26 19:11:38 | INFO | train_inner | epoch 026:     27 / 31 loss=11.029, ppl=2089.19, wps=16993.6, ups=12.9, wpb=1317.7, bsz=63.2, num_updates=800, lr=2.4e-06, gnorm=9.143, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=202
2022-02-26 19:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:39 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 10.851 | ppl 1846.91 | wps 29921.7 | wpb 591.2 | bsz 29.9 | num_updates 804 | best_loss 10.851
2022-02-26 19:11:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 804 updates
2022-02-26 19:11:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:42 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 26 @ 804 updates, score 10.851) (writing took 4.511411895000492 seconds)
2022-02-26 19:11:44 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-26 19:11:44 | INFO | train | epoch 026 | loss 11.093 | ppl 2185.07 | wps 5126.7 | ups 4.18 | wpb 1227.6 | bsz 61.6 | num_updates 804 | lr 2.412e-06 | gnorm 9.281 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 208
2022-02-26 19:11:44 | INFO | fairseq.trainer | begin training epoch 27
2022-02-26 19:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:45 | INFO | train_inner | epoch 027:     16 / 31 loss=11.013, ppl=2065.9, wps=3741.3, ups=3.06, wpb=1221, bsz=61.1, num_updates=820, lr=2.46e-06, gnorm=8.62, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=209
2022-02-26 19:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:46 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.76 | ppl 1733.8 | wps 30987.6 | wpb 591.2 | bsz 29.9 | num_updates 835 | best_loss 10.76
2022-02-26 19:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 835 updates
2022-02-26 19:11:46 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:49 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 27 @ 835 updates, score 10.76) (writing took 4.025151114008622 seconds)
2022-02-26 19:11:50 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-26 19:11:50 | INFO | train | epoch 027 | loss 10.966 | ppl 1999.96 | wps 5596.6 | ups 4.56 | wpb 1227.6 | bsz 61.6 | num_updates 835 | lr 2.505e-06 | gnorm 8.672 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 215
2022-02-26 19:11:50 | INFO | fairseq.trainer | begin training epoch 28
2022-02-26 19:11:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:51 | INFO | train_inner | epoch 028:      5 / 31 loss=10.988, ppl=2031.63, wps=3748.1, ups=3.34, wpb=1122.6, bsz=60.8, num_updates=840, lr=2.52e-06, gnorm=8.795, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=215
2022-02-26 19:11:52 | INFO | train_inner | epoch 028:     25 / 31 loss=10.788, ppl=1767.82, wps=17598.1, ups=14.49, wpb=1214.5, bsz=62.7, num_updates=860, lr=2.58e-06, gnorm=10.262, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=216
2022-02-26 19:11:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:11:53 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.691 | ppl 1652.8 | wps 32068.7 | wpb 591.2 | bsz 29.9 | num_updates 866 | best_loss 10.691
2022-02-26 19:11:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 866 updates
2022-02-26 19:11:53 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:56 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:11:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 28 @ 866 updates, score 10.691) (writing took 4.2252347379981074 seconds)
2022-02-26 19:11:57 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-26 19:11:57 | INFO | train | epoch 028 | loss 10.855 | ppl 1852.78 | wps 5546.8 | ups 4.52 | wpb 1227.6 | bsz 61.6 | num_updates 866 | lr 2.598e-06 | gnorm 9.585 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 221
2022-02-26 19:11:57 | INFO | fairseq.trainer | begin training epoch 29
2022-02-26 19:11:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:11:58 | INFO | train_inner | epoch 029:     14 / 31 loss=10.899, ppl=1909.23, wps=4019.5, ups=3.27, wpb=1228.5, bsz=60.3, num_updates=880, lr=2.64e-06, gnorm=8.296, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=222
2022-02-26 19:11:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:00 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.618 | ppl 1571.74 | wps 30675.9 | wpb 591.2 | bsz 29.9 | num_updates 897 | best_loss 10.618
2022-02-26 19:12:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 897 updates
2022-02-26 19:12:00 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:03 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 29 @ 897 updates, score 10.618) (writing took 3.971856525997282 seconds)
2022-02-26 19:12:04 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-26 19:12:04 | INFO | train | epoch 029 | loss 10.806 | ppl 1790.76 | wps 5707 | ups 4.65 | wpb 1227.6 | bsz 61.6 | num_updates 897 | lr 2.691e-06 | gnorm 8.207 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 228
2022-02-26 19:12:04 | INFO | fairseq.trainer | begin training epoch 30
2022-02-26 19:12:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:04 | INFO | train_inner | epoch 030:      3 / 31 loss=10.743, ppl=1713.4, wps=4390.5, ups=3.37, wpb=1303.6, bsz=62.4, num_updates=900, lr=2.7e-06, gnorm=8.117, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=228
2022-02-26 19:12:06 | INFO | train_inner | epoch 030:     23 / 31 loss=10.682, ppl=1643.05, wps=17442.3, ups=14.9, wpb=1170.5, bsz=61.9, num_updates=920, lr=2.76e-06, gnorm=8.06, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=230
2022-02-26 19:12:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:07 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.647 | ppl 1603.19 | wps 31159.7 | wpb 591.2 | bsz 29.9 | num_updates 928 | best_loss 10.618
2022-02-26 19:12:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 928 updates
2022-02-26 19:12:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:12:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:12:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 30 @ 928 updates, score 10.647) (writing took 2.567257739006891 seconds)
2022-02-26 19:12:09 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-26 19:12:09 | INFO | train | epoch 030 | loss 10.674 | ppl 1633.26 | wps 7310.9 | ups 5.96 | wpb 1227.6 | bsz 61.6 | num_updates 928 | lr 2.784e-06 | gnorm 8.007 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 233
2022-02-26 19:12:09 | INFO | fairseq.trainer | begin training epoch 31
2022-02-26 19:12:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:10 | INFO | train_inner | epoch 031:     12 / 31 loss=10.632, ppl=1587.05, wps=5672, ups=4.45, wpb=1273.7, bsz=62.4, num_updates=940, lr=2.82e-06, gnorm=8.049, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=234
2022-02-26 19:12:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:12 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.457 | ppl 1405.16 | wps 26767.1 | wpb 591.2 | bsz 29.9 | num_updates 959 | best_loss 10.457
2022-02-26 19:12:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 959 updates
2022-02-26 19:12:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 31 @ 959 updates, score 10.457) (writing took 4.0277933399920585 seconds)
2022-02-26 19:12:16 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-26 19:12:16 | INFO | train | epoch 031 | loss 10.615 | ppl 1568.13 | wps 5621.5 | ups 4.58 | wpb 1227.6 | bsz 61.6 | num_updates 959 | lr 2.877e-06 | gnorm 8.346 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 240
2022-02-26 19:12:16 | INFO | fairseq.trainer | begin training epoch 32
2022-02-26 19:12:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:16 | INFO | train_inner | epoch 032:      1 / 31 loss=10.614, ppl=1567.61, wps=4077, ups=3.33, wpb=1222.8, bsz=60.3, num_updates=960, lr=2.88e-06, gnorm=8.383, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=240
2022-02-26 19:12:17 | INFO | train_inner | epoch 032:     21 / 31 loss=10.572, ppl=1522.22, wps=17911.2, ups=14.52, wpb=1234, bsz=63.2, num_updates=980, lr=2.94e-06, gnorm=7.93, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=242
2022-02-26 19:12:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:19 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 10.473 | ppl 1421.34 | wps 29350.3 | wpb 591.2 | bsz 29.9 | num_updates 990 | best_loss 10.457
2022-02-26 19:12:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 990 updates
2022-02-26 19:12:19 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:12:21 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:12:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 32 @ 990 updates, score 10.473) (writing took 2.668422298011137 seconds)
2022-02-26 19:12:21 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-26 19:12:21 | INFO | train | epoch 032 | loss 10.551 | ppl 1499.93 | wps 7144.7 | ups 5.82 | wpb 1227.6 | bsz 61.6 | num_updates 990 | lr 2.97e-06 | gnorm 8.047 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 245
2022-02-26 19:12:21 | INFO | fairseq.trainer | begin training epoch 33
2022-02-26 19:12:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:22 | INFO | train_inner | epoch 033:     10 / 31 loss=10.603, ppl=1554.93, wps=5584.6, ups=4.42, wpb=1262.8, bsz=59, num_updates=1000, lr=3e-06, gnorm=7.961, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=246
2022-02-26 19:12:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:22 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.448 | ppl 1397.11 | wps 30748.5 | wpb 591.2 | bsz 29.9 | num_updates 1000 | best_loss 10.448
2022-02-26 19:12:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1000 updates
2022-02-26 19:12:22 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-26 19:12:25 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt
2022-02-26 19:12:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_33_1000.pt (epoch 33 @ 1000 updates, score 10.448) (writing took 6.152512922999449 seconds)
2022-02-26 19:12:30 | INFO | train_inner | epoch 033:     30 / 31 loss=10.363, ppl=1317.27, wps=2849.8, ups=2.38, wpb=1198.3, bsz=64, num_updates=1020, lr=3.06e-06, gnorm=7.931, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=255
2022-02-26 19:12:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:31 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.381 | ppl 1333.83 | wps 30042 | wpb 591.2 | bsz 29.9 | num_updates 1021 | best_loss 10.381
2022-02-26 19:12:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1021 updates
2022-02-26 19:12:31 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:33 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 33 @ 1021 updates, score 10.381) (writing took 4.063839041991741 seconds)
2022-02-26 19:12:35 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-26 19:12:35 | INFO | train | epoch 033 | loss 10.473 | ppl 1421.1 | wps 2768.2 | ups 2.25 | wpb 1227.6 | bsz 61.6 | num_updates 1021 | lr 3.063e-06 | gnorm 7.876 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 259
2022-02-26 19:12:35 | INFO | fairseq.trainer | begin training epoch 34
2022-02-26 19:12:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:37 | INFO | train_inner | epoch 034:     19 / 31 loss=10.387, ppl=1339.03, wps=3748.8, ups=3.09, wpb=1211.8, bsz=62.4, num_updates=1040, lr=3.12e-06, gnorm=8.232, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=261
2022-02-26 19:12:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:38 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 10.37 | ppl 1323.46 | wps 27523.2 | wpb 591.2 | bsz 29.9 | num_updates 1052 | best_loss 10.37
2022-02-26 19:12:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1052 updates
2022-02-26 19:12:38 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 34 @ 1052 updates, score 10.37) (writing took 4.3764925110008335 seconds)
2022-02-26 19:12:43 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-26 19:12:43 | INFO | train | epoch 034 | loss 10.416 | ppl 1366.01 | wps 5179.7 | ups 4.22 | wpb 1227.6 | bsz 61.6 | num_updates 1052 | lr 3.156e-06 | gnorm 8.081 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 267
2022-02-26 19:12:43 | INFO | fairseq.trainer | begin training epoch 35
2022-02-26 19:12:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:43 | INFO | train_inner | epoch 035:      8 / 31 loss=10.464, ppl=1412.53, wps=3767.2, ups=3.07, wpb=1227.2, bsz=60.3, num_updates=1060, lr=3.18e-06, gnorm=7.755, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=268
2022-02-26 19:12:45 | INFO | train_inner | epoch 035:     28 / 31 loss=10.297, ppl=1258.35, wps=19919.6, ups=15.37, wpb=1296, bsz=61.9, num_updates=1080, lr=3.24e-06, gnorm=7.923, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=269
2022-02-26 19:12:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:45 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 10.186 | ppl 1164.52 | wps 31484.2 | wpb 591.2 | bsz 29.9 | num_updates 1083 | best_loss 10.186
2022-02-26 19:12:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1083 updates
2022-02-26 19:12:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 35 @ 1083 updates, score 10.186) (writing took 3.9469108719931683 seconds)
2022-02-26 19:12:49 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-26 19:12:49 | INFO | train | epoch 035 | loss 10.339 | ppl 1294.92 | wps 5802.9 | ups 4.73 | wpb 1227.6 | bsz 61.6 | num_updates 1083 | lr 3.249e-06 | gnorm 7.947 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 273
2022-02-26 19:12:49 | INFO | fairseq.trainer | begin training epoch 36
2022-02-26 19:12:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:51 | INFO | train_inner | epoch 036:     17 / 31 loss=10.323, ppl=1281.35, wps=4354.9, ups=3.44, wpb=1265.6, bsz=61.1, num_updates=1100, lr=3.3e-06, gnorm=8.524, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=275
2022-02-26 19:12:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:52 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 10.132 | ppl 1121.9 | wps 27443 | wpb 591.2 | bsz 29.9 | num_updates 1114 | best_loss 10.132
2022-02-26 19:12:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1114 updates
2022-02-26 19:12:52 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:54 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:12:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 36 @ 1114 updates, score 10.132) (writing took 3.9177867389953462 seconds)
2022-02-26 19:12:56 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-26 19:12:56 | INFO | train | epoch 036 | loss 10.227 | ppl 1198.54 | wps 5742.9 | ups 4.68 | wpb 1227.6 | bsz 61.6 | num_updates 1114 | lr 3.342e-06 | gnorm 8.36 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 280
2022-02-26 19:12:56 | INFO | fairseq.trainer | begin training epoch 37
2022-02-26 19:12:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:12:56 | INFO | train_inner | epoch 037:      6 / 31 loss=10.092, ppl=1091.07, wps=3797.7, ups=3.35, wpb=1132.5, bsz=61.6, num_updates=1120, lr=3.36e-06, gnorm=7.921, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=281
2022-02-26 19:12:58 | INFO | train_inner | epoch 037:     26 / 31 loss=10.244, ppl=1212.5, wps=17669.7, ups=14.65, wpb=1205.9, bsz=61.9, num_updates=1140, lr=3.42e-06, gnorm=7.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=282
2022-02-26 19:12:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:12:59 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.967 | ppl 1000.76 | wps 31446.7 | wpb 591.2 | bsz 29.9 | num_updates 1145 | best_loss 9.967
2022-02-26 19:12:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1145 updates
2022-02-26 19:12:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 37 @ 1145 updates, score 9.967) (writing took 3.8547752570011653 seconds)
2022-02-26 19:13:03 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-26 19:13:03 | INFO | train | epoch 037 | loss 10.189 | ppl 1167.65 | wps 5759.8 | ups 4.69 | wpb 1227.6 | bsz 61.6 | num_updates 1145 | lr 3.435e-06 | gnorm 7.595 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 287
2022-02-26 19:13:03 | INFO | fairseq.trainer | begin training epoch 38
2022-02-26 19:13:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:04 | INFO | train_inner | epoch 038:     15 / 31 loss=10.075, ppl=1078.35, wps=4016.2, ups=3.4, wpb=1181, bsz=62.4, num_updates=1160, lr=3.48e-06, gnorm=7.75, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=288
2022-02-26 19:13:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:05 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 10.041 | ppl 1053.41 | wps 32089.2 | wpb 591.2 | bsz 29.9 | num_updates 1176 | best_loss 9.967
2022-02-26 19:13:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1176 updates
2022-02-26 19:13:05 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:08 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 38 @ 1176 updates, score 10.041) (writing took 2.951611247000983 seconds)
2022-02-26 19:13:08 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-26 19:13:08 | INFO | train | epoch 038 | loss 10.103 | ppl 1099.7 | wps 6641.8 | ups 5.41 | wpb 1227.6 | bsz 61.6 | num_updates 1176 | lr 3.528e-06 | gnorm 7.73 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 292
2022-02-26 19:13:08 | INFO | fairseq.trainer | begin training epoch 39
2022-02-26 19:13:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:09 | INFO | train_inner | epoch 039:      4 / 31 loss=10.085, ppl=1086.02, wps=5269.2, ups=4.07, wpb=1293.3, bsz=60.3, num_updates=1180, lr=3.54e-06, gnorm=7.527, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=293
2022-02-26 19:13:10 | INFO | train_inner | epoch 039:     24 / 31 loss=10.01, ppl=1031.02, wps=19044.3, ups=14.91, wpb=1277.4, bsz=62.7, num_updates=1200, lr=3.6e-06, gnorm=7.835, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=294
2022-02-26 19:13:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:11 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.903 | ppl 957.5 | wps 31190.5 | wpb 591.2 | bsz 29.9 | num_updates 1207 | best_loss 9.903
2022-02-26 19:13:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1207 updates
2022-02-26 19:13:11 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:13 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 39 @ 1207 updates, score 9.903) (writing took 3.9628922779957065 seconds)
2022-02-26 19:13:15 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-26 19:13:15 | INFO | train | epoch 039 | loss 9.996 | ppl 1020.98 | wps 5740.1 | ups 4.68 | wpb 1227.6 | bsz 61.6 | num_updates 1207 | lr 3.621e-06 | gnorm 7.737 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 299
2022-02-26 19:13:15 | INFO | fairseq.trainer | begin training epoch 40
2022-02-26 19:13:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:16 | INFO | train_inner | epoch 040:     13 / 31 loss=9.877, ppl=940.12, wps=3676.8, ups=3.38, wpb=1086.4, bsz=60.8, num_updates=1220, lr=3.66e-06, gnorm=7.614, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=300
2022-02-26 19:13:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:18 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.728 | ppl 848.31 | wps 32799.7 | wpb 591.2 | bsz 29.9 | num_updates 1238 | best_loss 9.728
2022-02-26 19:13:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1238 updates
2022-02-26 19:13:18 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:20 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 40 @ 1238 updates, score 9.728) (writing took 3.793460450004204 seconds)
2022-02-26 19:13:21 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-26 19:13:21 | INFO | train | epoch 040 | loss 9.915 | ppl 965.51 | wps 5923.4 | ups 4.83 | wpb 1227.6 | bsz 61.6 | num_updates 1238 | lr 3.714e-06 | gnorm 7.746 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 305
2022-02-26 19:13:21 | INFO | fairseq.trainer | begin training epoch 41
2022-02-26 19:13:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:22 | INFO | train_inner | epoch 041:      2 / 31 loss=10.012, ppl=1032.3, wps=4507.7, ups=3.5, wpb=1288, bsz=61.1, num_updates=1240, lr=3.72e-06, gnorm=8.023, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=306
2022-02-26 19:13:23 | INFO | train_inner | epoch 041:     22 / 31 loss=9.886, ppl=946.16, wps=19167.6, ups=14.39, wpb=1331.8, bsz=62.7, num_updates=1260, lr=3.78e-06, gnorm=7.616, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=307
2022-02-26 19:13:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:24 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.767 | ppl 871.4 | wps 33886.6 | wpb 591.2 | bsz 29.9 | num_updates 1269 | best_loss 9.728
2022-02-26 19:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1269 updates
2022-02-26 19:13:24 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:27 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 41 @ 1269 updates, score 9.767) (writing took 2.5023338650062215 seconds)
2022-02-26 19:13:27 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-26 19:13:27 | INFO | train | epoch 041 | loss 9.918 | ppl 967.22 | wps 7342.6 | ups 5.98 | wpb 1227.6 | bsz 61.6 | num_updates 1269 | lr 3.807e-06 | gnorm 7.718 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 311
2022-02-26 19:13:27 | INFO | fairseq.trainer | begin training epoch 42
2022-02-26 19:13:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:27 | INFO | train_inner | epoch 042:     11 / 31 loss=9.921, ppl=969.71, wps=5277.8, ups=4.53, wpb=1164.5, bsz=61.6, num_updates=1280, lr=3.84e-06, gnorm=7.611, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=312
2022-02-26 19:13:29 | INFO | train_inner | epoch 042:     31 / 31 loss=9.716, ppl=841.31, wps=18316.9, ups=15.08, wpb=1214.5, bsz=60.3, num_updates=1300, lr=3.9e-06, gnorm=7.924, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=313
2022-02-26 19:13:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:29 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.678 | ppl 819.2 | wps 32341.9 | wpb 591.2 | bsz 29.9 | num_updates 1300 | best_loss 9.678
2022-02-26 19:13:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 1300 updates
2022-02-26 19:13:29 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:32 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 42 @ 1300 updates, score 9.678) (writing took 3.7048442980012624 seconds)
2022-02-26 19:13:33 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-02-26 19:13:33 | INFO | train | epoch 042 | loss 9.786 | ppl 882.61 | wps 6013.7 | ups 4.9 | wpb 1227.6 | bsz 61.6 | num_updates 1300 | lr 3.9e-06 | gnorm 7.775 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 317
2022-02-26 19:13:33 | INFO | fairseq.trainer | begin training epoch 43
2022-02-26 19:13:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:34 | INFO | train_inner | epoch 043:     20 / 31 loss=9.795, ppl=888.08, wps=4541.5, ups=3.55, wpb=1280.2, bsz=61.9, num_updates=1320, lr=3.96e-06, gnorm=7.558, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=319
2022-02-26 19:13:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:36 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.652 | ppl 804.28 | wps 31285 | wpb 591.2 | bsz 29.9 | num_updates 1331 | best_loss 9.652
2022-02-26 19:13:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 1331 updates
2022-02-26 19:13:36 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:38 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 43 @ 1331 updates, score 9.652) (writing took 4.120866194003611 seconds)
2022-02-26 19:13:40 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-02-26 19:13:40 | INFO | train | epoch 043 | loss 9.771 | ppl 873.56 | wps 5618.3 | ups 4.58 | wpb 1227.6 | bsz 61.6 | num_updates 1331 | lr 3.993e-06 | gnorm 7.659 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 324
2022-02-26 19:13:40 | INFO | fairseq.trainer | begin training epoch 44
2022-02-26 19:13:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:40 | INFO | train_inner | epoch 044:      9 / 31 loss=9.687, ppl=824.56, wps=3909.7, ups=3.3, wpb=1184.5, bsz=62.4, num_updates=1340, lr=4.02e-06, gnorm=8.028, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=325
2022-02-26 19:13:42 | INFO | train_inner | epoch 044:     29 / 31 loss=9.612, ppl=782.46, wps=17721.3, ups=14.51, wpb=1221.7, bsz=62.7, num_updates=1360, lr=4.08e-06, gnorm=7.454, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=326
2022-02-26 19:13:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:42 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.624 | ppl 788.9 | wps 29326.1 | wpb 591.2 | bsz 29.9 | num_updates 1362 | best_loss 9.624
2022-02-26 19:13:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 1362 updates
2022-02-26 19:13:42 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:45 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 44 @ 1362 updates, score 9.624) (writing took 3.82695534999948 seconds)
2022-02-26 19:13:46 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-02-26 19:13:46 | INFO | train | epoch 044 | loss 9.66 | ppl 808.89 | wps 5797.2 | ups 4.72 | wpb 1227.6 | bsz 61.6 | num_updates 1362 | lr 4.086e-06 | gnorm 7.703 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 330
2022-02-26 19:13:46 | INFO | fairseq.trainer | begin training epoch 45
2022-02-26 19:13:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:48 | INFO | train_inner | epoch 045:     18 / 31 loss=9.634, ppl=794.66, wps=4131.2, ups=3.46, wpb=1195.7, bsz=60.3, num_updates=1380, lr=4.14e-06, gnorm=7.457, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=332
2022-02-26 19:13:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:49 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.52 | ppl 734.3 | wps 31101.7 | wpb 591.2 | bsz 29.9 | num_updates 1393 | best_loss 9.52
2022-02-26 19:13:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1393 updates
2022-02-26 19:13:49 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:51 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 45 @ 1393 updates, score 9.52) (writing took 3.9072922019986436 seconds)
2022-02-26 19:13:53 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-02-26 19:13:53 | INFO | train | epoch 045 | loss 9.561 | ppl 755.44 | wps 5756.1 | ups 4.69 | wpb 1227.6 | bsz 61.6 | num_updates 1393 | lr 4.179e-06 | gnorm 7.69 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 337
2022-02-26 19:13:53 | INFO | fairseq.trainer | begin training epoch 46
2022-02-26 19:13:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:13:53 | INFO | train_inner | epoch 046:      7 / 31 loss=9.535, ppl=741.64, wps=4325.6, ups=3.44, wpb=1258.5, bsz=61.6, num_updates=1400, lr=4.2e-06, gnorm=8.11, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=338
2022-02-26 19:13:55 | INFO | train_inner | epoch 046:     27 / 31 loss=9.51, ppl=729.21, wps=17443.7, ups=14.31, wpb=1219.4, bsz=61.9, num_updates=1420, lr=4.26e-06, gnorm=7.706, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=339
2022-02-26 19:13:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:13:56 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.591 | ppl 771.02 | wps 30384.3 | wpb 591.2 | bsz 29.9 | num_updates 1424 | best_loss 9.52
2022-02-26 19:13:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 1424 updates
2022-02-26 19:13:56 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:59 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:13:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 46 @ 1424 updates, score 9.591) (writing took 3.0463681620021816 seconds)
2022-02-26 19:13:59 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-02-26 19:13:59 | INFO | train | epoch 046 | loss 9.523 | ppl 735.7 | wps 6627.6 | ups 5.4 | wpb 1227.6 | bsz 61.6 | num_updates 1424 | lr 4.272e-06 | gnorm 7.805 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 343
2022-02-26 19:13:59 | INFO | fairseq.trainer | begin training epoch 47
2022-02-26 19:13:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:00 | INFO | train_inner | epoch 047:     16 / 31 loss=9.529, ppl=738.55, wps=4865.4, ups=4.03, wpb=1206, bsz=61.6, num_updates=1440, lr=4.32e-06, gnorm=7.821, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=344
2022-02-26 19:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:01 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.38 | ppl 666.3 | wps 31909.5 | wpb 591.2 | bsz 29.9 | num_updates 1455 | best_loss 9.38
2022-02-26 19:14:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 1455 updates
2022-02-26 19:14:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 47 @ 1455 updates, score 9.38) (writing took 3.9880868659965927 seconds)
2022-02-26 19:14:05 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-02-26 19:14:05 | INFO | train | epoch 047 | loss 9.443 | ppl 695.87 | wps 5750.6 | ups 4.68 | wpb 1227.6 | bsz 61.6 | num_updates 1455 | lr 4.365e-06 | gnorm 7.908 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 349
2022-02-26 19:14:05 | INFO | fairseq.trainer | begin training epoch 48
2022-02-26 19:14:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:06 | INFO | train_inner | epoch 048:      5 / 31 loss=9.411, ppl=680.69, wps=4242.3, ups=3.41, wpb=1244.2, bsz=59.8, num_updates=1460, lr=4.38e-06, gnorm=7.895, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=350
2022-02-26 19:14:07 | INFO | train_inner | epoch 048:     25 / 31 loss=9.321, ppl=639.61, wps=18914.5, ups=14.72, wpb=1285.3, bsz=63.2, num_updates=1480, lr=4.44e-06, gnorm=7.138, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=351
2022-02-26 19:14:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:08 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.397 | ppl 674.27 | wps 32077.8 | wpb 591.2 | bsz 29.9 | num_updates 1486 | best_loss 9.38
2022-02-26 19:14:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1486 updates
2022-02-26 19:14:08 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:14:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 48 @ 1486 updates, score 9.397) (writing took 2.5376265599916223 seconds)
2022-02-26 19:14:10 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-02-26 19:14:10 | INFO | train | epoch 048 | loss 9.37 | ppl 661.52 | wps 7350.4 | ups 5.99 | wpb 1227.6 | bsz 61.6 | num_updates 1486 | lr 4.458e-06 | gnorm 7.353 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 355
2022-02-26 19:14:10 | INFO | fairseq.trainer | begin training epoch 49
2022-02-26 19:14:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:11 | INFO | train_inner | epoch 049:     14 / 31 loss=9.437, ppl=693.28, wps=5702.6, ups=4.51, wpb=1265.4, bsz=62.4, num_updates=1500, lr=4.5e-06, gnorm=7.318, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=356
2022-02-26 19:14:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:12 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.329 | ppl 643.22 | wps 31269 | wpb 591.2 | bsz 29.9 | num_updates 1500 | best_loss 9.329
2022-02-26 19:14:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1500 updates
2022-02-26 19:14:12 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-26 19:14:14 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt
2022-02-26 19:14:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_49_1500.pt (epoch 49 @ 1500 updates, score 9.329) (writing took 7.330665714995121 seconds)
2022-02-26 19:14:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:21 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.28 | ppl 621.66 | wps 30719.2 | wpb 591.2 | bsz 29.9 | num_updates 1517 | best_loss 9.28
2022-02-26 19:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1517 updates
2022-02-26 19:14:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 49 @ 1517 updates, score 9.28) (writing took 3.9229340650053928 seconds)
2022-02-26 19:14:25 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-02-26 19:14:25 | INFO | train | epoch 049 | loss 9.284 | ppl 623.57 | wps 2606.3 | ups 2.12 | wpb 1227.6 | bsz 61.6 | num_updates 1517 | lr 4.551e-06 | gnorm 7.73 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 369
2022-02-26 19:14:25 | INFO | fairseq.trainer | begin training epoch 50
2022-02-26 19:14:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:25 | INFO | train_inner | epoch 050:      3 / 31 loss=9.082, ppl=541.94, wps=1585, ups=1.44, wpb=1100.2, bsz=60.3, num_updates=1520, lr=4.56e-06, gnorm=8.202, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=370
2022-02-26 19:14:27 | INFO | train_inner | epoch 050:     23 / 31 loss=9.277, ppl=620.56, wps=19169.4, ups=15.28, wpb=1254.2, bsz=61.9, num_updates=1540, lr=4.62e-06, gnorm=7.504, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=371
2022-02-26 19:14:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:28 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.271 | ppl 617.75 | wps 31290.7 | wpb 591.2 | bsz 29.9 | num_updates 1548 | best_loss 9.271
2022-02-26 19:14:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1548 updates
2022-02-26 19:14:28 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:30 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 50 @ 1548 updates, score 9.271) (writing took 3.9350640530028613 seconds)
2022-02-26 19:14:32 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-02-26 19:14:32 | INFO | train | epoch 050 | loss 9.25 | ppl 608.87 | wps 5836.2 | ups 4.75 | wpb 1227.6 | bsz 61.6 | num_updates 1548 | lr 4.644e-06 | gnorm 7.543 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 376
2022-02-26 19:14:32 | INFO | fairseq.trainer | begin training epoch 51
2022-02-26 19:14:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:32 | INFO | train_inner | epoch 051:     12 / 31 loss=9.361, ppl=657.67, wps=4494, ups=3.46, wpb=1299.3, bsz=60.3, num_updates=1560, lr=4.68e-06, gnorm=7.962, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=377
2022-02-26 19:14:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:34 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.257 | ppl 611.94 | wps 30578.3 | wpb 591.2 | bsz 29.9 | num_updates 1579 | best_loss 9.257
2022-02-26 19:14:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1579 updates
2022-02-26 19:14:34 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:37 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 51 @ 1579 updates, score 9.257) (writing took 3.8273983910039533 seconds)
2022-02-26 19:14:38 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-02-26 19:14:38 | INFO | train | epoch 051 | loss 9.225 | ppl 598.25 | wps 5871.8 | ups 4.78 | wpb 1227.6 | bsz 61.6 | num_updates 1579 | lr 4.737e-06 | gnorm 7.8 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 382
2022-02-26 19:14:38 | INFO | fairseq.trainer | begin training epoch 52
2022-02-26 19:14:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:38 | INFO | train_inner | epoch 052:      1 / 31 loss=9.133, ppl=561.34, wps=4024.7, ups=3.48, wpb=1157.5, bsz=62.4, num_updates=1580, lr=4.74e-06, gnorm=7.452, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=382
2022-02-26 19:14:40 | INFO | train_inner | epoch 052:     21 / 31 loss=9.101, ppl=549.15, wps=17632.9, ups=14.95, wpb=1179.5, bsz=62.7, num_updates=1600, lr=4.8e-06, gnorm=7.438, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=384
2022-02-26 19:14:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:41 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.17 | ppl 576.18 | wps 30014.6 | wpb 591.2 | bsz 29.9 | num_updates 1610 | best_loss 9.17
2022-02-26 19:14:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1610 updates
2022-02-26 19:14:41 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:43 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 52 @ 1610 updates, score 9.17) (writing took 3.831199442007346 seconds)
2022-02-26 19:14:45 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-02-26 19:14:45 | INFO | train | epoch 052 | loss 9.172 | ppl 577.02 | wps 5854.2 | ups 4.77 | wpb 1227.6 | bsz 61.6 | num_updates 1610 | lr 4.83e-06 | gnorm 7.504 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 389
2022-02-26 19:14:45 | INFO | fairseq.trainer | begin training epoch 53
2022-02-26 19:14:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:46 | INFO | train_inner | epoch 053:     10 / 31 loss=9.248, ppl=607.92, wps=4172, ups=3.29, wpb=1267.7, bsz=59.5, num_updates=1620, lr=4.86e-06, gnorm=7.46, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=390
2022-02-26 19:14:47 | INFO | train_inner | epoch 053:     30 / 31 loss=9.024, ppl=520.43, wps=18543.6, ups=14.4, wpb=1288.1, bsz=64, num_updates=1640, lr=4.92e-06, gnorm=7.523, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=391
2022-02-26 19:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:48 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.057 | ppl 532.74 | wps 32095.7 | wpb 591.2 | bsz 29.9 | num_updates 1641 | best_loss 9.057
2022-02-26 19:14:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1641 updates
2022-02-26 19:14:48 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:50 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 53 @ 1641 updates, score 9.057) (writing took 3.8129485249955906 seconds)
2022-02-26 19:14:51 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-02-26 19:14:51 | INFO | train | epoch 053 | loss 9.081 | ppl 541.46 | wps 5597.2 | ups 4.56 | wpb 1227.6 | bsz 61.6 | num_updates 1641 | lr 4.923e-06 | gnorm 7.519 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 395
2022-02-26 19:14:51 | INFO | fairseq.trainer | begin training epoch 54
2022-02-26 19:14:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:53 | INFO | train_inner | epoch 054:     19 / 31 loss=9.064, ppl=535.23, wps=4478.3, ups=3.5, wpb=1280.2, bsz=60.3, num_updates=1660, lr=4.98e-06, gnorm=7.437, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=397
2022-02-26 19:14:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:14:54 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.02 | ppl 519.27 | wps 27102.9 | wpb 591.2 | bsz 29.9 | num_updates 1672 | best_loss 9.02
2022-02-26 19:14:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1672 updates
2022-02-26 19:14:54 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:57 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:14:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 54 @ 1672 updates, score 9.02) (writing took 3.992239008002798 seconds)
2022-02-26 19:14:58 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-02-26 19:14:58 | INFO | train | epoch 054 | loss 9.038 | ppl 525.64 | wps 5686.9 | ups 4.63 | wpb 1227.6 | bsz 61.6 | num_updates 1672 | lr 5.016e-06 | gnorm 7.589 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 402
2022-02-26 19:14:58 | INFO | fairseq.trainer | begin training epoch 55
2022-02-26 19:14:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:14:59 | INFO | train_inner | epoch 055:      8 / 31 loss=9.001, ppl=512.28, wps=4060.9, ups=3.33, wpb=1220, bsz=62.4, num_updates=1680, lr=5.04e-06, gnorm=7.672, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=403
2022-02-26 19:15:00 | INFO | train_inner | epoch 055:     28 / 31 loss=8.935, ppl=489.36, wps=18705.3, ups=15.33, wpb=1219.8, bsz=61.9, num_updates=1700, lr=5.1e-06, gnorm=7.593, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=404
2022-02-26 19:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:01 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.04 | ppl 526.48 | wps 30292.6 | wpb 591.2 | bsz 29.9 | num_updates 1703 | best_loss 9.02
2022-02-26 19:15:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1703 updates
2022-02-26 19:15:01 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:04 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 55 @ 1703 updates, score 9.04) (writing took 3.0945771359984064 seconds)
2022-02-26 19:15:04 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-02-26 19:15:04 | INFO | train | epoch 055 | loss 8.944 | ppl 492.38 | wps 6538.1 | ups 5.33 | wpb 1227.6 | bsz 61.6 | num_updates 1703 | lr 5.109e-06 | gnorm 7.62 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 408
2022-02-26 19:15:04 | INFO | fairseq.trainer | begin training epoch 56
2022-02-26 19:15:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:05 | INFO | train_inner | epoch 056:     17 / 31 loss=8.831, ppl=455.37, wps=4492.2, ups=3.86, wpb=1164.4, bsz=61.1, num_updates=1720, lr=5.16e-06, gnorm=7.686, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=409
2022-02-26 19:15:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:07 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.985 | ppl 506.57 | wps 29202.3 | wpb 591.2 | bsz 29.9 | num_updates 1734 | best_loss 8.985
2022-02-26 19:15:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1734 updates
2022-02-26 19:15:07 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:09 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 56 @ 1734 updates, score 8.985) (writing took 3.9883567150100134 seconds)
2022-02-26 19:15:11 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-02-26 19:15:11 | INFO | train | epoch 056 | loss 8.9 | ppl 477.76 | wps 5516.2 | ups 4.49 | wpb 1227.6 | bsz 61.6 | num_updates 1734 | lr 5.202e-06 | gnorm 7.488 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 415
2022-02-26 19:15:11 | INFO | fairseq.trainer | begin training epoch 57
2022-02-26 19:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:11 | INFO | train_inner | epoch 057:      6 / 31 loss=8.975, ppl=503.13, wps=3930.2, ups=3.31, wpb=1186.1, bsz=60.3, num_updates=1740, lr=5.22e-06, gnorm=7.779, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=415
2022-02-26 19:15:13 | INFO | train_inner | epoch 057:     26 / 31 loss=8.748, ppl=429.85, wps=17424.9, ups=14.45, wpb=1205.9, bsz=63.2, num_updates=1760, lr=5.28e-06, gnorm=7.639, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=417
2022-02-26 19:15:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:13 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.917 | ppl 483.34 | wps 29920.8 | wpb 591.2 | bsz 29.9 | num_updates 1765 | best_loss 8.917
2022-02-26 19:15:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1765 updates
2022-02-26 19:15:13 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:16 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 57 @ 1765 updates, score 8.917) (writing took 4.484520123995026 seconds)
2022-02-26 19:15:18 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-02-26 19:15:18 | INFO | train | epoch 057 | loss 8.831 | ppl 455.29 | wps 5310.5 | ups 4.33 | wpb 1227.6 | bsz 61.6 | num_updates 1765 | lr 5.295e-06 | gnorm 7.701 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 422
2022-02-26 19:15:18 | INFO | fairseq.trainer | begin training epoch 58
2022-02-26 19:15:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:19 | INFO | train_inner | epoch 058:     15 / 31 loss=8.879, ppl=470.76, wps=4073.6, ups=3.1, wpb=1316, bsz=61.1, num_updates=1780, lr=5.34e-06, gnorm=7.282, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=423
2022-02-26 19:15:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:21 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.929 | ppl 487.52 | wps 29114.2 | wpb 591.2 | bsz 29.9 | num_updates 1796 | best_loss 8.917
2022-02-26 19:15:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1796 updates
2022-02-26 19:15:21 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:24 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 58 @ 1796 updates, score 8.929) (writing took 2.663104753999505 seconds)
2022-02-26 19:15:24 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-02-26 19:15:24 | INFO | train | epoch 058 | loss 8.785 | ppl 441.11 | wps 6788.4 | ups 5.53 | wpb 1227.6 | bsz 61.6 | num_updates 1796 | lr 5.388e-06 | gnorm 8.058 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 428
2022-02-26 19:15:24 | INFO | fairseq.trainer | begin training epoch 59
2022-02-26 19:15:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:24 | INFO | train_inner | epoch 059:      4 / 31 loss=8.758, ppl=432.86, wps=5130, ups=4.19, wpb=1223.8, bsz=61.6, num_updates=1800, lr=5.4e-06, gnorm=8.515, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=428
2022-02-26 19:15:25 | INFO | train_inner | epoch 059:     24 / 31 loss=8.653, ppl=402.6, wps=18568.2, ups=15.4, wpb=1205.6, bsz=61.9, num_updates=1820, lr=5.46e-06, gnorm=7.411, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=429
2022-02-26 19:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:26 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.931 | ppl 488.15 | wps 31450.7 | wpb 591.2 | bsz 29.9 | num_updates 1827 | best_loss 8.917
2022-02-26 19:15:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1827 updates
2022-02-26 19:15:26 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:29 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 59 @ 1827 updates, score 8.931) (writing took 2.9940937159990426 seconds)
2022-02-26 19:15:29 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-02-26 19:15:29 | INFO | train | epoch 059 | loss 8.739 | ppl 427.18 | wps 6683.9 | ups 5.44 | wpb 1227.6 | bsz 61.6 | num_updates 1827 | lr 5.481e-06 | gnorm 7.532 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 433
2022-02-26 19:15:29 | INFO | fairseq.trainer | begin training epoch 60
2022-02-26 19:15:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:30 | INFO | train_inner | epoch 060:     13 / 31 loss=8.761, ppl=433.7, wps=4889.4, ups=4.02, wpb=1217.8, bsz=60.3, num_updates=1840, lr=5.52e-06, gnorm=7.47, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=434
2022-02-26 19:15:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:32 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.915 | ppl 482.77 | wps 28254.1 | wpb 591.2 | bsz 29.9 | num_updates 1858 | best_loss 8.915
2022-02-26 19:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1858 updates
2022-02-26 19:15:32 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:35 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 60 @ 1858 updates, score 8.915) (writing took 4.247326691998751 seconds)
2022-02-26 19:15:36 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-02-26 19:15:36 | INFO | train | epoch 060 | loss 8.646 | ppl 400.62 | wps 5503.5 | ups 4.48 | wpb 1227.6 | bsz 61.6 | num_updates 1858 | lr 5.574e-06 | gnorm 7.534 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 440
2022-02-26 19:15:36 | INFO | fairseq.trainer | begin training epoch 61
2022-02-26 19:15:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:36 | INFO | train_inner | epoch 061:      2 / 31 loss=8.623, ppl=394.25, wps=3831.6, ups=3.23, wpb=1186.7, bsz=62.4, num_updates=1860, lr=5.58e-06, gnorm=7.731, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=441
2022-02-26 19:15:38 | INFO | train_inner | epoch 061:     22 / 31 loss=8.635, ppl=397.55, wps=18901, ups=15.32, wpb=1234, bsz=61.9, num_updates=1880, lr=5.64e-06, gnorm=7.428, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=442
2022-02-26 19:15:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:39 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.764 | ppl 434.7 | wps 27905.9 | wpb 591.2 | bsz 29.9 | num_updates 1889 | best_loss 8.764
2022-02-26 19:15:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1889 updates
2022-02-26 19:15:39 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:41 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 61 @ 1889 updates, score 8.764) (writing took 3.942211846995633 seconds)
2022-02-26 19:15:43 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-02-26 19:15:43 | INFO | train | epoch 061 | loss 8.636 | ppl 397.74 | wps 5759.1 | ups 4.69 | wpb 1227.6 | bsz 61.6 | num_updates 1889 | lr 5.667e-06 | gnorm 7.483 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 447
2022-02-26 19:15:43 | INFO | fairseq.trainer | begin training epoch 62
2022-02-26 19:15:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:44 | INFO | train_inner | epoch 062:     11 / 31 loss=8.617, ppl=392.56, wps=4175.9, ups=3.35, wpb=1246.4, bsz=61.6, num_updates=1900, lr=5.7e-06, gnorm=7.402, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=448
2022-02-26 19:15:45 | INFO | train_inner | epoch 062:     31 / 31 loss=8.522, ppl=367.65, wps=19040, ups=15.32, wpb=1243, bsz=61.1, num_updates=1920, lr=5.76e-06, gnorm=7.999, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=449
2022-02-26 19:15:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:45 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8.769 | ppl 436.37 | wps 29402.5 | wpb 591.2 | bsz 29.9 | num_updates 1920 | best_loss 8.764
2022-02-26 19:15:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1920 updates
2022-02-26 19:15:45 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:48 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:15:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 62 @ 1920 updates, score 8.769) (writing took 2.6607420329964953 seconds)
2022-02-26 19:15:48 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-02-26 19:15:48 | INFO | train | epoch 062 | loss 8.556 | ppl 376.26 | wps 7113.7 | ups 5.79 | wpb 1227.6 | bsz 61.6 | num_updates 1920 | lr 5.76e-06 | gnorm 7.729 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 452
2022-02-26 19:15:48 | INFO | fairseq.trainer | begin training epoch 63
2022-02-26 19:15:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:50 | INFO | train_inner | epoch 063:     20 / 31 loss=8.55, ppl=374.8, wps=5330, ups=4.35, wpb=1224.7, bsz=63.2, num_updates=1940, lr=5.82e-06, gnorm=7.253, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=454
2022-02-26 19:15:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:51 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 8.569 | ppl 379.78 | wps 28915.8 | wpb 591.2 | bsz 29.9 | num_updates 1951 | best_loss 8.569
2022-02-26 19:15:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 1951 updates
2022-02-26 19:15:51 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:53 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt
2022-02-26 19:15:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_best.pt (epoch 63 @ 1951 updates, score 8.569) (writing took 4.60081109899329 seconds)
2022-02-26 19:15:56 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-02-26 19:15:56 | INFO | train | epoch 063 | loss 8.53 | ppl 369.58 | wps 5154.7 | ups 4.2 | wpb 1227.6 | bsz 61.6 | num_updates 1951 | lr 5.853e-06 | gnorm 7.581 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 460
2022-02-26 19:15:56 | INFO | fairseq.trainer | begin training epoch 64
2022-02-26 19:15:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:15:56 | INFO | train_inner | epoch 064:      9 / 31 loss=8.421, ppl=342.78, wps=3404.5, ups=2.94, wpb=1158.3, bsz=61.1, num_updates=1960, lr=5.88e-06, gnorm=7.939, clip=100, loss_scale=32, train_wall=2, gb_free=20.9, wall=461
2022-02-26 19:15:58 | INFO | train_inner | epoch 064:     29 / 31 loss=8.559, ppl=377.07, wps=17901, ups=13.24, wpb=1352, bsz=61.9, num_updates=1980, lr=5.94e-06, gnorm=7.231, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=462
2022-02-26 19:15:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:15:59 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 8.677 | ppl 409.3 | wps 29792 | wpb 591.2 | bsz 29.9 | num_updates 1982 | best_loss 8.569
2022-02-26 19:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 1982 updates
2022-02-26 19:15:59 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:16:01 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:16:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 64 @ 1982 updates, score 8.677) (writing took 2.9681306069978746 seconds)
2022-02-26 19:16:02 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-02-26 19:16:02 | INFO | train | epoch 064 | loss 8.481 | ppl 357.42 | wps 6392.2 | ups 5.21 | wpb 1227.6 | bsz 61.6 | num_updates 1982 | lr 5.946e-06 | gnorm 7.468 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 466
2022-02-26 19:16:02 | INFO | fairseq.trainer | begin training epoch 65
2022-02-26 19:16:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-26 19:16:03 | INFO | train_inner | epoch 065:     18 / 31 loss=8.281, ppl=311.09, wps=4549.8, ups=3.93, wpb=1156.3, bsz=61.1, num_updates=2000, lr=6e-06, gnorm=7.597, clip=100, loss_scale=32, train_wall=1, gb_free=20.9, wall=467
2022-02-26 19:16:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:16:04 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.654 | ppl 402.69 | wps 30874.9 | wpb 591.2 | bsz 29.9 | num_updates 2000 | best_loss 8.569
2022-02-26 19:16:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 2000 updates
2022-02-26 19:16:04 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt
2022-02-26 19:16:06 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt
2022-02-26 19:16:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_65_2000.pt (epoch 65 @ 2000 updates, score 8.654) (writing took 4.825617766007781 seconds)
2022-02-26 19:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-26 19:16:10 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.671 | ppl 407.55 | wps 26552.7 | wpb 591.2 | bsz 29.9 | num_updates 2013 | best_loss 8.569
2022-02-26 19:16:10 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-02-26 19:16:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 2013 updates
2022-02-26 19:16:10 | INFO | fairseq.trainer | Saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:16:12 | INFO | fairseq.trainer | Finished saving checkpoint to bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt
2022-02-26 19:16:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint bartabst/checkpoints/bart.mlm/dev/checkpoint_last.pt (epoch 65 @ 2013 updates, score 8.671) (writing took 2.389349050994497 seconds)
2022-02-26 19:16:12 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-02-26 19:16:12 | INFO | train | epoch 065 | loss 8.41 | ppl 340.17 | wps 3555.2 | ups 2.9 | wpb 1227.6 | bsz 61.6 | num_updates 2013 | lr 6.039e-06 | gnorm 7.413 | clip 100 | loss_scale 32 | train_wall 2 | gb_free 20.9 | wall 476
2022-02-26 19:16:12 | INFO | fairseq_cli.train | done training in 471.2 seconds
